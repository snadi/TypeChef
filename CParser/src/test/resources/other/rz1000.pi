#line 1 "host/headerboth.h" 1
#if definedEx(CONFIG_X86_32)
#line 1 "host/header32.h" 1
#line 4 "host/headerboth.h" 2
#endif
#if !definedEx(CONFIG_X86_32)
#line 1 "host/header64.h" 1
#line 6 "host/headerboth.h" 2
#endif
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/completedConf.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/partialConf.h" 1
#if definedEx(CONFIG_SMP)
#endif
#if !definedEx(CONFIG_SMP)
#endif
//encode numeric parameter
#if definedEx(CONFIG_NEED_MULTIPLE_NODES)
#endif
//from scripts/Makefile.lib 
#if definedEx(CONFIG_DYNAMIC_DEBUG)
#endif
#if definedEx(CONFIG_MTRR_SANITIZER)
#endif
#if definedEx(CONFIG_BOOTPARAM_HUNG_TASK_PANIC_VALUE)
	#if definedEx(CONFIG_BOOTPARAM_HUNG_TASK_PANIC)
	#endif
#if !definedEx(CONFIG_BOOTPARAM_HUNG_TASK_PANIC)
	#endif
#endif
//Defines a 'if' macro, which triggers a bug: The preprocessor incorrectly
//expands this macro within "#if".
//Needs header provided by the user:
//Is always true on x86. If it's false it causes an error in
//arch/x86/include/asm/paravirt.h.
/////////////////////////////////////
//All the following macros are always false on x86, even if defined or anyhow included in the feature model:
//Second & last round of disabling:
/////////////////////////////////////
//Non-boolean features
#if definedEx(CONFIG_KMEMCHECK)
#endif
#if definedEx(CONFIG_ATM_FORE200E)
#endif
#if definedEx(CONFIG_CFAG12864B)
#endif
#if definedEx(CONFIG_KS0108)
#endif
#if definedEx(CONFIG_BLK_DEV_RAM)
#endif
#if definedEx(CONFIG_CDROM_PKTCDVD)
#endif
#if definedEx(CONFIG_LEGACY_PTYS)
#endif
#if definedEx(CONFIG_RAW_DRIVER)
#endif
#if definedEx(CONFIG_SCx200_I2C)
#endif
#if definedEx(CONFIG_SCx200_I2C)
#endif
#if definedEx(CONFIG_RADIO_RTRACK)
#endif
#if definedEx(CONFIG_RADIO_RTRACK2)
#endif
#if definedEx(CONFIG_RADIO_AZTECH)
#endif
#if definedEx(CONFIG_RADIO_GEMTEK)
#endif
#if definedEx(CONFIG_RADIO_TERRATEC)
#endif
#if definedEx(CONFIG_RADIO_TRUST)
#endif
#if definedEx(CONFIG_RADIO_TYPHOON)
#endif
#if definedEx(CONFIG_RADIO_TYPHOON)
#endif
#if definedEx(CONFIG_RADIO_ZOLTRIX)
#endif
#if definedEx(CONFIG_MTD_DEBUG)
#endif
#if definedEx(CONFIG_MTD_REDBOOT_PARTS)
#endif
#if definedEx(CONFIG_WIMAX_I2400M)
#endif
#if definedEx(CONFIG_RTC_HCTOSYS)
#endif
#if definedEx(CONFIG_SCSI_EATA)
#endif
#if definedEx(CONFIG_SCSI_SYM53C8XX_2)
#endif
#if definedEx(CONFIG_SCSI_SYM53C8XX_2)
#endif
#if definedEx(CONFIG_SCSI_SYM53C8XX_2)
#endif
#if definedEx(CONFIG_SCSI_NCR_Q720)
#endif
#if definedEx(CONFIG_SCSI_NCR_Q720)
#endif
#if definedEx(CONFIG_SCSI_NCR_Q720)
#endif
#if definedEx(CONFIG_SCSI_U14_34F)
#endif
#if definedEx(CONFIG_SMB_NLS_DEFAULT)
#endif
#if definedEx(CONFIG_SQUASHFS)
#endif
#if definedEx(CONFIG_WIMAX)
#endif
#if definedEx(CONFIG_SND_AC97_POWER_SAVE)
#endif
#if definedEx(CONFIG_SND_HDA_INPUT_BEEP)
#endif
#if definedEx(CONFIG_SND_HDA_POWER_SAVE)
#endif
#if definedEx(CONFIG_RADIO_GEMTEK)
#endif
#if definedEx(CONFIG_CS5535_MFGPT)
#endif
#if definedEx(CONFIG_SOUND_MSNDCLAS)
#endif
#if definedEx(CONFIG_SOUND_MSNDCLAS)
#endif
#if definedEx(CONFIG_SOUND_MSNDCLAS)
#endif
#if definedEx(CONFIG_SOUND_MSNDCLAS)
#endif
#if definedEx(CONFIG_SOUND_MSNDCLAS)
#endif
#if definedEx(CONFIG_SOUND_MSNDPIN)
#endif
#if definedEx(CONFIG_SOUND_MSNDPIN)
#endif
#if definedEx(CONFIG_SOUND_MSNDPIN)
#endif
#if definedEx(CONFIG_SOUND_MSNDPIN)
#endif
#if definedEx(CONFIG_SOUND_MSNDPIN)
#endif
#if definedEx(CONFIG_MSNDPIN_NONPNP)
#endif
#if definedEx(CONFIG_MSNDPIN_NONPNP)
#endif
#if definedEx(CONFIG_MSNDPIN_NONPNP)
#endif
#if definedEx(CONFIG_MSNDPIN_NONPNP)
#endif
#if definedEx(CONFIG_MSNDPIN_NONPNP)
#endif
#if definedEx(CONFIG_MSNDPIN_NONPNP)
#endif
#if definedEx(CONFIG_MSNDPIN_NONPNP)
#endif
#if !definedEx(CONFIG_SOUND_MSNDCLAS) && definedEx(CONFIG_SOUND_MSNDPIN) || definedEx(CONFIG_SOUND_MSNDCLAS)
#endif
#if definedEx(CONFIG_TRIX_HAVE_BOOT)
#endif
#if definedEx(CONFIG_PSS_HAVE_BOOT)
#endif
#if definedEx(CONFIG_SC6600)
#endif
#if definedEx(CONFIG_SC6600)
#endif
#if definedEx(CONFIG_KGDB_TESTS_ON_BOOT)
#endif
#if definedEx(CONFIG_MTD_MTDRAM)
#endif
#if definedEx(CONFIG_MTD_MTDRAM)
#endif
#if definedEx(CONFIG_MTD_MTDRAM)
#endif
#if definedEx(CONFIG_MTD_DOCPROBE)
#endif
#if definedEx(CONFIG_MTD_NAND_DISKONCHIP)
#endif
#if definedEx(CONFIG_MTD_UBI)
#endif
#if definedEx(CONFIG_MTD_UBI)
#endif
//lib/KConfig.debug
#if definedEx(CONFIG_DEBUG_OBJECTS)
#endif
#if definedEx(CONFIG_DEBUG_KMEMLEAK)
#endif
//mm/KConfig
//[kaestner@kananga linux]$ ./run.sh de.fosd.typechef.linux.KConfigReader l/net/dccp/ccids/Kconfig
#if definedEx(CONFIG_IP_DCCP_CCID3) && definedEx(CONFIG_EXPERIMENTAL)
#endif
#if definedEx(CONFIG_NET_EMATCH)
#endif
#if definedEx(CONFIG_TIPC_ADVANCED)
#endif
#if definedEx(CONFIG_TIPC_ADVANCED)
#endif
#if definedEx(CONFIG_TIPC_ADVANCED)
#endif
#if definedEx(CONFIG_TIPC_ADVANCED)
#endif
#if definedEx(CONFIG_TIPC_ADVANCED)
#endif
#if definedEx(CONFIG_TIPC_ADVANCED)
#endif
#if definedEx(CONFIG_SC6600)
#endif
#if definedEx(CONFIG_SC6600)
#endif
#if definedEx(CONFIG_SC6600_JOY)
#endif
#if !definedEx(CONFIG_MSDOS_FS) && definedEx(CONFIG_VFAT_FS) || definedEx(CONFIG_MSDOS_FS)
#endif
#if definedEx(CONFIG_VFAT_FS)
#endif
#if definedEx(CONFIG_VGACON_SOFT_SCROLLBACK)
#endif
#if definedEx(CONFIG_NLS)
#endif
//assumed to be 1 or not defined
#if definedEx(CONFIG_BUG)
#endif
#if definedEx(CONFIG_USB_SERIAL_SAFE_PADDED)
#endif
#if definedEx(CONFIG_PANEL)
#endif
#if definedEx(CONFIG_PANEL)
#endif
#if definedEx(CONFIG_SERIAL_8250)
#endif
#if definedEx(CONFIG_SERIAL_8250)
#endif
#if definedEx(CONFIG_DLCI)
#endif
#if definedEx(CONFIG_HIBERNATION)
#endif
#if definedEx(CONFIG_DETECT_SOFTLOCKUP)
  #if definedEx(CONFIG_BOOTPARAM_SOFTLOCKUP_PANIC)
  #endif
#if !definedEx(CONFIG_BOOTPARAM_SOFTLOCKUP_PANIC)
  #endif
#endif
//seem always selected by default and not deselectable
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/l/drivers/ide/rz1000.c" 1
/*
 *  Copyright (C) 1995-1998  Linus Torvalds & author (see below)
 */
/*
 *  Principal Author:  mlord@pobox.com (Mark Lord)
 *
 *  See linux/MAINTAINERS for address of current maintainer.
 *
 *  This file provides support for disabling the buggy read-ahead
 *  mode of the RZ1000 IDE chipset, commonly used on Intel motherboards.
 *
 *  Dunno if this fixes both ports, or only the primary port (?).
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/types.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/asm-generic/types.h" 1
/*
 * int-ll64 is used practically everywhere now,
 * so use it as a reasonable default.
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/asm-generic/int-ll64.h" 1
/*
 * asm-generic/int-ll64.h
 *
 * Integer declarations for architectures which use "long long"
 * for 64-bit types.
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/bitsperlong.h" 1
#if !definedEx(CONFIG_X86_32)
#endif
#if definedEx(CONFIG_X86_32)
#endif
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/asm-generic/bitsperlong.h" 1
/*
 * There seems to be no way of detecting this automatically from user
 * space, so 64 bit architectures should override this in their
 * bitsperlong.h. In particular, an architecture that supports
 * both 32 and 64 bit user space must not rely on CONFIG_64BIT
 * to decide it, but rather check a compiler provided macro.
 */
#if definedEx(CONFIG_64BIT)
#endif
#if !definedEx(CONFIG_64BIT)
#endif
/*
 * FIXME: The check currently breaks x86-64 build, so it's
 * temporarily disabled. Please fix x86-64 and reenable
 */
#line 12 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/bitsperlong.h" 2
#line 13 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/asm-generic/int-ll64.h" 2
/*
 * __xx is ok: it doesn't pollute the POSIX namespace. Use these in the
 * header files exported to user space
 */
typedef __signed__ char __s8;
typedef unsigned char __u8;
typedef __signed__ short __s16;
typedef unsigned short __u16;
typedef __signed__ int __s32;
typedef unsigned int __u32;
__extension__ typedef __signed__ long long __s64;
__extension__ typedef unsigned long long __u64;
typedef signed char s8;
typedef unsigned char u8;
typedef signed short s16;
typedef unsigned short u16;
typedef signed int s32;
typedef unsigned int u32;
typedef signed long long s64;
typedef unsigned long long u64;
#line 9 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/asm-generic/types.h" 2
typedef unsigned short umode_t;
/*
 * These aren't exported outside the kernel to avoid name space clashes
 */
/*
 * DMA addresses may be very different from physical addresses
 * and pointers. i386 and powerpc may have 64 bit DMA on 32 bit
 * systems, while sparc64 uses 32 bit DMA addresses for 64 bit
 * physical addresses.
 * This default defines dma_addr_t to have the same size as
 * phys_addr_t, which is the most common way.
 * Do not define the dma64_addr_t type, which never really
 * worked.
 */
#line 8 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/types.h" 2
typedef u64 dma64_addr_t;
#if !definedEx(CONFIG_X86_64) && definedEx(CONFIG_HIGHMEM64G) || definedEx(CONFIG_X86_64)
/* DMA addresses come in 32-bit and 64-bit flavours. */
typedef u64 dma_addr_t;
#endif
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_HIGHMEM64G)
typedef u32 dma_addr_t;
#endif
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/posix_types.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/stddef.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/compiler.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/compiler-gcc.h" 1
/*
 * Common definitions for all gcc versions go here.
 */
/* Optimization barrier */
/* The "volatile" is due to gcc bugs */
/*
 * This macro obfuscates arithmetic on a variable address so that gcc
 * shouldn't recognize the original var, and make assumptions about it.
 *
 * This is needed because the C standard makes it undefined to do
 * pointer arithmetic on "objects" outside their boundaries and the
 * gcc optimizers assume this is the case. In particular they
 * assume such arithmetic does not wrap.
 *
 * A miscompilation has been observed because of this on PPC.
 * To work around it we hide the relationship of the pointer and the object
 * using this macro.
 *
 * Versions of the ppc64 compiler before 4.1 had a bug where use of
 * RELOC_HIDE could trash r30. The bug can be worked around by changing
 * the inline assembly constraint from =g to =r, in this particular
 * case either is valid.
 */
/* &a[0] degrades to a pointer: a different type from an array */
/*
 * Force always-inline if the user requests it so via the .config,
 * or if gcc is too old:
 */
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
#endif
/*
 * it doesn't make sense on ARM (currently the only user of __naked) to trace
 * naked functions because then mcount is called without stack and frame pointer
 * being set up and there is no chance to restore the lr register to the value
 * before mcount was called.
 */
/*
 * From the GCC manual:
 *
 * Many functions have no effects except the return value and their
 * return value depends only on the parameters and/or global
 * variables.  Such a function can be subject to common subexpression
 * elimination and loop optimization just as an arithmetic operator
 * would be.
 * [...]
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/compiler-gcc4.h" 1
/* GCC 4.1.[01] miscompiles __weak */
/*
 * A trick to suppress uninitialized variable warning without generating any
 * code
 */
/* Mark functions as cold. gcc will assume any path leading to a call
   to them will be unlikely.  This means a lot of manual unlikely()s
   are unnecessary now for any paths leading to the usual suspects
   like BUG(), printk(), panic() etc. [but let's keep them for now for
   older compilers]
   Early snapshots of gcc 4.3 don't support this and we can't detect this
   in the preprocessor, but we can live with this because they're unreleased.
   Maketime probing would be overkill here.
   gcc also has a __attribute__((__hot__)) to move hot functions into
   a special section, but I don't see any sense in this right now in
   the kernel context */
/*
 * Mark a position in code as unreachable.  This can be used to
 * suppress control flow warnings after asm blocks that transfer
 * control elsewhere.
 *
 * Early snapshots of gcc 4.5 don't support this and we can't detect
 * this in the preprocessor, but we can live with this because they're
 * unreleased.  Really, we need to have autoconf for the kernel.
 */
#line 89 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/compiler-gcc.h" 2
#line 44 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/compiler.h" 2
/* Intel compiler defines __GNUC__. So we will overwrite implementations
 * coming from above header files here
 */
/*
 * Generic compiler-dependent macros required for kernel
 * build go below this comment. Actual compiler/compiler version
 * specific implementations come from the above header files
 */
struct ftrace_branch_data {
	const char *func;
	const char *file;
	unsigned line;
	union {
		struct {
			unsigned long correct;
			unsigned long incorrect;
		};
		struct {
			unsigned long miss;
			unsigned long hit;
		};
		unsigned long miss_hit[2];
	};
};
/*
 * Note: DISABLE_BRANCH_PROFILING can be used by special lowlevel code
 * to disable branch tracing on a per file basis.
 */
/* Optimization barrier */
/* Unreachable code */
/*
 * Allow us to mark functions as 'deprecated' and have gcc emit a nice
 * warning for each use, in hopes of speeding the functions removal.
 * Usage is:
 * 		int __deprecated foo(void)
 */
#if !definedEx(CONFIG_ENABLE_MUST_CHECK)
#endif
#if !definedEx(CONFIG_ENABLE_WARN_DEPRECATED)
#endif
/*
 * Allow us to avoid 'defined but not used' warnings on functions and data,
 * as well as force them to be emitted to the assembly file.
 *
 * As of gcc 3.4, static functions that are not marked with attribute((used))
 * may be elided from the assembly file.  As of gcc 3.4, static data not so
 * marked will not be elided, but this may change in a future gcc version.
 *
 * NOTE: Because distributions shipped with a backported unit-at-a-time
 * compiler in gcc 3.3, we must define __used to be __attribute__((used))
 * for gcc >=3.3 instead of 3.4.
 *
 * In prior versions of gcc, such functions and data would be emitted, but
 * would be warned about except with attribute((unused)).
 *
 * Mark functions that are referenced only in inline assembly as __used so
 * the code is emitted even though it appears to be unreferenced.
 */
/*
 * Rather then using noinline to prevent stack consumption, use
 * noinline_for_stack instead.  For documentaiton reasons.
 */
/*
 * From the GCC manual:
 *
 * Many functions do not examine any values except their arguments,
 * and have no effects except the return value.  Basically this is
 * just slightly more strict class than the `pure' attribute above,
 * since function is not allowed to read global memory.
 *
 * Note that a function that has pointer arguments and examines the
 * data pointed to must _not_ be declared `const'.  Likewise, a
 * function that calls a non-`const' function usually must not be
 * `const'.  It does not make sense for a `const' function to return
 * `void'.
 */
/*
 * Tell gcc if a function is cold. The compiler will assume any path
 * directly leading to the call is unlikely.
 */
/* Simple shorthand for a section definition */
/* Are two types/vars the same type (ignoring qualifiers)? */
/* Compile time object size, -1 for unknown */
/*
 * Prevent the compiler from merging or refetching accesses.  The compiler
 * is also forbidden from reordering successive instances of ACCESS_ONCE(),
 * but only when the compiler is aware of some particular ordering.  One way
 * to make the compiler aware of ordering is to put the two invocations of
 * ACCESS_ONCE() in different C statements.
 *
 * This macro does absolutely -nothing- to prevent the CPU from reordering,
 * merging, or refetching absolutely anything at any time.  Its main intended
 * use is to mediate communication between process-level code and irq/NMI
 * handlers, all running on the same CPU.
 */
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/stddef.h" 2
enum {
	false	= 0,
	true	= 1
};
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/posix_types.h" 2
/*
 * This allows for 1024 file descriptors: if NR_OPEN is ever grown
 * beyond that you'll have to change this too. But 1024 fd's seem to be
 * enough even for such "real" unices like OSF/1, so hopefully this is
 * one limit that doesn't have to be changed [again].
 *
 * Note that POSIX wants the FD_CLEAR(fd,fdsetp) defines to be in
 * <sys/time.h> (and thus <linux/time.h>) - but this is a more logical
 * place for them. Solved by having dummy defines in <sys/time.h>.
 */
/*
 * Those macros may have been defined in <gnu/types.h>. But we always
 * use the ones here. 
 */
typedef struct {
	unsigned long fds_bits [(1024/(8 * sizeof(unsigned long)))];
} __kernel_fd_set;
/* Type of a signal handler.  */
typedef void (*__kernel_sighandler_t)(int);
/* Type of a SYSV IPC key.  */
typedef int __kernel_key_t;
typedef int __kernel_mqd_t;
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/posix_types.h" 1
#if definedEx(CONFIG_X86_32)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/posix_types_32.h" 1
/*
 * This file is generally used by user-level software, so you need to
 * be a little careful about namespace pollution etc.  Also, we cannot
 * assume GCC is being used.
 */
typedef unsigned long	__kernel_ino_t;
typedef unsigned short	__kernel_mode_t;
typedef unsigned short	__kernel_nlink_t;
typedef long		__kernel_off_t;
typedef int		__kernel_pid_t;
typedef unsigned short	__kernel_ipc_pid_t;
typedef unsigned short	__kernel_uid_t;
typedef unsigned short	__kernel_gid_t;
typedef unsigned int	__kernel_size_t;
typedef int		__kernel_ssize_t;
typedef int		__kernel_ptrdiff_t;
typedef long		__kernel_time_t;
typedef long		__kernel_suseconds_t;
typedef long		__kernel_clock_t;
typedef int		__kernel_timer_t;
typedef int		__kernel_clockid_t;
typedef int		__kernel_daddr_t;
typedef char *		__kernel_caddr_t;
typedef unsigned short	__kernel_uid16_t;
typedef unsigned short	__kernel_gid16_t;
typedef unsigned int	__kernel_uid32_t;
typedef unsigned int	__kernel_gid32_t;
typedef unsigned short	__kernel_old_uid_t;
typedef unsigned short	__kernel_old_gid_t;
typedef unsigned short	__kernel_old_dev_t;
typedef long long	__kernel_loff_t;
typedef struct {
	int	val[2];
} __kernel_fsid_t;
#line 5 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/posix_types.h" 2
#endif
#if !definedEx(CONFIG_X86_32)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/posix_types_64.h" 1
/*
 * This file is generally used by user-level software, so you need to
 * be a little careful about namespace pollution etc.  Also, we cannot
 * assume GCC is being used.
 */
typedef unsigned long	__kernel_ino_t;
typedef unsigned int	__kernel_mode_t;
typedef unsigned long	__kernel_nlink_t;
typedef long		__kernel_off_t;
typedef int		__kernel_pid_t;
typedef int		__kernel_ipc_pid_t;
typedef unsigned int	__kernel_uid_t;
typedef unsigned int	__kernel_gid_t;
typedef unsigned long	__kernel_size_t;
typedef long		__kernel_ssize_t;
typedef long		__kernel_ptrdiff_t;
typedef long		__kernel_time_t;
typedef long		__kernel_suseconds_t;
typedef long		__kernel_clock_t;
typedef int		__kernel_timer_t;
typedef int		__kernel_clockid_t;
typedef int		__kernel_daddr_t;
typedef char *		__kernel_caddr_t;
typedef unsigned short	__kernel_uid16_t;
typedef unsigned short	__kernel_gid16_t;
typedef long long	__kernel_loff_t;
typedef struct {
	int	val[2];
} __kernel_fsid_t;
typedef unsigned short __kernel_old_uid_t;
typedef unsigned short __kernel_old_gid_t;
typedef __kernel_uid_t __kernel_uid32_t;
typedef __kernel_gid_t __kernel_gid32_t;
typedef unsigned long	__kernel_old_dev_t;
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __FD_SET(unsigned long fd, __kernel_fd_set *fdsetp)
{
	unsigned long _tmp = fd / (8 * sizeof(unsigned long));
	unsigned long _rem = fd % (8 * sizeof(unsigned long));
	fdsetp->fds_bits[_tmp] |= (1UL<<_rem);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __FD_CLR(unsigned long fd, __kernel_fd_set *fdsetp)
{
	unsigned long _tmp = fd / (8 * sizeof(unsigned long));
	unsigned long _rem = fd % (8 * sizeof(unsigned long));
	fdsetp->fds_bits[_tmp] &= ~(1UL<<_rem);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int __FD_ISSET(unsigned long fd, __const__ __kernel_fd_set *p)
{
	unsigned long _tmp = fd / (8 * sizeof(unsigned long));
	unsigned long _rem = fd % (8 * sizeof(unsigned long));
	return (p->fds_bits[_tmp] & (1UL<<_rem)) != 0;
}
/*
 * This will unroll the loop for the normal constant cases (8 or 32 longs,
 * for 256 and 1024-bit fd_sets respectively)
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __FD_ZERO(__kernel_fd_set *p)
{
	unsigned long *tmp = p->fds_bits;
	int i;
	if (__builtin_constant_p((1024/(8 * sizeof(unsigned long))))) {
		switch ((1024/(8 * sizeof(unsigned long)))) {
		case 32:
			tmp[ 0] = 0; tmp[ 1] = 0; tmp[ 2] = 0; tmp[ 3] = 0;
			tmp[ 4] = 0; tmp[ 5] = 0; tmp[ 6] = 0; tmp[ 7] = 0;
			tmp[ 8] = 0; tmp[ 9] = 0; tmp[10] = 0; tmp[11] = 0;
			tmp[12] = 0; tmp[13] = 0; tmp[14] = 0; tmp[15] = 0;
			tmp[16] = 0; tmp[17] = 0; tmp[18] = 0; tmp[19] = 0;
			tmp[20] = 0; tmp[21] = 0; tmp[22] = 0; tmp[23] = 0;
			tmp[24] = 0; tmp[25] = 0; tmp[26] = 0; tmp[27] = 0;
			tmp[28] = 0; tmp[29] = 0; tmp[30] = 0; tmp[31] = 0;
			return;
		case 16:
			tmp[ 0] = 0; tmp[ 1] = 0; tmp[ 2] = 0; tmp[ 3] = 0;
			tmp[ 4] = 0; tmp[ 5] = 0; tmp[ 6] = 0; tmp[ 7] = 0;
			tmp[ 8] = 0; tmp[ 9] = 0; tmp[10] = 0; tmp[11] = 0;
			tmp[12] = 0; tmp[13] = 0; tmp[14] = 0; tmp[15] = 0;
			return;
		case 8:
			tmp[ 0] = 0; tmp[ 1] = 0; tmp[ 2] = 0; tmp[ 3] = 0;
			tmp[ 4] = 0; tmp[ 5] = 0; tmp[ 6] = 0; tmp[ 7] = 0;
			return;
		case 4:
			tmp[ 0] = 0; tmp[ 1] = 0; tmp[ 2] = 0; tmp[ 3] = 0;
			return;
		}
	}
	i = (1024/(8 * sizeof(unsigned long)));
	while (i) {
		i--;
		*tmp = 0;
		tmp++;
	}
}
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/posix_types.h" 2
#endif
#line 49 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/posix_types.h" 2
#line 16 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 2
typedef __u32 __kernel_dev_t;
typedef __kernel_fd_set		fd_set;
typedef __kernel_dev_t		dev_t;
typedef __kernel_ino_t		ino_t;
typedef __kernel_mode_t		mode_t;
typedef __kernel_nlink_t	nlink_t;
typedef __kernel_off_t		off_t;
typedef __kernel_pid_t		pid_t;
typedef __kernel_daddr_t	daddr_t;
typedef __kernel_key_t		key_t;
typedef __kernel_suseconds_t	suseconds_t;
typedef __kernel_timer_t	timer_t;
typedef __kernel_clockid_t	clockid_t;
typedef __kernel_mqd_t		mqd_t;
typedef _Bool			bool;
typedef __kernel_uid32_t	uid_t;
typedef __kernel_gid32_t	gid_t;
typedef __kernel_uid16_t        uid16_t;
typedef __kernel_gid16_t        gid16_t;
typedef unsigned long		uintptr_t;
#if definedEx(CONFIG_UID16)
/* This is defined by include/asm-{arch}/posix_types.h */
typedef __kernel_old_uid_t	old_uid_t;
typedef __kernel_old_gid_t	old_gid_t;
#endif
typedef __kernel_loff_t		loff_t;
/*
 * The following typedefs are also protected by individual ifdefs for
 * historical reasons:
 */
typedef __kernel_size_t		size_t;
typedef __kernel_ssize_t	ssize_t;
typedef __kernel_ptrdiff_t	ptrdiff_t;
typedef __kernel_time_t		time_t;
typedef __kernel_clock_t	clock_t;
typedef __kernel_caddr_t	caddr_t;
/* bsd */
typedef unsigned char		u_char;
typedef unsigned short		u_short;
typedef unsigned int		u_int;
typedef unsigned long		u_long;
/* sysv */
typedef unsigned char		unchar;
typedef unsigned short		ushort;
typedef unsigned int		uint;
typedef unsigned long		ulong;
typedef		__u8		u_int8_t;
typedef		__s8		int8_t;
typedef		__u16		u_int16_t;
typedef		__s16		int16_t;
typedef		__u32		u_int32_t;
typedef		__s32		int32_t;
typedef		__u8		uint8_t;
typedef		__u16		uint16_t;
typedef		__u32		uint32_t;
typedef		__u64		uint64_t;
typedef		__u64		u_int64_t;
typedef		__s64		int64_t;
/* this is a special 64bit data type that is 8-byte aligned */
/**
 * The type used for indexing onto a disc or disc partition.
 *
 * Linux always considers sectors to be 512 bytes long independently
 * of the devices real block size.
 *
 * blkcnt_t is the type of the inode's block count.
 */
#if definedEx(CONFIG_LBDAF)
typedef u64 sector_t;
typedef u64 blkcnt_t;
#endif
#if !definedEx(CONFIG_LBDAF)
typedef unsigned long sector_t;
typedef unsigned long blkcnt_t;
#endif
/*
 * The type of an index into the pagecache.  Use a #define so asm/types.h
 * can override it.
 */
/*
 * Below are truly Linux-specific types that should never collide with
 * any application/library that wants linux/types.h.
 */
typedef __u16  __le16;
typedef __u16  __be16;
typedef __u32  __le32;
typedef __u32  __be32;
typedef __u64  __le64;
typedef __u64  __be64;
typedef __u16  __sum16;
typedef __u32  __wsum;
typedef unsigned  gfp_t;
typedef unsigned  fmode_t;
#if definedEx(CONFIG_PHYS_ADDR_T_64BIT)
typedef u64 phys_addr_t;
#endif
#if !definedEx(CONFIG_PHYS_ADDR_T_64BIT)
typedef u32 phys_addr_t;
#endif
typedef phys_addr_t resource_size_t;
typedef struct {
	volatile int counter;
} atomic_t;
#if definedEx(CONFIG_64BIT)
typedef struct {
	volatile long counter;
} atomic64_t;
#endif
struct ustat {
	__kernel_daddr_t	f_tfree;
	__kernel_ino_t		f_tinode;
	char			f_fname[6];
	char			f_fpack[6];
};
#line 18 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/l/drivers/ide/rz1000.c" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/module.h" 1
/*
 * Dynamic loading of modules into the kernel.
 *
 * Rewritten by Richard Henderson <rth@tamu.edu> Dec 1996
 * Rewritten again by Rusty Russell, 2002
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/list.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/stddef.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/list.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/poison.h" 1
/********** include/linux/list.h **********/
/*
 * Architectures might want to move the poison pointer offset
 * into some well-recognized area such as 0xdead000000000000,
 * that is also not mappable by user-space exploits:
 */
/*
 * These are non-NULL pointers that will result in page faults
 * under normal circumstances, used to verify that nobody uses
 * non-initialized list entries.
 */
/********** include/linux/timer.h **********/
/*
 * Magic number "tsta" to indicate a static timer initializer
 * for the object debugging code.
 */
/********** mm/debug-pagealloc.c **********/
/********** mm/slab.c **********/
/*
 * Magic nums for obj red zoning.
 * Placed in the first word before and the first word after an obj.
 */
/* ...and for poisoning */
/********** arch/$ARCH/mm/init.c **********/
/********** arch/ia64/hp/common/sba_iommu.c **********/
/*
 * arch/ia64/hp/common/sba_iommu.c uses a 16-byte poison string with a
 * value of "SBAIOMMU POISON\0" for spill-over poisoning.
 */
/********** fs/jbd/journal.c **********/
/********** drivers/base/dmapool.c **********/
/********** drivers/atm/ **********/
/********** net/ **********/
/********** kernel/mutexes **********/
/********** lib/flex_array.c **********/
/********** security/ **********/
/********** sound/oss/ **********/
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/list.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/prefetch.h" 1
/*
 *  Generic cache management functions. Everything is arch-specific,  
 *  but this header exists to make sure the defines/functions can be
 *  used in a generic way.
 *
 *  2000-11-13  Arjan van de Ven   <arjan@fenrus.demon.nl>
 *
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 15 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/prefetch.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/processor.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/processor-flags.h" 1
/* Various flags defined: can be included from assembler. */
/*
 * EFLAGS bits
 */
/*
 * Basic CPU control in CR0
 */
/*
 * Paging options in CR3
 */
/*
 * Intel CPU features in CR4
 */
/*
 * x86-64 Task Priority Register, CR8
 */
/*
 * AMD and Transmeta use MSRs for configuration; see <asm/msr-index.h>
 */
/*
 *      NSC/Cyrix CPU configuration register indexes
 */
#if definedEx(CONFIG_VM86)
#endif
#if !definedEx(CONFIG_VM86)
#endif
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/processor.h" 2
/* Forward declaration, a strange C thing */
struct task_struct;
struct mm_struct;
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/vm86.h" 1
/*
 * I'm guessing at the VIF/VIP flag usage, but hope that this is how
 * the Pentium uses them. Linux will return from vm86 mode when both
 * VIF and VIP is set.
 *
 * On a Pentium, we could probably optimize the virtual flags directly
 * in the eflags register instead of doing it "by hand" in vflags...
 *
 * Linus
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/processor-flags.h" 1
#line 17 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/vm86.h" 2
/*
 * Return values for the 'vm86()' system call
 */
/*
 * Additional return values when invoking new vm86()
 */
/*
 * function codes when invoking new vm86()
 */
/*
 * This is the stack-layout seen by the user space program when we have
 * done a translation of "SAVE_ALL" from vm86 mode. The real kernel layout
 * is 'kernel_vm86_regs' (see below).
 */
struct vm86_regs {
/*
 * normal regs, with special meaning for the segment descriptors..
 */
	long ebx;
	long ecx;
	long edx;
	long esi;
	long edi;
	long ebp;
	long eax;
	long __null_ds;
	long __null_es;
	long __null_fs;
	long __null_gs;
	long orig_eax;
	long eip;
	unsigned short cs, __csh;
	long eflags;
	long esp;
	unsigned short ss, __ssh;
/*
 * these are specific to v86 mode:
 */
	unsigned short es, __esh;
	unsigned short ds, __dsh;
	unsigned short fs, __fsh;
	unsigned short gs, __gsh;
};
struct revectored_struct {
	unsigned long __map[8];			/* 256 bits */
};
struct vm86_struct {
	struct vm86_regs regs;
	unsigned long flags;
	unsigned long screen_bitmap;
	unsigned long cpu_type;
	struct revectored_struct int_revectored;
	struct revectored_struct int21_revectored;
};
/*
 * flags masks
 */
struct vm86plus_info_struct {
	unsigned long force_return_for_pic:1;
	unsigned long vm86dbg_active:1;       /* for debugger */
	unsigned long vm86dbg_TFpendig:1;     /* for debugger */
	unsigned long unused:28;
	unsigned long is_vm86pus:1;	      /* for vm86 internal use */
	unsigned char vm86dbg_intxxtab[32];   /* for debugger */
};
struct vm86plus_struct {
	struct vm86_regs regs;
	unsigned long flags;
	unsigned long screen_bitmap;
	unsigned long cpu_type;
	struct revectored_struct int_revectored;
	struct revectored_struct int21_revectored;
	struct vm86plus_info_struct vm86plus;
};
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/ptrace.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/compiler.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/ptrace.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/ptrace-abi.h" 1
#if definedEx(CONFIG_X86_32)
#endif
#if !definedEx(CONFIG_X86_32)
/* top of stack page */
#endif
/* Arbitrarily choose the same ptrace numbers as used by the Sparc code. */
/* only useful for access 32bit programs / kernels */
#if !definedEx(CONFIG_X86_32)
#endif
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 86 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/ptrace-abi.h" 2
/* configuration/status structure used in PTRACE_BTS_CONFIG and
   PTRACE_BTS_STATUS commands.
*/
struct ptrace_bts_config {
	/* requested or actual size of BTS buffer in bytes */
	__u32 size;
	/* bitmask of below flags */
	__u32 flags;
	/* buffer overflow signal */
	__u32 signal;
	/* actual size of bts_struct in bytes */
	__u32 bts_size;
};
/* Configure branch trace recording.
   ADDR points to a struct ptrace_bts_config.
   DATA gives the size of that buffer.
   A new buffer is allocated, if requested in the flags.
   An overflow signal may only be requested for new buffers.
   Returns the number of bytes read.
*/
/* Return the current configuration in a struct ptrace_bts_config
   pointed to by ADDR; DATA gives the size of that buffer.
   Returns the number of bytes written.
*/
/* Return the number of available BTS records for draining.
   DATA and ADDR are ignored.
*/
/* Get a single BTS record.
   DATA defines the index into the BTS array, where 0 is the newest
   entry, and higher indices refer to older entries.
   ADDR is pointing to struct bts_struct (see asm/ds.h).
*/
/* Clear the BTS buffer.
   DATA and ADDR are ignored.
*/
/* Read all available BTS records and clear the buffer.
   ADDR points to an array of struct bts_struct.
   DATA gives the size of that buffer.
   BTS records are read from oldest to newest.
   Returns number of BTS records drained.
*/
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/ptrace.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/processor-flags.h" 1
#line 8 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/ptrace.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/segment.h" 1
/* Constructor for a conventional segment GDT (or LDT) entry */
/* This is a macro so it can be used in initializers */
/* Simple and small GDT entries for booting only */
#if definedEx(CONFIG_X86_32)
/*
 * The layout of the per-CPU GDT under Linux:
 *
 *   0 - null
 *   1 - reserved
 *   2 - reserved
 *   3 - reserved
 *
 *   4 - unused			<==== new cacheline
 *   5 - unused
 *
 *  ------- start of TLS (Thread-Local Storage) segments:
 *
 *   6 - TLS segment #1			[ glibc's TLS segment ]
 *   7 - TLS segment #2			[ Wine's %fs Win32 segment ]
 *   8 - TLS segment #3
 *   9 - reserved
 *  10 - reserved
 *  11 - reserved
 *
 *  ------- start of kernel segments:
 *
 *  12 - kernel code segment		<==== new cacheline
 *  13 - kernel data segment
 *  14 - default user CS
 *  15 - default user DS
 *  16 - TSS
 *  17 - LDT
 *  18 - PNPBIOS support (16->32 gate)
 *  19 - PNPBIOS support
 *  20 - PNPBIOS support
 *  21 - PNPBIOS support
 *  22 - PNPBIOS support
 *  23 - APM BIOS support
 *  24 - APM BIOS support
 *  25 - APM BIOS support
 *
 *  26 - ESPFIX small SS
 *  27 - per-cpu			[ offset to per-cpu data area ]
 *  28 - stack_canary-20		[ for stack protector ]
 *  29 - unused
 *  30 - unused
 *  31 - TSS for double fault handler
 */
#if definedEx(CONFIG_SMP)
#endif
#if !definedEx(CONFIG_SMP)
#endif
#if definedEx(CONFIG_CC_STACKPROTECTOR)
#endif
#if !definedEx(CONFIG_CC_STACKPROTECTOR)
#endif
/*
 * The GDT has 32 entries
 */
/* The PnP BIOS entries in the GDT */
/* The PnP BIOS selectors */
/* Bottom two bits of selector give the ring privilege level */
/* Bit 2 is table indicator (LDT/GDT) */
/* User mode is privilege level 3 */
/* LDT segment has TI set, GDT has it cleared */
/*
 * Matching rules for certain types of segments.
 */
/* Matches PNP_CS32 and PNP_CS16 (they must be consecutive) */
#endif
#if !definedEx(CONFIG_X86_32)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/cache.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/linkage.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/compiler.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/linkage.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/linkage.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/stringify.h" 1
/* Indirect stringification.  Doing two levels allows the parameter to be a
 * macro itself.  For example, compile with -DFOO=bar, __stringify(FOO)
 * converts to "bar".
 */
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/linkage.h" 2
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/linkage.h" 2
/*
 * For assembly routines.
 *
 * Note when using these that you must specify the appropriate
 * alignment directives yourself
 */
/*
 * This is used by architectures to keep arguments on the stack
 * untouched by the compiler by keeping them live until the end.
 * The argument stack may be owned by the assembly-language
 * caller, not the callee, and gcc doesn't always understand
 * that.
 *
 * We have the return value, and a maximum of six arguments.
 *
 * This should always be followed by a "return ret" for the
 * protection to work (ie no more work that the compiler might
 * end up needing stack temporaries for).
 */
/* Assembly files may be compiled with -traditional .. */
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/cache.h" 2
/* L1 cache line size */
#if definedEx(CONFIG_X86_VSMP)
#if definedEx(CONFIG_SMP)
#endif
#endif
#line 148 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/segment.h" 2
/*
 * we cannot use the same code segment descriptor for user and kernel
 * -- not even in the long flat mode, because of different DPL /kkeil
 * The segment offset needs to contain a RPL. Grr. -AK
 * GDT layout to get 64bit syscall right (sysret hardcodes gdt offsets)
 */
/* TLS indexes for 64bit - hardcoded in arch_prctl */
#endif
#if !definedEx(CONFIG_PARAVIRT)
#endif
/* User mode is privilege level 3 */
/* LDT segment has TI set, GDT has it cleared */
/* Bottom two bits of selector give the ring privilege level */
/* Bit 2 is table indicator (LDT/GDT) */
extern const char early_idt_handlers[32][10];
#line 11 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/ptrace.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/page_types.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/const.h" 1
/* const.h: Macros for dealing with constants.  */
/* Some constant macros are used in both assembler and
 * C code.  Therefore we cannot annotate them always with
 * 'UL' and other type specifiers unilaterally.  We
 * use the following macros to deal with this.
 *
 * Similarly, _AT() will cast an expression with a type in C, but
 * leave it unchanged in asm.
 */
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/page_types.h" 2
/* PAGE_SHIFT determines the page size */
/* Cast PAGE_MASK to a signed type so that it is sign-extended if
   virtual addresses are 32-bits but physical addresses are larger
   (ie, 32-bit PAE). */
#if definedEx(CONFIG_X86_64)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/page_64_types.h" 1
/*
 * Set __PAGE_OFFSET to the most negative possible address +
 * PGDIR_SIZE*16 (pgd slot 272).  The gap is to allow a space for a
 * hypervisor to fit.  Choosing 16 slots here is arbitrary, but it's
 * what Xen requires.
 */
/* See Documentation/x86/x86_64/mm.txt for a description of the memory map. */
/*
 * Kernel image size is limited to 512 MB (see level2_kernel_pgt in
 * arch/x86/kernel/head_64.S), and it is mapped here:
 */
void clear_page(void *page);
void copy_page(void *to, void *from);
/* duplicated to the one in bootmem.h */
extern unsigned long max_pfn;
extern unsigned long phys_base;
extern unsigned long __phys_addr(unsigned long);
extern void init_extra_mapping_uc(unsigned long phys, unsigned long size);
extern void init_extra_mapping_wb(unsigned long phys, unsigned long size);
#if definedEx(CONFIG_FLATMEM)
#endif
#line 38 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/page_types.h" 2
#endif
#if !definedEx(CONFIG_X86_64)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/page_32_types.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/const.h" 1
/* const.h: Macros for dealing with constants.  */
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/page_32_types.h" 2
/*
 * This handles the memory map.
 *
 * A __PAGE_OFFSET of 0xC0000000 means that the kernel has
 * a virtual address space of one gigabyte, which limits the
 * amount of physical memory you can use to about 950MB.
 *
 * If you want more physical memory than this then see the CONFIG_HIGHMEM4G
 * and CONFIG_HIGHMEM64G options in the kernel configuration.
 */
#if definedEx(CONFIG_4KSTACKS)
#endif
#if !definedEx(CONFIG_4KSTACKS)
#endif
#if definedEx(CONFIG_X86_PAE)
/* 44=32+12, the limit we can fit into an unsigned long pfn */
#endif
#if !definedEx(CONFIG_X86_PAE)
#endif
/*
 * Kernel image size is limited to 512 MB (see in arch/x86/kernel/head_32.S)
 */
/*
 * This much address space is reserved for vmalloc() and iomap()
 * as well as fixmap mappings.
 */
extern unsigned int __VMALLOC_RESERVE;
extern int sysctl_legacy_va_layout;
extern void find_low_pfn_range(void);
extern void setup_bootmem_allocator(void);
#line 40 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/page_types.h" 2
#endif
extern int page_is_ram(unsigned long pagenr);
extern int devmem_is_allowed(unsigned long pagenr);
extern unsigned long max_low_pfn_mapped;
extern unsigned long max_pfn_mapped;
extern unsigned long init_memory_mapping(unsigned long start,
					 unsigned long end);
extern void initmem_init(unsigned long start_pfn, unsigned long end_pfn,
				int acpi, int k8);
extern void free_initmem(void);
#line 12 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/ptrace.h" 2
#if definedEx(CONFIG_X86_32)
/* this struct defines the way the registers are stored on the
   stack during a system call. */
 struct pt_regs {
	unsigned long bx;
	unsigned long cx;
	unsigned long dx;
	unsigned long si;
	unsigned long di;
	unsigned long bp;
	unsigned long ax;
	unsigned long ds;
	unsigned long es;
	unsigned long fs;
	unsigned long gs;
	unsigned long orig_ax;
	unsigned long ip;
	unsigned long cs;
	unsigned long flags;
	unsigned long sp;
	unsigned long ss;
};
#endif
#if !definedEx(CONFIG_X86_32)
 struct pt_regs {
	unsigned long r15;
	unsigned long r14;
	unsigned long r13;
	unsigned long r12;
	unsigned long bp;
	unsigned long bx;
/* arguments: non interrupts/non tracing syscalls only save upto here*/
	unsigned long r11;
	unsigned long r10;
	unsigned long r9;
	unsigned long r8;
	unsigned long ax;
	unsigned long cx;
	unsigned long dx;
	unsigned long si;
	unsigned long di;
	unsigned long orig_ax;
/* end of arguments */
/* cpu exception frame or undefined */
	unsigned long ip;
	unsigned long cs;
	unsigned long flags;
	unsigned long sp;
	unsigned long ss;
/* top of stack page */
};
#endif
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/init.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/compiler.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/init.h" 2
/* These macros are used to mark some functions or 
 * initialized data (doesn't apply to uninitialized data)
 * as `initialization' functions. The kernel can take this
 * as hint that the function is used only during the initialization
 * phase and free up used memory resources after
 *
 * Usage:
 * For functions:
 * 
 * You should add __init immediately before the function name, like:
 *
 * static void __init initme(int x, int y)
 * {
 *    extern int z; z = x * y;
 * }
 *
 * If the function has a prototype somewhere, you can also add
 * __init between closing brace of the prototype and semicolon:
 *
 * extern int initialize_foobar_device(int, int, int) __init;
 *
 * For initialized data:
 * You should insert __initdata between the variable name and equal
 * sign followed by value, e.g.:
 *
 * static int init_variable __initdata = 0;
 * static const char linux_logo[] __initconst = { 0x32, 0x36, ... };
 *
 * Don't forget to initialize data not at file scope, i.e. within a function,
 * as gcc otherwise puts the data into the bss section and not into the init
 * section.
 * 
 * Also note, that this data cannot be "const".
 */
/* These are for everybody (although not all archs will actually
   discard it in modules) */
/* modpost check for section mismatches during the kernel build.
 * A section mismatch happens when there are references from a
 * code or data section to an init section (both code or data).
 * The init sections are (for most archs) discarded by the kernel
 * when early init has completed so all such references are potential bugs.
 * For exit sections the same issue exists.
 * The following markers are used for the cases where the reference to
 * the *init / *exit section (code or data) is valid and will teach
 * modpost not to issue a warning.
 * The markers follow same syntax rules as __init / __initdata. */
/* compatibility defines */
/* Used for HOTPLUG */
/* Used for HOTPLUG_CPU */
/* Used for MEMORY_HOTPLUG */
/* For assembly routines */
/* silence warnings when references are OK */
/*
 * Used for initialization calls..
 */
typedef int (*initcall_t)(void);
typedef void (*exitcall_t)(void);
extern initcall_t __con_initcall_start[], __con_initcall_end[];
extern initcall_t __security_initcall_start[], __security_initcall_end[];
/* Used for contructor calls. */
typedef void (*ctor_fn_t)(void);
/* Defined in init/main.c */
extern int do_one_initcall(initcall_t fn);
extern char __attribute__ ((__section__(".init.data"))) boot_command_line[];
extern char *saved_command_line;
extern unsigned int reset_devices;
/* used by init/main.c */
void setup_arch(char **);
void prepare_namespace(void);
extern void (*late_time_init)(void);
extern int initcall_debug;
/* initcalls are now grouped by functionality into separate 
 * subsections. Ordering inside the subsections is determined
 * by link order. 
 * For backwards compatibility, initcall() puts the call in 
 * the device init subsection.
 *
 * The `id' arg to __define_initcall() is needed so that multiple initcalls
 * can point at the same handler without causing duplicate-symbol build errors.
 */
/*
 * Early initcalls run before initializing SMP.
 *
 * Only for built-in code, not modules.
 */
/*
 * A "pure" initcall has no dependencies on anything else, and purely
 * initializes variables that couldn't be statically initialized.
 *
 * This only exists for built-in code, not for modules.
 */
struct obs_kernel_param {
	const char *str;
	int (*setup_func)(char *);
	int early;
};
/*
 * Only for really core code.  See moduleparam.h for the normal way.
 *
 * Force the alignment so the compiler doesn't space elements of the
 * obs_kernel_param "array" too far apart in .init.setup.
 */
/* NOTE: fn is as per module_param, not __setup!  Emits warning if fn
 * returns non-zero. */
/* Relies on boot_command_line being set */
void __attribute__ ((__section__(".init.text"))) __attribute__((__cold__)) __attribute__((no_instrument_function)) parse_early_param(void);
void __attribute__ ((__section__(".init.text"))) __attribute__((__cold__)) __attribute__((no_instrument_function)) parse_early_options(char *cmdline);
/**
 * module_init() - driver initialization entry point
 * @x: function to be run at kernel boot time or module insertion
 * 
 * module_init() will either be called during do_initcalls() (if
 * builtin) or at module insertion time (if a module).  There can only
 * be one per module.
 */
/**
 * module_exit() - driver exit entry point
 * @x: function to be run when driver is removed
 * 
 * module_exit() will wrap the driver clean-up code
 * with cleanup_module() when used with rmmod when
 * the driver is a module.  If the driver is statically
 * compiled into the kernel, module_exit() has no effect.
 * There can only be one per module.
 */
/* Data marked not to be saved by software suspend */
/* This means "can be init if no module support, otherwise module load
   may call it." */
#if definedEx(CONFIG_MODULES)
#endif
#if !definedEx(CONFIG_MODULES)
#endif
/* Functions marked as __devexit may be discarded at kernel link time, depending
   on config options.  Newer versions of binutils detect references from
   retained sections to discarded sections and flag an error.  Pointers to
   __devexit functions must use __devexit_p(function_name), the wrapper will
   insert either the function_name or NULL, depending on the config options.
 */
#if definedEx(CONFIG_HOTPLUG)
#endif
#if !definedEx(CONFIG_HOTPLUG)
#endif
#line 135 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/ptrace.h" 2
struct cpuinfo_x86;
struct task_struct;
extern unsigned long profile_pc(struct pt_regs *regs);
extern unsigned long
convert_ip_to_linear(struct task_struct *child, struct pt_regs *regs);
extern void send_sigtrap(struct task_struct *tsk, struct pt_regs *regs,
			 int error_code, int si_code);
void signal_fault(struct pt_regs *regs, void  *frame, char *where);
extern long syscall_trace_enter(struct pt_regs *);
extern void syscall_trace_leave(struct pt_regs *);
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long regs_return_value(struct pt_regs *regs)
{
	return regs->ax;
}
/*
 * user_mode_vm(regs) determines whether a register set came from user mode.
 * This is true if V8086 mode was enabled OR if the register set was from
 * protected mode with RPL-3 CS value.  This tricky test checks that with
 * one comparison.  Many places in the kernel can bypass this full check
 * if they have already ruled out V8086 mode, so user_mode(regs) can be used.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int user_mode(struct pt_regs *regs)
{
#if definedEx(CONFIG_X86_32)
	return (regs->cs & 0x3) == 0x3;
#endif
#if !definedEx(CONFIG_X86_32)
	return !!(regs->cs & 3);
#endif
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int user_mode_vm(struct pt_regs *regs)
{
#if definedEx(CONFIG_X86_32)
	return ((regs->cs & 0x3) | (regs->flags & 
#if definedEx(CONFIG_VM86)
0x00020000
#endif
#if !definedEx(CONFIG_VM86)
0
#endif
)) >=
		0x3;
#endif
#if !definedEx(CONFIG_X86_32)
	return user_mode(regs);
#endif
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int v8086_mode(struct pt_regs *regs)
{
#if definedEx(CONFIG_X86_32)
	return (regs->flags & 
#if definedEx(CONFIG_VM86)
0x00020000
#endif
#if !definedEx(CONFIG_VM86)
0
#endif
);
#endif
#if !definedEx(CONFIG_X86_32)
	return 0;	/* No V86 mode support in long mode */
#endif
}
/*
 * X86_32 CPUs don't save ss and esp if the CPU is already in kernel mode
 * when it traps.  The previous stack will be directly underneath the saved
 * registers, and 'sp/ss' won't even have been saved. Thus the '&regs->sp'.
 *
 * This is valid only for kernel mode traps.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long kernel_stack_pointer(struct pt_regs *regs)
{
#if definedEx(CONFIG_X86_32)
	return (unsigned long)(&regs->sp);
#endif
#if !definedEx(CONFIG_X86_32)
	return regs->sp;
#endif
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long instruction_pointer(struct pt_regs *regs)
{
	return regs->ip;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long frame_pointer(struct pt_regs *regs)
{
	return regs->bp;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long user_stack_pointer(struct pt_regs *regs)
{
	return regs->sp;
}
/* Query offset/name of register from its name/offset */
extern int regs_query_register_offset(const char *name);
extern const char *regs_query_register_name(unsigned int offset);
/**
 * regs_get_register() - get register value from its offset
 * @regs:	pt_regs from which register value is gotten.
 * @offset:	offset number of the register.
 *
 * regs_get_register returns the value of a register. The @offset is the
 * offset of the register in struct pt_regs address which specified by @regs.
 * If @offset is bigger than MAX_REG_OFFSET, this returns 0.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long regs_get_register(struct pt_regs *regs,
					      unsigned int offset)
{
	if (__builtin_expect(!!(offset > (__builtin_offsetof(struct pt_regs,ss))), 0))
		return 0;
	return *(unsigned long *)((unsigned long)regs + offset);
}
/**
 * regs_within_kernel_stack() - check the address in the stack
 * @regs:	pt_regs which contains kernel stack pointer.
 * @addr:	address which is checked.
 *
 * regs_within_kernel_stack() checks @addr is within the kernel stack page(s).
 * If @addr is within the kernel stack, it returns true. If not, returns false.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int regs_within_kernel_stack(struct pt_regs *regs,
					   unsigned long addr)
{
	return ((addr & ~((((1UL) << 12) << 
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_4KSTACKS) || definedEx(CONFIG_X86_64)
1
#endif
#if !definedEx(CONFIG_X86_64) && definedEx(CONFIG_4KSTACKS)
0
#endif
) - 1))  ==
		(kernel_stack_pointer(regs) & ~((((1UL) << 12) << 
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_4KSTACKS) || definedEx(CONFIG_X86_64)
1
#endif
#if !definedEx(CONFIG_X86_64) && definedEx(CONFIG_4KSTACKS)
0
#endif
) - 1)));
}
/**
 * regs_get_kernel_stack_nth() - get Nth entry of the stack
 * @regs:	pt_regs which contains kernel stack pointer.
 * @n:		stack entry number.
 *
 * regs_get_kernel_stack_nth() returns @n th entry of the kernel stack which
 * is specified by @regs. If the @n th entry is NOT in the kernel stack,
 * this returns 0.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long regs_get_kernel_stack_nth(struct pt_regs *regs,
						      unsigned int n)
{
	unsigned long *addr = (unsigned long *)kernel_stack_pointer(regs);
	addr += n;
	if (regs_within_kernel_stack(regs, (unsigned long)addr))
		return *addr;
	else
		return 0;
}
/* Get Nth argument at function call */
extern unsigned long regs_get_argument_nth(struct pt_regs *regs,
					   unsigned int n);
/*
 * These are defined as per linux/ptrace.h, which see.
 */
extern void user_enable_single_step(struct task_struct *);
extern void user_disable_single_step(struct task_struct *);
extern void user_enable_block_step(struct task_struct *);
#if definedEx(CONFIG_X86_DEBUGCTLMSR)
#endif
#if !definedEx(CONFIG_X86_DEBUGCTLMSR)
#endif
struct user_desc;
extern int do_get_thread_area(struct task_struct *p, int idx,
			      struct user_desc  *info);
extern int do_set_thread_area(struct task_struct *p, int idx,
			      struct user_desc  *info, int can_allocate);
#line 132 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/vm86.h" 2
/*
 * This is the (kernel) stack-layout when we have done a "SAVE_ALL" from vm86
 * mode - the main change is that the old segment descriptors aren't
 * useful any more and are forced to be zero by the kernel (and the
 * hardware when a trap occurs), and the real segment descriptors are
 * at the end of the structure. Look at ptrace.h to see the "normal"
 * setup. For user space layout see 'struct vm86_regs' above.
 */
struct kernel_vm86_regs {
/*
 * normal regs, with special meaning for the segment descriptors..
 */
	struct pt_regs pt;
/*
 * these are specific to v86 mode:
 */
	unsigned short es, __esh;
	unsigned short ds, __dsh;
	unsigned short fs, __fsh;
	unsigned short gs, __gsh;
};
struct kernel_vm86_struct {
	struct kernel_vm86_regs regs;
/*
 * the below part remains on the kernel stack while we are in VM86 mode.
 * 'tss.esp0' then contains the address of VM86_TSS_ESP0 below, and when we
 * get forced back from VM86, the CPU and "SAVE_ALL" will restore the above
 * 'struct kernel_vm86_regs' with the then actual values.
 * Therefore, pt_regs in fact points to a complete 'kernel_vm86_struct'
 * in kernelspace, hence we need not reget the data from userspace.
 */
	unsigned long flags;
	unsigned long screen_bitmap;
	unsigned long cpu_type;
	struct revectored_struct int_revectored;
	struct revectored_struct int21_revectored;
	struct vm86plus_info_struct vm86plus;
	struct pt_regs *regs32;   /* here we save the pointer to the old regs */
/*
 * The below is not part of the structure, but the stack layout continues
 * this way. In front of 'return-eip' may be some data, depending on
 * compilation, so we don't rely on this and save the pointer to 'oldregs'
 * in 'regs32' above.
 * However, with GCC-2.7.2 and the current CFLAGS you see exactly this:
	long return-eip;        from call to vm86()
	struct pt_regs oldregs;  user space registers as saved by syscall
 */
};
#if definedEx(CONFIG_VM86)
void handle_vm86_fault(struct kernel_vm86_regs *, long);
int handle_vm86_trap(struct kernel_vm86_regs *, long, int);
struct pt_regs *save_v86_state(struct kernel_vm86_regs *);
struct task_struct;
void release_vm86_irqs(struct task_struct *);
#endif
#if !definedEx(CONFIG_VM86)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int handle_vm86_trap(struct kernel_vm86_regs *a, long b, int c)
{
	return 0;
}
#endif
#line 12 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/processor.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/math_emu.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/ptrace.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/math_emu.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/vm86.h" 1
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/math_emu.h" 2
/* This structure matches the layout of the data saved to the stack
   following a device-not-present interrupt, part of it saved
   automatically by the 80386/80486.
   */
struct math_emu_info {
	long ___orig_eip;
	union {
		struct pt_regs *regs;
		struct kernel_vm86_regs *vm86;
	};
};
#line 13 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/processor.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/segment.h" 1
#line 14 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/processor.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/types.h" 1
#line 15 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/processor.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/sigcontext.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/compiler.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/sigcontext.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/sigcontext.h" 2
/*
 * bytes 464..511 in the current 512byte layout of fxsave/fxrstor frame
 * are reserved for SW usage. On cpu's supporting xsave/xrstor, these bytes
 * are used to extended the fpstate pointer in the sigcontext, which now
 * includes the extended state information along with fpstate information.
 *
 * Presence of FP_XSTATE_MAGIC1 at the beginning of this SW reserved
 * area and FP_XSTATE_MAGIC2 at the end of memory layout
 * (extended_size - FP_XSTATE_MAGIC2_SIZE) indicates the presence of the
 * extended state information in the memory layout pointed by the fpstate
 * pointer in sigcontext.
 */
struct _fpx_sw_bytes {
	__u32 magic1;		/* FP_XSTATE_MAGIC1 */
	__u32 extended_size;	/* total size of the layout referred by
				 * fpstate pointer in the sigcontext.
				 */
	__u64 xstate_bv;
				/* feature bit mask (including fp/sse/extended
				 * state) that is present in the memory
				 * layout.
				 */
	__u32 xstate_size;	/* actual xsave state size, based on the
				 * features saved in the layout.
				 * 'extended_size' will be greater than
				 * 'xstate_size'.
				 */
	__u32 padding[7];	/*  for future use. */
};
#if definedEx(CONFIG_X86_32)
/*
 * As documented in the iBCS2 standard..
 *
 * The first part of "struct _fpstate" is just the normal i387
 * hardware setup, the extra "status" word is used to save the
 * coprocessor status word before entering the handler.
 *
 * Pentium III FXSR, SSE support
 *	Gareth Hughes <gareth@valinux.com>, May 2000
 *
 * The FPU state data structure has had to grow to accommodate the
 * extended FPU state required by the Streaming SIMD Extensions.
 * There is no documented standard to accomplish this at the moment.
 */
struct _fpreg {
	unsigned short significand[4];
	unsigned short exponent;
};
struct _fpxreg {
	unsigned short significand[4];
	unsigned short exponent;
	unsigned short padding[3];
};
struct _xmmreg {
	unsigned long element[4];
};
struct _fpstate {
	/* Regular FPU environment */
	unsigned long	cw;
	unsigned long	sw;
	unsigned long	tag;
	unsigned long	ipoff;
	unsigned long	cssel;
	unsigned long	dataoff;
	unsigned long	datasel;
	struct _fpreg	_st[8];
	unsigned short	status;
	unsigned short	magic;		/* 0xffff = regular FPU data only */
	/* FXSR FPU environment */
	unsigned long	_fxsr_env[6];	/* FXSR FPU env is ignored */
	unsigned long	mxcsr;
	unsigned long	reserved;
	struct _fpxreg	_fxsr_st[8];	/* FXSR FPU reg data is ignored */
	struct _xmmreg	_xmm[8];
	unsigned long	padding1[44];
	union {
		unsigned long	padding2[12];
		struct _fpx_sw_bytes sw_reserved; /* represents the extended
						   * state info */
	};
};
struct sigcontext {
	unsigned short gs, __gsh;
	unsigned short fs, __fsh;
	unsigned short es, __esh;
	unsigned short ds, __dsh;
	unsigned long di;
	unsigned long si;
	unsigned long bp;
	unsigned long sp;
	unsigned long bx;
	unsigned long dx;
	unsigned long cx;
	unsigned long ax;
	unsigned long trapno;
	unsigned long err;
	unsigned long ip;
	unsigned short cs, __csh;
	unsigned long flags;
	unsigned long sp_at_signal;
	unsigned short ss, __ssh;
	/*
	 * fpstate is really (struct _fpstate *) or (struct _xstate *)
	 * depending on the FP_XSTATE_MAGIC1 encoded in the SW reserved
	 * bytes of (struct _fpstate) and FP_XSTATE_MAGIC2 present at the end
	 * of extended memory layout. See comments at the definition of
	 * (struct _fpx_sw_bytes)
	 */
	void  *fpstate;		/* zero when no FPU/extended context */
	unsigned long oldmask;
	unsigned long cr2;
};
#endif
#if !definedEx(CONFIG_X86_32)
/* FXSAVE frame */
/* Note: reserved1/2 may someday contain valuable data. Always save/restore
   them when you change signal frames. */
struct _fpstate {
	__u16	cwd;
	__u16	swd;
	__u16	twd;		/* Note this is not the same as the
				   32bit/x87/FSAVE twd */
	__u16	fop;
	__u64	rip;
	__u64	rdp;
	__u32	mxcsr;
	__u32	mxcsr_mask;
	__u32	st_space[32];	/* 8*16 bytes for each FP-reg */
	__u32	xmm_space[64];	/* 16*16 bytes for each XMM-reg  */
	__u32	reserved2[12];
	union {
		__u32	reserved3[12];
		struct _fpx_sw_bytes sw_reserved; /* represents the extended
						   * state information */
	};
};
struct sigcontext {
	unsigned long r8;
	unsigned long r9;
	unsigned long r10;
	unsigned long r11;
	unsigned long r12;
	unsigned long r13;
	unsigned long r14;
	unsigned long r15;
	unsigned long di;
	unsigned long si;
	unsigned long bp;
	unsigned long bx;
	unsigned long dx;
	unsigned long ax;
	unsigned long cx;
	unsigned long sp;
	unsigned long ip;
	unsigned long flags;
	unsigned short cs;
	unsigned short gs;
	unsigned short fs;
	unsigned short __pad0;
	unsigned long err;
	unsigned long trapno;
	unsigned long oldmask;
	unsigned long cr2;
	/*
	 * fpstate is really (struct _fpstate *) or (struct _xstate *)
	 * depending on the FP_XSTATE_MAGIC1 encoded in the SW reserved
	 * bytes of (struct _fpstate) and FP_XSTATE_MAGIC2 present at the end
	 * of extended memory layout. See comments at the definition of
	 * (struct _fpx_sw_bytes)
	 */
	void  *fpstate;		/* zero when no FPU/extended context */
	unsigned long reserved1[8];
};
#endif
struct _xsave_hdr {
	__u64 xstate_bv;
	__u64 reserved1[2];
	__u64 reserved2[5];
};
struct _ymmh_state {
	/* 16 * 16 bytes for each YMMH-reg */
	__u32 ymmh_space[64];
};
/*
 * Extended state pointed by the fpstate pointer in the sigcontext.
 * In addition to the fpstate, information encoded in the xstate_hdr
 * indicates the presence of other extended state information
 * supported by the processor and OS.
 */
struct _xstate {
	struct _fpstate fpstate;
	struct _xsave_hdr xstate_hdr;
	struct _ymmh_state ymmh;
	/* new processor state extensions go here */
};
#line 16 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/processor.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/current.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/compiler.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/current.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/percpu.h" 1
#if definedEx(CONFIG_X86_64)
#endif
#if !definedEx(CONFIG_X86_64)
#endif
 #line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/kernel.h" 1
/*
 * 'kernel.h' contains some often-used function prototypes etc
 */
#line 1 "systems/redhat/usr/lib/gcc/x86_64-redhat-linux/4.4.4/include/stdarg.h" 1
/* Copyright (C) 1989, 1997, 1998, 1999, 2000, 2009 Free Software Foundation, Inc.
This file is part of GCC.
GCC is free software; you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation; either version 3, or (at your option)
any later version.
GCC is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.
Under Section 7 of GPL version 3, you are granted additional
permissions described in the GCC Runtime Library Exception, version
3.1, as published by the Free Software Foundation.
You should have received a copy of the GNU General Public License and
a copy of the GCC Runtime Library Exception along with this program;
see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see
<http://www.gnu.org/licenses/>.  */
/*
 * ISO C Standard:  7.15  Variable arguments  <stdarg.h>
 */
/* Define __gnuc_va_list.  */
typedef __builtin_va_list __gnuc_va_list;
/* Define the standard macros for the user,
   if this invocation was from the user program.  */
/* Define va_list, if desired, from __gnuc_va_list. */
/* We deliberately do not define va_list when called from
   stdio.h, because ANSI C says that stdio.h is not supposed to define
   va_list.  stdio.h needs to have access to that data type, 
   but must not use that name.  It should use the name __gnuc_va_list,
   which is safe because it is reserved for the implementation.  */
 /* The macro _VA_LIST_ is the same thing used by this file in Ultrix.
   But on BSD NET2 we must not test or define or undef it.
   (Note that the comments in NET 2's ansi.h
   are incorrect for _VA_LIST_--see stdio.h!)  */
/* The macro _VA_LIST_DEFINED is used in Windows NT 3.5  */
/* The macro _VA_LIST is used in SCO Unix 3.2.  */
/* The macro _VA_LIST_T_H is used in the Bull dpx2  */
/* The macro __va_list__ is used by BeOS.  */
typedef __gnuc_va_list va_list;
#line 12 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/kernel.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/linkage.h" 1
#if definedEx(CONFIG_X86_32)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/compiler.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/linkage.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/linkage.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/stringify.h" 1
/* Indirect stringification.  Doing two levels allows the parameter to be a
 * macro itself.  For example, compile with -DFOO=bar, __stringify(FOO)
 * converts to "bar".
 */
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/linkage.h" 2
/*
 * For 32-bit UML - mark functions implemented in assembly that use
 * regparm input parameters:
 */
/*
 * Make sure the compiler doesn't do anything stupid with the
 * arguments on the stack - they are owned by the *caller*, not
 * the callee. This just fools gcc into not spilling into them,
 * and keeps it from doing tailcall recursion and/or using the
 * stack slots for temporaries, since they are live and "used"
 * all the way to the end of the function.
 *
 * NOTE! On x86-64, all the arguments are in registers, so this
 * only matters on a 32-bit kernel.
 */
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/linkage.h" 2
/*
 * For assembly routines.
 *
 * Note when using these that you must specify the appropriate
 * alignment directives yourself
 */
/*
 * This is used by architectures to keep arguments on the stack
 * untouched by the compiler by keeping them live until the end.
 * The argument stack may be owned by the assembly-language
 * caller, not the callee, and gcc doesn't always understand
 * that.
 *
 * We have the return value, and a maximum of six arguments.
 *
 * This should always be followed by a "return ret" for the
 * protection to work (ie no more work that the compiler might
 * end up needing stack temporaries for).
 */
/* Assembly files may be compiled with -traditional .. */
#endif
#line 13 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/kernel.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/stddef.h" 1
#line 14 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/kernel.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 15 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/kernel.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/compiler.h" 1
#line 16 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/kernel.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/bitops.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/types.h" 1
#line 5 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/bitops.h" 2
/*
 * Include this here because some architectures need generic_ffs/fls in
 * scope
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/bitops.h" 1
/*
 * Copyright 1992, Linus Torvalds.
 *
 * Note: inlines with more than a single statement should be marked
 * __always_inline to avoid problems with older gcc's inlining heuristics.
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/compiler.h" 1
#line 17 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/bitops.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/alternative.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/alternative.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/stddef.h" 1
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/alternative.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/stringify.h" 1
#line 8 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/alternative.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/asm.h" 1
#if definedEx(CONFIG_X86_32)
#endif
#if !definedEx(CONFIG_X86_32)
#endif
/* Exception table entry */
#line 9 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/alternative.h" 2
/*
 * Alternative inline assembly for SMP.
 *
 * The LOCK_PREFIX macro defined here replaces the LOCK and
 * LOCK_PREFIX macros used everywhere in the source tree.
 *
 * SMP alternatives use the same data structures as the other
 * alternatives and the X86_FEATURE_UP flag to indicate the case of a
 * UP system running a SMP kernel.  The existing apply_alternatives()
 * works fine for patching a SMP kernel for UP.
 *
 * The SMP alternative tables can be kept after boot and contain both
 * UP and SMP versions of the instructions to allow switching back to
 * SMP at runtime, when hotplugging in a new CPU, which is especially
 * useful in virtualized environments.
 *
 * The very common lock prefix is handled as special case in a
 * separate table which is a pure address list without replacement ptr
 * and size information.  That keeps the table sizes small.
 */
#if definedEx(CONFIG_SMP)
#endif
#if !definedEx(CONFIG_SMP)
#endif
/* This must be included *after* the definition of LOCK_PREFIX */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/cpufeature.h" 1
/*
 * Defines x86 CPU feature bits
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/required-features.h" 1
/* Define minimum CPUID feature set for kernel These bits are checked
   really early to actually display a visible error message before the
   kernel dies.  Make sure to assign features to the proper mask!
   Some requirements that are not in CPUID yet are also in the
   CONFIG_X86_MINIMUM_CPU_FAMILY which is checked too.
   The real information is in arch/x86/Kconfig.cpu, this just converts
   the CONFIGs into a bitmask */
#if !definedEx(CONFIG_MATH_EMULATION)
#endif
#if definedEx(CONFIG_MATH_EMULATION)
#endif
#if !definedEx(CONFIG_X86_PAE) && definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_PAE)
#endif
#if !definedEx(CONFIG_X86_PAE) && !definedEx(CONFIG_X86_64)
#endif
#if definedEx(CONFIG_X86_CMPXCHG64)
#endif
#if !definedEx(CONFIG_X86_CMPXCHG64)
#endif
#if !definedEx(CONFIG_X86_64) && definedEx(CONFIG_X86_CMOV) || definedEx(CONFIG_X86_64)
#endif
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_X86_CMOV)
#endif
#if definedEx(CONFIG_X86_USE_3DNOW)
#endif
#if !definedEx(CONFIG_X86_USE_3DNOW)
#endif
#if !definedEx(CONFIG_X86_64) && definedEx(CONFIG_X86_P6_NOP) || definedEx(CONFIG_X86_64)
#endif
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_X86_P6_NOP)
#endif
#if definedEx(CONFIG_X86_64)
#if definedEx(CONFIG_PARAVIRT)
/* Paravirtualized systems may not have PSE or PGE available */
#endif
#if !definedEx(CONFIG_PARAVIRT)
#endif
#endif
#if !definedEx(CONFIG_X86_64)
#endif
#line 9 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/cpufeature.h" 2
/*
 * Note: If the comment begins with a quoted string, that string is used
 * in /proc/cpuinfo instead of the macro name.  If the string is "",
 * this feature bit is not displayed in /proc/cpuinfo at all.
 */
/* Intel-defined CPU features, CPUID level 0x00000001 (edx), word 0 */
					  /* (plus FCMOVcc, FCOMI with FPU) */
/* AMD-defined CPU features, CPUID level 0x80000001, word 1 */
/* Don't duplicate feature flags which are redundant with Intel! */
/* Transmeta-defined CPU features, CPUID level 0x80860001, word 2 */
/* Other features, Linux-defined mapping, word 3 */
/* This range is used for feature bits which conflict or are synthesized */
/* cpu types for specific tunings: */
/* Intel-defined CPU features, CPUID level 0x00000001 (ecx), word 4 */
/* VIA/Cyrix/Centaur-defined CPU features, CPUID level 0xC0000001, word 5 */
/* More extended AMD flags: CPUID level 0x80000001, ecx, word 6 */
/*
 * Auxiliary flags: Linux defined - For features scattered in various
 * CPUID levels like 0x6, 0xA etc
 */
/* Virtualization flags: Linux defined */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/bitops.h" 1
#line 176 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/cpufeature.h" 2
extern const char * const x86_cap_flags[9*32];
extern const char * const x86_power_flags[32];
#if !definedEx(CONFIG_X86_64) && definedEx(CONFIG_X86_INVLPG) || definedEx(CONFIG_X86_64)
#endif
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_X86_INVLPG)
#endif
#if definedEx(CONFIG_X86_64)
#endif
#line 45 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/alternative.h" 2
struct alt_instr {
	u8 *instr;		/* original instruction */
	u8 *replacement;
	u8  cpuid;		/* cpuid bit set for replacement */
	u8  instrlen;		/* length of original instruction */
	u8  replacementlen;	/* length of new instruction, <= instrlen */
	u8  pad1;
#if definedEx(CONFIG_X86_64)
	u32 pad2;
#endif
};
extern void alternative_instructions(void);
extern void apply_alternatives(struct alt_instr *start, struct alt_instr *end);
struct module;
#if definedEx(CONFIG_SMP)
extern void alternatives_smp_module_add(struct module *mod, char *name,
					void *locks, void *locks_end,
					void *text, void *text_end);
extern void alternatives_smp_module_del(struct module *mod);
extern void alternatives_smp_switch(int smp);
#endif
#if !definedEx(CONFIG_SMP)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void alternatives_smp_module_add(struct module *mod, char *name,
					       void *locks, void *locks_end,
					       void *text, void *text_end) {}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void alternatives_smp_module_del(struct module *mod) {}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void alternatives_smp_switch(int smp) {}
#endif
/* alternative assembly primitive: */
/*
 * Alternative instructions for different CPU types or capabilities.
 *
 * This allows to use optimized instructions even on generic binary
 * kernels.
 *
 * length of oldinstr must be longer or equal the length of newinstr
 * It can be padded with nops as needed.
 *
 * For non barrier like inlines please define new variants
 * without volatile and memory clobber.
 */
/*
 * Alternative inline assembly with input.
 *
 * Pecularities:
 * No memory clobber here.
 * Argument numbers start with 1.
 * Best is to use constraints that are fixed size (like (%1) ... "r")
 * If you use variable sized constraints like "m" or "g" in the
 * replacement make sure to pad to the worst case length.
 * Leaving an unused argument 0 to keep API compatibility.
 */
/* Like alternative_input, but with a single output argument */
/*
 * use this macro(s) if you need more than one output parameter
 * in alternative_io
 */
struct paravirt_patch_site;
#if definedEx(CONFIG_PARAVIRT)
void apply_paravirt(struct paravirt_patch_site *start,
		    struct paravirt_patch_site *end);
#endif
#if !definedEx(CONFIG_PARAVIRT)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void apply_paravirt(struct paravirt_patch_site *start,
				  struct paravirt_patch_site *end)
{}
#endif
/*
 * Clear and restore the kernel write-protection flag on the local CPU.
 * Allows the kernel to edit read-only pages.
 * Side-effect: any interrupt handler running between save and restore will have
 * the ability to write to read-only pages.
 *
 * Warning:
 * Code patching in the UP case is safe if NMIs and MCE handlers are stopped and
 * no thread can be preempted in the instructions being modified (no iret to an
 * invalid instruction possible) or if the instructions are changed from a
 * consistent state to another consistent state atomically.
 * More care must be taken when modifying code in the SMP case because of
 * Intel's errata.
 * On the local CPU you need to be protected again NMI or MCE handlers seeing an
 * inconsistent instruction while you patch.
 */
extern void *text_poke(void *addr, const void *opcode, size_t len);
#line 18 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/bitops.h" 2
/*
 * These have to be done with inline assembly: that way the bit-setting
 * is guaranteed to be atomic. All bit operations return 0 if the bit
 * was cleared before the operation and != 0 if it was not.
 *
 * bit 0 is the LSB of addr; bit 32 is the LSB of (addr+1).
 */
/*
 * We do the locked ops that don't return the old value as
 * a mask operation on a byte.
 */
/**
 * set_bit - Atomically set a bit in memory
 * @nr: the bit to set
 * @addr: the address to start counting from
 *
 * This function is atomic and may not be reordered.  See __set_bit()
 * if you do not require the atomic guarantees.
 *
 * Note: there are no guarantees that this function will not be reordered
 * on non x86 architectures, so if you are writing portable code,
 * make sure not to rely on its reordering guarantees.
 *
 * Note that @nr may be almost arbitrarily large; this function is not
 * restricted to acting on a single-word quantity.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __attribute__((always_inline)) void
set_bit(unsigned int nr, volatile unsigned long *addr)
{
	if ((__builtin_constant_p(nr))) {
		asm volatile(
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "orb %1,%0"
			: "+m" (*(volatile long *) ((void *)(addr) +((nr)>>3)))
			: "iq" ((u8)(1 << ((nr) & 7)))
			: "memory");
	} else {
		asm volatile(
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "bts %1,%0"
			: "+m" (*(volatile long *) (addr)) : "Ir" (nr) : "memory");
	}
}
/**
 * __set_bit - Set a bit in memory
 * @nr: the bit to set
 * @addr: the address to start counting from
 *
 * Unlike set_bit(), this function is non-atomic and may be reordered.
 * If it's called on the same region of memory simultaneously, the effect
 * may be that only one operation succeeds.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __set_bit(int nr, volatile unsigned long *addr)
{
	asm volatile("bts %1,%0" : "+m" (*(volatile long *) (addr)) : "Ir" (nr) : "memory");
}
/**
 * clear_bit - Clears a bit in memory
 * @nr: Bit to clear
 * @addr: Address to start counting from
 *
 * clear_bit() is atomic and may not be reordered.  However, it does
 * not contain a memory barrier, so if it is used for locking purposes,
 * you should call smp_mb__before_clear_bit() and/or smp_mb__after_clear_bit()
 * in order to ensure changes are visible on other processors.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __attribute__((always_inline)) void
clear_bit(int nr, volatile unsigned long *addr)
{
	if ((__builtin_constant_p(nr))) {
		asm volatile(
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "andb %1,%0"
			: "+m" (*(volatile long *) ((void *)(addr) +((nr)>>3)))
			: "iq" ((u8)~(1 << ((nr) & 7))));
	} else {
		asm volatile(
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "btr %1,%0"
			: "+m" (*(volatile long *) (addr))
			: "Ir" (nr));
	}
}
/*
 * clear_bit_unlock - Clears a bit in memory
 * @nr: Bit to clear
 * @addr: Address to start counting from
 *
 * clear_bit() is atomic and implies release semantics before the memory
 * operation. It can be used for an unlock.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void clear_bit_unlock(unsigned nr, volatile unsigned long *addr)
{
	__asm__ __volatile__("": : :"memory");
	clear_bit(nr, addr);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __clear_bit(int nr, volatile unsigned long *addr)
{
	asm volatile("btr %1,%0" : "+m" (*(volatile long *) (addr)) : "Ir" (nr));
}
/*
 * __clear_bit_unlock - Clears a bit in memory
 * @nr: Bit to clear
 * @addr: Address to start counting from
 *
 * __clear_bit() is non-atomic and implies release semantics before the memory
 * operation. It can be used for an unlock if no other CPUs can concurrently
 * modify other bits in the word.
 *
 * No memory barrier is required here, because x86 cannot reorder stores past
 * older loads. Same principle as spin_unlock.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __clear_bit_unlock(unsigned nr, volatile unsigned long *addr)
{
	__asm__ __volatile__("": : :"memory");
	__clear_bit(nr, addr);
}
/**
 * __change_bit - Toggle a bit in memory
 * @nr: the bit to change
 * @addr: the address to start counting from
 *
 * Unlike change_bit(), this function is non-atomic and may be reordered.
 * If it's called on the same region of memory simultaneously, the effect
 * may be that only one operation succeeds.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __change_bit(int nr, volatile unsigned long *addr)
{
	asm volatile("btc %1,%0" : "+m" (*(volatile long *) (addr)) : "Ir" (nr));
}
/**
 * change_bit - Toggle a bit in memory
 * @nr: Bit to change
 * @addr: Address to start counting from
 *
 * change_bit() is atomic and may not be reordered.
 * Note that @nr may be almost arbitrarily large; this function is not
 * restricted to acting on a single-word quantity.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void change_bit(int nr, volatile unsigned long *addr)
{
	if ((__builtin_constant_p(nr))) {
		asm volatile(
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "xorb %1,%0"
			: "+m" (*(volatile long *) ((void *)(addr) +((nr)>>3)))
			: "iq" ((u8)(1 << ((nr) & 7))));
	} else {
		asm volatile(
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "btc %1,%0"
			: "+m" (*(volatile long *) (addr))
			: "Ir" (nr));
	}
}
/**
 * test_and_set_bit - Set a bit and return its old value
 * @nr: Bit to set
 * @addr: Address to count from
 *
 * This operation is atomic and cannot be reordered.
 * It also implies a memory barrier.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int test_and_set_bit(int nr, volatile unsigned long *addr)
{
	int oldbit;
	asm volatile(
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "bts %2,%1\n\t"
		     "sbb %0,%0" : "=r" (oldbit), "+m" (*(volatile long *) (addr)) : "Ir" (nr) : "memory");
	return oldbit;
}
/**
 * test_and_set_bit_lock - Set a bit and return its old value for lock
 * @nr: Bit to set
 * @addr: Address to count from
 *
 * This is the same as test_and_set_bit on x86.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __attribute__((always_inline)) int
test_and_set_bit_lock(int nr, volatile unsigned long *addr)
{
	return test_and_set_bit(nr, addr);
}
/**
 * __test_and_set_bit - Set a bit and return its old value
 * @nr: Bit to set
 * @addr: Address to count from
 *
 * This operation is non-atomic and can be reordered.
 * If two examples of this operation race, one can appear to succeed
 * but actually fail.  You must protect multiple accesses with a lock.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int __test_and_set_bit(int nr, volatile unsigned long *addr)
{
	int oldbit;
	asm("bts %2,%1\n\t"
	    "sbb %0,%0"
	    : "=r" (oldbit), "+m" (*(volatile long *) (addr))
	    : "Ir" (nr));
	return oldbit;
}
/**
 * test_and_clear_bit - Clear a bit and return its old value
 * @nr: Bit to clear
 * @addr: Address to count from
 *
 * This operation is atomic and cannot be reordered.
 * It also implies a memory barrier.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int test_and_clear_bit(int nr, volatile unsigned long *addr)
{
	int oldbit;
	asm volatile(
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "btr %2,%1\n\t"
		     "sbb %0,%0"
		     : "=r" (oldbit), "+m" (*(volatile long *) (addr)) : "Ir" (nr) : "memory");
	return oldbit;
}
/**
 * __test_and_clear_bit - Clear a bit and return its old value
 * @nr: Bit to clear
 * @addr: Address to count from
 *
 * This operation is non-atomic and can be reordered.
 * If two examples of this operation race, one can appear to succeed
 * but actually fail.  You must protect multiple accesses with a lock.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int __test_and_clear_bit(int nr, volatile unsigned long *addr)
{
	int oldbit;
	asm volatile("btr %2,%1\n\t"
		     "sbb %0,%0"
		     : "=r" (oldbit), "+m" (*(volatile long *) (addr))
		     : "Ir" (nr));
	return oldbit;
}
/* WARNING: non atomic and it can be reordered! */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int __test_and_change_bit(int nr, volatile unsigned long *addr)
{
	int oldbit;
	asm volatile("btc %2,%1\n\t"
		     "sbb %0,%0"
		     : "=r" (oldbit), "+m" (*(volatile long *) (addr))
		     : "Ir" (nr) : "memory");
	return oldbit;
}
/**
 * test_and_change_bit - Change a bit and return its old value
 * @nr: Bit to change
 * @addr: Address to count from
 *
 * This operation is atomic and cannot be reordered.
 * It also implies a memory barrier.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int test_and_change_bit(int nr, volatile unsigned long *addr)
{
	int oldbit;
	asm volatile(
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "btc %2,%1\n\t"
		     "sbb %0,%0"
		     : "=r" (oldbit), "+m" (*(volatile long *) (addr)) : "Ir" (nr) : "memory");
	return oldbit;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __attribute__((always_inline)) int constant_test_bit(unsigned int nr, const volatile unsigned long *addr)
{
	return ((1UL << (nr % 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
)) &
		(((unsigned long *)addr)[nr / 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
])) != 0;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int variable_test_bit(int nr, volatile const unsigned long *addr)
{
	int oldbit;
	asm volatile("bt %2,%1\n\t"
		     "sbb %0,%0"
		     : "=r" (oldbit)
		     : "m" (*(unsigned long *)addr), "Ir" (nr));
	return oldbit;
}
/**
 * __ffs - find first set bit in word
 * @word: The word to search
 *
 * Undefined if no bit exists, so code should check against 0 first.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long __ffs(unsigned long word)
{
	asm("bsf %1,%0"
		: "=r" (word)
		: "rm" (word));
	return word;
}
/**
 * ffz - find first zero bit in word
 * @word: The word to search
 *
 * Undefined if no zero exists, so code should check against ~0UL first.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long ffz(unsigned long word)
{
	asm("bsf %1,%0"
		: "=r" (word)
		: "r" (~word));
	return word;
}
/*
 * __fls: find last set bit in word
 * @word: The word to search
 *
 * Undefined if no set bit exists, so code should check against 0 first.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long __fls(unsigned long word)
{
	asm("bsr %1,%0"
	    : "=r" (word)
	    : "rm" (word));
	return word;
}
/**
 * ffs - find first set bit in word
 * @x: the word to search
 *
 * This is defined the same way as the libc and compiler builtin ffs
 * routines, therefore differs in spirit from the other bitops.
 *
 * ffs(value) returns 0 if value is 0 or the position of the first
 * set bit if value is nonzero. The first (least significant) bit
 * is at position 1.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int ffs(int x)
{
	int r;
#if definedEx(CONFIG_X86_CMOV)
	asm("bsfl %1,%0\n\t"
	    "cmovzl %2,%0"
	    : "=r" (r) : "rm" (x), "r" (-1));
#endif
#if !definedEx(CONFIG_X86_CMOV)
	asm("bsfl %1,%0\n\t"
	    "jnz 1f\n\t"
	    "movl $-1,%0\n"
	    "1:" : "=r" (r) : "rm" (x));
#endif
	return r + 1;
}
/**
 * fls - find last set bit in word
 * @x: the word to search
 *
 * This is defined in a similar way as the libc and compiler builtin
 * ffs, but returns the position of the most significant set bit.
 *
 * fls(value) returns 0 if value is 0 or the position of the last
 * set bit if value is nonzero. The last (most significant) bit is
 * at position 32.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int fls(int x)
{
	int r;
#if definedEx(CONFIG_X86_CMOV)
	asm("bsrl %1,%0\n\t"
	    "cmovzl %2,%0"
	    : "=&r" (r) : "rm" (x), "rm" (-1));
#endif
#if !definedEx(CONFIG_X86_CMOV)
	asm("bsrl %1,%0\n\t"
	    "jnz 1f\n\t"
	    "movl $-1,%0\n"
	    "1:" : "=r" (r) : "rm" (x));
#endif
	return r + 1;
}
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/asm-generic/bitops/sched.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/compiler.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/asm-generic/bitops/sched.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/types.h" 1
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/asm-generic/bitops/sched.h" 2
/*
 * Every architecture must define this function. It's the fastest
 * way of searching a 100-bit bitmap.  It's guaranteed that at least
 * one of the 100 bits is cleared.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int sched_find_first_bit(const unsigned long *b)
{
#if definedEx(CONFIG_64BIT)
	if (b[0])
		return __ffs(b[0]);
	return __ffs(b[1]) + 64;
#endif
#if !definedEx(CONFIG_64BIT)
	if (b[0])
		return __ffs(b[0]);
	if (b[1])
		return __ffs(b[1]) + 32;
	if (b[2])
		return __ffs(b[2]) + 64;
	return __ffs(b[3]) + 96;
#endif
}
#line 445 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/bitops.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/asm-generic/bitops/hweight.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/types.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/asm-generic/bitops/hweight.h" 2
extern unsigned int hweight32(unsigned int w);
extern unsigned int hweight16(unsigned int w);
extern unsigned int hweight8(unsigned int w);
extern unsigned long hweight64(__u64 w);
#line 449 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/bitops.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/asm-generic/bitops/fls64.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/types.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/asm-generic/bitops/fls64.h" 2
/**
 * fls64 - find last set bit in a 64-bit word
 * @x: the word to search
 *
 * This is defined in a similar way as the libc and compiler builtin
 * ffsll, but returns the position of the most significant set bit.
 *
 * fls64(value) returns 0 if value is 0 or the position of the last
 * set bit if value is nonzero. The last (most significant) bit is
 * at position 64.
 */
#if !definedEx(CONFIG_64BIT)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __attribute__((always_inline)) int fls64(__u64 x)
{
	__u32 h = x >> 32;
	if (h)
		return fls(h) + 32;
	return fls(x);
}
#endif
#if definedEx(CONFIG_64BIT)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __attribute__((always_inline)) int fls64(__u64 x)
{
	if (x == 0)
		return 0;
	return __fls(x) + 1;
}
#endif
#line 453 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/bitops.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/asm-generic/bitops/ext2-non-atomic.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/asm-generic/bitops/le.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/types.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/asm-generic/bitops/le.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/byteorder.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/byteorder/little_endian.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 13 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/byteorder/little_endian.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/swab.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/swab.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/compiler.h" 1
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/swab.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/swab.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/swab.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/compiler.h" 1
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/swab.h" 2
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __attribute__((__const__)) __u32 __arch_swab32(__u32 val)
{
#if definedEx(CONFIG_X86_32)
#if definedEx(CONFIG_X86_BSWAP)
	asm("bswap %0" : "=r" (val) : "0" (val));
#endif
#if !definedEx(CONFIG_X86_BSWAP)
	asm("xchgb %b0,%h0\n\t"	/* swap lower bytes	*/
	    "rorl $16,%0\n\t"	/* swap words		*/
	    "xchgb %b0,%h0"	/* swap higher bytes	*/
	    : "=q" (val)
	    : "0" (val));
#endif
#endif
#if !definedEx(CONFIG_X86_32)
	asm("bswapl %0"
	    : "=r" (val)
	    : "0" (val));
#endif
	return val;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __attribute__((__const__)) __u64 __arch_swab64(__u64 val)
{
#if definedEx(CONFIG_X86_32)
	union {
		struct {
			__u32 a;
			__u32 b;
		} s;
		__u64 u;
	} v;
	v.u = val;
#if definedEx(CONFIG_X86_BSWAP)
	asm("bswapl %0 ; bswapl %1 ; xchgl %0,%1"
	    : "=r" (v.s.a), "=r" (v.s.b)
	    : "0" (v.s.a), "1" (v.s.b));
#endif
#if !definedEx(CONFIG_X86_BSWAP)
	v.s.a = __arch_swab32(v.s.a);
	v.s.b = __arch_swab32(v.s.b);
	asm("xchgl %0,%1"
	    : "=r" (v.s.a), "=r" (v.s.b)
	    : "0" (v.s.a), "1" (v.s.b));
#endif
	return v.u;
#endif
#if !definedEx(CONFIG_X86_32)
	asm("bswapq %0"
	    : "=r" (val)
	    : "0" (val));
	return val;
#endif
}
#line 8 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/swab.h" 2
/*
 * casts are necessary for constants, because we never know how for sure
 * how U/UL/ULL map to __u16, __u32, __u64. At least not in a portable way.
 */
/*
 * Implement the following as inlines, but define the interface using
 * macros to allow constant folding when possible:
 * ___swab16, ___swab32, ___swab64, ___swahw32, ___swahb32
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __attribute__((__const__)) __u16 __fswab16(__u16 val)
{
 	return ((__u16)( (((__u16)(val) & (__u16)0x00ffU) << 8) | (((__u16)(val) & (__u16)0xff00U) >> 8)));
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __attribute__((__const__)) __u32 __fswab32(__u32 val)
{
	return __arch_swab32(val);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __attribute__((__const__)) __u64 __fswab64(__u64 val)
{
	return __arch_swab64(val);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __attribute__((__const__)) __u32 __fswahw32(__u32 val)
{
 	return ((__u32)( (((__u32)(val) & (__u32)0x0000ffffUL) << 16) | (((__u32)(val) & (__u32)0xffff0000UL) >> 16)));
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __attribute__((__const__)) __u32 __fswahb32(__u32 val)
{
 	return ((__u32)( (((__u32)(val) & (__u32)0x00ff00ffUL) << 8) | (((__u32)(val) & (__u32)0xff00ff00UL) >> 8)));
}
/**
 * __swab16 - return a byteswapped 16-bit value
 * @x: value to byteswap
 */
/**
 * __swab32 - return a byteswapped 32-bit value
 * @x: value to byteswap
 */
/**
 * __swab64 - return a byteswapped 64-bit value
 * @x: value to byteswap
 */
/**
 * __swahw32 - return a word-swapped 32-bit value
 * @x: value to wordswap
 *
 * __swahw32(0x12340000) is 0x00001234
 */
/**
 * __swahb32 - return a high and low byte-swapped 32-bit value
 * @x: value to byteswap
 *
 * __swahb32(0x12345678) is 0x34127856
 */
/**
 * __swab16p - return a byteswapped 16-bit value from a pointer
 * @p: pointer to a naturally-aligned 16-bit value
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __u16 __swab16p(const __u16 *p)
{
 	return (__builtin_constant_p((__u16)(*p)) ? ((__u16)( (((__u16)(*p) & (__u16)0x00ffU) << 8) | (((__u16)(*p) & (__u16)0xff00U) >> 8))) : __fswab16(*p));
}
/**
 * __swab32p - return a byteswapped 32-bit value from a pointer
 * @p: pointer to a naturally-aligned 32-bit value
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __u32 __swab32p(const __u32 *p)
{
 	return (__builtin_constant_p((__u32)(*p)) ? ((__u32)( (((__u32)(*p) & (__u32)0x000000ffUL) << 24) | (((__u32)(*p) & (__u32)0x0000ff00UL) << 8) | (((__u32)(*p) & (__u32)0x00ff0000UL) >> 8) | (((__u32)(*p) & (__u32)0xff000000UL) >> 24))) : __fswab32(*p));
}
/**
 * __swab64p - return a byteswapped 64-bit value from a pointer
 * @p: pointer to a naturally-aligned 64-bit value
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __u64 __swab64p(const __u64 *p)
{
 	return (__builtin_constant_p((__u64)(*p)) ? ((__u64)( (((__u64)(*p) & (__u64)0x00000000000000ffULL) << 56) | (((__u64)(*p) & (__u64)0x000000000000ff00ULL) << 40) | (((__u64)(*p) & (__u64)0x0000000000ff0000ULL) << 24) | (((__u64)(*p) & (__u64)0x00000000ff000000ULL) << 8) | (((__u64)(*p) & (__u64)0x000000ff00000000ULL) >> 8) | (((__u64)(*p) & (__u64)0x0000ff0000000000ULL) >> 24) | (((__u64)(*p) & (__u64)0x00ff000000000000ULL) >> 40) | (((__u64)(*p) & (__u64)0xff00000000000000ULL) >> 56))) : __fswab64(*p));
}
/**
 * __swahw32p - return a wordswapped 32-bit value from a pointer
 * @p: pointer to a naturally-aligned 32-bit value
 *
 * See __swahw32() for details of wordswapping.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __u32 __swahw32p(const __u32 *p)
{
 	return (__builtin_constant_p((__u32)(*p)) ? ((__u32)( (((__u32)(*p) & (__u32)0x0000ffffUL) << 16) | (((__u32)(*p) & (__u32)0xffff0000UL) >> 16))) : __fswahw32(*p));
}
/**
 * __swahb32p - return a high and low byteswapped 32-bit value from a pointer
 * @p: pointer to a naturally-aligned 32-bit value
 *
 * See __swahb32() for details of high/low byteswapping.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __u32 __swahb32p(const __u32 *p)
{
 	return (__builtin_constant_p((__u32)(*p)) ? ((__u32)( (((__u32)(*p) & (__u32)0x00ff00ffUL) << 8) | (((__u32)(*p) & (__u32)0xff00ff00UL) >> 8))) : __fswahb32(*p));
}
/**
 * __swab16s - byteswap a 16-bit value in-place
 * @p: pointer to a naturally-aligned 16-bit value
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __swab16s(__u16 *p)
{
 	*p = __swab16p(p);
}
/**
 * __swab32s - byteswap a 32-bit value in-place
 * @p: pointer to a naturally-aligned 32-bit value
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __swab32s(__u32 *p)
{
 	*p = __swab32p(p);
}
/**
 * __swab64s - byteswap a 64-bit value in-place
 * @p: pointer to a naturally-aligned 64-bit value
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __swab64s(__u64 *p)
{
 	*p = __swab64p(p);
}
/**
 * __swahw32s - wordswap a 32-bit value in-place
 * @p: pointer to a naturally-aligned 32-bit value
 *
 * See __swahw32() for details of wordswapping
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __swahw32s(__u32 *p)
{
 	*p = __swahw32p(p);
}
/**
 * __swahb32s - high and low byteswap a 32-bit value in-place
 * @p: pointer to a naturally-aligned 32-bit value
 *
 * See __swahb32() for details of high and low byte swapping
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __swahb32s(__u32 *p)
{
 	*p = __swahb32p(p);
}
#line 14 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/byteorder/little_endian.h" 2
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __le64 __cpu_to_le64p(const __u64 *p)
{
	return ( __le64)*p;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __u64 __le64_to_cpup(const __le64 *p)
{
	return ( __u64)*p;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __le32 __cpu_to_le32p(const __u32 *p)
{
	return ( __le32)*p;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __u32 __le32_to_cpup(const __le32 *p)
{
	return ( __u32)*p;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __le16 __cpu_to_le16p(const __u16 *p)
{
	return ( __le16)*p;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __u16 __le16_to_cpup(const __le16 *p)
{
	return ( __u16)*p;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __be64 __cpu_to_be64p(const __u64 *p)
{
	return ( __be64)__swab64p(p);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __u64 __be64_to_cpup(const __be64 *p)
{
	return __swab64p((__u64 *)p);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __be32 __cpu_to_be32p(const __u32 *p)
{
	return ( __be32)__swab32p(p);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __u32 __be32_to_cpup(const __be32 *p)
{
	return __swab32p((__u32 *)p);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __be16 __cpu_to_be16p(const __u16 *p)
{
	return ( __be16)__swab16p(p);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __u16 __be16_to_cpup(const __be16 *p)
{
	return __swab16p((__u16 *)p);
}
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/byteorder/generic.h" 1
/*
 * linux/byteorder_generic.h
 * Generic Byte-reordering support
 *
 * The "... p" macros, like le64_to_cpup, can be used with pointers
 * to unaligned data, but there will be a performance penalty on 
 * some architectures.  Use get_unaligned for unaligned data.
 *
 * Francois-Rene Rideau <fare@tunes.org> 19970707
 *    gathered all the good ideas from all asm-foo/byteorder.h into one file,
 *    cleaned them up.
 *    I hope it is compliant with non-GCC compilers.
 *    I decided to put __BYTEORDER_HAS_U64__ in byteorder.h,
 *    because I wasn't sure it would be ok to put it in types.h
 *    Upgraded it to 2.1.43
 * Francois-Rene Rideau <fare@tunes.org> 19971012
 *    Upgraded it to 2.1.57
 *    to please Linus T., replaced huge #ifdef's between little/big endian
 *    by nestedly #include'd files.
 * Francois-Rene Rideau <fare@tunes.org> 19971205
 *    Made it to 2.1.71; now a facelift:
 *    Put files under include/linux/byteorder/
 *    Split swab from generic support.
 *
 * TODO:
 *   = Regular kernel maintainers could also replace all these manual
 *    byteswap macros that remain, disseminated among drivers,
 *    after some grep or the sources...
 *   = Linus might want to rename all these macros and files to fit his taste,
 *    to fit his personal naming scheme.
 *   = it seems that a few drivers would also appreciate
 *    nybble swapping support...
 *   = every architecture could add their byteswap macro in asm/byteorder.h
 *    see how some architectures already do (i386, alpha, ppc, etc)
 *   = cpu_to_beXX and beXX_to_cpu might some day need to be well
 *    distinguished throughout the kernel. This is not the case currently,
 *    since little endian, big endian, and pdp endian machines needn't it.
 *    But this might be the case for, say, a port of Linux to 20/21 bit
 *    architectures (and F21 Linux addict around?).
 */
/*
 * The following macros are to be defined by <asm/byteorder.h>:
 *
 * Conversion of long and short int between network and host format
 *	ntohl(__u32 x)
 *	ntohs(__u16 x)
 *	htonl(__u32 x)
 *	htons(__u16 x)
 * It seems that some programs (which? where? or perhaps a standard? POSIX?)
 * might like the above to be functions, not macros (why?).
 * if that's true, then detect them, and take measures.
 * Anyway, the measure is: define only ___ntohl as a macro instead,
 * and in a separate file, have
 * unsigned long inline ntohl(x){return ___ntohl(x);}
 *
 * The same for constant arguments
 *	__constant_ntohl(__u32 x)
 *	__constant_ntohs(__u16 x)
 *	__constant_htonl(__u32 x)
 *	__constant_htons(__u16 x)
 *
 * Conversion of XX-bit integers (16- 32- or 64-)
 * between native CPU format and little/big endian format
 * 64-bit stuff only defined for proper architectures
 *	cpu_to_[bl]eXX(__uXX x)
 *	[bl]eXX_to_cpu(__uXX x)
 *
 * The same, but takes a pointer to the value to convert
 *	cpu_to_[bl]eXXp(__uXX x)
 *	[bl]eXX_to_cpup(__uXX x)
 *
 * The same, but change in situ
 *	cpu_to_[bl]eXXs(__uXX x)
 *	[bl]eXX_to_cpus(__uXX x)
 *
 * See asm-foo/byteorder.h for examples of how to provide
 * architecture-optimized versions
 *
 */
/*
 * They have to be macros in order to do the constant folding
 * correctly - if the argument passed into a inline function
 * it is no longer constant according to gcc..
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void le16_add_cpu(__le16 *var, u16 val)
{
	*var = (( __le16)(__u16)((( __u16)(__le16)(*var)) + val));
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void le32_add_cpu(__le32 *var, u32 val)
{
	*var = (( __le32)(__u32)((( __u32)(__le32)(*var)) + val));
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void le64_add_cpu(__le64 *var, u64 val)
{
	*var = (( __le64)(__u64)((( __u64)(__le64)(*var)) + val));
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void be16_add_cpu(__be16 *var, u16 val)
{
	*var = (( __be16)(__builtin_constant_p((__u16)(((__builtin_constant_p((__u16)(( __u16)(__be16)(*var))) ?((__u16)((((__u16)(( __u16)(__be16)(*var)) &(__u16)0x00ffU) << 8) |(((__u16)(( __u16)(__be16)(*var)) &(__u16)0xff00U) >> 8))) : __fswab16(( __u16)(__be16)(*var))) + val))) ? ((__u16)( (((__u16)(((__builtin_constant_p((__u16)(( __u16)(__be16)(*var))) ?((__u16)((((__u16)(( __u16)(__be16)(*var)) &(__u16)0x00ffU) << 8) |(((__u16)(( __u16)(__be16)(*var)) &(__u16)0xff00U) >> 8))) : __fswab16(( __u16)(__be16)(*var))) + val)) & (__u16)0x00ffU) << 8) | (((__u16)(((__builtin_constant_p((__u16)(( __u16)(__be16)(*var))) ?((__u16)((((__u16)(( __u16)(__be16)(*var)) &(__u16)0x00ffU) << 8) |(((__u16)(( __u16)(__be16)(*var)) &(__u16)0xff00U) >> 8))) : __fswab16(( __u16)(__be16)(*var))) + val)) & (__u16)0xff00U) >> 8))) : __fswab16(((__builtin_constant_p((__u16)(( __u16)(__be16)(*var))) ?((__u16)((((__u16)(( __u16)(__be16)(*var)) &(__u16)0x00ffU) << 8) |(((__u16)(( __u16)(__be16)(*var)) &(__u16)0xff00U) >> 8))) : __fswab16(( __u16)(__be16)(*var))) + val))));
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void be32_add_cpu(__be32 *var, u32 val)
{
	*var = (( __be32)(__builtin_constant_p((__u32)(((__builtin_constant_p((__u32)(( __u32)(__be32)(*var))) ?((__u32)((((__u32)(( __u32)(__be32)(*var)) &(__u32)0x000000ffUL) << 24) |(((__u32)(( __u32)(__be32)(*var)) &(__u32)0x0000ff00UL) << 8) |(((__u32)(( __u32)(__be32)(*var)) &(__u32)0x00ff0000UL) >> 8) |(((__u32)(( __u32)(__be32)(*var)) &(__u32)0xff000000UL) >> 24))) : __fswab32(( __u32)(__be32)(*var))) + val))) ? ((__u32)( (((__u32)(((__builtin_constant_p((__u32)(( __u32)(__be32)(*var))) ?((__u32)((((__u32)(( __u32)(__be32)(*var)) &(__u32)0x000000ffUL) << 24) |(((__u32)(( __u32)(__be32)(*var)) &(__u32)0x0000ff00UL) << 8) |(((__u32)(( __u32)(__be32)(*var)) &(__u32)0x00ff0000UL) >> 8) |(((__u32)(( __u32)(__be32)(*var)) &(__u32)0xff000000UL) >> 24))) : __fswab32(( __u32)(__be32)(*var))) + val)) & (__u32)0x000000ffUL) << 24) | (((__u32)(((__builtin_constant_p((__u32)(( __u32)(__be32)(*var))) ?((__u32)((((__u32)(( __u32)(__be32)(*var)) &(__u32)0x000000ffUL) << 24) |(((__u32)(( __u32)(__be32)(*var)) &(__u32)0x0000ff00UL) << 8) |(((__u32)(( __u32)(__be32)(*var)) &(__u32)0x00ff0000UL) >> 8) |(((__u32)(( __u32)(__be32)(*var)) &(__u32)0xff000000UL) >> 24))) : __fswab32(( __u32)(__be32)(*var))) + val)) & (__u32)0x0000ff00UL) << 8) | (((__u32)(((__builtin_constant_p((__u32)(( __u32)(__be32)(*var))) ?((__u32)((((__u32)(( __u32)(__be32)(*var)) &(__u32)0x000000ffUL) << 24) |(((__u32)(( __u32)(__be32)(*var)) &(__u32)0x0000ff00UL) << 8) |(((__u32)(( __u32)(__be32)(*var)) &(__u32)0x00ff0000UL) >> 8) |(((__u32)(( __u32)(__be32)(*var)) &(__u32)0xff000000UL) >> 24))) : __fswab32(( __u32)(__be32)(*var))) + val)) & (__u32)0x00ff0000UL) >> 8) | (((__u32)(((__builtin_constant_p((__u32)(( __u32)(__be32)(*var))) ?((__u32)((((__u32)(( __u32)(__be32)(*var)) &(__u32)0x000000ffUL) << 24) |(((__u32)(( __u32)(__be32)(*var)) &(__u32)0x0000ff00UL) << 8) |(((__u32)(( __u32)(__be32)(*var)) &(__u32)0x00ff0000UL) >> 8) |(((__u32)(( __u32)(__be32)(*var)) &(__u32)0xff000000UL) >> 24))) : __fswab32(( __u32)(__be32)(*var))) + val)) & (__u32)0xff000000UL) >> 24))) : __fswab32(((__builtin_constant_p((__u32)(( __u32)(__be32)(*var))) ?((__u32)((((__u32)(( __u32)(__be32)(*var)) &(__u32)0x000000ffUL) << 24) |(((__u32)(( __u32)(__be32)(*var)) &(__u32)0x0000ff00UL) << 8) |(((__u32)(( __u32)(__be32)(*var)) &(__u32)0x00ff0000UL) >> 8) |(((__u32)(( __u32)(__be32)(*var)) &(__u32)0xff000000UL) >> 24))) : __fswab32(( __u32)(__be32)(*var))) + val))));
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void be64_add_cpu(__be64 *var, u64 val)
{
	*var = (( __be64)(__builtin_constant_p((__u64)(((__builtin_constant_p((__u64)(( __u64)(__be64)(*var))) ?((__u64)((((__u64)(( __u64)(__be64)(*var)) &(__u64)0x00000000000000ffULL) << 56) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0x000000000000ff00ULL) << 40) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0x0000000000ff0000ULL) << 24) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0x00000000ff000000ULL) << 8) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0x000000ff00000000ULL) >> 8) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0x0000ff0000000000ULL) >> 24) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0x00ff000000000000ULL) >> 40) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0xff00000000000000ULL) >> 56))) : __fswab64(( __u64)(__be64)(*var))) + val))) ? ((__u64)( (((__u64)(((__builtin_constant_p((__u64)(( __u64)(__be64)(*var))) ?((__u64)((((__u64)(( __u64)(__be64)(*var)) &(__u64)0x00000000000000ffULL) << 56) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0x000000000000ff00ULL) << 40) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0x0000000000ff0000ULL) << 24) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0x00000000ff000000ULL) << 8) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0x000000ff00000000ULL) >> 8) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0x0000ff0000000000ULL) >> 24) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0x00ff000000000000ULL) >> 40) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0xff00000000000000ULL) >> 56))) : __fswab64(( __u64)(__be64)(*var))) + val)) & (__u64)0x00000000000000ffULL) << 56) | (((__u64)(((__builtin_constant_p((__u64)(( __u64)(__be64)(*var))) ?((__u64)((((__u64)(( __u64)(__be64)(*var)) &(__u64)0x00000000000000ffULL) << 56) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0x000000000000ff00ULL) << 40) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0x0000000000ff0000ULL) << 24) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0x00000000ff000000ULL) << 8) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0x000000ff00000000ULL) >> 8) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0x0000ff0000000000ULL) >> 24) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0x00ff000000000000ULL) >> 40) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0xff00000000000000ULL) >> 56))) : __fswab64(( __u64)(__be64)(*var))) + val)) & (__u64)0x000000000000ff00ULL) << 40) | (((__u64)(((__builtin_constant_p((__u64)(( __u64)(__be64)(*var))) ?((__u64)((((__u64)(( __u64)(__be64)(*var)) &(__u64)0x00000000000000ffULL) << 56) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0x000000000000ff00ULL) << 40) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0x0000000000ff0000ULL) << 24) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0x00000000ff000000ULL) << 8) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0x000000ff00000000ULL) >> 8) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0x0000ff0000000000ULL) >> 24) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0x00ff000000000000ULL) >> 40) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0xff00000000000000ULL) >> 56))) : __fswab64(( __u64)(__be64)(*var))) + val)) & (__u64)0x0000000000ff0000ULL) << 24) | (((__u64)(((__builtin_constant_p((__u64)(( __u64)(__be64)(*var))) ?((__u64)((((__u64)(( __u64)(__be64)(*var)) &(__u64)0x00000000000000ffULL) << 56) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0x000000000000ff00ULL) << 40) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0x0000000000ff0000ULL) << 24) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0x00000000ff000000ULL) << 8) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0x000000ff00000000ULL) >> 8) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0x0000ff0000000000ULL) >> 24) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0x00ff000000000000ULL) >> 40) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0xff00000000000000ULL) >> 56))) : __fswab64(( __u64)(__be64)(*var))) + val)) & (__u64)0x00000000ff000000ULL) << 8) | (((__u64)(((__builtin_constant_p((__u64)(( __u64)(__be64)(*var))) ?((__u64)((((__u64)(( __u64)(__be64)(*var)) &(__u64)0x00000000000000ffULL) << 56) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0x000000000000ff00ULL) << 40) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0x0000000000ff0000ULL) << 24) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0x00000000ff000000ULL) << 8) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0x000000ff00000000ULL) >> 8) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0x0000ff0000000000ULL) >> 24) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0x00ff000000000000ULL) >> 40) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0xff00000000000000ULL) >> 56))) : __fswab64(( __u64)(__be64)(*var))) + val)) & (__u64)0x000000ff00000000ULL) >> 8) | (((__u64)(((__builtin_constant_p((__u64)(( __u64)(__be64)(*var))) ?((__u64)((((__u64)(( __u64)(__be64)(*var)) &(__u64)0x00000000000000ffULL) << 56) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0x000000000000ff00ULL) << 40) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0x0000000000ff0000ULL) << 24) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0x00000000ff000000ULL) << 8) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0x000000ff00000000ULL) >> 8) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0x0000ff0000000000ULL) >> 24) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0x00ff000000000000ULL) >> 40) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0xff00000000000000ULL) >> 56))) : __fswab64(( __u64)(__be64)(*var))) + val)) & (__u64)0x0000ff0000000000ULL) >> 24) | (((__u64)(((__builtin_constant_p((__u64)(( __u64)(__be64)(*var))) ?((__u64)((((__u64)(( __u64)(__be64)(*var)) &(__u64)0x00000000000000ffULL) << 56) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0x000000000000ff00ULL) << 40) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0x0000000000ff0000ULL) << 24) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0x00000000ff000000ULL) << 8) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0x000000ff00000000ULL) >> 8) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0x0000ff0000000000ULL) >> 24) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0x00ff000000000000ULL) >> 40) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0xff00000000000000ULL) >> 56))) : __fswab64(( __u64)(__be64)(*var))) + val)) & (__u64)0x00ff000000000000ULL) >> 40) | (((__u64)(((__builtin_constant_p((__u64)(( __u64)(__be64)(*var))) ?((__u64)((((__u64)(( __u64)(__be64)(*var)) &(__u64)0x00000000000000ffULL) << 56) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0x000000000000ff00ULL) << 40) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0x0000000000ff0000ULL) << 24) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0x00000000ff000000ULL) << 8) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0x000000ff00000000ULL) >> 8) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0x0000ff0000000000ULL) >> 24) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0x00ff000000000000ULL) >> 40) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0xff00000000000000ULL) >> 56))) : __fswab64(( __u64)(__be64)(*var))) + val)) & (__u64)0xff00000000000000ULL) >> 56))) : __fswab64(((__builtin_constant_p((__u64)(( __u64)(__be64)(*var))) ?((__u64)((((__u64)(( __u64)(__be64)(*var)) &(__u64)0x00000000000000ffULL) << 56) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0x000000000000ff00ULL) << 40) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0x0000000000ff0000ULL) << 24) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0x00000000ff000000ULL) << 8) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0x000000ff00000000ULL) >> 8) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0x0000ff0000000000ULL) >> 24) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0x00ff000000000000ULL) >> 40) |(((__u64)(( __u64)(__be64)(*var)) &(__u64)0xff00000000000000ULL) >> 56))) : __fswab64(( __u64)(__be64)(*var))) + val))));
}
#line 107 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/byteorder/little_endian.h" 2
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/byteorder.h" 2
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/asm-generic/bitops/le.h" 2
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/asm-generic/bitops/ext2-non-atomic.h" 2
#line 457 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/bitops.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/asm-generic/bitops/minix.h" 1
#line 464 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/bitops.h" 2
#line 19 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/bitops.h" 2
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
__inline__ __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
__inline__
#endif
 int get_bitmask_order(unsigned int count)
{
	int order;
	order = fls(count);
	return order;	/* We could be slightly more clever with -1 here... */
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
__inline__ __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
__inline__
#endif
 int get_count_order(unsigned int count)
{
	int order;
	order = fls(count) - 1;
	if (count & (count - 1))
		order++;
	return order;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long hweight_long(unsigned long w)
{
	return sizeof(w) == 4 ? hweight32(w) : hweight64(w);
}
/**
 * rol32 - rotate a 32-bit value left
 * @word: value to rotate
 * @shift: bits to roll
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __u32 rol32(__u32 word, unsigned int shift)
{
	return (word << shift) | (word >> (32 - shift));
}
/**
 * ror32 - rotate a 32-bit value right
 * @word: value to rotate
 * @shift: bits to roll
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __u32 ror32(__u32 word, unsigned int shift)
{
	return (word >> shift) | (word << (32 - shift));
}
/**
 * rol16 - rotate a 16-bit value left
 * @word: value to rotate
 * @shift: bits to roll
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __u16 rol16(__u16 word, unsigned int shift)
{
	return (word << shift) | (word >> (16 - shift));
}
/**
 * ror16 - rotate a 16-bit value right
 * @word: value to rotate
 * @shift: bits to roll
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __u16 ror16(__u16 word, unsigned int shift)
{
	return (word >> shift) | (word << (16 - shift));
}
/**
 * rol8 - rotate an 8-bit value left
 * @word: value to rotate
 * @shift: bits to roll
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __u8 rol8(__u8 word, unsigned int shift)
{
	return (word << shift) | (word >> (8 - shift));
}
/**
 * ror8 - rotate an 8-bit value right
 * @word: value to rotate
 * @shift: bits to roll
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __u8 ror8(__u8 word, unsigned int shift)
{
	return (word >> shift) | (word << (8 - shift));
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned fls_long(unsigned long l)
{
	if (sizeof(l) == 4)
		return fls(l);
	return fls64(l);
}
/**
 * __ffs64 - find first set bit in a 64 bit word
 * @word: The 64 bit word
 *
 * On 64 bit arches this is a synomyn for __ffs
 * The result is not defined if no bits are set, so check that @word
 * is non-zero before calling this.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long __ffs64(u64 word)
{
#if !definedEx(CONFIG_64BIT)
	if (((u32)word) == 0UL)
		return __ffs((u32)(word >> 32)) + 32;
#endif
	return __ffs((unsigned long)word);
}
/**
 * find_first_bit - find the first set bit in a memory region
 * @addr: The address to start the search at
 * @size: The maximum size to search
 *
 * Returns the bit number of the first set bit.
 */
extern unsigned long find_first_bit(const unsigned long *addr,
				    unsigned long size);
/**
 * find_first_zero_bit - find the first cleared bit in a memory region
 * @addr: The address to start the search at
 * @size: The maximum size to search
 *
 * Returns the bit number of the first cleared bit.
 */
extern unsigned long find_first_zero_bit(const unsigned long *addr,
					 unsigned long size);
/**
 * find_last_bit - find the last set bit in a memory region
 * @addr: The address to start the search at
 * @size: The maximum size to search
 *
 * Returns the bit number of the first set bit, or size.
 */
extern unsigned long find_last_bit(const unsigned long *addr,
				   unsigned long size);
/**
 * find_next_bit - find the next set bit in a memory region
 * @addr: The address to base the search on
 * @offset: The bitnumber to start searching at
 * @size: The bitmap size in bits
 */
extern unsigned long find_next_bit(const unsigned long *addr,
				   unsigned long size, unsigned long offset);
/**
 * find_next_zero_bit - find the next cleared bit in a memory region
 * @addr: The address to base the search on
 * @offset: The bitnumber to start searching at
 * @size: The bitmap size in bits
 */
extern unsigned long find_next_zero_bit(const unsigned long *addr,
					unsigned long size,
					unsigned long offset);
#line 17 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/kernel.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/log2.h" 1
/* Integer base 2 logarithm calculation
 *
 * Copyright (C) 2006 Red Hat, Inc. All Rights Reserved.
 * Written by David Howells (dhowells@redhat.com)
 *
 * This program is free software; you can redistribute it and/or
 * modify it under the terms of the GNU General Public License
 * as published by the Free Software Foundation; either version
 * 2 of the License, or (at your option) any later version.
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 17 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/log2.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/bitops.h" 1
#line 18 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/log2.h" 2
/*
 * deal with unrepresentable constant logarithms
 */
extern __attribute__((const, noreturn))
int ____ilog2_NaN(void);
/*
 * non-constant log of base 2 calculators
 * - the arch may override these in asm/bitops.h if they can be implemented
 *   more efficiently than using fls() and fls64()
 * - the arch is not required to handle n==0 if implementing the fallback
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __attribute__((const))
int __ilog2_u32(u32 n)
{
	return fls(n) - 1;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __attribute__((const))
int __ilog2_u64(u64 n)
{
	return fls64(n) - 1;
}
/*
 *  Determine whether some value is a power of two, where zero is
 * *not* considered a power of two.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __attribute__((const))
bool is_power_of_2(unsigned long n)
{
	return (n != 0 && ((n & (n - 1)) == 0));
}
/*
 * round up to nearest power of two
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __attribute__((const))
unsigned long __roundup_pow_of_two(unsigned long n)
{
	return 1UL << fls_long(n - 1);
}
/*
 * round down to nearest power of two
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __attribute__((const))
unsigned long __rounddown_pow_of_two(unsigned long n)
{
	return 1UL << (fls_long(n) - 1);
}
/**
 * ilog2 - log of base 2 of 32-bit or a 64-bit unsigned value
 * @n - parameter
 *
 * constant-capable log of base 2 calculation
 * - this can be used to initialise global variables from constant data, hence
 *   the massive ternary operator construction
 *
 * selects the appropriately-sized optimised version depending on sizeof(n)
 */
/**
 * roundup_pow_of_two - round the given value up to nearest power of two
 * @n - parameter
 *
 * round the given value up to the nearest power of two
 * - the result is undefined when n == 0
 * - this can be used to initialise global variables from constant data
 */
/**
 * rounddown_pow_of_two - round the given value down to nearest power of two
 * @n - parameter
 *
 * round the given value down to the nearest power of two
 * - the result is undefined when n == 0
 * - this can be used to initialise global variables from constant data
 */
/**
 * order_base_2 - calculate the (rounded up) base 2 order of the argument
 * @n: parameter
 *
 * The first few values calculated by this routine:
 *  ob2(0) = 0
 *  ob2(1) = 0
 *  ob2(2) = 1
 *  ob2(3) = 2
 *  ob2(4) = 2
 *  ob2(5) = 3
 *  ... and so on.
 */
#line 18 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/kernel.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/typecheck.h" 1
/*
 * Check at compile time that something is of a particular type.
 * Always evaluates to 1 so you may use it easily in comparisons.
 */
/*
 * Check at compile time that 'function' is a certain type, or is a pointer
 * to that type (needs to use typedef for the function type.)
 */
#line 19 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/kernel.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/dynamic_debug.h" 1
/* dynamic_printk_enabled, and dynamic_printk_enabled2 are bitmasks in which
 * bit n is set to 1 if any modname hashes into the bucket n, 0 otherwise. They
 * use independent hash functions, to reduce the chance of false positives.
 */
extern long long dynamic_debug_enabled;
extern long long dynamic_debug_enabled2;
/*
 * An instance of this structure is created in a special
 * ELF section at every dynamic debug callsite.  At runtime,
 * the special section is treated as an array of these.
 */
struct _ddebug {
	/*
	 * These fields are used to drive the user interface
	 * for selecting and displaying debug callsites.
	 */
	const char *modname;
	const char *function;
	const char *filename;
	const char *format;
	char primary_hash;
	char secondary_hash;
	unsigned int lineno:24;
	/*
 	 * The flags field controls the behaviour at the callsite.
 	 * The bits here are changed dynamically when the user
 	 * writes commands to <debugfs>/dynamic_debug/ddebug
	 */
	unsigned int flags:8;
} __attribute__((aligned(8)));
int ddebug_add_module(struct _ddebug *tab, unsigned int n,
				const char *modname);
#if definedEx(CONFIG_DYNAMIC_DEBUG)
extern int ddebug_remove_module(char *mod_name);
#endif
#if !definedEx(CONFIG_DYNAMIC_DEBUG)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int ddebug_remove_module(char *mod)
{
	return 0;
}
#endif
#line 20 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/kernel.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/byteorder.h" 1
#line 21 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/kernel.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/bug.h" 1
#if definedEx(CONFIG_BUG)
#if definedEx(CONFIG_DEBUG_BUGVERBOSE)
#if definedEx(CONFIG_X86_32)
#endif
#if !definedEx(CONFIG_X86_32)
#endif
#endif
#if !definedEx(CONFIG_DEBUG_BUGVERBOSE)
#endif
#endif
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/asm-generic/bug.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/compiler.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/asm-generic/bug.h" 2
#if definedEx(CONFIG_BUG)
#if definedEx(CONFIG_GENERIC_BUG)
struct bug_entry {
#if !definedEx(CONFIG_GENERIC_BUG_RELATIVE_POINTERS)
	unsigned long	bug_addr;
#endif
#if definedEx(CONFIG_GENERIC_BUG_RELATIVE_POINTERS)
	signed int	bug_addr_disp;
#endif
#if definedEx(CONFIG_DEBUG_BUGVERBOSE)
#if !definedEx(CONFIG_GENERIC_BUG_RELATIVE_POINTERS)
	const char	*file;
#endif
#if definedEx(CONFIG_GENERIC_BUG_RELATIVE_POINTERS)
	signed int	file_disp;
#endif
	unsigned short	line;
#endif
	unsigned short	flags;
};
#endif
/*
 * Don't use BUG() or BUG_ON() unless there's really no way out; one
 * example might be detecting data structure corruption in the middle
 * of an operation that can't be backed out of.  If the (sub)system
 * can somehow continue operating, perhaps with reduced functionality,
 * it's probably not BUG-worthy.
 *
 * If you're tempted to BUG(), think again:  is completely giving up
 * really the *only* solution?  There are usually better options, where
 * users don't need to reboot ASAP and can mostly shut down cleanly.
 */
/*
 * WARN(), WARN_ON(), WARN_ON_ONCE, and so on can be used to report
 * significant issues that need prompt attention if they should ever
 * appear at runtime.  Use the versions with printk format strings
 * to provide better diagnostics.
 */
extern void warn_slowpath_fmt(const char *file, const int line,
		const char *fmt, ...) __attribute__((format(printf, 3, 4)));
extern void warn_slowpath_null(const char *file, const int line);
#endif
#if !definedEx(CONFIG_BUG)
#endif
#if definedEx(CONFIG_SMP)
#endif
#if !definedEx(CONFIG_SMP)
#endif
#line 40 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/bug.h" 2
#line 22 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/kernel.h" 2
extern const char linux_banner[];
extern const char linux_proc_banner[];
#if definedEx(CONFIG_LBDAF)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/div64.h" 1
#if definedEx(CONFIG_X86_32)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 8 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/div64.h" 2
/*
 * do_div() is NOT a C function. It wants to return
 * two values (the quotient and the remainder), but
 * since that doesn't work very well in C, what it
 * does is:
 *
 * - modifies the 64-bit dividend _in_place_
 * - returns the 32-bit remainder
 *
 * This ends up being the most efficient "calling
 * convention" on x86.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 u64 div_u64_rem(u64 dividend, u32 divisor, u32 *remainder)
{
	union {
		u64 v64;
		u32 v32[2];
	} d = { dividend };
	u32 upper;
	upper = d.v32[1];
	d.v32[1] = 0;
	if (upper >= divisor) {
		d.v32[1] = upper / divisor;
		upper %= divisor;
	}
	asm ("divl %2" : "=a" (d.v32[0]), "=d" (*remainder) :
		"rm" (divisor), "0" (d.v32[0]), "1" (upper));
	return d.v64;
}
#endif
#if !definedEx(CONFIG_X86_32)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/asm-generic/div64.h" 1
/*
 * Copyright (C) 2003 Bernardo Innocenti <bernie@develer.com>
 * Based on former asm-ppc/div64.h and asm-m68knommu/div64.h
 *
 * The semantics of do_div() are:
 *
 * uint32_t do_div(uint64_t *n, uint32_t base)
 * {
 * 	uint32_t remainder = *n % base;
 * 	*n = *n / base;
 * 	return remainder;
 * }
 *
 * NOTE: macro parameter n is evaluated multiple times,
 *       beware of side effects!
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 22 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/asm-generic/div64.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/compiler.h" 1
#line 23 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/asm-generic/div64.h" 2
#line 59 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/div64.h" 2
#endif
#line 63 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/kernel.h" 2
#endif
#if !definedEx(CONFIG_LBDAF)
#endif
/**
 * upper_32_bits - return bits 32-63 of a number
 * @n: the number we're accessing
 *
 * A basic shift-right of a 64- or 32-bit quantity.  Use this to suppress
 * the "right shift count >= width of type" warning when that quantity is
 * 32-bits.
 */
/**
 * lower_32_bits - return bits 0-31 of a number
 * @n: the number we're accessing
 */
/* Use the default kernel loglevel */
/*
 * Annotation for a "continued" line of log printout (only done after a
 * line that had no enclosing \n). Only to be used by core/arch code
 * during early bootup (a continued line is not SMP-safe otherwise).
 */
extern int console_printk[];
struct completion;
struct pt_regs;
struct user;
#if definedEx(CONFIG_PREEMPT_VOLUNTARY)
extern int _cond_resched(void);
#endif
#if !definedEx(CONFIG_PREEMPT_VOLUNTARY)
#endif
#if definedEx(CONFIG_DEBUG_SPINLOCK_SLEEP)
  void __might_sleep(char *file, int line, int preempt_offset);
/**
 * might_sleep - annotation for functions that can sleep
 *
 * this macro will print a stack trace if it is executed in an atomic
 * context (spinlock, irq-handler, ...).
 *
 * This is a useful debugging help to be able to catch problems early and not
 * be bitten later when the calling function happens to sleep when it is not
 * supposed to.
 */
#endif
#if !definedEx(CONFIG_DEBUG_SPINLOCK_SLEEP)
  static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __might_sleep(char *file, int line, int preempt_offset) { }
#endif
#if definedEx(CONFIG_PROVE_LOCKING)
void might_fault(void);
#endif
#if !definedEx(CONFIG_PROVE_LOCKING)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void might_fault(void)
{
#if definedEx(CONFIG_DEBUG_SPINLOCK_SLEEP)
do { __might_sleep("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/kernel.h", 157, 0); 
#if definedEx(CONFIG_PREEMPT_VOLUNTARY)
_cond_resched()
#endif
#if !definedEx(CONFIG_PREEMPT_VOLUNTARY)
do { } while (0)
#endif
; } while (0)
#endif
#if !definedEx(CONFIG_DEBUG_SPINLOCK_SLEEP)
do { 
#if definedEx(CONFIG_PREEMPT_VOLUNTARY)
_cond_resched()
#endif
#if !definedEx(CONFIG_PREEMPT_VOLUNTARY)
do { } while (0)
#endif
; } while (0)
#endif
;
}
#endif
extern struct atomic_notifier_head panic_notifier_list;
extern long (*panic_blink)(long time);
 void panic(const char * fmt, ...)
	__attribute__ ((noreturn, format (printf, 1, 2))) __attribute__((__cold__));
extern void oops_enter(void);
extern void oops_exit(void);
extern int oops_may_print(void);
 void do_exit(long error_code)
	__attribute__((noreturn));
 void complete_and_exit(struct completion *, long)
	__attribute__((noreturn));
extern unsigned long simple_strtoul(const char *,char **,unsigned int);
extern long simple_strtol(const char *,char **,unsigned int);
extern unsigned long long simple_strtoull(const char *,char **,unsigned int);
extern long long simple_strtoll(const char *,char **,unsigned int);
extern int strict_strtoul(const char *, unsigned int, unsigned long *);
extern int strict_strtol(const char *, unsigned int, long *);
extern int strict_strtoull(const char *, unsigned int, unsigned long long *);
extern int strict_strtoll(const char *, unsigned int, long long *);
extern int sprintf(char * buf, const char * fmt, ...)
	__attribute__ ((format (printf, 2, 3)));
extern int vsprintf(char *buf, const char *, va_list)
	__attribute__ ((format (printf, 2, 0)));
extern int snprintf(char * buf, size_t size, const char * fmt, ...)
	__attribute__ ((format (printf, 3, 4)));
extern int vsnprintf(char *buf, size_t size, const char *fmt, va_list args)
	__attribute__ ((format (printf, 3, 0)));
extern int scnprintf(char * buf, size_t size, const char * fmt, ...)
	__attribute__ ((format (printf, 3, 4)));
extern int vscnprintf(char *buf, size_t size, const char *fmt, va_list args)
	__attribute__ ((format (printf, 3, 0)));
extern char *kasprintf(gfp_t gfp, const char *fmt, ...)
	__attribute__ ((format (printf, 2, 3)));
extern char *kvasprintf(gfp_t gfp, const char *fmt, va_list args);
extern int sscanf(const char *, const char *, ...)
	__attribute__ ((format (scanf, 2, 3)));
extern int vsscanf(const char *, const char *, va_list)
	__attribute__ ((format (scanf, 2, 0)));
extern int get_option(char **str, int *pint);
extern char *get_options(const char *str, int nints, int *ints);
extern unsigned long long memparse(const char *ptr, char **retptr);
extern int core_kernel_text(unsigned long addr);
extern int __kernel_text_address(unsigned long addr);
extern int kernel_text_address(unsigned long addr);
extern int func_ptr_is_kernel_text(void *ptr);
struct pid;
extern struct pid *session_of_pgrp(struct pid *pgrp);
/*
 * FW_BUG
 * Add this to a message where you are sure the firmware is buggy or behaves
 * really stupid or out of spec. Be aware that the responsible BIOS developer
 * should be able to fix this issue or at least get a concrete idea of the
 * problem by reading your message without the need of looking at the kernel
 * code.
 * 
 * Use it for definite and high priority BIOS bugs.
 *
 * FW_WARN
 * Use it for not that clear (e.g. could the kernel messed up things already?)
 * and medium priority BIOS bugs.
 *
 * FW_INFO
 * Use this one if you want to tell the user or vendor about something
 * suspicious, but generally harmless related to the firmware.
 *
 * Use it for information or very low priority BIOS bugs.
 */
#if definedEx(CONFIG_PRINTK)
#if !definedEx(CONFIG_X86_32)
#endif
#if definedEx(CONFIG_X86_32)
 __attribute__((regparm(0)))
#endif
 int vprintk(const char *fmt, va_list args)
	__attribute__ ((format (printf, 1, 0)));
#if !definedEx(CONFIG_X86_32)
#endif
#if definedEx(CONFIG_X86_32)
 __attribute__((regparm(0)))
#endif
 int printk(const char * fmt, ...)
	__attribute__ ((format (printf, 1, 2))) __attribute__((__cold__));
extern int __printk_ratelimit(const char *func);
extern bool printk_timed_ratelimit(unsigned long *caller_jiffies,
				   unsigned int interval_msec);
extern int printk_delay_msec;
/*
 * Print a one-time message (analogous to WARN_ONCE() et al):
 */
void log_buf_kexec_setup(void);
#endif
#if !definedEx(CONFIG_PRINTK)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int vprintk(const char *s, va_list args)
	__attribute__ ((format (printf, 1, 0)));
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int vprintk(const char *s, va_list args) { return 0; }
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int printk(const char *s, ...)
	__attribute__ ((format (printf, 1, 2)));
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int __attribute__((__cold__)) printk(const char *s, ...) { return 0; }
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int printk_ratelimit(void) { return 0; }
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 bool printk_timed_ratelimit(unsigned long *caller_jiffies, 					  unsigned int interval_msec)			{ return false; }
/* No effect, but we still get type checking even in the !PRINTK case: */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void log_buf_kexec_setup(void)
{
}
#endif
extern int printk_needs_cpu(int cpu);
extern void printk_tick(void);
extern void 
#if !definedEx(CONFIG_X86_32)
#endif
#if definedEx(CONFIG_X86_32)
 __attribute__((regparm(0)))
#endif
 __attribute__((format(printf, 1, 2)))
	early_printk(const char *fmt, ...);
unsigned long int_sqrt(unsigned long);
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void console_silent(void)
{
	(console_printk[0]) = 0;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void console_verbose(void)
{
	if ((console_printk[0]))
		(console_printk[0]) = 15;
}
extern void bust_spinlocks(int yes);
extern void wake_up_klogd(void);
extern int oops_in_progress;		/* If set, an oops, panic(), BUG() or die() is in progress */
extern int panic_timeout;
extern int panic_on_oops;
extern int panic_on_unrecovered_nmi;
extern int panic_on_io_nmi;
extern const char *print_tainted(void);
extern void add_taint(unsigned flag);
extern int test_taint(unsigned flag);
extern unsigned long get_taint(void);
extern int root_mountflags;
/* Values used for system_state */
extern enum system_states {
	SYSTEM_BOOTING,
	SYSTEM_RUNNING,
	SYSTEM_HALT,
	SYSTEM_POWER_OFF,
	SYSTEM_RESTART,
	SYSTEM_SUSPEND_DISK,
} system_state;
extern void dump_stack(void) __attribute__((__cold__));
enum {
	DUMP_PREFIX_NONE,
	DUMP_PREFIX_ADDRESS,
	DUMP_PREFIX_OFFSET
};
extern void hex_dump_to_buffer(const void *buf, size_t len,
				int rowsize, int groupsize,
				char *linebuf, size_t linebuflen, bool ascii);
extern void print_hex_dump(const char *level, const char *prefix_str,
				int prefix_type, int rowsize, int groupsize,
				const void *buf, size_t len, bool ascii);
extern void print_hex_dump_bytes(const char *prefix_str, int prefix_type,
			const void *buf, size_t len);
extern const char hex_asc[];
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 char *pack_hex_byte(char *buf, u8 byte)
{
	*buf++ = hex_asc[((byte) & 0xf0) >> 4];
	*buf++ = hex_asc[((byte) & 0x0f)];
	return buf;
}
/* pr_devel() should produce zero code unless DEBUG is defined */
/* If you are writing a driver, please use dev_dbg instead */
#if definedEx(CONFIG_DYNAMIC_DEBUG)
/* dynamic_pr_debug() uses pr_fmt() internally so we don't need it here */
#endif
#if !definedEx(CONFIG_DYNAMIC_DEBUG)
#endif
/*
 * ratelimited messages with local ratelimit_state,
 * no local ratelimit_state used in the !PRINTK case
 */
#if definedEx(CONFIG_PRINTK)
#endif
#if !definedEx(CONFIG_PRINTK)
/* No effect, but we still get type checking even in the !PRINTK case: */
#endif
/* no pr_cont_ratelimited, don't do that... */
/* If you are writing a driver, please use dev_dbg instead */
/*
 * General tracing related utility functions - trace_printk(),
 * tracing_on/tracing_off and tracing_start()/tracing_stop
 *
 * Use tracing_on/tracing_off when you want to quickly turn on or off
 * tracing. It simply enables or disables the recording of the trace events.
 * This also corresponds to the user space /sys/kernel/debug/tracing/tracing_on
 * file, which gives a means for the kernel and userspace to interact.
 * Place a tracing_off() in the kernel where you want tracing to end.
 * From user space, examine the trace, and then echo 1 > tracing_on
 * to continue tracing.
 *
 * tracing_stop/tracing_start has slightly more overhead. It is used
 * by things like suspend to ram where disabling the recording of the
 * trace is not enough, but tracing must actually stop because things
 * like calling smp_processor_id() may crash the system.
 *
 * Most likely, you want to use tracing_on/tracing_off.
 */
#if definedEx(CONFIG_RING_BUFFER)
void tracing_on(void);
void tracing_off(void);
/* trace_off_permanent stops recording with no way to bring it back */
void tracing_off_permanent(void);
int tracing_is_on(void);
#endif
#if !definedEx(CONFIG_RING_BUFFER)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void tracing_on(void) { }
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void tracing_off(void) { }
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void tracing_off_permanent(void) { }
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int tracing_is_on(void) { return 0; }
#endif
#if definedEx(CONFIG_TRACING)
extern void tracing_start(void);
extern void tracing_stop(void);
extern void ftrace_off_permanent(void);
extern void
ftrace_special(unsigned long arg1, unsigned long arg2, unsigned long arg3);
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __attribute__ ((format (printf, 1, 2)))
____trace_printk_check_format(const char *fmt, ...)
{
}
/**
 * trace_printk - printf formatting in the ftrace buffer
 * @fmt: the printf format for printing
 *
 * Note: __trace_printk is an internal function for trace_printk and
 *       the @ip is passed in via the trace_printk macro.
 *
 * This function allows a kernel developer to debug fast path sections
 * that printk is not appropriate for. By scattering in various
 * printk like tracing in the code, a developer can quickly see
 * where problems are occurring.
 *
 * This is intended as a debugging tool for the developer only.
 * Please refrain from leaving trace_printks scattered around in
 * your code.
 */
extern int
__trace_bprintk(unsigned long ip, const char *fmt, ...)
	__attribute__ ((format (printf, 2, 3)));
extern int
__trace_printk(unsigned long ip, const char *fmt, ...)
	__attribute__ ((format (printf, 2, 3)));
extern void trace_dump_stack(void);
/*
 * The double __builtin_constant_p is because gcc will give us an error
 * if we try to allocate the static variable to fmt if it is not a
 * constant. Even with the outer if statement.
 */
extern int
__ftrace_vbprintk(unsigned long ip, const char *fmt, va_list ap);
extern int
__ftrace_vprintk(unsigned long ip, const char *fmt, va_list ap);
extern void ftrace_dump(void);
#endif
#if !definedEx(CONFIG_TRACING)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void
ftrace_special(unsigned long arg1, unsigned long arg2, unsigned long arg3) { }
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int
trace_printk(const char *fmt, ...) __attribute__ ((format (printf, 1, 2)));
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void tracing_start(void) { }
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void tracing_stop(void) { }
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void ftrace_off_permanent(void) { }
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void trace_dump_stack(void) { }
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int
trace_printk(const char *fmt, ...)
{
	return 0;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int
ftrace_vprintk(const char *fmt, va_list ap)
{
	return 0;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void ftrace_dump(void) { }
#endif
/*
 *      Display an IP address in readable format.
 */
/*
 * min()/max()/clamp() macros that also do
 * strict type-checking.. See the
 * "unnecessary" pointer comparison.
 */
/**
 * clamp - return a value clamped to a given range with strict typechecking
 * @val: current value
 * @min: minimum allowable value
 * @max: maximum allowable value
 *
 * This macro does strict typechecking of min/max to make sure they are of the
 * same type as val.  See the unnecessary pointer comparisons.
 */
/*
 * ..and if you can't take the strict
 * types, you can specify one yourself.
 *
 * Or not use min/max/clamp at all, of course.
 */
/**
 * clamp_t - return a value clamped to a given range using a given type
 * @type: the type of variable to use
 * @val: current value
 * @min: minimum allowable value
 * @max: maximum allowable value
 *
 * This macro does no typechecking and uses temporary variables of type
 * 'type' to make all the comparisons.
 */
/**
 * clamp_val - return a value clamped to a given range using val's type
 * @val: current value
 * @min: minimum allowable value
 * @max: maximum allowable value
 *
 * This macro does no typechecking and uses temporary variables of whatever
 * type the input argument 'val' is.  This is useful when val is an unsigned
 * type and min and max are literals that will otherwise be assigned a signed
 * integer type.
 */
/*
 * swap - swap value of @a and @b
 */
/**
 * container_of - cast a member of a structure out to the containing structure
 * @ptr:	the pointer to the member.
 * @type:	the type of the container struct this is embedded in.
 * @member:	the name of the member within the struct.
 *
 */
struct sysinfo;
extern int do_sysinfo(struct sysinfo *info);
struct sysinfo {
	long uptime;			/* Seconds since boot */
	unsigned long loads[3];		/* 1, 5, and 15 minute load averages */
	unsigned long totalram;		/* Total usable main memory size */
	unsigned long freeram;		/* Available memory size */
	unsigned long sharedram;	/* Amount of shared memory */
	unsigned long bufferram;	/* Memory used by buffers */
	unsigned long totalswap;	/* Total swap space size */
	unsigned long freeswap;		/* swap space still available */
	unsigned short procs;		/* Number of current processes */
	unsigned short pad;		/* explicit padding for m68k */
	unsigned long totalhigh;	/* Total high memory size */
	unsigned long freehigh;		/* Available high memory size */
	unsigned int mem_unit;		/* Memory unit size in bytes */
	char _f[20-2*sizeof(long)-sizeof(int)];	/* Padding: libc5 uses this.. */
};
/* Force a compilation error if condition is true */
/* Force a compilation error if condition is constant and true */
/* Force a compilation error if a constant expression is not a power of 2 */
/* Force a compilation error if condition is true, but also produce a
   result (of value 0 and type size_t), so the expression can be used
   e.g. in a structure initializer (or where-ever else comma expressions
   aren't permitted). */
/* Trap pasters of __FUNCTION__ at compile-time */
/* This helps us to avoid #ifdef CONFIG_NUMA */
#if definedEx(CONFIG_NUMA)
#endif
#if !definedEx(CONFIG_NUMA)
#endif
/* Rebuild everything on CONFIG_FTRACE_MCOUNT_RECORD */
#if definedEx(CONFIG_FTRACE_MCOUNT_RECORD)
#endif
#line 47 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/percpu.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/stringify.h" 1
#line 48 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/percpu.h" 2
#if definedEx(CONFIG_SMP)
#endif
#if !definedEx(CONFIG_SMP)
#endif
/*
 * Initialized pointers to per-cpu variables needed for the boot
 * processor need to use these macros to get the proper address
 * offset from __per_cpu_load on SMP.
 *
 * There also must be an entry in vmlinux_64.lds.S
 */
#if definedEx(CONFIG_X86_64_SMP)
#endif
#if !definedEx(CONFIG_X86_64_SMP)
#endif
/* For arch-specific code, we can use direct single-insn ops (they
 * don't give an lvalue though). */
extern void __bad_percpu_size(void);
/*
 * percpu_read() makes gcc load the percpu variable every time it is
 * accessed while percpu_read_stable() allows the value to be cached.
 * percpu_read_stable() is more efficient and can be used if its value
 * is guaranteed to be valid across cpus.  The current users include
 * get_current() and get_thread_info() both of which are actually
 * per-thread variables implemented as per-cpu variables and thus
 * stable for the duration of the respective task.
 */
/*
 * Per cpu atomic 64 bit operations are only available under 64 bit.
 * 32 bit must fall back to generic operations.
 */
#if definedEx(CONFIG_X86_64)
#endif
/* This is not atomic against other CPUs -- CPU preemption needs to be off */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/asm-generic/percpu.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/compiler.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/asm-generic/percpu.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/threads.h" 1
/*
 * The default limit for the nr of threads is now in
 * /proc/sys/kernel/threads-max.
 */
/*
 * Maximum supported processors.  Setting this smaller saves quite a
 * bit of memory.  Use nr_cpu_ids instead of this except for static bitmaps.
 */
/* Places which use this should consider cpumask_var_t. */
/*
 * This controls the default maximum pid allocated to a process
 */
/*
 * A maximum of 4 million PIDs should be enough for a while.
 * [NOTE: PID/TIDs are limited to 2^29 ~= 500+ million, see futex.h.]
 */
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/asm-generic/percpu.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/percpu-defs.h" 1
/*
 * Determine the real variable name from the name visible in the
 * kernel sources.
 */
/*
 * Base implementations of per-CPU variable declarations and definitions, where
 * the section in which the variable is to be placed is provided by the
 * 'sec' argument.  This may be used to affect the parameters governing the
 * variable's storage.
 *
 * NOTE!  The sections for the DECLARE and for the DEFINE must match, lest
 * linkage errors occur due the compiler generating the wrong code to access
 * that section.
 */
/*
 * s390 and alpha modules require percpu variables to be defined as
 * weak to force the compiler to generate GOT based external
 * references for them.  This is necessary because percpu sections
 * will be located outside of the usually addressable area.
 *
 * This definition puts the following two extra restrictions when
 * defining percpu variables.
 *
 * 1. The symbol must be globally unique, even the static ones.
 * 2. Static percpu variables cannot be defined inside a function.
 *
 * Archs which need weak percpu definitions should define
 * ARCH_NEEDS_WEAK_PER_CPU in asm/percpu.h when necessary.
 *
 * To ensure that the generic code observes the above two
 * restrictions, if CONFIG_DEBUG_FORCE_WEAK_PER_CPU is set weak
 * definition is used for all cases.
 */
#if definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
/*
 * __pcpu_scope_* dummy variable is used to enforce scope.  It
 * receives the static modifier when it's used in front of
 * DEFINE_PER_CPU() and will trigger build failure if
 * DECLARE_PER_CPU() is used for the same variable.
 *
 * __pcpu_unique_* dummy variable is used to enforce symbol uniqueness
 * such that hidden weak symbol collision, which will cause unrelated
 * variables to share the same address, can be detected during build.
 */
#endif
#if !definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
/*
 * Normal declaration and definition macros.
 */
#endif
/*
 * Variant on the per-CPU variable declaration/definition theme used for
 * ordinary per-CPU variables.
 */
/*
 * Declaration/definition used for per-CPU variables that must come first in
 * the set of variables.
 */
/*
 * Declaration/definition used for per-CPU variables that must be cacheline
 * aligned under SMP conditions so that, whilst a particular instance of the
 * data corresponds to a particular CPU, inefficiencies due to direct access by
 * other CPUs are reduced by preventing the data from unnecessarily spanning
 * cachelines.
 *
 * An example of this would be statistical data, where each CPU's set of data
 * is updated by that CPU alone, but the data from across all CPUs is collated
 * by a CPU processing a read from a proc file.
 */
/*
 * Declaration/definition used for per-CPU variables that must be page aligned.
 */
/*
 * Intermodule exports for per-CPU variables.
 */
#line 8 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/asm-generic/percpu.h" 2
#if definedEx(CONFIG_SMP)
/*
 * per_cpu_offset() is the offset that has to be added to a
 * percpu variable to get to the instance for a certain processor.
 *
 * Most arches use the __per_cpu_offset array for those offsets but
 * some arches have their own ways of determining the offset (x86_64, s390).
 */
extern unsigned long __per_cpu_offset[8];
/*
 * Determine the offset for the currently active processor.
 * An arch may define __my_cpu_offset to provide a more effective
 * means of obtaining the offset to the per cpu variables of the
 * current processor.
 */
#if definedEx(CONFIG_DEBUG_PREEMPT)
#endif
#if !definedEx(CONFIG_DEBUG_PREEMPT)
#endif
/*
 * Add a offset to a pointer but keep the pointer as is.
 *
 * Only S390 provides its own means of moving the pointer.
 */
/*
 * A percpu variable may point to a discarded regions. The following are
 * established ways to produce a usable pointer from the percpu variable
 * offset.
 */
extern void setup_per_cpu_areas(void);
#endif
#if !definedEx(CONFIG_SMP)
#endif
#if definedEx(CONFIG_SMP)
#endif
#if !definedEx(CONFIG_SMP)
#endif
#if definedEx(CONFIG_SMP)
#endif
#if !definedEx(CONFIG_SMP)
#endif
#line 246 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/percpu.h" 2
/* We can use this directly for local CPU (faster). */
#if definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(".discard"), unused)) char __pcpu_scope_this_cpu_off; extern __attribute__((section(
#if definedEx(CONFIG_SMP)
".data.percpu"
#endif
#if !definedEx(CONFIG_SMP)
".data"
#endif
 "")))  __typeof__(unsigned long) per_cpu__this_cpu_off
#endif
#if !definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(
#if definedEx(CONFIG_SMP)
".data.percpu"
#endif
#if !definedEx(CONFIG_SMP)
".data"
#endif
 "")))  __typeof__(unsigned long) per_cpu__this_cpu_off
#endif
;
#if definedEx(CONFIG_SMP)
/*
 * Define the "EARLY_PER_CPU" macros.  These are used for some per_cpu
 * variables that are initialized and accessed before there are per_cpu
 * areas allocated.
 */
#endif
#if !definedEx(CONFIG_SMP)
/* no early_per_cpu_map() */
#endif
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/current.h" 2
struct task_struct;
#if definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(".discard"), unused)) char __pcpu_scope_current_task; extern __attribute__((section(
#if definedEx(CONFIG_SMP)
".data.percpu"
#endif
#if !definedEx(CONFIG_SMP)
".data"
#endif
 "")))  __typeof__(struct task_struct *) per_cpu__current_task
#endif
#if !definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(
#if definedEx(CONFIG_SMP)
".data.percpu"
#endif
#if !definedEx(CONFIG_SMP)
".data"
#endif
 "")))  __typeof__(struct task_struct *) per_cpu__current_task
#endif
;
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __attribute__((always_inline)) struct task_struct *get_current(void)
{
	return ({ typeof(per_cpu__current_task) pfo_ret__; switch (sizeof(per_cpu__current_task)) { case 1: asm("mov" "b "
#if definedEx(CONFIG_SMP)
"%%""#if definedEx(CONFIG_X86_64)\ngs\n#elif !definedEx(CONFIG_X86_64)\nfs\n#endif\n"":%P" "1"
#endif
#if !definedEx(CONFIG_SMP)
"%P" "1"
#endif
",%0" : "=q" (pfo_ret__) : "p"(&per_cpu__current_task)); break; case 2: asm("mov" "w "
#if definedEx(CONFIG_SMP)
"%%""#if definedEx(CONFIG_X86_64)\ngs\n#elif !definedEx(CONFIG_X86_64)\nfs\n#endif\n"":%P" "1"
#endif
#if !definedEx(CONFIG_SMP)
"%P" "1"
#endif
",%0" : "=r" (pfo_ret__) : "p"(&per_cpu__current_task)); break; case 4: asm("mov" "l "
#if definedEx(CONFIG_SMP)
"%%""#if definedEx(CONFIG_X86_64)\ngs\n#elif !definedEx(CONFIG_X86_64)\nfs\n#endif\n"":%P" "1"
#endif
#if !definedEx(CONFIG_SMP)
"%P" "1"
#endif
",%0" : "=r" (pfo_ret__) : "p"(&per_cpu__current_task)); break; case 8: asm("mov" "q "
#if definedEx(CONFIG_SMP)
"%%""#if definedEx(CONFIG_X86_64)\ngs\n#elif !definedEx(CONFIG_X86_64)\nfs\n#endif\n"":%P" "1"
#endif
#if !definedEx(CONFIG_SMP)
"%P" "1"
#endif
",%0" : "=r" (pfo_ret__) : "p"(&per_cpu__current_task)); break; default: __bad_percpu_size(); } pfo_ret__; });
}
#line 17 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/processor.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/cpufeature.h" 1
/*
 * Defines x86 CPU feature bits
 */
#line 18 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/processor.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/system.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/asm.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/system.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/segment.h" 1
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/system.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/cpufeature.h" 1
/*
 * Defines x86 CPU feature bits
 */
#line 8 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/system.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/cmpxchg.h" 1
#if definedEx(CONFIG_X86_32)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/cmpxchg_32.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/bitops.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/cmpxchg_32.h" 2
/*
 * Note: if you use set64_bit(), __cmpxchg64(), or their variants, you
 *       you need to test for the feature in boot_cpu_data.
 */
extern void __xchg_wrong_size(void);
/*
 * Note: no "lock" prefix even on SMP: xchg always implies lock anyway
 * Note 2: xchg has side effect, so that attribute volatile is necessary,
 *	  but generally the primitive is invalid, *ptr is output argument. --ANK
 */
struct __xchg_dummy {
	unsigned long a[100];
};
/*
 * The semantics of XCHGCMP8B are a bit strange, this is why
 * there is a loop and the loading of %%eax and %%edx has to
 * be inside. This inlines well in most cases, the cached
 * cost is around ~38 cycles. (in the future we might want
 * to do an SIMD/3DNOW!/MMX/FPU 64-bit store here, but that
 * might have an implicit FPU-save as a cost, so it's not
 * clear which path to go.)
 *
 * cmpxchg8b must be used with the lock prefix here to allow
 * the instruction to be executed atomically, see page 3-102
 * of the instruction set reference 24319102.pdf. We need
 * the reader side to see the coherent 64bit value.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __set_64bit(unsigned long long *ptr,
			       unsigned int low, unsigned int high)
{
	asm volatile("\n1:\t"
		     "movl (%0), %%eax\n\t"
		     "movl 4(%0), %%edx\n\t"
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" " " ".balign 4" " " "\n" " " ".long" " " "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "cmpxchg8b (%0)\n\t"
		     "jnz 1b"
		     : /* no outputs */
		     : "D"(ptr),
		       "b"(low),
		       "c"(high)
		     : "ax", "dx", "memory");
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __set_64bit_constant(unsigned long long *ptr,
					unsigned long long value)
{
	__set_64bit(ptr, (unsigned int)value, (unsigned int)(value >> 32));
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __set_64bit_var(unsigned long long *ptr,
				   unsigned long long value)
{
	__set_64bit(ptr, *(((unsigned int *)&(value)) + 0), *(((unsigned int *)&(value)) + 1));
}
extern void __cmpxchg_wrong_size(void);
/*
 * Atomic compare and exchange.  Compare OLD with MEM, if identical,
 * store NEW in MEM.  Return the initial value in MEM.  Success is
 * indicated by comparing RETURN with OLD.
 */
#if definedEx(CONFIG_X86_CMPXCHG)
#endif
#if definedEx(CONFIG_X86_CMPXCHG64)
#endif
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long long __cmpxchg64(volatile void *ptr,
					     unsigned long long old,
					     unsigned long long new)
{
	unsigned long long prev;
	asm volatile(
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" " " ".balign 4" " " "\n" " " ".long" " " "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "cmpxchg8b %3"
		     : "=A"(prev)
		     : "b"((unsigned long)new),
		       "c"((unsigned long)(new >> 32)),
		       "m"(*((struct __xchg_dummy *)(ptr))),
		       "0"(old)
		     : "memory");
	return prev;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long long __cmpxchg64_local(volatile void *ptr,
						   unsigned long long old,
						   unsigned long long new)
{
	unsigned long long prev;
	asm volatile("cmpxchg8b %3"
		     : "=A"(prev)
		     : "b"((unsigned long)new),
		       "c"((unsigned long)(new >> 32)),
		       "m"(*((struct __xchg_dummy *)(ptr))),
		       "0"(old)
		     : "memory");
	return prev;
}
#if !definedEx(CONFIG_X86_CMPXCHG)
/*
 * Building a kernel capable running on 80386. It may be necessary to
 * simulate the cmpxchg on the 80386 CPU. For that purpose we define
 * a function for each of the sizes we support.
 */
extern unsigned long cmpxchg_386_u8(volatile void *, u8, u8);
extern unsigned long cmpxchg_386_u16(volatile void *, u16, u16);
extern unsigned long cmpxchg_386_u32(volatile void *, u32, u32);
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long cmpxchg_386(volatile void *ptr, unsigned long old,
					unsigned long new, int size)
{
	switch (size) {
	case 1:
		return cmpxchg_386_u8(ptr, old, new);
	case 2:
		return cmpxchg_386_u16(ptr, old, new);
	case 4:
		return cmpxchg_386_u32(ptr, old, new);
	}
	return old;
}
#endif
#if !definedEx(CONFIG_X86_CMPXCHG64)
/*
 * Building a kernel capable running on 80386 and 80486. It may be necessary
 * to simulate the cmpxchg8b on the 80386 and 80486 CPU.
 */
extern unsigned long long cmpxchg_486_u64(volatile void *, u64, u64);
#endif
#line 4 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/cmpxchg.h" 2
#endif
#if !definedEx(CONFIG_X86_32)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/cmpxchg_64.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/alternative.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/cmpxchg_64.h" 2
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void set_64bit(volatile unsigned long *ptr, unsigned long val)
{
	*ptr = val;
}
extern void __xchg_wrong_size(void);
extern void __cmpxchg_wrong_size(void);
/*
 * Note: no "lock" prefix even on SMP: xchg always implies lock anyway
 * Note 2: xchg has side effect, so that attribute volatile is necessary,
 *	  but generally the primitive is invalid, *ptr is output argument. --ANK
 */
/*
 * Atomic compare and exchange.  Compare OLD with MEM, if identical,
 * store NEW in MEM.  Return the initial value in MEM.  Success is
 * indicated by comparing RETURN with OLD.
 */
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/cmpxchg.h" 2
#endif
#line 9 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/system.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/nops.h" 1
/* Define nops for use with alternative() */
/* generic versions from gas
   1: nop
   the following instructions are NOT nops in 64-bit mode,
   for 64-bit mode use K8 or P6 nops instead
   2: movl %esi,%esi
   3: leal 0x00(%esi),%esi
   4: leal 0x00(,%esi,1),%esi
   6: leal 0x00000000(%esi),%esi
   7: leal 0x00000000(,%esi,1),%esi
*/
/* Opteron 64bit nops
   1: nop
   2: osp nop
   3: osp osp nop
   4: osp osp osp nop
*/
/* K7 nops
   uses eax dependencies (arbitary choice)
   1: nop
   2: movl %eax,%eax
   3: leal (,%eax,1),%eax
   4: leal 0x00(,%eax,1),%eax
   6: leal 0x00000000(%eax),%eax
   7: leal 0x00000000(,%eax,1),%eax
*/
/* P6 nops
   uses eax dependencies (Intel-recommended choice)
   1: nop
   2: osp nop
   3: nopl (%eax)
   4: nopl 0x00(%eax)
   5: nopl 0x00(%eax,%eax,1)
   6: osp nopl 0x00(%eax,%eax,1)
   7: nopl 0x00000000(%eax)
   8: nopl 0x00000000(%eax,%eax,1)
   Note: All the above are assumed to be a single instruction.
	There is kernel code that depends on this.
*/
#if definedEx(CONFIG_MK7)
#endif
#if !definedEx(CONFIG_MK7) && definedEx(CONFIG_X86_P6_NOP)
#endif
#if !definedEx(CONFIG_MK7) && definedEx(CONFIG_X86_64) && !definedEx(CONFIG_X86_P6_NOP)
#endif
#if !definedEx(CONFIG_MK7) && !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_X86_P6_NOP)
#endif
#line 10 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/system.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/kernel.h" 1
#line 12 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/system.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/irqflags.h" 1
/*
 * include/linux/irqflags.h
 *
 * IRQ flags tracing: follow the state of the hardirq and softirq flags and
 * provide callbacks for transitions between ON and OFF states.
 *
 * This file gets included from lowlevel asm headers too, to provide
 * wrapped versions of the local_irq_*() APIs, based on the
 * raw_local_irq_*() macros from the lowlevel headers.
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/typecheck.h" 1
#line 16 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/irqflags.h" 2
#if definedEx(CONFIG_TRACE_IRQFLAGS)
  extern void trace_softirqs_on(unsigned long ip);
  extern void trace_softirqs_off(unsigned long ip);
  extern void trace_hardirqs_on(void);
  extern void trace_hardirqs_off(void);
#endif
#if !definedEx(CONFIG_TRACE_IRQFLAGS)
#endif
#if !definedEx(CONFIG_IRQSOFF_TRACER) && definedEx(CONFIG_PREEMPT_TRACER) || definedEx(CONFIG_IRQSOFF_TRACER)
 extern void stop_critical_timings(void);
 extern void start_critical_timings(void);
#endif
#if !definedEx(CONFIG_IRQSOFF_TRACER) && !definedEx(CONFIG_PREEMPT_TRACER)
#endif
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/irqflags.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/processor-flags.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/irqflags.h" 2
/*
 * Interrupt control:
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long native_save_fl(void)
{
	unsigned long flags;
	/*
	 * "=rm" is safe here, because "pop" adjusts the stack before
	 * it evaluates its effective address -- this is part of the
	 * documented behavior of the "pop" instruction.
	 */
	asm volatile("# __raw_save_flags\n\t"
		     "pushf ; pop %0"
		     : "=rm" (flags)
		     : /* no input */
		     : "memory");
	return flags;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void native_restore_fl(unsigned long flags)
{
	asm volatile("push %0 ; popf"
		     : /* no output */
		     :"g" (flags)
		     :"memory", "cc");
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void native_irq_disable(void)
{
	asm volatile("cli": : :"memory");
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void native_irq_enable(void)
{
	asm volatile("sti": : :"memory");
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void native_safe_halt(void)
{
	asm volatile("sti; hlt": : :"memory");
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void native_halt(void)
{
	asm volatile("hlt": : :"memory");
}
#if definedEx(CONFIG_PARAVIRT)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h" 1
/* Various instructions on x86 need to be replaced for
 * para-virtualization: those hooks are defined here. */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/pgtable_types.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/const.h" 1
/* const.h: Macros for dealing with constants.  */
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/pgtable_types.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/page_types.h" 1
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/pgtable_types.h" 2
/* If _PAGE_BIT_PRESENT is clear, we use these: */
/* - if the user mapped it with PROT_NONE; pte_present gives true */
/* - set: nonlinear file mapping, saved PTE; unset:swap */
#if definedEx(CONFIG_KMEMCHECK)
#endif
#if !definedEx(CONFIG_KMEMCHECK)
#endif
#if !definedEx(CONFIG_X86_PAE) && definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_PAE)
#endif
#if !definedEx(CONFIG_X86_PAE) && !definedEx(CONFIG_X86_64)
#endif
/* Set of bits not changed in pte_modify */
/*         xwr */
/*
 * early identity mapping  pte attrib macros.
 */
#if definedEx(CONFIG_X86_64)
#endif
#if !definedEx(CONFIG_X86_64)
/*
 * For PDE_IDENT_ATTR include USER bit. As the PDE and PTE protection
 * bits are combined, this will alow user to access the high address mapped
 * VDSO in the presence of CONFIG_COMPAT_VDSO
 */
#endif
#if definedEx(CONFIG_X86_32)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/pgtable_32_types.h" 1
/*
 * The Linux x86 paging architecture is 'compile-time dual-mode', it
 * implements both the traditional 2-level x86 page tables and the
 * newer 3-level PAE-mode page tables.
 */
#if definedEx(CONFIG_X86_PAE)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/pgtable-3level_types.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/pgtable-3level_types.h" 2
typedef u64	pteval_t;
typedef u64	pmdval_t;
typedef u64	pudval_t;
typedef u64	pgdval_t;
typedef u64	pgprotval_t;
typedef union {
	struct {
		unsigned long pte_low, pte_high;
	};
	pteval_t pte;
} pte_t;
/*
 * PGDIR_SHIFT determines what a top-level page table entry can map
 */
/*
 * PMD_SHIFT determines the size of the area a middle-level
 * page table can map
 */
/*
 * entries per page directory level
 */
#line 12 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/pgtable_32_types.h" 2
#endif
#if !definedEx(CONFIG_X86_PAE)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/pgtable-2level_types.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/pgtable-2level_types.h" 2
typedef unsigned long	pteval_t;
typedef unsigned long	pmdval_t;
typedef unsigned long	pudval_t;
typedef unsigned long	pgdval_t;
typedef unsigned long	pgprotval_t;
typedef union {
	pteval_t pte;
	pteval_t pte_low;
} pte_t;
/*
 * traditional i386 two-level paging structure:
 */
/*
 * the i386 is two-level, so we don't really have any
 * PMD directory physically.
 */
#line 16 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/pgtable_32_types.h" 2
#endif
/* Just any arbitrary offset to the start of the vmalloc VM area: the
 * current 8MB value just means that there will be a 8MB "hole" after the
 * physical memory until the kernel virtual memory starts.  That means that
 * any out-of-bounds memory accesses will hopefully be caught.
 * The vmalloc() routines leaves a hole of 4kB between each vmalloced
 * area for the same reason. ;)
 */
extern bool __vmalloc_start_set; /* set once high_memory is set */
#if definedEx(CONFIG_X86_PAE)
#endif
#if !definedEx(CONFIG_X86_PAE)
#endif
#if definedEx(CONFIG_HIGHMEM)
#endif
#if !definedEx(CONFIG_HIGHMEM)
#endif
#line 174 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/pgtable_types.h" 2
#endif
#if !definedEx(CONFIG_X86_32)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/pgtable_64_types.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/pgtable_64_types.h" 2
/*
 * These are used to make use of C type-checking..
 */
typedef unsigned long	pteval_t;
typedef unsigned long	pmdval_t;
typedef unsigned long	pudval_t;
typedef unsigned long	pgdval_t;
typedef unsigned long	pgprotval_t;
typedef struct { pteval_t pte; } pte_t;
/*
 * PGDIR_SHIFT determines what a top-level page table entry can map
 */
/*
 * 3rd level page
 */
/*
 * PMD_SHIFT determines the size of the area a middle-level
 * page table can map
 */
/*
 * entries per page directory level
 */
/* See Documentation/x86/x86_64/mm.txt for a description of the memory map. */
#line 176 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/pgtable_types.h" 2
#endif
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 181 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/pgtable_types.h" 2
/* PTE_PFN_MASK extracts the PFN from a (pte|pmd|pud|pgd)val_t */
/* PTE_FLAGS_MASK extracts the flags from a (pte|pmd|pud|pgd)val_t */
typedef struct pgprot { pgprotval_t pgprot; } pgprot_t;
typedef struct { pgdval_t pgd; } pgd_t;
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 pgd_t native_make_pgd(pgdval_t val)
{
	return (pgd_t) { val };
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 pgdval_t native_pgd_val(pgd_t pgd)
{
	return pgd.pgd;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 pgdval_t pgd_flags(pgd_t pgd)
{
	return native_pgd_val(pgd) & (~((pteval_t)(((signed long)(~(((1UL) << 12)-1))) & ((phys_addr_t)(1ULL << 
#if definedEx(CONFIG_X86_64)
46
#endif
#if definedEx(CONFIG_X86_PAE) && !definedEx(CONFIG_X86_64)
44
#endif
#if !definedEx(CONFIG_X86_PAE) && !definedEx(CONFIG_X86_64)
32
#endif
) - 1))));
}
#if !definedEx(CONFIG_X86_32) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_PARAVIRT)
typedef struct { pudval_t pud; } pud_t;
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 pud_t native_make_pud(pmdval_t val)
{
	return (pud_t) { val };
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 pudval_t native_pud_val(pud_t pud)
{
	return pud.pud;
}
#endif
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/asm-generic/pgtable-nopud.h" 1
/*
 * Having the pud type consist of a pgd gets the size right, and allows
 * us to conceptually access the pgd entry that this pud is folded into
 * without casting.
 */
typedef struct { pgd_t pgd; } pud_t;
/*
 * The "pgd_xxx()" functions here are trivial for a folded two-level
 * setup: the pud is never bad, and a pud always exists (as it's folded
 * into the pgd entry)
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int pgd_none(pgd_t pgd)		{ return 0; }
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int pgd_bad(pgd_t pgd)		{ return 0; }
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int pgd_present(pgd_t pgd)	{ return 1; }
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void pgd_clear(pgd_t *pgd)	{ }
/*
 * (puds are folded into pgds so this doesn't get actually called,
 * but the define is needed for a generic inline function.)
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 pud_t * pud_offset(pgd_t * pgd, unsigned long address)
{
	return (pud_t *)pgd;
}
/*
 * allocating and freeing a pud is trivial: the 1-entry pud is
 * inside the pgd, so has no extra memory associated with it.
 */
#line 221 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/pgtable_types.h" 2
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 pudval_t native_pud_val(pud_t pud)
{
	return native_pgd_val(pud.pgd);
}
#endif
#if !definedEx(CONFIG_X86_32) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_X86_PAE) && !definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_X86_PAE)
typedef struct { pmdval_t pmd; } pmd_t;
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 pmd_t native_make_pmd(pmdval_t val)
{
	return (pmd_t) { val };
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 pmdval_t native_pmd_val(pmd_t pmd)
{
	return pmd.pmd;
}
#endif
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_X86_PAE) && definedEx(CONFIG_PARAVIRT)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/asm-generic/pgtable-nopmd.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/asm-generic/pgtable-nopud.h" 1
#line 8 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/asm-generic/pgtable-nopmd.h" 2
struct mm_struct;
/*
 * Having the pmd type consist of a pud gets the size right, and allows
 * us to conceptually access the pud entry that this pmd is folded into
 * without casting.
 */
typedef struct { pud_t pud; } pmd_t;
/*
 * The "pud_xxx()" functions here are trivial for a folded two-level
 * setup: the pmd is never bad, and a pmd always exists (as it's folded
 * into the pud entry)
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int pud_none(pud_t pud)		{ return 0; }
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int pud_bad(pud_t pud)		{ return 0; }
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int pud_present(pud_t pud)	{ return 1; }
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void pud_clear(pud_t *pud)	{ }
/*
 * (pmds are folded into puds so this doesn't get actually called,
 * but the define is needed for a generic inline function.)
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 pmd_t * pmd_offset(pud_t * pud, unsigned long address)
{
	return (pmd_t *)pud;
}
/*
 * allocating and freeing a pmd is trivial: the 1-entry pmd is
 * inside the pud, so has no extra memory associated with it.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void pmd_free(struct mm_struct *mm, pmd_t *pmd)
{
}
#line 242 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/pgtable_types.h" 2
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 pmdval_t native_pmd_val(pmd_t pmd)
{
	return native_pgd_val(pmd.pud.pgd);
}
#endif
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 pudval_t pud_flags(pud_t pud)
{
	return native_pud_val(pud) & (~((pteval_t)(((signed long)(~(((1UL) << 12)-1))) & ((phys_addr_t)(1ULL << 
#if definedEx(CONFIG_X86_64)
46
#endif
#if definedEx(CONFIG_X86_PAE) && !definedEx(CONFIG_X86_64)
44
#endif
#if !definedEx(CONFIG_X86_PAE) && !definedEx(CONFIG_X86_64)
32
#endif
) - 1))));
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 pmdval_t pmd_flags(pmd_t pmd)
{
	return native_pmd_val(pmd) & (~((pteval_t)(((signed long)(~(((1UL) << 12)-1))) & ((phys_addr_t)(1ULL << 
#if definedEx(CONFIG_X86_64)
46
#endif
#if definedEx(CONFIG_X86_PAE) && !definedEx(CONFIG_X86_64)
44
#endif
#if !definedEx(CONFIG_X86_PAE) && !definedEx(CONFIG_X86_64)
32
#endif
) - 1))));
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 pte_t native_make_pte(pteval_t val)
{
	return (pte_t) { .pte = val };
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 pteval_t native_pte_val(pte_t pte)
{
	return pte.pte;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 pteval_t pte_flags(pte_t pte)
{
	return native_pte_val(pte) & (~((pteval_t)(((signed long)(~(((1UL) << 12)-1))) & ((phys_addr_t)(1ULL << 
#if definedEx(CONFIG_X86_64)
46
#endif
#if definedEx(CONFIG_X86_PAE) && !definedEx(CONFIG_X86_64)
44
#endif
#if !definedEx(CONFIG_X86_PAE) && !definedEx(CONFIG_X86_64)
32
#endif
) - 1))));
}
typedef struct page *pgtable_t;
extern pteval_t __supported_pte_mask;
extern void set_nx(void);
extern int nx_enabled;
extern pgprot_t pgprot_writecombine(pgprot_t prot);
/* Indicate that x86 has its own track and untrack pfn vma functions */
struct file;
pgprot_t phys_mem_access_prot(struct file *file, unsigned long pfn,
                              unsigned long size, pgprot_t vma_prot);
int phys_mem_access_prot_allowed(struct file *file, unsigned long pfn,
                              unsigned long size, pgprot_t *vma_prot);
/* Install a pte for a particular vaddr in kernel space. */
void set_pte_vaddr(unsigned long vaddr, pte_t pte);
#if definedEx(CONFIG_X86_32)
extern void native_pagetable_setup_start(pgd_t *base);
extern void native_pagetable_setup_done(pgd_t *base);
#endif
#if !definedEx(CONFIG_X86_32)
#endif
struct seq_file;
extern void arch_report_meminfo(struct seq_file *m);
enum {
	PG_LEVEL_NONE,
	PG_LEVEL_4K,
	PG_LEVEL_2M,
	PG_LEVEL_1G,
	PG_LEVEL_NUM
};
#if definedEx(CONFIG_PROC_FS)
extern void update_page_count(int level, unsigned long pages);
#endif
#if !definedEx(CONFIG_PROC_FS)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void update_page_count(int level, unsigned long pages) { }
#endif
/*
 * Helper function that returns the kernel pagetable entry controlling
 * the virtual address 'address'. NULL means no pagetable entry present.
 * NOTE: the return type is pte_t but if the pmd is PSE then we return it
 * as a pte too.
 */
extern pte_t *lookup_address(unsigned long address, unsigned int *level);
#line 9 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/asm.h" 1
#line 10 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt_types.h" 1
/* Bitmask of what can be clobbered: usually at least eax. */
#if definedEx(CONFIG_X86_32)
/* CLBR_ANY should match all regs platform has. For i386, that's just it */
#endif
#if !definedEx(CONFIG_X86_32)
#endif
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/desc_defs.h" 1
/* Written 2000 by Andi Kleen */
/*
 * Segment descriptor structure definitions, usable from both x86_64 and i386
 * archs.
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 14 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/desc_defs.h" 2
/*
 * FIXME: Accessing the desc_struct through its fields is more elegant,
 * and should be the one valid thing to do. However, a lot of open code
 * still touches the a and b accessors, and doing this allow us to do it
 * incrementally. We keep the signature as a struct, rather than an union,
 * so we can get rid of it transparently in the future -- glommer
 */
/* 8 byte segment descriptor */
struct desc_struct {
	union {
		struct {
			unsigned int a;
			unsigned int b;
		};
		struct {
			u16 limit0;
			u16 base0;
			unsigned base1: 8, type: 4, s: 1, dpl: 2, p: 1;
			unsigned limit: 4, avl: 1, l: 1, d: 1, g: 1, base2: 8;
		};
	};
} __attribute__((packed));
enum {
	GATE_INTERRUPT = 0xE,
	GATE_TRAP = 0xF,
	GATE_CALL = 0xC,
	GATE_TASK = 0x5,
};
/* 16byte gate */
struct gate_struct64 {
	u16 offset_low;
	u16 segment;
	unsigned ist : 3, zero0 : 5, type : 5, dpl : 2, p : 1;
	u16 offset_middle;
	u32 offset_high;
	u32 zero1;
} __attribute__((packed));
enum {
	DESC_TSS = 0x9,
	DESC_LDT = 0x2,
	DESCTYPE_S = 0x10,	/* !system */
};
/* LDT or TSS descriptor in the GDT. 16 bytes. */
struct ldttss_desc64 {
	u16 limit0;
	u16 base0;
	unsigned base1 : 8, type : 5, dpl : 2, p : 1;
	unsigned limit1 : 4, zero0 : 3, g : 1, base2 : 8;
	u32 base3;
	u32 zero1;
} __attribute__((packed));
#if definedEx(CONFIG_X86_64)
typedef struct gate_struct64 gate_desc;
typedef struct ldttss_desc64 ldt_desc;
typedef struct ldttss_desc64 tss_desc;
#endif
#if !definedEx(CONFIG_X86_64)
typedef struct desc_struct gate_desc;
typedef struct desc_struct ldt_desc;
typedef struct desc_struct tss_desc;
#endif
struct desc_ptr {
	unsigned short size;
	unsigned long address;
} __attribute__((packed)) ;
#line 44 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt_types.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/kmap_types.h" 1
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_DEBUG_HIGHMEM)
#endif
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/asm-generic/kmap_types.h" 1
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_DEBUG_HIGHMEM)
#endif
#if !definedEx(CONFIG_X86_32) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_DEBUG_HIGHMEM)
#endif
enum km_type {
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_DEBUG_HIGHMEM)
__KM_FENCE_0 ,
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_DEBUG_HIGHMEM)
#endif
	KM_BOUNCE_READ,
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_DEBUG_HIGHMEM)
__KM_FENCE_1 ,
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_DEBUG_HIGHMEM)
#endif
	KM_SKB_SUNRPC_DATA,
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_DEBUG_HIGHMEM)
__KM_FENCE_2 ,
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_DEBUG_HIGHMEM)
#endif
	KM_SKB_DATA_SOFTIRQ,
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_DEBUG_HIGHMEM)
__KM_FENCE_3 ,
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_DEBUG_HIGHMEM)
#endif
	KM_USER0,
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_DEBUG_HIGHMEM)
__KM_FENCE_4 ,
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_DEBUG_HIGHMEM)
#endif
	KM_USER1,
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_DEBUG_HIGHMEM)
__KM_FENCE_5 ,
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_DEBUG_HIGHMEM)
#endif
	KM_BIO_SRC_IRQ,
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_DEBUG_HIGHMEM)
__KM_FENCE_6 ,
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_DEBUG_HIGHMEM)
#endif
	KM_BIO_DST_IRQ,
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_DEBUG_HIGHMEM)
__KM_FENCE_7 ,
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_DEBUG_HIGHMEM)
#endif
	KM_PTE0,
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_DEBUG_HIGHMEM)
__KM_FENCE_8 ,
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_DEBUG_HIGHMEM)
#endif
	KM_PTE1,
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_DEBUG_HIGHMEM)
__KM_FENCE_9 ,
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_DEBUG_HIGHMEM)
#endif
	KM_IRQ0,
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_DEBUG_HIGHMEM)
__KM_FENCE_10 ,
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_DEBUG_HIGHMEM)
#endif
	KM_IRQ1,
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_DEBUG_HIGHMEM)
__KM_FENCE_11 ,
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_DEBUG_HIGHMEM)
#endif
	KM_SOFTIRQ0,
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_DEBUG_HIGHMEM)
__KM_FENCE_12 ,
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_DEBUG_HIGHMEM)
#endif
	KM_SOFTIRQ1,
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_DEBUG_HIGHMEM)
__KM_FENCE_13 ,
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_DEBUG_HIGHMEM)
#endif
	KM_SYNC_ICACHE,
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_DEBUG_HIGHMEM)
__KM_FENCE_14 ,
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_DEBUG_HIGHMEM)
#endif
	KM_SYNC_DCACHE,
/* UML specific, for copy_*_user - used in do_op_one_page */
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_DEBUG_HIGHMEM)
__KM_FENCE_15 ,
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_DEBUG_HIGHMEM)
#endif
	KM_UML_USERCOPY,
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_DEBUG_HIGHMEM)
__KM_FENCE_16 ,
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_DEBUG_HIGHMEM)
#endif
	KM_IRQ_PTE,
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_DEBUG_HIGHMEM)
__KM_FENCE_17 ,
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_DEBUG_HIGHMEM)
#endif
	KM_NMI,
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_DEBUG_HIGHMEM)
__KM_FENCE_18 ,
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_DEBUG_HIGHMEM)
#endif
	KM_NMI_PTE,
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_DEBUG_HIGHMEM)
__KM_FENCE_19 ,
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_DEBUG_HIGHMEM)
#endif
	KM_TYPE_NR
};
#line 10 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/kmap_types.h" 2
#line 45 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt_types.h" 2
struct page;
struct thread_struct;
struct desc_ptr;
struct tss_struct;
struct mm_struct;
struct desc_struct;
struct task_struct;
struct cpumask;
/*
 * Wrapper type for pointers to code which uses the non-standard
 * calling convention.  See PV_CALL_SAVE_REGS_THUNK below.
 */
struct paravirt_callee_save {
	void *func;
};
/* general info */
struct pv_info {
	unsigned int kernel_rpl;
	int shared_kernel_pmd;
	int paravirt_enabled;
	const char *name;
};
struct pv_init_ops {
	/*
	 * Patch may replace one of the defined code sequences with
	 * arbitrary code, subject to the same register constraints.
	 * This generally means the code is not free to clobber any
	 * registers other than EAX.  The patch function should return
	 * the number of bytes of code generated, as we nop pad the
	 * rest in generic code.
	 */
	unsigned (*patch)(u8 type, u16 clobber, void *insnbuf,
			  unsigned long addr, unsigned len);
};
struct pv_lazy_ops {
	/* Set deferred update mode, used for batching operations. */
	void (*enter)(void);
	void (*leave)(void);
};
struct pv_time_ops {
	unsigned long long (*sched_clock)(void);
	unsigned long (*get_tsc_khz)(void);
};
struct pv_cpu_ops {
	/* hooks for various privileged instructions */
	unsigned long (*get_debugreg)(int regno);
	void (*set_debugreg)(int regno, unsigned long value);
	void (*clts)(void);
	unsigned long (*read_cr0)(void);
	void (*write_cr0)(unsigned long);
	unsigned long (*read_cr4_safe)(void);
	unsigned long (*read_cr4)(void);
	void (*write_cr4)(unsigned long);
#if definedEx(CONFIG_X86_64)
	unsigned long (*read_cr8)(void);
	void (*write_cr8)(unsigned long);
#endif
	/* Segment descriptor handling */
	void (*load_tr_desc)(void);
	void (*load_gdt)(const struct desc_ptr *);
	void (*load_idt)(const struct desc_ptr *);
	void (*store_gdt)(struct desc_ptr *);
	void (*store_idt)(struct desc_ptr *);
	void (*set_ldt)(const void *desc, unsigned entries);
	unsigned long (*store_tr)(void);
	void (*load_tls)(struct thread_struct *t, unsigned int cpu);
#if definedEx(CONFIG_X86_64)
	void (*load_gs_index)(unsigned int idx);
#endif
	void (*write_ldt_entry)(struct desc_struct *ldt, int entrynum,
				const void *desc);
	void (*write_gdt_entry)(struct desc_struct *,
				int entrynum, const void *desc, int size);
	void (*write_idt_entry)(gate_desc *,
				int entrynum, const gate_desc *gate);
	void (*alloc_ldt)(struct desc_struct *ldt, unsigned entries);
	void (*free_ldt)(struct desc_struct *ldt, unsigned entries);
	void (*load_sp0)(struct tss_struct *tss, struct thread_struct *t);
	void (*set_iopl_mask)(unsigned mask);
	void (*wbinvd)(void);
	void (*io_delay)(void);
	/* cpuid emulation, mostly so that caps bits can be disabled */
	void (*cpuid)(unsigned int *eax, unsigned int *ebx,
		      unsigned int *ecx, unsigned int *edx);
	/* MSR, PMC and TSR operations.
	   err = 0/-EFAULT.  wrmsr returns 0/-EFAULT. */
	u64 (*read_msr)(unsigned int msr, int *err);
	int (*rdmsr_regs)(u32 *regs);
	int (*write_msr)(unsigned int msr, unsigned low, unsigned high);
	int (*wrmsr_regs)(u32 *regs);
	u64 (*read_tsc)(void);
	u64 (*read_pmc)(int counter);
	unsigned long long (*read_tscp)(unsigned int *aux);
	/*
	 * Atomically enable interrupts and return to userspace.  This
	 * is only ever used to return to 32-bit processes; in a
	 * 64-bit kernel, it's used for 32-on-64 compat processes, but
	 * never native 64-bit processes.  (Jump, not call.)
	 */
	void (*irq_enable_sysexit)(void);
	/*
	 * Switch to usermode gs and return to 64-bit usermode using
	 * sysret.  Only used in 64-bit kernels to return to 64-bit
	 * processes.  Usermode register state, including %rsp, must
	 * already be restored.
	 */
	void (*usergs_sysret64)(void);
	/*
	 * Switch to usermode gs and return to 32-bit usermode using
	 * sysret.  Used to return to 32-on-64 compat processes.
	 * Other usermode register state, including %esp, must already
	 * be restored.
	 */
	void (*usergs_sysret32)(void);
	/* Normal iret.  Jump to this with the standard iret stack
	   frame set up. */
	void (*iret)(void);
	void (*swapgs)(void);
	void (*start_context_switch)(struct task_struct *prev);
	void (*end_context_switch)(struct task_struct *next);
};
struct pv_irq_ops {
	/*
	 * Get/set interrupt state.  save_fl and restore_fl are only
	 * expected to use X86_EFLAGS_IF; all other bits
	 * returned from save_fl are undefined, and may be ignored by
	 * restore_fl.
	 *
	 * NOTE: These functions callers expect the callee to preserve
	 * more registers than the standard C calling convention.
	 */
	struct paravirt_callee_save save_fl;
	struct paravirt_callee_save restore_fl;
	struct paravirt_callee_save irq_disable;
	struct paravirt_callee_save irq_enable;
	void (*safe_halt)(void);
	void (*halt)(void);
#if definedEx(CONFIG_X86_64)
	void (*adjust_exception_frame)(void);
#endif
};
struct pv_apic_ops {
#if definedEx(CONFIG_X86_LOCAL_APIC)
	void (*startup_ipi_hook)(int phys_apicid,
				 unsigned long start_eip,
				 unsigned long start_esp);
#endif
};
struct pv_mmu_ops {
	unsigned long (*read_cr2)(void);
	void (*write_cr2)(unsigned long);
	unsigned long (*read_cr3)(void);
	void (*write_cr3)(unsigned long);
	/*
	 * Hooks for intercepting the creation/use/destruction of an
	 * mm_struct.
	 */
	void (*activate_mm)(struct mm_struct *prev,
			    struct mm_struct *next);
	void (*dup_mmap)(struct mm_struct *oldmm,
			 struct mm_struct *mm);
	void (*exit_mmap)(struct mm_struct *mm);
	/* TLB operations */
	void (*flush_tlb_user)(void);
	void (*flush_tlb_kernel)(void);
	void (*flush_tlb_single)(unsigned long addr);
	void (*flush_tlb_others)(const struct cpumask *cpus,
				 struct mm_struct *mm,
				 unsigned long va);
	/* Hooks for allocating and freeing a pagetable top-level */
	int  (*pgd_alloc)(struct mm_struct *mm);
	void (*pgd_free)(struct mm_struct *mm, pgd_t *pgd);
	/*
	 * Hooks for allocating/releasing pagetable pages when they're
	 * attached to a pagetable
	 */
	void (*alloc_pte)(struct mm_struct *mm, unsigned long pfn);
	void (*alloc_pmd)(struct mm_struct *mm, unsigned long pfn);
	void (*alloc_pmd_clone)(unsigned long pfn, unsigned long clonepfn, unsigned long start, unsigned long count);
	void (*alloc_pud)(struct mm_struct *mm, unsigned long pfn);
	void (*release_pte)(unsigned long pfn);
	void (*release_pmd)(unsigned long pfn);
	void (*release_pud)(unsigned long pfn);
	/* Pagetable manipulation functions */
	void (*set_pte)(pte_t *ptep, pte_t pteval);
	void (*set_pte_at)(struct mm_struct *mm, unsigned long addr,
			   pte_t *ptep, pte_t pteval);
	void (*set_pmd)(pmd_t *pmdp, pmd_t pmdval);
	void (*pte_update)(struct mm_struct *mm, unsigned long addr,
			   pte_t *ptep);
	void (*pte_update_defer)(struct mm_struct *mm,
				 unsigned long addr, pte_t *ptep);
	pte_t (*ptep_modify_prot_start)(struct mm_struct *mm, unsigned long addr,
					pte_t *ptep);
	void (*ptep_modify_prot_commit)(struct mm_struct *mm, unsigned long addr,
					pte_t *ptep, pte_t pte);
	struct paravirt_callee_save pte_val;
	struct paravirt_callee_save make_pte;
	struct paravirt_callee_save pgd_val;
	struct paravirt_callee_save make_pgd;
#if !definedEx(CONFIG_X86_32) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_X86_PAE) && !definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_X86_PAE)
#if definedEx(CONFIG_X86_PAE)
	void (*set_pte_atomic)(pte_t *ptep, pte_t pteval);
	void (*pte_clear)(struct mm_struct *mm, unsigned long addr,
			  pte_t *ptep);
	void (*pmd_clear)(pmd_t *pmdp);
#endif
	void (*set_pud)(pud_t *pudp, pud_t pudval);
	struct paravirt_callee_save pmd_val;
	struct paravirt_callee_save make_pmd;
#if !definedEx(CONFIG_X86_32) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_X86_PAE) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_X86_PAE) && !definedEx(CONFIG_PARAVIRT)
	struct paravirt_callee_save pud_val;
	struct paravirt_callee_save make_pud;
	void (*set_pgd)(pgd_t *pudp, pgd_t pgdval);
#endif
#endif
#if definedEx(CONFIG_HIGHPTE)
	void *(*kmap_atomic_pte)(struct page *page, enum km_type type);
#endif
	struct pv_lazy_ops lazy_mode;
	/* dom0 ops */
	/* Sometimes the physical address is a pfn, and sometimes its
	   an mfn.  We can tell which is which from the index. */
	void (*set_fixmap)(unsigned /* enum fixed_addresses */ idx,
			   phys_addr_t phys, pgprot_t flags);
};
struct arch_spinlock;
struct pv_lock_ops {
	int (*spin_is_locked)(struct arch_spinlock *lock);
	int (*spin_is_contended)(struct arch_spinlock *lock);
	void (*spin_lock)(struct arch_spinlock *lock);
	void (*spin_lock_flags)(struct arch_spinlock *lock, unsigned long flags);
	int (*spin_trylock)(struct arch_spinlock *lock);
	void (*spin_unlock)(struct arch_spinlock *lock);
};
/* This contains all the paravirt structures: we get a convenient
 * number for each function using the offset which we use to indicate
 * what to patch. */
struct paravirt_patch_template {
	struct pv_init_ops pv_init_ops;
	struct pv_time_ops pv_time_ops;
	struct pv_cpu_ops pv_cpu_ops;
	struct pv_irq_ops pv_irq_ops;
	struct pv_apic_ops pv_apic_ops;
	struct pv_mmu_ops pv_mmu_ops;
	struct pv_lock_ops pv_lock_ops;
};
extern struct pv_info pv_info;
extern struct pv_init_ops pv_init_ops;
extern struct pv_time_ops pv_time_ops;
extern struct pv_cpu_ops pv_cpu_ops;
extern struct pv_irq_ops pv_irq_ops;
extern struct pv_apic_ops pv_apic_ops;
extern struct pv_mmu_ops pv_mmu_ops;
extern struct pv_lock_ops pv_lock_ops;
/*
 * Generate some code, and mark it as patchable by the
 * apply_paravirt() alternate instruction patcher.
 */
/* Generate patchable code, with the default asm parameters. */
/* Simple instruction patching code. */
unsigned paravirt_patch_nop(void);
unsigned paravirt_patch_ident_32(void *insnbuf, unsigned len);
unsigned paravirt_patch_ident_64(void *insnbuf, unsigned len);
unsigned paravirt_patch_ignore(unsigned len);
unsigned paravirt_patch_call(void *insnbuf,
			     const void *target, u16 tgt_clobbers,
			     unsigned long addr, u16 site_clobbers,
			     unsigned len);
unsigned paravirt_patch_jmp(void *insnbuf, const void *target,
			    unsigned long addr, unsigned len);
unsigned paravirt_patch_default(u8 type, u16 clobbers, void *insnbuf,
				unsigned long addr, unsigned len);
unsigned paravirt_patch_insns(void *insnbuf, unsigned len,
			      const char *start, const char *end);
unsigned native_patch(u8 type, u16 clobbers, void *ibuf,
		      unsigned long addr, unsigned len);
int paravirt_disable_iospace(void);
/*
 * This generates an indirect call based on the operation type number.
 * The type number, computed in PARAVIRT_PATCH, is derived from the
 * offset into the paravirt_patch_template structure, and can therefore be
 * freely converted back into a structure offset.
 */
/*
 * These macros are intended to wrap calls through one of the paravirt
 * ops structs, so that they can be later identified and patched at
 * runtime.
 *
 * Normally, a call to a pv_op function is a simple indirect call:
 * (pv_op_struct.operations)(args...).
 *
 * Unfortunately, this is a relatively slow operation for modern CPUs,
 * because it cannot necessarily determine what the destination
 * address is.  In this case, the address is a runtime constant, so at
 * the very least we can patch the call to e a simple direct call, or
 * ideally, patch an inline implementation into the callsite.  (Direct
 * calls are essentially free, because the call and return addresses
 * are completely predictable.)
 *
 * For i386, these macros rely on the standard gcc "regparm(3)" calling
 * convention, in which the first three arguments are placed in %eax,
 * %edx, %ecx (in that order), and the remaining arguments are placed
 * on the stack.  All caller-save registers (eax,edx,ecx) are expected
 * to be modified (either clobbered or used for return values).
 * X86_64, on the other hand, already specifies a register-based calling
 * conventions, returning at %rax, with parameteres going on %rdi, %rsi,
 * %rdx, and %rcx. Note that for this reason, x86_64 does not need any
 * special handling for dealing with 4 arguments, unlike i386.
 * However, x86_64 also have to clobber all caller saved registers, which
 * unfortunately, are quite a bit (r8 - r11)
 *
 * The call instruction itself is marked by placing its start address
 * and size into the .parainstructions section, so that
 * apply_paravirt() in arch/i386/kernel/alternative.c can do the
 * appropriate patching under the control of the backend pv_init_ops
 * implementation.
 *
 * Unfortunately there's no way to get gcc to generate the args setup
 * for the call, and then allow the call itself to be generated by an
 * inline asm.  Because of this, we must do the complete arg setup and
 * return value handling from within these macros.  This is fairly
 * cumbersome.
 *
 * There are 5 sets of PVOP_* macros for dealing with 0-4 arguments.
 * It could be extended to more arguments, but there would be little
 * to be gained from that.  For each number of arguments, there are
 * the two VCALL and CALL variants for void and non-void functions.
 *
 * When there is a return value, the invoker of the macro must specify
 * the return type.  The macro then uses sizeof() on that type to
 * determine whether its a 32 or 64 bit value, and places the return
 * in the right register(s) (just %eax for 32-bit, and %edx:%eax for
 * 64-bit). For x86_64 machines, it just returns at %rax regardless of
 * the return value size.
 *
 * 64-bit arguments are passed as a pair of adjacent 32-bit arguments
 * i386 also passes 64-bit arguments as a pair of adjacent 32-bit arguments
 * in low,high order
 *
 * Small structures are passed and returned in registers.  The macro
 * calling convention can't directly deal with this, so the wrapper
 * functions must do this.
 *
 * These PVOP_* macros are only defined within this header.  This
 * means that all uses must be wrapped in inline functions.  This also
 * makes sure the incoming and outgoing types are always correct.
 */
#if definedEx(CONFIG_X86_32)
#endif
#if !definedEx(CONFIG_X86_32)
/* [re]ax isn't an arg, but the return val */
/* void functions are still allowed [re]ax for scratch */
#endif
#if definedEx(CONFIG_PARAVIRT_DEBUG)
#endif
#if !definedEx(CONFIG_PARAVIRT_DEBUG)
#endif
/* This is the only difference in x86_64. We can make it much simpler */
#if definedEx(CONFIG_X86_32)
#endif
#if !definedEx(CONFIG_X86_32)
#endif
/* Lazy mode for batching updates / context switch */
enum paravirt_lazy_mode {
	PARAVIRT_LAZY_NONE,
	PARAVIRT_LAZY_MMU,
	PARAVIRT_LAZY_CPU,
};
enum paravirt_lazy_mode paravirt_get_lazy_mode(void);
void paravirt_start_context_switch(struct task_struct *prev);
void paravirt_end_context_switch(struct task_struct *next);
void paravirt_enter_lazy_mmu(void);
void paravirt_leave_lazy_mmu(void);
void _paravirt_nop(void);
u32 _paravirt_ident_32(u32);
u64 _paravirt_ident_64(u64);
/* These all sit in the .parainstructions section to tell us what to patch. */
struct paravirt_patch_site {
	u8 *instr; 		/* original instructions */
	u8 instrtype;		/* type of this instruction */
	u8 len;			/* length of original instruction */
	u16 clobbers;		/* what registers you may clobber */
};
extern struct paravirt_patch_site __parainstructions[],
	__parainstructions_end[];
#line 12 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 15 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/cpumask.h" 1
/*
 * Cpumasks provide a bitmap suitable for representing the
 * set of CPU's in a system, one bit position per CPU number.  In general,
 * only nr_cpu_ids (<= NR_CPUS) bits are valid.
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/kernel.h" 1
#line 11 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/cpumask.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/threads.h" 1
#line 12 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/cpumask.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/bitmap.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 8 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/bitmap.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/bitops.h" 1
#line 9 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/bitmap.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/string.h" 1
/* We don't want strings.h stuff being used by user stuff by accident */
 #line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/compiler.h" 1
#line 12 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/string.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 13 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/string.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/stddef.h" 1
#line 14 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/string.h" 2
#line 1 "systems/redhat/usr/lib/gcc/x86_64-redhat-linux/4.4.4/include/stdarg.h" 1
/* Copyright (C) 1989, 1997, 1998, 1999, 2000, 2009 Free Software Foundation, Inc.
This file is part of GCC.
GCC is free software; you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation; either version 3, or (at your option)
any later version.
GCC is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.
Under Section 7 of GPL version 3, you are granted additional
permissions described in the GCC Runtime Library Exception, version
3.1, as published by the Free Software Foundation.
You should have received a copy of the GNU General Public License and
a copy of the GCC Runtime Library Exception along with this program;
see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see
<http://www.gnu.org/licenses/>.  */
/*
 * ISO C Standard:  7.15  Variable arguments  <stdarg.h>
 */
#line 15 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/string.h" 2
extern char *strndup_user(const char  *, long);
extern void *memdup_user(const void  *, size_t);
/*
 * Include machine specific inline routines
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/string.h" 1
#if definedEx(CONFIG_X86_32)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/string_32.h" 1
/* Let gcc decide whether to inline or use the out of line functions */
extern char *strcpy(char *dest, const char *src);
extern char *strncpy(char *dest, const char *src, size_t count);
extern char *strcat(char *dest, const char *src);
extern char *strncat(char *dest, const char *src, size_t count);
extern int strcmp(const char *cs, const char *ct);
extern int strncmp(const char *cs, const char *ct, size_t count);
extern char *strchr(const char *s, int c);
extern size_t strlen(const char *s);
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __attribute__((always_inline)) void *__memcpy(void *to, const void *from, size_t n)
{
	int d0, d1, d2;
	asm volatile("rep ; movsl\n\t"
		     "movl %4,%%ecx\n\t"
		     "andl $3,%%ecx\n\t"
		     "jz 1f\n\t"
		     "rep ; movsb\n\t"
		     "1:"
		     : "=&c" (d0), "=&D" (d1), "=&S" (d2)
		     : "0" (n / 4), "g" (n), "1" ((long)to), "2" ((long)from)
		     : "memory");
	return to;
}
/*
 * This looks ugly, but the compiler can optimize it totally,
 * as the count is constant.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __attribute__((always_inline)) void *__constant_memcpy(void *to, const void *from,
					       size_t n)
{
	long esi, edi;
	if (!n)
		return to;
	switch (n) {
	case 1:
		*(char *)to = *(char *)from;
		return to;
	case 2:
		*(short *)to = *(short *)from;
		return to;
	case 4:
		*(int *)to = *(int *)from;
		return to;
	case 3:
		*(short *)to = *(short *)from;
		*((char *)to + 2) = *((char *)from + 2);
		return to;
	case 5:
		*(int *)to = *(int *)from;
		*((char *)to + 4) = *((char *)from + 4);
		return to;
	case 6:
		*(int *)to = *(int *)from;
		*((short *)to + 2) = *((short *)from + 2);
		return to;
	case 8:
		*(int *)to = *(int *)from;
		*((int *)to + 1) = *((int *)from + 1);
		return to;
	}
	esi = (long)from;
	edi = (long)to;
	if (n >= 5 * 4) {
		/* large block: use rep prefix */
		int ecx;
		asm volatile("rep ; movsl"
			     : "=&c" (ecx), "=&D" (edi), "=&S" (esi)
			     : "0" (n / 4), "1" (edi), "2" (esi)
			     : "memory"
		);
	} else {
		/* small block: don't clobber ecx + smaller code */
		if (n >= 4 * 4)
			asm volatile("movsl"
				     : "=&D"(edi), "=&S"(esi)
				     : "0"(edi), "1"(esi)
				     : "memory");
		if (n >= 3 * 4)
			asm volatile("movsl"
				     : "=&D"(edi), "=&S"(esi)
				     : "0"(edi), "1"(esi)
				     : "memory");
		if (n >= 2 * 4)
			asm volatile("movsl"
				     : "=&D"(edi), "=&S"(esi)
				     : "0"(edi), "1"(esi)
				     : "memory");
		if (n >= 1 * 4)
			asm volatile("movsl"
				     : "=&D"(edi), "=&S"(esi)
				     : "0"(edi), "1"(esi)
				     : "memory");
	}
	switch (n % 4) {
		/* tail */
	case 0:
		return to;
	case 1:
		asm volatile("movsb"
			     : "=&D"(edi), "=&S"(esi)
			     : "0"(edi), "1"(esi)
			     : "memory");
		return to;
	case 2:
		asm volatile("movsw"
			     : "=&D"(edi), "=&S"(esi)
			     : "0"(edi), "1"(esi)
			     : "memory");
		return to;
	default:
		asm volatile("movsw\n\tmovsb"
			     : "=&D"(edi), "=&S"(esi)
			     : "0"(edi), "1"(esi)
			     : "memory");
		return to;
	}
}
#if definedEx(CONFIG_X86_USE_3DNOW)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/mmx.h" 1
/*
 *	MMX 3Dnow! helper operations
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 10 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/mmx.h" 2
extern void *_mmx_memcpy(void *to, const void *from, size_t size);
extern void mmx_clear_page(void *page);
extern void mmx_copy_page(void *to, void *from);
#line 150 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/string_32.h" 2
/*
 *	This CPU favours 3DNow strongly (eg AMD Athlon)
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void *__constant_memcpy3d(void *to, const void *from, size_t len)
{
	if (len < 512)
		return __constant_memcpy(to, from, len);
	return _mmx_memcpy(to, from, len);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void *__memcpy3d(void *to, const void *from, size_t len)
{
	if (len < 512)
		return __memcpy(to, from, len);
	return _mmx_memcpy(to, from, len);
}
#endif
#if !definedEx(CONFIG_X86_USE_3DNOW)
/*
 *	No 3D Now!
 */
#if !definedEx(CONFIG_KMEMCHECK)
#endif
#if definedEx(CONFIG_KMEMCHECK)
/*
 * kmemcheck becomes very happy if we use the REP instructions unconditionally,
 * because it means that we know both memory operands in advance.
 */
#endif
#endif
void *memmove(void *dest, const void *src, size_t n);
extern void *memchr(const void *cs, int c, size_t count);
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void *__memset_generic(void *s, char c, size_t count)
{
	int d0, d1;
	asm volatile("rep\n\t"
		     "stosb"
		     : "=&c" (d0), "=&D" (d1)
		     : "a" (c), "1" (s), "0" (count)
		     : "memory");
	return s;
}
/* we might want to write optimized versions of these later */
/*
 * memset(x, 0, y) is a reasonably common thing to do, so we want to fill
 * things 32 bits at a time even when we don't know the size of the
 * area at compile-time..
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __attribute__((always_inline))
void *__constant_c_memset(void *s, unsigned long c, size_t count)
{
	int d0, d1;
	asm volatile("rep ; stosl\n\t"
		     "testb $2,%b3\n\t"
		     "je 1f\n\t"
		     "stosw\n"
		     "1:\ttestb $1,%b3\n\t"
		     "je 2f\n\t"
		     "stosb\n"
		     "2:"
		     : "=&c" (d0), "=&D" (d1)
		     : "a" (c), "q" (count), "0" (count/4), "1" ((long)s)
		     : "memory");
	return s;
}
/* Added by Gertjan van Wingerde to make minix and sysv module work */
extern size_t strnlen(const char *s, size_t count);
/* end of additional stuff */
extern char *strstr(const char *cs, const char *ct);
/*
 * This looks horribly ugly, but the compiler can optimize it totally,
 * as we by now know that both pattern and count is constant..
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __attribute__((always_inline))
void *__constant_c_and_count_memset(void *s, unsigned long pattern,
				    size_t count)
{
	switch (count) {
	case 0:
		return s;
	case 1:
		*(unsigned char *)s = pattern & 0xff;
		return s;
	case 2:
		*(unsigned short *)s = pattern & 0xffff;
		return s;
	case 3:
		*(unsigned short *)s = pattern & 0xffff;
		*((unsigned char *)s + 2) = pattern & 0xff;
		return s;
	case 4:
		*(unsigned long *)s = pattern;
		return s;
	}
	{
		int d0, d1;
 		unsigned long eax = pattern;
		switch (count % 4) {
		case 0:
			asm volatile("rep ; stosl" "" : "=&c" (d0), "=&D" (d1) : "a" (eax), "0" (count/4), "1" ((long)s) : "memory");
			return s;
		case 1:
			asm volatile("rep ; stosl" "\n\tstosb" : "=&c" (d0), "=&D" (d1) : "a" (eax), "0" (count/4), "1" ((long)s) : "memory");
			return s;
		case 2:
			asm volatile("rep ; stosl" "\n\tstosw" : "=&c" (d0), "=&D" (d1) : "a" (eax), "0" (count/4), "1" ((long)s) : "memory");
			return s;
		default:
			asm volatile("rep ; stosl" "\n\tstosw\n\tstosb" : "=&c" (d0), "=&D" (d1) : "a" (eax), "0" (count/4), "1" ((long)s) : "memory");
			return s;
		}
	}
}
/*
 * find the first occurrence of byte 'c', or 1 past the area if none
 */
extern void *memscan(void *addr, int c, size_t size);
#line 4 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/string.h" 2
#endif
#if !definedEx(CONFIG_X86_32)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/string_64.h" 1
/* Written 2002 by Andi Kleen */
/* Only used for special circumstances. Stolen from i386/string.h */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __attribute__((always_inline)) void *__inline_memcpy(void *to, const void *from, size_t n)
{
	unsigned long d0, d1, d2;
	asm volatile("rep ; movsl\n\t"
		     "testb $2,%b4\n\t"
		     "je 1f\n\t"
		     "movsw\n"
		     "1:\ttestb $1,%b4\n\t"
		     "je 2f\n\t"
		     "movsb\n"
		     "2:"
		     : "=&c" (d0), "=&D" (d1), "=&S" (d2)
		     : "0" (n / 4), "q" (n), "1" ((long)to), "2" ((long)from)
		     : "memory");
	return to;
}
/* Even with __builtin_ the compiler may decide to use the out of line
   function. */
#if !definedEx(CONFIG_KMEMCHECK)
extern void *memcpy(void *to, const void *from, size_t len);
#endif
#if definedEx(CONFIG_KMEMCHECK)
/*
 * kmemcheck becomes very happy if we use the REP instructions unconditionally,
 * because it means that we know both memory operands in advance.
 */
#endif
void *memset(void *s, int c, size_t n);
void *memmove(void *dest, const void *src, size_t count);
int memcmp(const void *cs, const void *ct, size_t count);
size_t strlen(const char *s);
char *strcpy(char *dest, const char *src);
char *strcat(char *dest, const char *src);
int strcmp(const char *cs, const char *ct);
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/string.h" 2
#endif
#line 23 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/string.h" 2
#if !definedEx(CONFIG_X86_32) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_PARAVIRT)
extern char * strcpy(char *,const char *);
#endif
#if !definedEx(CONFIG_X86_32) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_PARAVIRT)
extern char * strncpy(char *,const char *, __kernel_size_t);
#endif
size_t strlcpy(char *, const char *, size_t);
#if !definedEx(CONFIG_X86_32) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_PARAVIRT)
extern char * strcat(char *, const char *);
#endif
#if !definedEx(CONFIG_X86_32) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_PARAVIRT)
extern char * strncat(char *, const char *, __kernel_size_t);
#endif
extern size_t strlcat(char *, const char *, __kernel_size_t);
#if !definedEx(CONFIG_X86_32) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_PARAVIRT)
extern int strcmp(const char *,const char *);
#endif
#if !definedEx(CONFIG_X86_32) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_PARAVIRT)
extern int strncmp(const char *,const char *,__kernel_size_t);
#endif
extern int strnicmp(const char *, const char *, __kernel_size_t);
extern int strcasecmp(const char *s1, const char *s2);
extern int strncasecmp(const char *s1, const char *s2, size_t n);
#if !definedEx(CONFIG_X86_32) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_PARAVIRT)
extern char * strchr(const char *,int);
#endif
extern char * strnchr(const char *, size_t, int);
extern char * strrchr(const char *,int);
extern char * 
#if definedEx(CONFIG_ENABLE_MUST_CHECK)
__attribute__((warn_unused_result))
#endif
#if !definedEx(CONFIG_ENABLE_MUST_CHECK)
#endif
 skip_spaces(const char *);
extern char *strim(char *);
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
#if definedEx(CONFIG_ENABLE_MUST_CHECK)
__attribute__((warn_unused_result))
#endif
#if !definedEx(CONFIG_ENABLE_MUST_CHECK)
#endif
 char *strstrip(char *str)
{
	return strim(str);
}
#if !definedEx(CONFIG_X86_32) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_PARAVIRT)
extern char * strstr(const char *, const char *);
#endif
extern char * strnstr(const char *, const char *, size_t);
#if !definedEx(CONFIG_X86_32) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_PARAVIRT)
extern __kernel_size_t strlen(const char *);
#endif
#if !definedEx(CONFIG_X86_32) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_PARAVIRT)
extern __kernel_size_t strnlen(const char *,__kernel_size_t);
#endif
extern char * strpbrk(const char *,const char *);
extern char * strsep(char **,const char *);
extern __kernel_size_t strspn(const char *,const char *);
extern __kernel_size_t strcspn(const char *,const char *);
#if !definedEx(CONFIG_X86_32) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_PARAVIRT)
extern void * memscan(void *,int,__kernel_size_t);
#endif
extern int 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
__builtin_memcmp
#endif
#if !definedEx(CONFIG_X86_32) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_PARAVIRT)
memcmp
#endif
(const void *,const void *,__kernel_size_t);
#if !definedEx(CONFIG_X86_32) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_PARAVIRT)
extern void * memchr(const void *,int,__kernel_size_t);
#endif
extern char *kstrdup(const char *s, gfp_t gfp);
extern char *kstrndup(const char *s, size_t len, gfp_t gfp);
extern void *kmemdup(const void *src, size_t len, gfp_t gfp);
extern char **argv_split(gfp_t gfp, const char *str, int *argcp);
extern void argv_free(char **argv);
extern bool sysfs_streq(const char *s1, const char *s2);
#if definedEx(CONFIG_BINARY_PRINTF)
int vbin_printf(u32 *bin_buf, size_t size, const char *fmt, va_list args);
int bstr_printf(char *buf, size_t size, const char *fmt, const u32 *bin_buf);
int bprintf(u32 *bin_buf, size_t size, const char *fmt, ...) __attribute__((format(printf,3,4)));
#endif
extern ssize_t memory_read_from_buffer(void *to, size_t count, loff_t *ppos,
			const void *from, size_t available);
/**
 * strstarts - does @str start with @prefix?
 * @str: string to examine
 * @prefix: prefix to look for.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 bool strstarts(const char *str, const char *prefix)
{
	return strncmp(str, prefix, strlen(prefix)) == 0;
}
#line 10 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/bitmap.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/kernel.h" 1
#line 11 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/bitmap.h" 2
/*
 * bitmaps provide bit arrays that consume one or more unsigned
 * longs.  The bitmap interface and available operations are listed
 * here, in bitmap.h
 *
 * Function implementations generic to all architectures are in
 * lib/bitmap.c.  Functions implementations that are architecture
 * specific are in various include/asm-<arch>/bitops.h headers
 * and other arch/<arch> specific files.
 *
 * See lib/bitmap.c for more details.
 */
/*
 * The available bitmap operations and their rough meaning in the
 * case that the bitmap is a single unsigned long are thus:
 *
 * Note that nbits should be always a compile time evaluable constant.
 * Otherwise many inlines will generate horrible code.
 *
 * bitmap_zero(dst, nbits)			*dst = 0UL
 * bitmap_fill(dst, nbits)			*dst = ~0UL
 * bitmap_copy(dst, src, nbits)			*dst = *src
 * bitmap_and(dst, src1, src2, nbits)		*dst = *src1 & *src2
 * bitmap_or(dst, src1, src2, nbits)		*dst = *src1 | *src2
 * bitmap_xor(dst, src1, src2, nbits)		*dst = *src1 ^ *src2
 * bitmap_andnot(dst, src1, src2, nbits)	*dst = *src1 & ~(*src2)
 * bitmap_complement(dst, src, nbits)		*dst = ~(*src)
 * bitmap_equal(src1, src2, nbits)		Are *src1 and *src2 equal?
 * bitmap_intersects(src1, src2, nbits) 	Do *src1 and *src2 overlap?
 * bitmap_subset(src1, src2, nbits)		Is *src1 a subset of *src2?
 * bitmap_empty(src, nbits)			Are all bits zero in *src?
 * bitmap_full(src, nbits)			Are all bits set in *src?
 * bitmap_weight(src, nbits)			Hamming Weight: number set bits
 * bitmap_set(dst, pos, nbits)			Set specified bit area
 * bitmap_clear(dst, pos, nbits)		Clear specified bit area
 * bitmap_find_next_zero_area(buf, len, pos, n, mask)	Find bit free area
 * bitmap_shift_right(dst, src, n, nbits)	*dst = *src >> n
 * bitmap_shift_left(dst, src, n, nbits)	*dst = *src << n
 * bitmap_remap(dst, src, old, new, nbits)	*dst = map(old, new)(src)
 * bitmap_bitremap(oldbit, old, new, nbits)	newbit = map(old, new)(oldbit)
 * bitmap_onto(dst, orig, relmap, nbits)	*dst = orig relative to relmap
 * bitmap_fold(dst, orig, sz, nbits)		dst bits = orig bits mod sz
 * bitmap_scnprintf(buf, len, src, nbits)	Print bitmap src to buf
 * bitmap_parse(buf, buflen, dst, nbits)	Parse bitmap dst from kernel buf
 * bitmap_parse_user(ubuf, ulen, dst, nbits)	Parse bitmap dst from user buf
 * bitmap_scnlistprintf(buf, len, src, nbits)	Print bitmap src as list to buf
 * bitmap_parselist(buf, dst, nbits)		Parse bitmap dst from list
 * bitmap_find_free_region(bitmap, bits, order)	Find and allocate bit region
 * bitmap_release_region(bitmap, pos, order)	Free specified bit region
 * bitmap_allocate_region(bitmap, pos, order)	Allocate specified bit region
 */
/*
 * Also the following operations in asm/bitops.h apply to bitmaps.
 *
 * set_bit(bit, addr)			*addr |= bit
 * clear_bit(bit, addr)			*addr &= ~bit
 * change_bit(bit, addr)		*addr ^= bit
 * test_bit(bit, addr)			Is bit set in *addr?
 * test_and_set_bit(bit, addr)		Set bit and return old value
 * test_and_clear_bit(bit, addr)	Clear bit and return old value
 * test_and_change_bit(bit, addr)	Change bit and return old value
 * find_first_zero_bit(addr, nbits)	Position first zero bit in *addr
 * find_first_bit(addr, nbits)		Position first set bit in *addr
 * find_next_zero_bit(addr, nbits, bit)	Position next zero bit in *addr >= bit
 * find_next_bit(addr, nbits, bit)	Position next set bit in *addr >= bit
 */
/*
 * The DECLARE_BITMAP(name,bits) macro, in linux/types.h, can be used
 * to declare an array named 'name' of just enough unsigned longs to
 * contain all bit positions from 0 to 'bits' - 1.
 */
/*
 * lib/bitmap.c provides these functions:
 */
extern int __bitmap_empty(const unsigned long *bitmap, int bits);
extern int __bitmap_full(const unsigned long *bitmap, int bits);
extern int __bitmap_equal(const unsigned long *bitmap1,
                	const unsigned long *bitmap2, int bits);
extern void __bitmap_complement(unsigned long *dst, const unsigned long *src,
			int bits);
extern void __bitmap_shift_right(unsigned long *dst,
                        const unsigned long *src, int shift, int bits);
extern void __bitmap_shift_left(unsigned long *dst,
                        const unsigned long *src, int shift, int bits);
extern int __bitmap_and(unsigned long *dst, const unsigned long *bitmap1,
			const unsigned long *bitmap2, int bits);
extern void __bitmap_or(unsigned long *dst, const unsigned long *bitmap1,
			const unsigned long *bitmap2, int bits);
extern void __bitmap_xor(unsigned long *dst, const unsigned long *bitmap1,
			const unsigned long *bitmap2, int bits);
extern int __bitmap_andnot(unsigned long *dst, const unsigned long *bitmap1,
			const unsigned long *bitmap2, int bits);
extern int __bitmap_intersects(const unsigned long *bitmap1,
			const unsigned long *bitmap2, int bits);
extern int __bitmap_subset(const unsigned long *bitmap1,
			const unsigned long *bitmap2, int bits);
extern int __bitmap_weight(const unsigned long *bitmap, int bits);
extern void bitmap_set(unsigned long *map, int i, int len);
extern void bitmap_clear(unsigned long *map, int start, int nr);
extern unsigned long bitmap_find_next_zero_area(unsigned long *map,
					 unsigned long size,
					 unsigned long start,
					 unsigned int nr,
					 unsigned long align_mask);
extern int bitmap_scnprintf(char *buf, unsigned int len,
			const unsigned long *src, int nbits);
extern int __bitmap_parse(const char *buf, unsigned int buflen, int is_user,
			unsigned long *dst, int nbits);
extern int bitmap_parse_user(const char  *ubuf, unsigned int ulen,
			unsigned long *dst, int nbits);
extern int bitmap_scnlistprintf(char *buf, unsigned int len,
			const unsigned long *src, int nbits);
extern int bitmap_parselist(const char *buf, unsigned long *maskp,
			int nmaskbits);
extern void bitmap_remap(unsigned long *dst, const unsigned long *src,
		const unsigned long *old, const unsigned long *new, int bits);
extern int bitmap_bitremap(int oldbit,
		const unsigned long *old, const unsigned long *new, int bits);
extern void bitmap_onto(unsigned long *dst, const unsigned long *orig,
		const unsigned long *relmap, int bits);
extern void bitmap_fold(unsigned long *dst, const unsigned long *orig,
		int sz, int bits);
extern int bitmap_find_free_region(unsigned long *bitmap, int bits, int order);
extern void bitmap_release_region(unsigned long *bitmap, int pos, int order);
extern int bitmap_allocate_region(unsigned long *bitmap, int pos, int order);
extern void bitmap_copy_le(void *dst, const unsigned long *src, int nbits);
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void bitmap_zero(unsigned long *dst, int nbits)
{
	if ((__builtin_constant_p(nbits) && (nbits) <= 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
))
		*dst = 0UL;
	else {
		int len = (((nbits) + (8 * sizeof(long)) - 1) / (8 * sizeof(long))) * sizeof(unsigned long);
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
__builtin_memset(dst, 0, len)
#endif
#if !definedEx(CONFIG_X86_32) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_PARAVIRT)
memset(dst, 0, len)
#endif
;
	}
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void bitmap_fill(unsigned long *dst, int nbits)
{
	size_t nlongs = (((nbits) + (8 * sizeof(long)) - 1) / (8 * sizeof(long)));
	if (!(__builtin_constant_p(nbits) && (nbits) <= 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
)) {
		int len = (nlongs - 1) * sizeof(unsigned long);
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
__builtin_memset(dst, 0xff, len)
#endif
#if !definedEx(CONFIG_X86_32) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_PARAVIRT)
memset(dst, 0xff,  len)
#endif
;
	}
	dst[nlongs - 1] = ( ((nbits) % 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
) ? (1UL<<((nbits) % 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
))-1 : ~0UL );
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void bitmap_copy(unsigned long *dst, const unsigned long *src,
			int nbits)
{
	if ((__builtin_constant_p(nbits) && (nbits) <= 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
))
		*dst = *src;
	else {
		int len = (((nbits) + (8 * sizeof(long)) - 1) / (8 * sizeof(long))) * sizeof(unsigned long);
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_X86_USE_3DNOW) && definedEx(CONFIG_PARAVIRT)
(__builtin_constant_p((len)) ? __constant_memcpy3d((dst), (src), (len)) : __memcpy3d((dst), (src), (len)))
#endif
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_X86_USE_3DNOW) && !definedEx(CONFIG_KMEMCHECK) && definedEx(CONFIG_PARAVIRT)
__builtin_memcpy(dst, src, len)
#endif
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_X86_USE_3DNOW) && definedEx(CONFIG_KMEMCHECK) && definedEx(CONFIG_PARAVIRT)
__memcpy((dst), (src), (len))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_KMEMCHECK) && definedEx(CONFIG_PARAVIRT)
__inline_memcpy((dst), (src), (len))
#endif
#if !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_KMEMCHECK) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_KMEMCHECK) && !definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_PARAVIRT)
memcpy(dst, src, len)
#endif
;
	}
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int bitmap_and(unsigned long *dst, const unsigned long *src1,
			const unsigned long *src2, int nbits)
{
	if ((__builtin_constant_p(nbits) && (nbits) <= 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
))
		return (*dst = *src1 & *src2) != 0;
	return __bitmap_and(dst, src1, src2, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void bitmap_or(unsigned long *dst, const unsigned long *src1,
			const unsigned long *src2, int nbits)
{
	if ((__builtin_constant_p(nbits) && (nbits) <= 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
))
		*dst = *src1 | *src2;
	else
		__bitmap_or(dst, src1, src2, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void bitmap_xor(unsigned long *dst, const unsigned long *src1,
			const unsigned long *src2, int nbits)
{
	if ((__builtin_constant_p(nbits) && (nbits) <= 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
))
		*dst = *src1 ^ *src2;
	else
		__bitmap_xor(dst, src1, src2, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int bitmap_andnot(unsigned long *dst, const unsigned long *src1,
			const unsigned long *src2, int nbits)
{
	if ((__builtin_constant_p(nbits) && (nbits) <= 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
))
		return (*dst = *src1 & ~(*src2)) != 0;
	return __bitmap_andnot(dst, src1, src2, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void bitmap_complement(unsigned long *dst, const unsigned long *src,
			int nbits)
{
	if ((__builtin_constant_p(nbits) && (nbits) <= 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
))
		*dst = ~(*src) & ( ((nbits) % 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
) ? (1UL<<((nbits) % 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
))-1 : ~0UL );
	else
		__bitmap_complement(dst, src, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int bitmap_equal(const unsigned long *src1,
			const unsigned long *src2, int nbits)
{
	if ((__builtin_constant_p(nbits) && (nbits) <= 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
))
		return ! ((*src1 ^ *src2) & ( ((nbits) % 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
) ? (1UL<<((nbits) % 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
))-1 : ~0UL ));
	else
		return __bitmap_equal(src1, src2, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int bitmap_intersects(const unsigned long *src1,
			const unsigned long *src2, int nbits)
{
	if ((__builtin_constant_p(nbits) && (nbits) <= 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
))
		return ((*src1 & *src2) & ( ((nbits) % 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
) ? (1UL<<((nbits) % 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
))-1 : ~0UL )) != 0;
	else
		return __bitmap_intersects(src1, src2, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int bitmap_subset(const unsigned long *src1,
			const unsigned long *src2, int nbits)
{
	if ((__builtin_constant_p(nbits) && (nbits) <= 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
))
		return ! ((*src1 & ~(*src2)) & ( ((nbits) % 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
) ? (1UL<<((nbits) % 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
))-1 : ~0UL ));
	else
		return __bitmap_subset(src1, src2, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int bitmap_empty(const unsigned long *src, int nbits)
{
	if ((__builtin_constant_p(nbits) && (nbits) <= 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
))
		return ! (*src & ( ((nbits) % 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
) ? (1UL<<((nbits) % 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
))-1 : ~0UL ));
	else
		return __bitmap_empty(src, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int bitmap_full(const unsigned long *src, int nbits)
{
	if ((__builtin_constant_p(nbits) && (nbits) <= 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
))
		return ! (~(*src) & ( ((nbits) % 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
) ? (1UL<<((nbits) % 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
))-1 : ~0UL ));
	else
		return __bitmap_full(src, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int bitmap_weight(const unsigned long *src, int nbits)
{
	if ((__builtin_constant_p(nbits) && (nbits) <= 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
))
		return hweight_long(*src & ( ((nbits) % 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
) ? (1UL<<((nbits) % 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
))-1 : ~0UL ));
	return __bitmap_weight(src, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void bitmap_shift_right(unsigned long *dst,
			const unsigned long *src, int n, int nbits)
{
	if ((__builtin_constant_p(nbits) && (nbits) <= 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
))
		*dst = *src >> n;
	else
		__bitmap_shift_right(dst, src, n, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void bitmap_shift_left(unsigned long *dst,
			const unsigned long *src, int n, int nbits)
{
	if ((__builtin_constant_p(nbits) && (nbits) <= 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
))
		*dst = (*src << n) & ( ((nbits) % 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
) ? (1UL<<((nbits) % 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
))-1 : ~0UL );
	else
		__bitmap_shift_left(dst, src, n, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int bitmap_parse(const char *buf, unsigned int buflen,
			unsigned long *maskp, int nmaskbits)
{
	return __bitmap_parse(buf, buflen, 0, maskp, nmaskbits);
}
#line 13 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/cpumask.h" 2
typedef struct cpumask { unsigned long bits[(((
#if definedEx(CONFIG_SMP)
8
#endif
#if !definedEx(CONFIG_SMP)
1
#endif
) + (8 * sizeof(long)) - 1) / (8 * sizeof(long)))]; } cpumask_t;
/**
 * cpumask_bits - get the bits in a cpumask
 * @maskp: the struct cpumask *
 *
 * You should only assume nr_cpu_ids bits of this mask are valid.  This is
 * a macro so it's const-correct.
 */
#if !definedEx(CONFIG_SMP)
#endif
#if definedEx(CONFIG_SMP)
extern int nr_cpu_ids;
#endif
#if definedEx(CONFIG_CPUMASK_OFFSTACK)
/* Assuming NR_CPUS is huge, a runtime limit is more efficient.  Also,
 * not all bits may be allocated. */
#endif
#if !definedEx(CONFIG_CPUMASK_OFFSTACK)
#endif
/*
 * The following particular system cpumasks and operations manage
 * possible, present, active and online cpus.
 *
 *     cpu_possible_mask- has bit 'cpu' set iff cpu is populatable
 *     cpu_present_mask - has bit 'cpu' set iff cpu is populated
 *     cpu_online_mask  - has bit 'cpu' set iff cpu available to scheduler
 *     cpu_active_mask  - has bit 'cpu' set iff cpu available to migration
 *
 *  If !CONFIG_HOTPLUG_CPU, present == possible, and active == online.
 *
 *  The cpu_possible_mask is fixed at boot time, as the set of CPU id's
 *  that it is possible might ever be plugged in at anytime during the
 *  life of that system boot.  The cpu_present_mask is dynamic(*),
 *  representing which CPUs are currently plugged in.  And
 *  cpu_online_mask is the dynamic subset of cpu_present_mask,
 *  indicating those CPUs available for scheduling.
 *
 *  If HOTPLUG is enabled, then cpu_possible_mask is forced to have
 *  all NR_CPUS bits set, otherwise it is just the set of CPUs that
 *  ACPI reports present at boot.
 *
 *  If HOTPLUG is enabled, then cpu_present_mask varies dynamically,
 *  depending on what ACPI reports as currently plugged in, otherwise
 *  cpu_present_mask is just a copy of cpu_possible_mask.
 *
 *  (*) Well, cpu_present_mask is dynamic in the hotplug case.  If not
 *      hotplug, it's a copy of cpu_possible_mask, hence fixed at boot.
 *
 * Subtleties:
 * 1) UP arch's (NR_CPUS == 1, CONFIG_SMP not defined) hardcode
 *    assumption that their single CPU is online.  The UP
 *    cpu_{online,possible,present}_masks are placebos.  Changing them
 *    will have no useful affect on the following num_*_cpus()
 *    and cpu_*() macros in the UP case.  This ugliness is a UP
 *    optimization - don't waste any instructions or memory references
 *    asking if you're online or how many CPUs there are if there is
 *    only one CPU.
 */
extern const struct cpumask *const cpu_possible_mask;
extern const struct cpumask *const cpu_online_mask;
extern const struct cpumask *const cpu_present_mask;
extern const struct cpumask *const cpu_active_mask;
#if definedEx(CONFIG_SMP)
#endif
#if !definedEx(CONFIG_SMP)
#endif
/* verify cpu argument to cpumask_* operators */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned int cpumask_check(unsigned int cpu)
{
#if definedEx(CONFIG_DEBUG_PER_CPU_MAPS)
	({ static bool __warned; int __ret_warn_once = !!(cpu >= 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_CPUMASK_OFFSTACK)
#if !definedEx(CONFIG_SMP) && definedEx(CONFIG_PARAVIRT)
1
#endif
#if !definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_SMP)
nr_cpu_ids
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_CPUMASK_OFFSTACK)
#if definedEx(CONFIG_SMP)
8
#endif
#if !definedEx(CONFIG_SMP)
1
#endif
#endif
); if (__builtin_expect(!!(__ret_warn_once), 0)) if (
#if definedEx(CONFIG_BUG)
({ int __ret_warn_on = !!(!__warned); if (__builtin_expect(!!(__ret_warn_on), 0)) warn_slowpath_null("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/cpumask.h", 107); __builtin_expect(!!(__ret_warn_on), 0); })
#endif
#if !definedEx(CONFIG_BUG)
({ int __ret_warn_on = !!(!__warned); __builtin_expect(!!(__ret_warn_on), 0); })
#endif
) __warned = true; __builtin_expect(!!(__ret_warn_once), 0); });
#endif
	return cpu;
}
#if !definedEx(CONFIG_SMP)
/* Uniprocessor.  Assume all masks are "1". */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned int cpumask_first(const struct cpumask *srcp)
{
	return 0;
}
/* Valid inputs for n are -1 and 0. */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned int cpumask_next(int n, const struct cpumask *srcp)
{
	return n+1;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned int cpumask_next_zero(int n, const struct cpumask *srcp)
{
	return n+1;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned int cpumask_next_and(int n,
					    const struct cpumask *srcp,
					    const struct cpumask *andp)
{
	return n+1;
}
/* cpu must be a valid cpu, ie 0, so there's no other choice. */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned int cpumask_any_but(const struct cpumask *mask,
					   unsigned int cpu)
{
	return 1;
}
#endif
#if definedEx(CONFIG_SMP)
/**
 * cpumask_first - get the first cpu in a cpumask
 * @srcp: the cpumask pointer
 *
 * Returns >= nr_cpu_ids if no cpus set.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned int cpumask_first(const struct cpumask *srcp)
{
	return find_first_bit(((srcp)->bits), 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_CPUMASK_OFFSTACK)
nr_cpu_ids
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_CPUMASK_OFFSTACK)
8
#endif
);
}
/**
 * cpumask_next - get the next cpu in a cpumask
 * @n: the cpu prior to the place to search (ie. return will be > @n)
 * @srcp: the cpumask pointer
 *
 * Returns >= nr_cpu_ids if no further cpus set.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned int cpumask_next(int n, const struct cpumask *srcp)
{
	/* -1 is a legal arg here. */
	if (n != -1)
		cpumask_check(n);
	return find_next_bit(((srcp)->bits), 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_CPUMASK_OFFSTACK)
nr_cpu_ids
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_CPUMASK_OFFSTACK)
8
#endif
, n+1);
}
/**
 * cpumask_next_zero - get the next unset cpu in a cpumask
 * @n: the cpu prior to the place to search (ie. return will be > @n)
 * @srcp: the cpumask pointer
 *
 * Returns >= nr_cpu_ids if no further cpus unset.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned int cpumask_next_zero(int n, const struct cpumask *srcp)
{
	/* -1 is a legal arg here. */
	if (n != -1)
		cpumask_check(n);
	return find_next_zero_bit(((srcp)->bits), 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_CPUMASK_OFFSTACK)
nr_cpu_ids
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_CPUMASK_OFFSTACK)
8
#endif
, n+1);
}
int cpumask_next_and(int n, const struct cpumask *, const struct cpumask *);
int cpumask_any_but(const struct cpumask *mask, unsigned int cpu);
/**
 * for_each_cpu - iterate over every cpu in a mask
 * @cpu: the (optionally unsigned) integer iterator
 * @mask: the cpumask pointer
 *
 * After the loop, cpu is >= nr_cpu_ids.
 */
/**
 * for_each_cpu_and - iterate over every cpu in both masks
 * @cpu: the (optionally unsigned) integer iterator
 * @mask: the first cpumask pointer
 * @and: the second cpumask pointer
 *
 * This saves a temporary CPU mask in many places.  It is equivalent to:
 *	struct cpumask tmp;
 *	cpumask_and(&tmp, &mask, &and);
 *	for_each_cpu(cpu, &tmp)
 *		...
 *
 * After the loop, cpu is >= nr_cpu_ids.
 */
#endif
/**
 * cpumask_set_cpu - set a cpu in a cpumask
 * @cpu: cpu number (< nr_cpu_ids)
 * @dstp: the cpumask pointer
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void cpumask_set_cpu(unsigned int cpu, struct cpumask *dstp)
{
	set_bit(cpumask_check(cpu), ((dstp)->bits));
}
/**
 * cpumask_clear_cpu - clear a cpu in a cpumask
 * @cpu: cpu number (< nr_cpu_ids)
 * @dstp: the cpumask pointer
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void cpumask_clear_cpu(int cpu, struct cpumask *dstp)
{
	clear_bit(cpumask_check(cpu), ((dstp)->bits));
}
/**
 * cpumask_test_cpu - test for a cpu in a cpumask
 * @cpu: cpu number (< nr_cpu_ids)
 * @cpumask: the cpumask pointer
 *
 * No static inline type checking - see Subtlety (1) above.
 */
/**
 * cpumask_test_and_set_cpu - atomically test and set a cpu in a cpumask
 * @cpu: cpu number (< nr_cpu_ids)
 * @cpumask: the cpumask pointer
 *
 * test_and_set_bit wrapper for cpumasks.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int cpumask_test_and_set_cpu(int cpu, struct cpumask *cpumask)
{
	return test_and_set_bit(cpumask_check(cpu), ((cpumask)->bits));
}
/**
 * cpumask_test_and_clear_cpu - atomically test and clear a cpu in a cpumask
 * @cpu: cpu number (< nr_cpu_ids)
 * @cpumask: the cpumask pointer
 *
 * test_and_clear_bit wrapper for cpumasks.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int cpumask_test_and_clear_cpu(int cpu, struct cpumask *cpumask)
{
	return test_and_clear_bit(cpumask_check(cpu), ((cpumask)->bits));
}
/**
 * cpumask_setall - set all cpus (< nr_cpu_ids) in a cpumask
 * @dstp: the cpumask pointer
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void cpumask_setall(struct cpumask *dstp)
{
	bitmap_fill(((dstp)->bits), 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_CPUMASK_OFFSTACK)
#if !definedEx(CONFIG_SMP) && definedEx(CONFIG_PARAVIRT)
1
#endif
#if !definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_SMP)
nr_cpu_ids
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_CPUMASK_OFFSTACK)
#if definedEx(CONFIG_SMP)
8
#endif
#if !definedEx(CONFIG_SMP)
1
#endif
#endif
);
}
/**
 * cpumask_clear - clear all cpus (< nr_cpu_ids) in a cpumask
 * @dstp: the cpumask pointer
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void cpumask_clear(struct cpumask *dstp)
{
	bitmap_zero(((dstp)->bits), 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_CPUMASK_OFFSTACK)
#if !definedEx(CONFIG_SMP) && definedEx(CONFIG_PARAVIRT)
1
#endif
#if !definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_SMP)
nr_cpu_ids
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_CPUMASK_OFFSTACK)
#if definedEx(CONFIG_SMP)
8
#endif
#if !definedEx(CONFIG_SMP)
1
#endif
#endif
);
}
/**
 * cpumask_and - *dstp = *src1p & *src2p
 * @dstp: the cpumask result
 * @src1p: the first input
 * @src2p: the second input
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int cpumask_and(struct cpumask *dstp,
			       const struct cpumask *src1p,
			       const struct cpumask *src2p)
{
	return bitmap_and(((dstp)->bits), ((src1p)->bits),
				       ((src2p)->bits), 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_CPUMASK_OFFSTACK)
#if !definedEx(CONFIG_SMP) && definedEx(CONFIG_PARAVIRT)
1
#endif
#if !definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_SMP)
nr_cpu_ids
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_CPUMASK_OFFSTACK)
#if definedEx(CONFIG_SMP)
8
#endif
#if !definedEx(CONFIG_SMP)
1
#endif
#endif
);
}
/**
 * cpumask_or - *dstp = *src1p | *src2p
 * @dstp: the cpumask result
 * @src1p: the first input
 * @src2p: the second input
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void cpumask_or(struct cpumask *dstp, const struct cpumask *src1p,
			      const struct cpumask *src2p)
{
	bitmap_or(((dstp)->bits), ((src1p)->bits),
				      ((src2p)->bits), 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_CPUMASK_OFFSTACK)
#if !definedEx(CONFIG_SMP) && definedEx(CONFIG_PARAVIRT)
1
#endif
#if !definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_SMP)
nr_cpu_ids
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_CPUMASK_OFFSTACK)
#if definedEx(CONFIG_SMP)
8
#endif
#if !definedEx(CONFIG_SMP)
1
#endif
#endif
);
}
/**
 * cpumask_xor - *dstp = *src1p ^ *src2p
 * @dstp: the cpumask result
 * @src1p: the first input
 * @src2p: the second input
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void cpumask_xor(struct cpumask *dstp,
			       const struct cpumask *src1p,
			       const struct cpumask *src2p)
{
	bitmap_xor(((dstp)->bits), ((src1p)->bits),
				       ((src2p)->bits), 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_CPUMASK_OFFSTACK)
#if !definedEx(CONFIG_SMP) && definedEx(CONFIG_PARAVIRT)
1
#endif
#if !definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_SMP)
nr_cpu_ids
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_CPUMASK_OFFSTACK)
#if definedEx(CONFIG_SMP)
8
#endif
#if !definedEx(CONFIG_SMP)
1
#endif
#endif
);
}
/**
 * cpumask_andnot - *dstp = *src1p & ~*src2p
 * @dstp: the cpumask result
 * @src1p: the first input
 * @src2p: the second input
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int cpumask_andnot(struct cpumask *dstp,
				  const struct cpumask *src1p,
				  const struct cpumask *src2p)
{
	return bitmap_andnot(((dstp)->bits), ((src1p)->bits),
					  ((src2p)->bits), 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_CPUMASK_OFFSTACK)
#if !definedEx(CONFIG_SMP) && definedEx(CONFIG_PARAVIRT)
1
#endif
#if !definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_SMP)
nr_cpu_ids
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_CPUMASK_OFFSTACK)
#if definedEx(CONFIG_SMP)
8
#endif
#if !definedEx(CONFIG_SMP)
1
#endif
#endif
);
}
/**
 * cpumask_complement - *dstp = ~*srcp
 * @dstp: the cpumask result
 * @srcp: the input to invert
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void cpumask_complement(struct cpumask *dstp,
				      const struct cpumask *srcp)
{
	bitmap_complement(((dstp)->bits), ((srcp)->bits),
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_CPUMASK_OFFSTACK)
#if !definedEx(CONFIG_SMP) && definedEx(CONFIG_PARAVIRT)
1
#endif
#if !definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_SMP)
nr_cpu_ids
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_CPUMASK_OFFSTACK)
#if definedEx(CONFIG_SMP)
8
#endif
#if !definedEx(CONFIG_SMP)
1
#endif
#endif
);
}
/**
 * cpumask_equal - *src1p == *src2p
 * @src1p: the first input
 * @src2p: the second input
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 bool cpumask_equal(const struct cpumask *src1p,
				const struct cpumask *src2p)
{
	return bitmap_equal(((src1p)->bits), ((src2p)->bits),
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_CPUMASK_OFFSTACK)
#if !definedEx(CONFIG_SMP) && definedEx(CONFIG_PARAVIRT)
1
#endif
#if !definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_SMP)
nr_cpu_ids
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_CPUMASK_OFFSTACK)
#if definedEx(CONFIG_SMP)
8
#endif
#if !definedEx(CONFIG_SMP)
1
#endif
#endif
);
}
/**
 * cpumask_intersects - (*src1p & *src2p) != 0
 * @src1p: the first input
 * @src2p: the second input
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 bool cpumask_intersects(const struct cpumask *src1p,
				     const struct cpumask *src2p)
{
	return bitmap_intersects(((src1p)->bits), ((src2p)->bits),
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_CPUMASK_OFFSTACK)
#if !definedEx(CONFIG_SMP) && definedEx(CONFIG_PARAVIRT)
1
#endif
#if !definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_SMP)
nr_cpu_ids
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_CPUMASK_OFFSTACK)
#if definedEx(CONFIG_SMP)
8
#endif
#if !definedEx(CONFIG_SMP)
1
#endif
#endif
);
}
/**
 * cpumask_subset - (*src1p & ~*src2p) == 0
 * @src1p: the first input
 * @src2p: the second input
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int cpumask_subset(const struct cpumask *src1p,
				 const struct cpumask *src2p)
{
	return bitmap_subset(((src1p)->bits), ((src2p)->bits),
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_CPUMASK_OFFSTACK)
#if !definedEx(CONFIG_SMP) && definedEx(CONFIG_PARAVIRT)
1
#endif
#if !definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_SMP)
nr_cpu_ids
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_CPUMASK_OFFSTACK)
#if definedEx(CONFIG_SMP)
8
#endif
#if !definedEx(CONFIG_SMP)
1
#endif
#endif
);
}
/**
 * cpumask_empty - *srcp == 0
 * @srcp: the cpumask to that all cpus < nr_cpu_ids are clear.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 bool cpumask_empty(const struct cpumask *srcp)
{
	return bitmap_empty(((srcp)->bits), 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_CPUMASK_OFFSTACK)
#if !definedEx(CONFIG_SMP) && definedEx(CONFIG_PARAVIRT)
1
#endif
#if !definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_SMP)
nr_cpu_ids
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_CPUMASK_OFFSTACK)
#if definedEx(CONFIG_SMP)
8
#endif
#if !definedEx(CONFIG_SMP)
1
#endif
#endif
);
}
/**
 * cpumask_full - *srcp == 0xFFFFFFFF...
 * @srcp: the cpumask to that all cpus < nr_cpu_ids are set.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 bool cpumask_full(const struct cpumask *srcp)
{
	return bitmap_full(((srcp)->bits), 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_CPUMASK_OFFSTACK)
#if !definedEx(CONFIG_SMP) && definedEx(CONFIG_PARAVIRT)
1
#endif
#if !definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_SMP)
nr_cpu_ids
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_CPUMASK_OFFSTACK)
#if definedEx(CONFIG_SMP)
8
#endif
#if !definedEx(CONFIG_SMP)
1
#endif
#endif
);
}
/**
 * cpumask_weight - Count of bits in *srcp
 * @srcp: the cpumask to count bits (< nr_cpu_ids) in.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned int cpumask_weight(const struct cpumask *srcp)
{
	return bitmap_weight(((srcp)->bits), 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_CPUMASK_OFFSTACK)
#if !definedEx(CONFIG_SMP) && definedEx(CONFIG_PARAVIRT)
1
#endif
#if !definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_SMP)
nr_cpu_ids
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_CPUMASK_OFFSTACK)
#if definedEx(CONFIG_SMP)
8
#endif
#if !definedEx(CONFIG_SMP)
1
#endif
#endif
);
}
/**
 * cpumask_shift_right - *dstp = *srcp >> n
 * @dstp: the cpumask result
 * @srcp: the input to shift
 * @n: the number of bits to shift by
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void cpumask_shift_right(struct cpumask *dstp,
				       const struct cpumask *srcp, int n)
{
	bitmap_shift_right(((dstp)->bits), ((srcp)->bits), n,
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_CPUMASK_OFFSTACK)
#if !definedEx(CONFIG_SMP) && definedEx(CONFIG_PARAVIRT)
1
#endif
#if !definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_SMP)
nr_cpu_ids
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_CPUMASK_OFFSTACK)
#if definedEx(CONFIG_SMP)
8
#endif
#if !definedEx(CONFIG_SMP)
1
#endif
#endif
);
}
/**
 * cpumask_shift_left - *dstp = *srcp << n
 * @dstp: the cpumask result
 * @srcp: the input to shift
 * @n: the number of bits to shift by
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void cpumask_shift_left(struct cpumask *dstp,
				      const struct cpumask *srcp, int n)
{
	bitmap_shift_left(((dstp)->bits), ((srcp)->bits), n,
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_CPUMASK_OFFSTACK)
#if !definedEx(CONFIG_SMP) && definedEx(CONFIG_PARAVIRT)
1
#endif
#if !definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_SMP)
nr_cpu_ids
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_CPUMASK_OFFSTACK)
#if definedEx(CONFIG_SMP)
8
#endif
#if !definedEx(CONFIG_SMP)
1
#endif
#endif
);
}
/**
 * cpumask_copy - *dstp = *srcp
 * @dstp: the result
 * @srcp: the input cpumask
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void cpumask_copy(struct cpumask *dstp,
				const struct cpumask *srcp)
{
	bitmap_copy(((dstp)->bits), ((srcp)->bits), 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_CPUMASK_OFFSTACK)
#if !definedEx(CONFIG_SMP) && definedEx(CONFIG_PARAVIRT)
1
#endif
#if !definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_SMP)
nr_cpu_ids
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_CPUMASK_OFFSTACK)
#if definedEx(CONFIG_SMP)
8
#endif
#if !definedEx(CONFIG_SMP)
1
#endif
#endif
);
}
/**
 * cpumask_any - pick a "random" cpu from *srcp
 * @srcp: the input cpumask
 *
 * Returns >= nr_cpu_ids if no cpus set.
 */
/**
 * cpumask_first_and - return the first cpu from *srcp1 & *srcp2
 * @src1p: the first input
 * @src2p: the second input
 *
 * Returns >= nr_cpu_ids if no cpus set in both.  See also cpumask_next_and().
 */
/**
 * cpumask_any_and - pick a "random" cpu from *mask1 & *mask2
 * @mask1: the first input cpumask
 * @mask2: the second input cpumask
 *
 * Returns >= nr_cpu_ids if no cpus set.
 */
/**
 * cpumask_of - the cpumask containing just a given cpu
 * @cpu: the cpu (<= nr_cpu_ids)
 */
/**
 * cpumask_scnprintf - print a cpumask into a string as comma-separated hex
 * @buf: the buffer to sprintf into
 * @len: the length of the buffer
 * @srcp: the cpumask to print
 *
 * If len is zero, returns zero.  Otherwise returns the length of the
 * (nul-terminated) @buf string.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int cpumask_scnprintf(char *buf, int len,
				    const struct cpumask *srcp)
{
	return bitmap_scnprintf(buf, len, ((srcp)->bits), 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_CPUMASK_OFFSTACK)
#if !definedEx(CONFIG_SMP) && definedEx(CONFIG_PARAVIRT)
1
#endif
#if !definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_SMP)
nr_cpu_ids
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_CPUMASK_OFFSTACK)
#if definedEx(CONFIG_SMP)
8
#endif
#if !definedEx(CONFIG_SMP)
1
#endif
#endif
);
}
/**
 * cpumask_parse_user - extract a cpumask from a user string
 * @buf: the buffer to extract from
 * @len: the length of the buffer
 * @dstp: the cpumask to set.
 *
 * Returns -errno, or 0 for success.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int cpumask_parse_user(const char  *buf, int len,
				     struct cpumask *dstp)
{
	return bitmap_parse_user(buf, len, ((dstp)->bits), 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_CPUMASK_OFFSTACK)
#if !definedEx(CONFIG_SMP) && definedEx(CONFIG_PARAVIRT)
1
#endif
#if !definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_SMP)
nr_cpu_ids
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_CPUMASK_OFFSTACK)
#if definedEx(CONFIG_SMP)
8
#endif
#if !definedEx(CONFIG_SMP)
1
#endif
#endif
);
}
/**
 * cpulist_scnprintf - print a cpumask into a string as comma-separated list
 * @buf: the buffer to sprintf into
 * @len: the length of the buffer
 * @srcp: the cpumask to print
 *
 * If len is zero, returns zero.  Otherwise returns the length of the
 * (nul-terminated) @buf string.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int cpulist_scnprintf(char *buf, int len,
				    const struct cpumask *srcp)
{
	return bitmap_scnlistprintf(buf, len, ((srcp)->bits),
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_CPUMASK_OFFSTACK)
#if !definedEx(CONFIG_SMP) && definedEx(CONFIG_PARAVIRT)
1
#endif
#if !definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_SMP)
nr_cpu_ids
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_CPUMASK_OFFSTACK)
#if definedEx(CONFIG_SMP)
8
#endif
#if !definedEx(CONFIG_SMP)
1
#endif
#endif
);
}
/**
 * cpulist_parse_user - extract a cpumask from a user string of ranges
 * @buf: the buffer to extract from
 * @len: the length of the buffer
 * @dstp: the cpumask to set.
 *
 * Returns -errno, or 0 for success.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int cpulist_parse(const char *buf, struct cpumask *dstp)
{
	return bitmap_parselist(buf, ((dstp)->bits), 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_CPUMASK_OFFSTACK)
#if !definedEx(CONFIG_SMP) && definedEx(CONFIG_PARAVIRT)
1
#endif
#if !definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_SMP)
nr_cpu_ids
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_CPUMASK_OFFSTACK)
#if definedEx(CONFIG_SMP)
8
#endif
#if !definedEx(CONFIG_SMP)
1
#endif
#endif
);
}
/**
 * cpumask_size - size to allocate for a 'struct cpumask' in bytes
 *
 * This will eventually be a runtime variable, depending on nr_cpu_ids.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 size_t cpumask_size(void)
{
	/* FIXME: Once all cpumask assignments are eliminated, this
	 * can be nr_cpumask_bits */
	return (((
#if definedEx(CONFIG_SMP)
8
#endif
#if !definedEx(CONFIG_SMP)
1
#endif
) + (8 * sizeof(long)) - 1) / (8 * sizeof(long))) * sizeof(long);
}
/*
 * cpumask_var_t: struct cpumask for stack usage.
 *
 * Oh, the wicked games we play!  In order to make kernel coding a
 * little more difficult, we typedef cpumask_var_t to an array or a
 * pointer: doing &mask on an array is a noop, so it still works.
 *
 * ie.
 *	cpumask_var_t tmpmask;
 *	if (!alloc_cpumask_var(&tmpmask, GFP_KERNEL))
 *		return -ENOMEM;
 *
 *	  ... use 'tmpmask' like a normal struct cpumask * ...
 *
 *	free_cpumask_var(tmpmask);
 */
#if definedEx(CONFIG_CPUMASK_OFFSTACK)
typedef struct cpumask *cpumask_var_t;
bool alloc_cpumask_var_node(cpumask_var_t *mask, gfp_t flags, int node);
bool alloc_cpumask_var(cpumask_var_t *mask, gfp_t flags);
bool zalloc_cpumask_var_node(cpumask_var_t *mask, gfp_t flags, int node);
bool zalloc_cpumask_var(cpumask_var_t *mask, gfp_t flags);
void alloc_bootmem_cpumask_var(cpumask_var_t *mask);
void free_cpumask_var(cpumask_var_t mask);
void free_bootmem_cpumask_var(cpumask_var_t mask);
#endif
#if !definedEx(CONFIG_CPUMASK_OFFSTACK)
typedef struct cpumask cpumask_var_t[1];
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 bool alloc_cpumask_var(cpumask_var_t *mask, gfp_t flags)
{
	return true;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 bool alloc_cpumask_var_node(cpumask_var_t *mask, gfp_t flags,
					  int node)
{
	return true;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 bool zalloc_cpumask_var(cpumask_var_t *mask, gfp_t flags)
{
	cpumask_clear(*mask);
	return true;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 bool zalloc_cpumask_var_node(cpumask_var_t *mask, gfp_t flags,
					  int node)
{
	cpumask_clear(*mask);
	return true;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void alloc_bootmem_cpumask_var(cpumask_var_t *mask)
{
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void free_cpumask_var(cpumask_var_t mask)
{
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void free_bootmem_cpumask_var(cpumask_var_t mask)
{
}
#endif
/* It's common to want to use cpu_all_mask in struct member initializers,
 * so it has to refer to an address rather than a pointer. */
extern const unsigned long cpu_all_bits[(((
#if definedEx(CONFIG_SMP)
8
#endif
#if !definedEx(CONFIG_SMP)
1
#endif
) + (8 * sizeof(long)) - 1) / (8 * sizeof(long)))];
/* First bits of cpu_bit_bitmap are in fact unset. */
/* Wrappers for arch boot code to manipulate normally-constant masks */
void set_cpu_possible(unsigned int cpu, bool possible);
void set_cpu_present(unsigned int cpu, bool present);
void set_cpu_online(unsigned int cpu, bool online);
void set_cpu_active(unsigned int cpu, bool active);
void init_cpu_present(const struct cpumask *src);
void init_cpu_possible(const struct cpumask *src);
void init_cpu_online(const struct cpumask *src);
/**
 * to_cpumask - convert an NR_CPUS bitmap to a struct cpumask *
 * @bitmap: the bitmap
 *
 * There are a few places where cpumask_var_t isn't appropriate and
 * static cpumasks must be used (eg. very early boot), yet we don't
 * expose the definition of 'struct cpumask'.
 *
 * This does the conversion, and can be used as a constant initializer.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int __check_is_bitmap(const unsigned long *bitmap)
{
	return 1;
}
/*
 * Special-case data structure for "single bit set only" constant CPU masks.
 *
 * We pre-generate all the 64 (or 32) possible bit positions, with enough
 * padding to the left and the right, and return the constant pointer
 * appropriately offset.
 */
extern const unsigned long
	cpu_bit_bitmap[
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
+1][(((
#if definedEx(CONFIG_SMP)
8
#endif
#if !definedEx(CONFIG_SMP)
1
#endif
) + (8 * sizeof(long)) - 1) / (8 * sizeof(long)))];
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 const struct cpumask *get_cpu_mask(unsigned int cpu)
{
	const unsigned long *p = cpu_bit_bitmap[1 + cpu % 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
];
	p -= cpu / 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
;
	return ((struct cpumask *)(1 ? (p) : (void *)sizeof(__check_is_bitmap(p))));
}
/*
 *
 * From here down, all obsolete.  Use cpumask_ variants!
 *
 */
/* These strip const, as traditionally they weren't const. */
#if !definedEx(CONFIG_SMP)
#endif
#if definedEx(CONFIG_SMP)
int __first_cpu(const cpumask_t *srcp);
int __next_cpu(int n, const cpumask_t *srcp);
int __any_online_cpu(const cpumask_t *mask);
#endif
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __cpu_set(int cpu, volatile cpumask_t *dstp)
{
	set_bit(cpu, dstp->bits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __cpu_clear(int cpu, volatile cpumask_t *dstp)
{
	clear_bit(cpu, dstp->bits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __cpus_setall(cpumask_t *dstp, int nbits)
{
	bitmap_fill(dstp->bits, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __cpus_clear(cpumask_t *dstp, int nbits)
{
	bitmap_zero(dstp->bits, nbits);
}
/* No static inline type checking - see Subtlety (1) above. */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int __cpu_test_and_set(int cpu, cpumask_t *addr)
{
	return test_and_set_bit(cpu, addr->bits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int __cpus_and(cpumask_t *dstp, const cpumask_t *src1p,
					const cpumask_t *src2p, int nbits)
{
	return bitmap_and(dstp->bits, src1p->bits, src2p->bits, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __cpus_or(cpumask_t *dstp, const cpumask_t *src1p,
					const cpumask_t *src2p, int nbits)
{
	bitmap_or(dstp->bits, src1p->bits, src2p->bits, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __cpus_xor(cpumask_t *dstp, const cpumask_t *src1p,
					const cpumask_t *src2p, int nbits)
{
	bitmap_xor(dstp->bits, src1p->bits, src2p->bits, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int __cpus_andnot(cpumask_t *dstp, const cpumask_t *src1p,
					const cpumask_t *src2p, int nbits)
{
	return bitmap_andnot(dstp->bits, src1p->bits, src2p->bits, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int __cpus_equal(const cpumask_t *src1p,
					const cpumask_t *src2p, int nbits)
{
	return bitmap_equal(src1p->bits, src2p->bits, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int __cpus_intersects(const cpumask_t *src1p,
					const cpumask_t *src2p, int nbits)
{
	return bitmap_intersects(src1p->bits, src2p->bits, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int __cpus_subset(const cpumask_t *src1p,
					const cpumask_t *src2p, int nbits)
{
	return bitmap_subset(src1p->bits, src2p->bits, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int __cpus_empty(const cpumask_t *srcp, int nbits)
{
	return bitmap_empty(srcp->bits, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int __cpus_weight(const cpumask_t *srcp, int nbits)
{
	return bitmap_weight(srcp->bits, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __cpus_shift_left(cpumask_t *dstp,
					const cpumask_t *srcp, int n, int nbits)
{
	bitmap_shift_left(dstp->bits, srcp->bits, n, nbits);
}
#line 16 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h" 2
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int paravirt_enabled(void)
{
	return pv_info.paravirt_enabled;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void load_sp0(struct tss_struct *tss,
			     struct thread_struct *thread)
{
	({ 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_cpu_ops.load_sp0 ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (24), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_cpu_ops.load_sp0 == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_cpu_ops.load_sp0)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_cpu_ops.load_sp0) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_cpu_ops.load_sp0)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(tss))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(tss))
#endif
, 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"d" ((unsigned long)(thread))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"S" ((unsigned long)(thread))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "rax", "r8", "r9", "r10", "r11"
#endif
); });
}
/* The paravirtualized CPUID instruction. */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __cpuid(unsigned int *eax, unsigned int *ebx,
			   unsigned int *ecx, unsigned int *edx)
{
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
({ unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_cpu_ops.cpuid ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b, %c0\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (31), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_cpu_ops.cpuid == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_cpu_ops.cpuid)
#endif
; asm volatile("push %[_arg4];" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" " " ".balign 4" " " "\n" " " ".long" " " " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "lea 4(%%esp),%%esp;" : "=a" (__eax), "=d" (__edx), "=c" (__ecx) : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_cpu_ops.cpuid) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_cpu_ops.cpuid)), [paravirt_clobber] "i" (((1 << 4) - 1)),"0"((u32)(eax)), "1"((u32)(ebx)), "2"((u32)(ecx)), [_arg4] "mr"((u32)(edx)) : "memory", "cc" ); })
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
({ unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_cpu_ops.cpuid ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (31), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_cpu_ops.cpuid == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_cpu_ops.cpuid)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" " " ".balign 8" " " "\n" " " ".quad" " " " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : "=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx) : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_cpu_ops.cpuid) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_cpu_ops.cpuid)), [paravirt_clobber] "i" (((1 << 9) - 1)),"D" ((unsigned long)(eax)), "S" ((unsigned long)(ebx)), "d" ((unsigned long)(ecx)), "c" ((unsigned long)(edx)) : "memory", "cc" , "rax", "r8", "r9", "r10", "r11"); })
#endif
;
}
/*
 * These special macros can be used to get or set a debugging register
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long paravirt_get_debugreg(int reg)
{
	return ({ unsigned long __ret; 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_cpu_ops.get_debugreg ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (39), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_cpu_ops.get_debugreg == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_cpu_ops.get_debugreg)
#endif
; if (sizeof(unsigned long) > sizeof(unsigned long)) { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
 "=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
, "=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_cpu_ops.get_debugreg) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_cpu_ops.get_debugreg)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(reg))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(reg))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "r8", "r9", "r10", "r11"
#endif
); __ret = (unsigned long)((((u64)__edx) << 32) | __eax); } else { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
 "=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
, "=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_cpu_ops.get_debugreg) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_cpu_ops.get_debugreg)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(reg))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(reg))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "r8", "r9", "r10", "r11"
#endif
); __ret = (unsigned long)__eax; } __ret; });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void set_debugreg(unsigned long val, int reg)
{
	({ 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_cpu_ops.set_debugreg ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (44), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_cpu_ops.set_debugreg == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_cpu_ops.set_debugreg)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_cpu_ops.set_debugreg) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_cpu_ops.set_debugreg)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(reg))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(reg))
#endif
, 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"d" ((unsigned long)(val))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"S" ((unsigned long)(val))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "rax", "r8", "r9", "r10", "r11"
#endif
); });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void clts(void)
{
	({ 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_cpu_ops.clts ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (49), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_cpu_ops.clts == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_cpu_ops.clts)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_cpu_ops.clts) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_cpu_ops.clts)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
) : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "rax", "r8", "r9", "r10", "r11"
#endif
); });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long read_cr0(void)
{
	return ({ unsigned long __ret; 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_cpu_ops.read_cr0 ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (54), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_cpu_ops.read_cr0 == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_cpu_ops.read_cr0)
#endif
; if (sizeof(unsigned long) > sizeof(unsigned long)) { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
 "=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
, "=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_cpu_ops.read_cr0) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_cpu_ops.read_cr0)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
) : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "r8", "r9", "r10", "r11"
#endif
); __ret = (unsigned long)((((u64)__edx) << 32) | __eax); } else { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
 "=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
, "=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_cpu_ops.read_cr0) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_cpu_ops.read_cr0)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
) : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "r8", "r9", "r10", "r11"
#endif
); __ret = (unsigned long)__eax; } __ret; });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void write_cr0(unsigned long x)
{
	({ 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_cpu_ops.write_cr0 ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (59), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_cpu_ops.write_cr0 == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_cpu_ops.write_cr0)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_cpu_ops.write_cr0) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_cpu_ops.write_cr0)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(x))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(x))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "rax", "r8", "r9", "r10", "r11"
#endif
); });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long read_cr2(void)
{
	return ({ unsigned long __ret; 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_mmu_ops.read_cr2 ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (64), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_mmu_ops.read_cr2 == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_mmu_ops.read_cr2)
#endif
; if (sizeof(unsigned long) > sizeof(unsigned long)) { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
 "=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
, "=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.read_cr2) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.read_cr2)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
) : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "r8", "r9", "r10", "r11"
#endif
); __ret = (unsigned long)((((u64)__edx) << 32) | __eax); } else { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
 "=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
, "=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.read_cr2) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.read_cr2)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
) : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "r8", "r9", "r10", "r11"
#endif
); __ret = (unsigned long)__eax; } __ret; });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void write_cr2(unsigned long x)
{
	({ 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_mmu_ops.write_cr2 ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (69), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_mmu_ops.write_cr2 == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_mmu_ops.write_cr2)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.write_cr2) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.write_cr2)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(x))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(x))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "rax", "r8", "r9", "r10", "r11"
#endif
); });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long read_cr3(void)
{
	return ({ unsigned long __ret; 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_mmu_ops.read_cr3 ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (74), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_mmu_ops.read_cr3 == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_mmu_ops.read_cr3)
#endif
; if (sizeof(unsigned long) > sizeof(unsigned long)) { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
 "=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
, "=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.read_cr3) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.read_cr3)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
) : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "r8", "r9", "r10", "r11"
#endif
); __ret = (unsigned long)((((u64)__edx) << 32) | __eax); } else { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
 "=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
, "=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.read_cr3) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.read_cr3)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
) : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "r8", "r9", "r10", "r11"
#endif
); __ret = (unsigned long)__eax; } __ret; });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void write_cr3(unsigned long x)
{
	({ 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_mmu_ops.write_cr3 ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (79), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_mmu_ops.write_cr3 == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_mmu_ops.write_cr3)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.write_cr3) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.write_cr3)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(x))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(x))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "rax", "r8", "r9", "r10", "r11"
#endif
); });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long read_cr4(void)
{
	return ({ unsigned long __ret; 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_cpu_ops.read_cr4 ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (84), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_cpu_ops.read_cr4 == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_cpu_ops.read_cr4)
#endif
; if (sizeof(unsigned long) > sizeof(unsigned long)) { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
 "=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
, "=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_cpu_ops.read_cr4) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_cpu_ops.read_cr4)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
) : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "r8", "r9", "r10", "r11"
#endif
); __ret = (unsigned long)((((u64)__edx) << 32) | __eax); } else { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
 "=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
, "=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_cpu_ops.read_cr4) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_cpu_ops.read_cr4)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
) : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "r8", "r9", "r10", "r11"
#endif
); __ret = (unsigned long)__eax; } __ret; });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long read_cr4_safe(void)
{
	return ({ unsigned long __ret; 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_cpu_ops.read_cr4_safe ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (88), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_cpu_ops.read_cr4_safe == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_cpu_ops.read_cr4_safe)
#endif
; if (sizeof(unsigned long) > sizeof(unsigned long)) { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
 "=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
, "=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_cpu_ops.read_cr4_safe) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_cpu_ops.read_cr4_safe)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
) : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "r8", "r9", "r10", "r11"
#endif
); __ret = (unsigned long)((((u64)__edx) << 32) | __eax); } else { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
 "=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
, "=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_cpu_ops.read_cr4_safe) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_cpu_ops.read_cr4_safe)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
) : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "r8", "r9", "r10", "r11"
#endif
); __ret = (unsigned long)__eax; } __ret; });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void write_cr4(unsigned long x)
{
	({ 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_cpu_ops.write_cr4 ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (93), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_cpu_ops.write_cr4 == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_cpu_ops.write_cr4)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_cpu_ops.write_cr4) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_cpu_ops.write_cr4)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(x))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(x))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "rax", "r8", "r9", "r10", "r11"
#endif
); });
}
#if definedEx(CONFIG_X86_64)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long read_cr8(void)
{
	return ({ unsigned long __ret; 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_cpu_ops.read_cr8 ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (99), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_cpu_ops.read_cr8 == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_cpu_ops.read_cr8)
#endif
; if (sizeof(unsigned long) > sizeof(unsigned long)) { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
 "=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
, "=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_cpu_ops.read_cr8) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_cpu_ops.read_cr8)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
) : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "r8", "r9", "r10", "r11"
#endif
); __ret = (unsigned long)((((u64)__edx) << 32) | __eax); } else { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
 "=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
, "=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_cpu_ops.read_cr8) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_cpu_ops.read_cr8)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
) : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "r8", "r9", "r10", "r11"
#endif
); __ret = (unsigned long)__eax; } __ret; });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void write_cr8(unsigned long x)
{
	({ 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_cpu_ops.write_cr8 ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (104), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_cpu_ops.write_cr8 == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_cpu_ops.write_cr8)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_cpu_ops.write_cr8) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_cpu_ops.write_cr8)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(x))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(x))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "rax", "r8", "r9", "r10", "r11"
#endif
); });
}
#endif
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void raw_safe_halt(void)
{
	({ 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_irq_ops.safe_halt ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (110), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_irq_ops.safe_halt == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_irq_ops.safe_halt)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_irq_ops.safe_halt) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_irq_ops.safe_halt)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
) : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "rax", "r8", "r9", "r10", "r11"
#endif
); });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void halt(void)
{
	({ 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_irq_ops.safe_halt ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (115), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_irq_ops.safe_halt == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_irq_ops.safe_halt)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_irq_ops.safe_halt) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_irq_ops.safe_halt)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
) : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "rax", "r8", "r9", "r10", "r11"
#endif
); });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void wbinvd(void)
{
	({ 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_cpu_ops.wbinvd ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (120), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_cpu_ops.wbinvd == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_cpu_ops.wbinvd)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_cpu_ops.wbinvd) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_cpu_ops.wbinvd)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
) : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "rax", "r8", "r9", "r10", "r11"
#endif
); });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 u64 paravirt_read_msr(unsigned msr, int *err)
{
	return ({ u64 __ret; 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_cpu_ops.read_msr ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (127), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_cpu_ops.read_msr == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_cpu_ops.read_msr)
#endif
; if (sizeof(u64) > sizeof(unsigned long)) { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
 "=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
, "=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_cpu_ops.read_msr) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_cpu_ops.read_msr)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(msr))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(msr))
#endif
, 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"d" ((unsigned long)(err))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"S" ((unsigned long)(err))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "r8", "r9", "r10", "r11"
#endif
); __ret = (u64)((((u64)__edx) << 32) | __eax); } else { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
 "=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
, "=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_cpu_ops.read_msr) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_cpu_ops.read_msr)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(msr))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(msr))
#endif
, 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"d" ((unsigned long)(err))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"S" ((unsigned long)(err))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "r8", "r9", "r10", "r11"
#endif
); __ret = (u64)__eax; } __ret; });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int paravirt_rdmsr_regs(u32 *regs)
{
	return ({ int __ret; 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_cpu_ops.rdmsr_regs ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (132), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_cpu_ops.rdmsr_regs == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_cpu_ops.rdmsr_regs)
#endif
; if (sizeof(int) > sizeof(unsigned long)) { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
 "=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
, "=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_cpu_ops.rdmsr_regs) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_cpu_ops.rdmsr_regs)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(regs))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(regs))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "r8", "r9", "r10", "r11"
#endif
); __ret = (int)((((u64)__edx) << 32) | __eax); } else { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
 "=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
, "=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_cpu_ops.rdmsr_regs) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_cpu_ops.rdmsr_regs)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(regs))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(regs))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "r8", "r9", "r10", "r11"
#endif
); __ret = (int)__eax; } __ret; });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int paravirt_write_msr(unsigned msr, unsigned low, unsigned high)
{
	return ({ int __ret; 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_cpu_ops.write_msr ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (137), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_cpu_ops.write_msr == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_cpu_ops.write_msr)
#endif
; if (sizeof(int) > sizeof(unsigned long)) { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
 "=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
, "=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_cpu_ops.write_msr) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_cpu_ops.write_msr)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(msr))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(msr))
#endif
, 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"d" ((unsigned long)(low))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"S" ((unsigned long)(low))
#endif
, 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"c" ((unsigned long)(high))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"d" ((unsigned long)(high))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "r8", "r9", "r10", "r11"
#endif
); __ret = (int)((((u64)__edx) << 32) | __eax); } else { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
 "=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
, "=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_cpu_ops.write_msr) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_cpu_ops.write_msr)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(msr))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(msr))
#endif
, 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"d" ((unsigned long)(low))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"S" ((unsigned long)(low))
#endif
, 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"c" ((unsigned long)(high))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"d" ((unsigned long)(high))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "r8", "r9", "r10", "r11"
#endif
); __ret = (int)__eax; } __ret; });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int paravirt_wrmsr_regs(u32 *regs)
{
	return ({ int __ret; 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_cpu_ops.wrmsr_regs ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (142), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_cpu_ops.wrmsr_regs == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_cpu_ops.wrmsr_regs)
#endif
; if (sizeof(int) > sizeof(unsigned long)) { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
 "=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
, "=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_cpu_ops.wrmsr_regs) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_cpu_ops.wrmsr_regs)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(regs))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(regs))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "r8", "r9", "r10", "r11"
#endif
); __ret = (int)((((u64)__edx) << 32) | __eax); } else { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
 "=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
, "=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_cpu_ops.wrmsr_regs) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_cpu_ops.wrmsr_regs)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(regs))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(regs))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "r8", "r9", "r10", "r11"
#endif
); __ret = (int)__eax; } __ret; });
}
/* These should all do BUG_ON(_err), but our headers are too tangled. */
/* rdmsr with exception handling */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int rdmsrl_safe(unsigned msr, unsigned long long *p)
{
	int err;
	*p = paravirt_read_msr(msr, &err);
	return err;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int rdmsrl_amd_safe(unsigned msr, unsigned long long *p)
{
	u32 gprs[8] = { 0 };
	int err;
	gprs[1] = msr;
	gprs[7] = 0x9c5a203a;
	err = paravirt_rdmsr_regs(gprs);
	*p = gprs[0] | ((u64)gprs[2] << 32);
	return err;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int wrmsrl_amd_safe(unsigned msr, unsigned long long val)
{
	u32 gprs[8] = { 0 };
	gprs[0] = (u32)val;
	gprs[1] = msr;
	gprs[2] = val >> 32;
	gprs[7] = 0x9c5a203a;
	return paravirt_wrmsr_regs(gprs);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 u64 paravirt_read_tsc(void)
{
	return ({ u64 __ret; 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_cpu_ops.read_tsc ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (217), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_cpu_ops.read_tsc == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_cpu_ops.read_tsc)
#endif
; if (sizeof(u64) > sizeof(unsigned long)) { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
 "=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
, "=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_cpu_ops.read_tsc) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_cpu_ops.read_tsc)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
) : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "r8", "r9", "r10", "r11"
#endif
); __ret = (u64)((((u64)__edx) << 32) | __eax); } else { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
 "=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
, "=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_cpu_ops.read_tsc) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_cpu_ops.read_tsc)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
) : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "r8", "r9", "r10", "r11"
#endif
); __ret = (u64)__eax; } __ret; });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long long paravirt_sched_clock(void)
{
	return ({ unsigned long long __ret; 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_time_ops.sched_clock ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (230), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_time_ops.sched_clock == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_time_ops.sched_clock)
#endif
; if (sizeof(unsigned long long) > sizeof(unsigned long)) { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
 "=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
, "=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_time_ops.sched_clock) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_time_ops.sched_clock)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
) : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "r8", "r9", "r10", "r11"
#endif
); __ret = (unsigned long long)((((u64)__edx) << 32) | __eax); } else { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
 "=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
, "=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_time_ops.sched_clock) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_time_ops.sched_clock)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
) : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "r8", "r9", "r10", "r11"
#endif
); __ret = (unsigned long long)__eax; } __ret; });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long long paravirt_read_pmc(int counter)
{
	return ({ u64 __ret; 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_cpu_ops.read_pmc ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (235), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_cpu_ops.read_pmc == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_cpu_ops.read_pmc)
#endif
; if (sizeof(u64) > sizeof(unsigned long)) { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
 "=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
, "=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_cpu_ops.read_pmc) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_cpu_ops.read_pmc)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(counter))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(counter))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "r8", "r9", "r10", "r11"
#endif
); __ret = (u64)((((u64)__edx) << 32) | __eax); } else { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
 "=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
, "=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_cpu_ops.read_pmc) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_cpu_ops.read_pmc)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(counter))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(counter))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "r8", "r9", "r10", "r11"
#endif
); __ret = (u64)__eax; } __ret; });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long long paravirt_rdtscp(unsigned int *aux)
{
	return ({ u64 __ret; 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_cpu_ops.read_tscp ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (247), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_cpu_ops.read_tscp == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_cpu_ops.read_tscp)
#endif
; if (sizeof(u64) > sizeof(unsigned long)) { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
 "=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
, "=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_cpu_ops.read_tscp) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_cpu_ops.read_tscp)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(aux))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(aux))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "r8", "r9", "r10", "r11"
#endif
); __ret = (u64)((((u64)__edx) << 32) | __eax); } else { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
 "=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
, "=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_cpu_ops.read_tscp) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_cpu_ops.read_tscp)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(aux))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(aux))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "r8", "r9", "r10", "r11"
#endif
); __ret = (u64)__eax; } __ret; });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void paravirt_alloc_ldt(struct desc_struct *ldt, unsigned entries)
{
	({ 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_cpu_ops.alloc_ldt ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (268), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_cpu_ops.alloc_ldt == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_cpu_ops.alloc_ldt)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_cpu_ops.alloc_ldt) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_cpu_ops.alloc_ldt)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(ldt))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(ldt))
#endif
, 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"d" ((unsigned long)(entries))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"S" ((unsigned long)(entries))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "rax", "r8", "r9", "r10", "r11"
#endif
); });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void paravirt_free_ldt(struct desc_struct *ldt, unsigned entries)
{
	({ 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_cpu_ops.free_ldt ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (273), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_cpu_ops.free_ldt == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_cpu_ops.free_ldt)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_cpu_ops.free_ldt) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_cpu_ops.free_ldt)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(ldt))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(ldt))
#endif
, 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"d" ((unsigned long)(entries))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"S" ((unsigned long)(entries))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "rax", "r8", "r9", "r10", "r11"
#endif
); });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void load_TR_desc(void)
{
	({ 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_cpu_ops.load_tr_desc ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (278), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_cpu_ops.load_tr_desc == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_cpu_ops.load_tr_desc)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_cpu_ops.load_tr_desc) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_cpu_ops.load_tr_desc)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
) : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "rax", "r8", "r9", "r10", "r11"
#endif
); });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void load_gdt(const struct desc_ptr *dtr)
{
	({ 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_cpu_ops.load_gdt ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (282), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_cpu_ops.load_gdt == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_cpu_ops.load_gdt)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_cpu_ops.load_gdt) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_cpu_ops.load_gdt)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(dtr))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(dtr))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "rax", "r8", "r9", "r10", "r11"
#endif
); });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void load_idt(const struct desc_ptr *dtr)
{
	({ 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_cpu_ops.load_idt ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (286), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_cpu_ops.load_idt == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_cpu_ops.load_idt)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_cpu_ops.load_idt) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_cpu_ops.load_idt)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(dtr))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(dtr))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "rax", "r8", "r9", "r10", "r11"
#endif
); });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void set_ldt(const void *addr, unsigned entries)
{
	({ 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_cpu_ops.set_ldt ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (290), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_cpu_ops.set_ldt == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_cpu_ops.set_ldt)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_cpu_ops.set_ldt) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_cpu_ops.set_ldt)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(addr))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(addr))
#endif
, 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"d" ((unsigned long)(entries))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"S" ((unsigned long)(entries))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "rax", "r8", "r9", "r10", "r11"
#endif
); });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void store_gdt(struct desc_ptr *dtr)
{
	({ 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_cpu_ops.store_gdt ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (294), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_cpu_ops.store_gdt == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_cpu_ops.store_gdt)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_cpu_ops.store_gdt) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_cpu_ops.store_gdt)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(dtr))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(dtr))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "rax", "r8", "r9", "r10", "r11"
#endif
); });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void store_idt(struct desc_ptr *dtr)
{
	({ 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_cpu_ops.store_idt ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (298), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_cpu_ops.store_idt == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_cpu_ops.store_idt)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_cpu_ops.store_idt) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_cpu_ops.store_idt)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(dtr))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(dtr))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "rax", "r8", "r9", "r10", "r11"
#endif
); });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long paravirt_store_tr(void)
{
	return ({ unsigned long __ret; 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_cpu_ops.store_tr ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (302), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_cpu_ops.store_tr == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_cpu_ops.store_tr)
#endif
; if (sizeof(unsigned long) > sizeof(unsigned long)) { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
 "=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
, "=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_cpu_ops.store_tr) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_cpu_ops.store_tr)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
) : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "r8", "r9", "r10", "r11"
#endif
); __ret = (unsigned long)((((u64)__edx) << 32) | __eax); } else { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
 "=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
, "=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_cpu_ops.store_tr) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_cpu_ops.store_tr)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
) : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "r8", "r9", "r10", "r11"
#endif
); __ret = (unsigned long)__eax; } __ret; });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void load_TLS(struct thread_struct *t, unsigned cpu)
{
	({ 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_cpu_ops.load_tls ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (307), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_cpu_ops.load_tls == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_cpu_ops.load_tls)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_cpu_ops.load_tls) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_cpu_ops.load_tls)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(t))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(t))
#endif
, 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"d" ((unsigned long)(cpu))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"S" ((unsigned long)(cpu))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "rax", "r8", "r9", "r10", "r11"
#endif
); });
}
#if definedEx(CONFIG_X86_64)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void load_gs_index(unsigned int gs)
{
	({ 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_cpu_ops.load_gs_index ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (313), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_cpu_ops.load_gs_index == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_cpu_ops.load_gs_index)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_cpu_ops.load_gs_index) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_cpu_ops.load_gs_index)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(gs))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(gs))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "rax", "r8", "r9", "r10", "r11"
#endif
); });
}
#endif
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void write_ldt_entry(struct desc_struct *dt, int entry,
				   const void *desc)
{
	({ 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_cpu_ops.write_ldt_entry ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (320), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_cpu_ops.write_ldt_entry == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_cpu_ops.write_ldt_entry)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_cpu_ops.write_ldt_entry) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_cpu_ops.write_ldt_entry)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(dt))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(dt))
#endif
, 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"d" ((unsigned long)(entry))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"S" ((unsigned long)(entry))
#endif
, 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"c" ((unsigned long)(desc))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"d" ((unsigned long)(desc))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "rax", "r8", "r9", "r10", "r11"
#endif
); });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void write_gdt_entry(struct desc_struct *dt, int entry,
				   void *desc, int type)
{
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
({ unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_cpu_ops.write_gdt_entry ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b, %c0\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (326), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_cpu_ops.write_gdt_entry == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_cpu_ops.write_gdt_entry)
#endif
; asm volatile("push %[_arg4];" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" " " ".balign 4" " " "\n" " " ".long" " " " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "lea 4(%%esp),%%esp;" : "=a" (__eax), "=d" (__edx), "=c" (__ecx) : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_cpu_ops.write_gdt_entry) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_cpu_ops.write_gdt_entry)), [paravirt_clobber] "i" (((1 << 4) - 1)),"0"((u32)(dt)), "1"((u32)(entry)), "2"((u32)(desc)), [_arg4] "mr"((u32)(type)) : "memory", "cc" ); })
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
({ unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_cpu_ops.write_gdt_entry ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (326), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_cpu_ops.write_gdt_entry == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_cpu_ops.write_gdt_entry)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" " " ".balign 8" " " "\n" " " ".quad" " " " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : "=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx) : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_cpu_ops.write_gdt_entry) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_cpu_ops.write_gdt_entry)), [paravirt_clobber] "i" (((1 << 9) - 1)),"D" ((unsigned long)(dt)), "S" ((unsigned long)(entry)), "d" ((unsigned long)(desc)), "c" ((unsigned long)(type)) : "memory", "cc" , "rax", "r8", "r9", "r10", "r11"); })
#endif
;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void write_idt_entry(gate_desc *dt, int entry, const gate_desc *g)
{
	({ 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_cpu_ops.write_idt_entry ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (331), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_cpu_ops.write_idt_entry == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_cpu_ops.write_idt_entry)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_cpu_ops.write_idt_entry) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_cpu_ops.write_idt_entry)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(dt))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(dt))
#endif
, 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"d" ((unsigned long)(entry))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"S" ((unsigned long)(entry))
#endif
, 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"c" ((unsigned long)(g))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"d" ((unsigned long)(g))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "rax", "r8", "r9", "r10", "r11"
#endif
); });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void set_iopl_mask(unsigned mask)
{
	({ 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_cpu_ops.set_iopl_mask ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (335), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_cpu_ops.set_iopl_mask == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_cpu_ops.set_iopl_mask)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_cpu_ops.set_iopl_mask) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_cpu_ops.set_iopl_mask)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(mask))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(mask))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "rax", "r8", "r9", "r10", "r11"
#endif
); });
}
/* The paravirtualized I/O functions */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void slow_down_io(void)
{
	pv_cpu_ops.io_delay();
}
#if definedEx(CONFIG_SMP)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void startup_ipi_hook(int phys_apicid, unsigned long start_eip,
				    unsigned long start_esp)
{
	({ 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_apic_ops.startup_ipi_hook ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (354), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_apic_ops.startup_ipi_hook == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_apic_ops.startup_ipi_hook)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_apic_ops.startup_ipi_hook) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_apic_ops.startup_ipi_hook)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(
 phys_apicid))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(
 phys_apicid))
#endif
, 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"d" ((unsigned long)(start_eip))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"S" ((unsigned long)(start_eip))
#endif
, 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"c" ((unsigned long)(start_esp))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"d" ((unsigned long)(start_esp))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "rax", "r8", "r9", "r10", "r11"
#endif
); });
}
#endif
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void paravirt_activate_mm(struct mm_struct *prev,
					struct mm_struct *next)
{
	({ 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_mmu_ops.activate_mm ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (361), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_mmu_ops.activate_mm == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_mmu_ops.activate_mm)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.activate_mm) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.activate_mm)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(prev))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(prev))
#endif
, 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"d" ((unsigned long)(next))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"S" ((unsigned long)(next))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "rax", "r8", "r9", "r10", "r11"
#endif
); });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void arch_dup_mmap(struct mm_struct *oldmm,
				 struct mm_struct *mm)
{
	({ 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_mmu_ops.dup_mmap ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (367), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_mmu_ops.dup_mmap == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_mmu_ops.dup_mmap)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.dup_mmap) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.dup_mmap)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(oldmm))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(oldmm))
#endif
, 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"d" ((unsigned long)(mm))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"S" ((unsigned long)(mm))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "rax", "r8", "r9", "r10", "r11"
#endif
); });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void arch_exit_mmap(struct mm_struct *mm)
{
	({ 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_mmu_ops.exit_mmap ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (372), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_mmu_ops.exit_mmap == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_mmu_ops.exit_mmap)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.exit_mmap) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.exit_mmap)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(mm))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(mm))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "rax", "r8", "r9", "r10", "r11"
#endif
); });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __flush_tlb(void)
{
	({ 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_mmu_ops.flush_tlb_user ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (377), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_mmu_ops.flush_tlb_user == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_mmu_ops.flush_tlb_user)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.flush_tlb_user) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.flush_tlb_user)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
) : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "rax", "r8", "r9", "r10", "r11"
#endif
); });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __flush_tlb_global(void)
{
	({ 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_mmu_ops.flush_tlb_kernel ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (381), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_mmu_ops.flush_tlb_kernel == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_mmu_ops.flush_tlb_kernel)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.flush_tlb_kernel) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.flush_tlb_kernel)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
) : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "rax", "r8", "r9", "r10", "r11"
#endif
); });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __flush_tlb_single(unsigned long addr)
{
	({ 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_mmu_ops.flush_tlb_single ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (385), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_mmu_ops.flush_tlb_single == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_mmu_ops.flush_tlb_single)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.flush_tlb_single) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.flush_tlb_single)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(addr))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(addr))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "rax", "r8", "r9", "r10", "r11"
#endif
); });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void flush_tlb_others(const struct cpumask *cpumask,
				    struct mm_struct *mm,
				    unsigned long va)
{
	({ 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_mmu_ops.flush_tlb_others ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (392), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_mmu_ops.flush_tlb_others == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_mmu_ops.flush_tlb_others)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.flush_tlb_others) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.flush_tlb_others)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(cpumask))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(cpumask))
#endif
, 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"d" ((unsigned long)(mm))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"S" ((unsigned long)(mm))
#endif
, 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"c" ((unsigned long)(va))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"d" ((unsigned long)(va))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "rax", "r8", "r9", "r10", "r11"
#endif
); });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int paravirt_pgd_alloc(struct mm_struct *mm)
{
	return ({ int __ret; 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_mmu_ops.pgd_alloc ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (397), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_mmu_ops.pgd_alloc == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_mmu_ops.pgd_alloc)
#endif
; if (sizeof(int) > sizeof(unsigned long)) { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
 "=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
, "=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.pgd_alloc) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.pgd_alloc)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(mm))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(mm))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "r8", "r9", "r10", "r11"
#endif
); __ret = (int)((((u64)__edx) << 32) | __eax); } else { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
 "=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
, "=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.pgd_alloc) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.pgd_alloc)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(mm))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(mm))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "r8", "r9", "r10", "r11"
#endif
); __ret = (int)__eax; } __ret; });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void paravirt_pgd_free(struct mm_struct *mm, pgd_t *pgd)
{
	({ 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_mmu_ops.pgd_free ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (402), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_mmu_ops.pgd_free == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_mmu_ops.pgd_free)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.pgd_free) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.pgd_free)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(mm))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(mm))
#endif
, 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"d" ((unsigned long)(pgd))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"S" ((unsigned long)(pgd))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "rax", "r8", "r9", "r10", "r11"
#endif
); });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void paravirt_alloc_pte(struct mm_struct *mm, unsigned long pfn)
{
	({ 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_mmu_ops.alloc_pte ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (407), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_mmu_ops.alloc_pte == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_mmu_ops.alloc_pte)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.alloc_pte) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.alloc_pte)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(mm))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(mm))
#endif
, 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"d" ((unsigned long)(pfn))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"S" ((unsigned long)(pfn))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "rax", "r8", "r9", "r10", "r11"
#endif
); });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void paravirt_release_pte(unsigned long pfn)
{
	({ 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_mmu_ops.release_pte ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (411), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_mmu_ops.release_pte == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_mmu_ops.release_pte)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.release_pte) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.release_pte)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(pfn))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(pfn))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "rax", "r8", "r9", "r10", "r11"
#endif
); });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void paravirt_alloc_pmd(struct mm_struct *mm, unsigned long pfn)
{
	({ 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_mmu_ops.alloc_pmd ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (416), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_mmu_ops.alloc_pmd == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_mmu_ops.alloc_pmd)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.alloc_pmd) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.alloc_pmd)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(mm))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(mm))
#endif
, 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"d" ((unsigned long)(pfn))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"S" ((unsigned long)(pfn))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "rax", "r8", "r9", "r10", "r11"
#endif
); });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void paravirt_alloc_pmd_clone(unsigned long pfn, unsigned long clonepfn,
					    unsigned long start, unsigned long count)
{
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
({ unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_mmu_ops.alloc_pmd_clone ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b, %c0\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (422), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_mmu_ops.alloc_pmd_clone == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_mmu_ops.alloc_pmd_clone)
#endif
; asm volatile("push %[_arg4];" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" " " ".balign 4" " " "\n" " " ".long" " " " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "lea 4(%%esp),%%esp;" : "=a" (__eax), "=d" (__edx), "=c" (__ecx) : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.alloc_pmd_clone) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.alloc_pmd_clone)), [paravirt_clobber] "i" (((1 << 4) - 1)),"0"((u32)(pfn)), "1"((u32)(clonepfn)), "2"((u32)(start)), [_arg4] "mr"((u32)(count)) : "memory", "cc" ); })
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
({ unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_mmu_ops.alloc_pmd_clone ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (422), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_mmu_ops.alloc_pmd_clone == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_mmu_ops.alloc_pmd_clone)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" " " ".balign 8" " " "\n" " " ".quad" " " " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : "=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx) : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.alloc_pmd_clone) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.alloc_pmd_clone)), [paravirt_clobber] "i" (((1 << 9) - 1)),"D" ((unsigned long)(pfn)), "S" ((unsigned long)(clonepfn)), "d" ((unsigned long)(start)), "c" ((unsigned long)(count)) : "memory", "cc" , "rax", "r8", "r9", "r10", "r11"); })
#endif
;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void paravirt_release_pmd(unsigned long pfn)
{
	({ 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_mmu_ops.release_pmd ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (426), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_mmu_ops.release_pmd == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_mmu_ops.release_pmd)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.release_pmd) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.release_pmd)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(pfn))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(pfn))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "rax", "r8", "r9", "r10", "r11"
#endif
); });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void paravirt_alloc_pud(struct mm_struct *mm, unsigned long pfn)
{
	({ 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_mmu_ops.alloc_pud ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (431), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_mmu_ops.alloc_pud == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_mmu_ops.alloc_pud)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.alloc_pud) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.alloc_pud)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(mm))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(mm))
#endif
, 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"d" ((unsigned long)(pfn))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"S" ((unsigned long)(pfn))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "rax", "r8", "r9", "r10", "r11"
#endif
); });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void paravirt_release_pud(unsigned long pfn)
{
	({ 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_mmu_ops.release_pud ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (435), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_mmu_ops.release_pud == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_mmu_ops.release_pud)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.release_pud) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.release_pud)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(pfn))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(pfn))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "rax", "r8", "r9", "r10", "r11"
#endif
); });
}
#if definedEx(CONFIG_HIGHPTE)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void *kmap_atomic_pte(struct page *page, enum km_type type)
{
	unsigned long ret;
	ret = ({ unsigned long __ret; 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_mmu_ops.kmap_atomic_pte ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (442), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_mmu_ops.kmap_atomic_pte == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_mmu_ops.kmap_atomic_pte)
#endif
; if (sizeof(unsigned long) > sizeof(unsigned long)) { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
 "=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
, "=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.kmap_atomic_pte) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.kmap_atomic_pte)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(page))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(page))
#endif
, 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"d" ((unsigned long)(type))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"S" ((unsigned long)(type))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "r8", "r9", "r10", "r11"
#endif
); __ret = (unsigned long)((((u64)__edx) << 32) | __eax); } else { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
 "=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
, "=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.kmap_atomic_pte) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.kmap_atomic_pte)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(page))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(page))
#endif
, 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"d" ((unsigned long)(type))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"S" ((unsigned long)(type))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "r8", "r9", "r10", "r11"
#endif
); __ret = (unsigned long)__eax; } __ret; });
	return (void *)ret;
}
#endif
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void pte_update(struct mm_struct *mm, unsigned long addr,
			      pte_t *ptep)
{
	({ 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_mmu_ops.pte_update ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (450), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_mmu_ops.pte_update == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_mmu_ops.pte_update)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.pte_update) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.pte_update)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(mm))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(mm))
#endif
, 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"d" ((unsigned long)(addr))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"S" ((unsigned long)(addr))
#endif
, 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"c" ((unsigned long)(ptep))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"d" ((unsigned long)(ptep))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "rax", "r8", "r9", "r10", "r11"
#endif
); });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void pte_update_defer(struct mm_struct *mm, unsigned long addr,
				    pte_t *ptep)
{
	({ 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_mmu_ops.pte_update_defer ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (456), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_mmu_ops.pte_update_defer == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_mmu_ops.pte_update_defer)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.pte_update_defer) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.pte_update_defer)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(mm))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(mm))
#endif
, 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"d" ((unsigned long)(addr))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"S" ((unsigned long)(addr))
#endif
, 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"c" ((unsigned long)(ptep))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"d" ((unsigned long)(ptep))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "rax", "r8", "r9", "r10", "r11"
#endif
); });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 pte_t __pte(pteval_t val)
{
	pteval_t ret;
	if (sizeof(pteval_t) > sizeof(long))
		ret = ({ pteval_t __ret; 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(
 pv_mmu_ops.make_pte.func ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (466), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (
 pv_mmu_ops.make_pte.func == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)
 pv_mmu_ops.make_pte.func)
#endif
; if (sizeof(pteval_t) > sizeof(unsigned long)) { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,
 pv_mmu_ops.make_pte.func) / sizeof(void *))), [paravirt_opptr] "i" (&(
 pv_mmu_ops.make_pte.func)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 0) |(1 << 2))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
(
(1 << 0)
)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(
 val))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(
 val))
#endif
, 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"d" ((unsigned long)((u64)val >> 32))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"S" ((unsigned long)((u64)val >> 32))
#endif
 : "memory", "cc" ); __ret = (pteval_t)((((u64)__edx) << 32) | __eax); } else { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,
 pv_mmu_ops.make_pte.func) / sizeof(void *))), [paravirt_opptr] "i" (&(
 pv_mmu_ops.make_pte.func)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 0) |(1 << 2))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
(
(1 << 0)
)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(
 val))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(
 val))
#endif
, 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"d" ((unsigned long)((u64)val >> 32))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"S" ((unsigned long)((u64)val >> 32))
#endif
 : "memory", "cc" ); __ret = (pteval_t)__eax; } __ret; });
	else
		ret = ({ pteval_t __ret; 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(
 pv_mmu_ops.make_pte.func ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (470), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (
 pv_mmu_ops.make_pte.func == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)
 pv_mmu_ops.make_pte.func)
#endif
; if (sizeof(pteval_t) > sizeof(unsigned long)) { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,
 pv_mmu_ops.make_pte.func) / sizeof(void *))), [paravirt_opptr] "i" (&(
 pv_mmu_ops.make_pte.func)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 0) |(1 << 2))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
(
(1 << 0)
)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(
 val))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(
 val))
#endif
 : "memory", "cc" ); __ret = (pteval_t)((((u64)__edx) << 32) | __eax); } else { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,
 pv_mmu_ops.make_pte.func) / sizeof(void *))), [paravirt_opptr] "i" (&(
 pv_mmu_ops.make_pte.func)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 0) |(1 << 2))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
(
(1 << 0)
)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(
 val))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(
 val))
#endif
 : "memory", "cc" ); __ret = (pteval_t)__eax; } __ret; });
	return (pte_t) { .pte = ret };
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 pteval_t pte_val(pte_t pte)
{
	pteval_t ret;
	if (sizeof(pteval_t) > sizeof(long))
		ret = ({ pteval_t __ret; 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_mmu_ops.pte_val.func ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (481), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_mmu_ops.pte_val.func == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_mmu_ops.pte_val.func)
#endif
; if (sizeof(pteval_t) > sizeof(unsigned long)) { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.pte_val.func) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.pte_val.func)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 0) |(1 << 2))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
(
(1 << 0)
)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(
 pte.pte))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(
 pte.pte))
#endif
, 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"d" ((unsigned long)((u64)pte.pte >> 32))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"S" ((unsigned long)((u64)pte.pte >> 32))
#endif
 : "memory", "cc" ); __ret = (pteval_t)((((u64)__edx) << 32) | __eax); } else { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.pte_val.func) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.pte_val.func)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 0) |(1 << 2))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
(
(1 << 0)
)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(
 pte.pte))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(
 pte.pte))
#endif
, 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"d" ((unsigned long)((u64)pte.pte >> 32))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"S" ((unsigned long)((u64)pte.pte >> 32))
#endif
 : "memory", "cc" ); __ret = (pteval_t)__eax; } __ret; });
	else
		ret = ({ pteval_t __ret; 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_mmu_ops.pte_val.func ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (484), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_mmu_ops.pte_val.func == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_mmu_ops.pte_val.func)
#endif
; if (sizeof(pteval_t) > sizeof(unsigned long)) { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.pte_val.func) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.pte_val.func)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 0) |(1 << 2))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
(
(1 << 0)
)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(
 pte.pte))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(
 pte.pte))
#endif
 : "memory", "cc" ); __ret = (pteval_t)((((u64)__edx) << 32) | __eax); } else { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.pte_val.func) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.pte_val.func)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 0) |(1 << 2))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
(
(1 << 0)
)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(
 pte.pte))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(
 pte.pte))
#endif
 : "memory", "cc" ); __ret = (pteval_t)__eax; } __ret; });
	return ret;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 pgd_t __pgd(pgdval_t val)
{
	pgdval_t ret;
	if (sizeof(pgdval_t) > sizeof(long))
		ret = ({ pgdval_t __ret; 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_mmu_ops.make_pgd.func ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (495), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_mmu_ops.make_pgd.func == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_mmu_ops.make_pgd.func)
#endif
; if (sizeof(pgdval_t) > sizeof(unsigned long)) { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.make_pgd.func) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.make_pgd.func)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 0) |(1 << 2))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
(
(1 << 0)
)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(
 val))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(
 val))
#endif
, 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"d" ((unsigned long)((u64)val >> 32))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"S" ((unsigned long)((u64)val >> 32))
#endif
 : "memory", "cc" ); __ret = (pgdval_t)((((u64)__edx) << 32) | __eax); } else { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.make_pgd.func) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.make_pgd.func)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 0) |(1 << 2))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
(
(1 << 0)
)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(
 val))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(
 val))
#endif
, 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"d" ((unsigned long)((u64)val >> 32))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"S" ((unsigned long)((u64)val >> 32))
#endif
 : "memory", "cc" ); __ret = (pgdval_t)__eax; } __ret; });
	else
		ret = ({ pgdval_t __ret; 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_mmu_ops.make_pgd.func ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (498), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_mmu_ops.make_pgd.func == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_mmu_ops.make_pgd.func)
#endif
; if (sizeof(pgdval_t) > sizeof(unsigned long)) { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.make_pgd.func) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.make_pgd.func)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 0) |(1 << 2))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
(
(1 << 0)
)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(
 val))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(
 val))
#endif
 : "memory", "cc" ); __ret = (pgdval_t)((((u64)__edx) << 32) | __eax); } else { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.make_pgd.func) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.make_pgd.func)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 0) |(1 << 2))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
(
(1 << 0)
)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(
 val))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(
 val))
#endif
 : "memory", "cc" ); __ret = (pgdval_t)__eax; } __ret; });
	return (pgd_t) { ret };
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 pgdval_t pgd_val(pgd_t pgd)
{
	pgdval_t ret;
	if (sizeof(pgdval_t) > sizeof(long))
		ret =  ({ pgdval_t __ret; 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_mmu_ops.pgd_val.func ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (509), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_mmu_ops.pgd_val.func == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_mmu_ops.pgd_val.func)
#endif
; if (sizeof(pgdval_t) > sizeof(unsigned long)) { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.pgd_val.func) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.pgd_val.func)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 0) |(1 << 2))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
(
(1 << 0)
)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(
 pgd.pgd))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(
 pgd.pgd))
#endif
, 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"d" ((unsigned long)((u64)pgd.pgd >> 32))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"S" ((unsigned long)((u64)pgd.pgd >> 32))
#endif
 : "memory", "cc" ); __ret = (pgdval_t)((((u64)__edx) << 32) | __eax); } else { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.pgd_val.func) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.pgd_val.func)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 0) |(1 << 2))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
(
(1 << 0)
)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(
 pgd.pgd))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(
 pgd.pgd))
#endif
, 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"d" ((unsigned long)((u64)pgd.pgd >> 32))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"S" ((unsigned long)((u64)pgd.pgd >> 32))
#endif
 : "memory", "cc" ); __ret = (pgdval_t)__eax; } __ret; });
	else
		ret =  ({ pgdval_t __ret; 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_mmu_ops.pgd_val.func ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (512), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_mmu_ops.pgd_val.func == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_mmu_ops.pgd_val.func)
#endif
; if (sizeof(pgdval_t) > sizeof(unsigned long)) { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.pgd_val.func) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.pgd_val.func)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 0) |(1 << 2))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
(
(1 << 0)
)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(
 pgd.pgd))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(
 pgd.pgd))
#endif
 : "memory", "cc" ); __ret = (pgdval_t)((((u64)__edx) << 32) | __eax); } else { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.pgd_val.func) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.pgd_val.func)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 0) |(1 << 2))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
(
(1 << 0)
)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(
 pgd.pgd))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(
 pgd.pgd))
#endif
 : "memory", "cc" ); __ret = (pgdval_t)__eax; } __ret; });
	return ret;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 pte_t ptep_modify_prot_start(struct mm_struct *mm, unsigned long addr,
					   pte_t *ptep)
{
	pteval_t ret;
	ret = ({ pteval_t __ret; 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_mmu_ops.ptep_modify_prot_start ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (524), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_mmu_ops.ptep_modify_prot_start == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_mmu_ops.ptep_modify_prot_start)
#endif
; if (sizeof(pteval_t) > sizeof(unsigned long)) { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
 "=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
, "=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.ptep_modify_prot_start) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.ptep_modify_prot_start)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(
 mm))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(
 mm))
#endif
, 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"d" ((unsigned long)(addr))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"S" ((unsigned long)(addr))
#endif
, 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"c" ((unsigned long)(ptep))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"d" ((unsigned long)(ptep))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "r8", "r9", "r10", "r11"
#endif
); __ret = (pteval_t)((((u64)__edx) << 32) | __eax); } else { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
 "=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
, "=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.ptep_modify_prot_start) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.ptep_modify_prot_start)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(
 mm))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(
 mm))
#endif
, 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"d" ((unsigned long)(addr))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"S" ((unsigned long)(addr))
#endif
, 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"c" ((unsigned long)(ptep))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"d" ((unsigned long)(ptep))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "r8", "r9", "r10", "r11"
#endif
); __ret = (pteval_t)__eax; } __ret; });
	return (pte_t) { .pte = ret };
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void ptep_modify_prot_commit(struct mm_struct *mm, unsigned long addr,
					   pte_t *ptep, pte_t pte)
{
	if (sizeof(pteval_t) > sizeof(long))
		/* 5 arg words */
		pv_mmu_ops.ptep_modify_prot_commit(mm, addr, ptep, pte);
	else
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
({ unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_mmu_ops.ptep_modify_prot_commit ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b, %c0\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (537), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_mmu_ops.ptep_modify_prot_commit == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_mmu_ops.ptep_modify_prot_commit)
#endif
; asm volatile("push %[_arg4];" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" " " ".balign 4" " " "\n" " " ".long" " " " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "lea 4(%%esp),%%esp;" : "=a" (__eax), "=d" (__edx), "=c" (__ecx) : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.ptep_modify_prot_commit) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.ptep_modify_prot_commit)), [paravirt_clobber] "i" (((1 << 4) - 1)),"0"((u32)(
 mm)), "1"((u32)(addr)), "2"((u32)(ptep)), [_arg4] "mr"((u32)(pte.pte)) : "memory", "cc" ); })
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
({ unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_mmu_ops.ptep_modify_prot_commit ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (537), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_mmu_ops.ptep_modify_prot_commit == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_mmu_ops.ptep_modify_prot_commit)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" " " ".balign 8" " " "\n" " " ".quad" " " " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : "=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx) : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.ptep_modify_prot_commit) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.ptep_modify_prot_commit)), [paravirt_clobber] "i" (((1 << 9) - 1)),"D" ((unsigned long)(
 mm)), "S" ((unsigned long)(addr)), "d" ((unsigned long)(ptep)), "c" ((unsigned long)(pte.pte)) : "memory", "cc" , "rax", "r8", "r9", "r10", "r11"); })
#endif
;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void set_pte(pte_t *ptep, pte_t pte)
{
	if (sizeof(pteval_t) > sizeof(long))
		({ 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_mmu_ops.set_pte ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (544), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_mmu_ops.set_pte == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_mmu_ops.set_pte)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.set_pte) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.set_pte)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(ptep))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(ptep))
#endif
, 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"d" ((unsigned long)(
 pte.pte))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"S" ((unsigned long)(
 pte.pte))
#endif
, 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"c" ((unsigned long)((u64)pte.pte >> 32))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"d" ((unsigned long)((u64)pte.pte >> 32))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "rax", "r8", "r9", "r10", "r11"
#endif
); });
	else
		({ 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_mmu_ops.set_pte ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (547), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_mmu_ops.set_pte == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_mmu_ops.set_pte)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.set_pte) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.set_pte)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(ptep))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(ptep))
#endif
, 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"d" ((unsigned long)(
 pte.pte))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"S" ((unsigned long)(
 pte.pte))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "rax", "r8", "r9", "r10", "r11"
#endif
); });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void set_pte_at(struct mm_struct *mm, unsigned long addr,
			      pte_t *ptep, pte_t pte)
{
	if (sizeof(pteval_t) > sizeof(long))
		/* 5 arg words */
		pv_mmu_ops.set_pte_at(mm, addr, ptep, pte);
	else
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
({ unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_mmu_ops.set_pte_at ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b, %c0\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (557), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_mmu_ops.set_pte_at == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_mmu_ops.set_pte_at)
#endif
; asm volatile("push %[_arg4];" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" " " ".balign 4" " " "\n" " " ".long" " " " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "lea 4(%%esp),%%esp;" : "=a" (__eax), "=d" (__edx), "=c" (__ecx) : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.set_pte_at) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.set_pte_at)), [paravirt_clobber] "i" (((1 << 4) - 1)),"0"((u32)(mm)), "1"((u32)(addr)), "2"((u32)(ptep)), [_arg4] "mr"((u32)(pte.pte)) : "memory", "cc" ); })
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
({ unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_mmu_ops.set_pte_at ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (557), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_mmu_ops.set_pte_at == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_mmu_ops.set_pte_at)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" " " ".balign 8" " " "\n" " " ".quad" " " " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : "=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx) : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.set_pte_at) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.set_pte_at)), [paravirt_clobber] "i" (((1 << 9) - 1)),"D" ((unsigned long)(mm)), "S" ((unsigned long)(addr)), "d" ((unsigned long)(ptep)), "c" ((unsigned long)(pte.pte)) : "memory", "cc" , "rax", "r8", "r9", "r10", "r11"); })
#endif
;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void set_pmd(pmd_t *pmdp, pmd_t pmd)
{
	pmdval_t val = native_pmd_val(pmd);
	if (sizeof(pmdval_t) > sizeof(long))
		({ 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_mmu_ops.set_pmd ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (565), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_mmu_ops.set_pmd == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_mmu_ops.set_pmd)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.set_pmd) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.set_pmd)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(pmdp))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(pmdp))
#endif
, 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"d" ((unsigned long)(val))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"S" ((unsigned long)(val))
#endif
, 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"c" ((unsigned long)((u64)val >> 32))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"d" ((unsigned long)((u64)val >> 32))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "rax", "r8", "r9", "r10", "r11"
#endif
); });
	else
		({ 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_mmu_ops.set_pmd ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (567), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_mmu_ops.set_pmd == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_mmu_ops.set_pmd)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.set_pmd) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.set_pmd)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(pmdp))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(pmdp))
#endif
, 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"d" ((unsigned long)(val))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"S" ((unsigned long)(val))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "rax", "r8", "r9", "r10", "r11"
#endif
); });
}
#if !definedEx(CONFIG_X86_32) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_X86_PAE) && !definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_X86_PAE)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 pmd_t __pmd(pmdval_t val)
{
	pmdval_t ret;
	if (sizeof(pmdval_t) > sizeof(long))
		ret = ({ pmdval_t __ret; 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_mmu_ops.make_pmd.func ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (577), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_mmu_ops.make_pmd.func == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_mmu_ops.make_pmd.func)
#endif
; if (sizeof(pmdval_t) > sizeof(unsigned long)) { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.make_pmd.func) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.make_pmd.func)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 0) |(1 << 2))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
(
(1 << 0)
)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(
 val))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(
 val))
#endif
, 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"d" ((unsigned long)((u64)val >> 32))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"S" ((unsigned long)((u64)val >> 32))
#endif
 : "memory", "cc" ); __ret = (pmdval_t)((((u64)__edx) << 32) | __eax); } else { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.make_pmd.func) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.make_pmd.func)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 0) |(1 << 2))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
(
(1 << 0)
)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(
 val))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(
 val))
#endif
, 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"d" ((unsigned long)((u64)val >> 32))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"S" ((unsigned long)((u64)val >> 32))
#endif
 : "memory", "cc" ); __ret = (pmdval_t)__eax; } __ret; });
	else
		ret = ({ pmdval_t __ret; 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_mmu_ops.make_pmd.func ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (580), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_mmu_ops.make_pmd.func == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_mmu_ops.make_pmd.func)
#endif
; if (sizeof(pmdval_t) > sizeof(unsigned long)) { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.make_pmd.func) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.make_pmd.func)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 0) |(1 << 2))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
(
(1 << 0)
)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(
 val))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(
 val))
#endif
 : "memory", "cc" ); __ret = (pmdval_t)((((u64)__edx) << 32) | __eax); } else { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.make_pmd.func) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.make_pmd.func)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 0) |(1 << 2))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
(
(1 << 0)
)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(
 val))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(
 val))
#endif
 : "memory", "cc" ); __ret = (pmdval_t)__eax; } __ret; });
	return (pmd_t) { ret };
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 pmdval_t pmd_val(pmd_t pmd)
{
	pmdval_t ret;
	if (sizeof(pmdval_t) > sizeof(long))
		ret =  ({ pmdval_t __ret; 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_mmu_ops.pmd_val.func ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (591), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_mmu_ops.pmd_val.func == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_mmu_ops.pmd_val.func)
#endif
; if (sizeof(pmdval_t) > sizeof(unsigned long)) { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.pmd_val.func) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.pmd_val.func)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 0) |(1 << 2))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
(
(1 << 0)
)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(
 pmd.pmd))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(
 pmd.pmd))
#endif
, 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"d" ((unsigned long)((u64)pmd.pmd >> 32))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"S" ((unsigned long)((u64)pmd.pmd >> 32))
#endif
 : "memory", "cc" ); __ret = (pmdval_t)((((u64)__edx) << 32) | __eax); } else { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.pmd_val.func) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.pmd_val.func)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 0) |(1 << 2))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
(
(1 << 0)
)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(
 pmd.pmd))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(
 pmd.pmd))
#endif
, 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"d" ((unsigned long)((u64)pmd.pmd >> 32))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"S" ((unsigned long)((u64)pmd.pmd >> 32))
#endif
 : "memory", "cc" ); __ret = (pmdval_t)__eax; } __ret; });
	else
		ret =  ({ pmdval_t __ret; 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_mmu_ops.pmd_val.func ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (594), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_mmu_ops.pmd_val.func == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_mmu_ops.pmd_val.func)
#endif
; if (sizeof(pmdval_t) > sizeof(unsigned long)) { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.pmd_val.func) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.pmd_val.func)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 0) |(1 << 2))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
(
(1 << 0)
)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(
 pmd.pmd))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(
 pmd.pmd))
#endif
 : "memory", "cc" ); __ret = (pmdval_t)((((u64)__edx) << 32) | __eax); } else { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.pmd_val.func) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.pmd_val.func)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 0) |(1 << 2))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
(
(1 << 0)
)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(
 pmd.pmd))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(
 pmd.pmd))
#endif
 : "memory", "cc" ); __ret = (pmdval_t)__eax; } __ret; });
	return ret;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void set_pud(pud_t *pudp, pud_t pud)
{
	pudval_t val = native_pud_val(pud);
	if (sizeof(pudval_t) > sizeof(long))
		({ 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_mmu_ops.set_pud ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (605), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_mmu_ops.set_pud == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_mmu_ops.set_pud)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.set_pud) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.set_pud)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(pudp))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(pudp))
#endif
, 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"d" ((unsigned long)(
 val))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"S" ((unsigned long)(
 val))
#endif
, 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"c" ((unsigned long)((u64)val >> 32))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"d" ((unsigned long)((u64)val >> 32))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "rax", "r8", "r9", "r10", "r11"
#endif
); });
	else
		({ 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_mmu_ops.set_pud ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (608), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_mmu_ops.set_pud == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_mmu_ops.set_pud)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.set_pud) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.set_pud)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(pudp))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(pudp))
#endif
, 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"d" ((unsigned long)(
 val))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"S" ((unsigned long)(
 val))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "rax", "r8", "r9", "r10", "r11"
#endif
); });
}
#if !definedEx(CONFIG_X86_32) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_X86_PAE) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_X86_PAE) && !definedEx(CONFIG_PARAVIRT)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 pud_t __pud(pudval_t val)
{
	pudval_t ret;
	if (sizeof(pudval_t) > sizeof(long))
		ret = ({ pudval_t __ret; unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_mmu_ops.make_pud.func ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (617), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_mmu_ops.make_pud.func == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_mmu_ops.make_pud.func)
#endif
; if (sizeof(pudval_t) > sizeof(unsigned long)) { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" " " ".balign 8" " " "\n" " " ".quad" " " " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : "=a" (__eax) : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.make_pud.func) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.make_pud.func)), [paravirt_clobber] "i" (((1 << 0))),"D" ((unsigned long)(
 val)), "S" ((unsigned long)((u64)val >> 32)) : "memory", "cc" ); __ret = (pudval_t)((((u64)__edx) << 32) | __eax); } else { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" " " ".balign 8" " " "\n" " " ".quad" " " " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : "=a" (__eax) : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.make_pud.func) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.make_pud.func)), [paravirt_clobber] "i" (((1 << 0))),"D" ((unsigned long)(
 val)), "S" ((unsigned long)((u64)val >> 32)) : "memory", "cc" ); __ret = (pudval_t)__eax; } __ret; });
	else
		ret = ({ pudval_t __ret; unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_mmu_ops.make_pud.func ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (620), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_mmu_ops.make_pud.func == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_mmu_ops.make_pud.func)
#endif
; if (sizeof(pudval_t) > sizeof(unsigned long)) { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" " " ".balign 8" " " "\n" " " ".quad" " " " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : "=a" (__eax) : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.make_pud.func) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.make_pud.func)), [paravirt_clobber] "i" (((1 << 0))),"D" ((unsigned long)(
 val)) : "memory", "cc" ); __ret = (pudval_t)((((u64)__edx) << 32) | __eax); } else { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" " " ".balign 8" " " "\n" " " ".quad" " " " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : "=a" (__eax) : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.make_pud.func) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.make_pud.func)), [paravirt_clobber] "i" (((1 << 0))),"D" ((unsigned long)(
 val)) : "memory", "cc" ); __ret = (pudval_t)__eax; } __ret; });
	return (pud_t) { ret };
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 pudval_t pud_val(pud_t pud)
{
	pudval_t ret;
	if (sizeof(pudval_t) > sizeof(long))
		ret =  ({ pudval_t __ret; unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_mmu_ops.pud_val.func ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (631), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_mmu_ops.pud_val.func == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_mmu_ops.pud_val.func)
#endif
; if (sizeof(pudval_t) > sizeof(unsigned long)) { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" " " ".balign 8" " " "\n" " " ".quad" " " " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : "=a" (__eax) : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.pud_val.func) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.pud_val.func)), [paravirt_clobber] "i" (((1 << 0))),"D" ((unsigned long)(
 pud.pud)), "S" ((unsigned long)((u64)pud.pud >> 32)) : "memory", "cc" ); __ret = (pudval_t)((((u64)__edx) << 32) | __eax); } else { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" " " ".balign 8" " " "\n" " " ".quad" " " " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : "=a" (__eax) : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.pud_val.func) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.pud_val.func)), [paravirt_clobber] "i" (((1 << 0))),"D" ((unsigned long)(
 pud.pud)), "S" ((unsigned long)((u64)pud.pud >> 32)) : "memory", "cc" ); __ret = (pudval_t)__eax; } __ret; });
	else
		ret =  ({ pudval_t __ret; unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_mmu_ops.pud_val.func ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (634), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_mmu_ops.pud_val.func == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_mmu_ops.pud_val.func)
#endif
; if (sizeof(pudval_t) > sizeof(unsigned long)) { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" " " ".balign 8" " " "\n" " " ".quad" " " " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : "=a" (__eax) : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.pud_val.func) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.pud_val.func)), [paravirt_clobber] "i" (((1 << 0))),"D" ((unsigned long)(
 pud.pud)) : "memory", "cc" ); __ret = (pudval_t)((((u64)__edx) << 32) | __eax); } else { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" " " ".balign 8" " " "\n" " " ".quad" " " " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : "=a" (__eax) : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.pud_val.func) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.pud_val.func)), [paravirt_clobber] "i" (((1 << 0))),"D" ((unsigned long)(
 pud.pud)) : "memory", "cc" ); __ret = (pudval_t)__eax; } __ret; });
	return ret;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void set_pgd(pgd_t *pgdp, pgd_t pgd)
{
	pgdval_t val = native_pgd_val(pgd);
	if (sizeof(pgdval_t) > sizeof(long))
		({ unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_mmu_ops.set_pgd ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (645), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_mmu_ops.set_pgd == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_mmu_ops.set_pgd)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" " " ".balign 8" " " "\n" " " ".quad" " " " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : "=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx) : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.set_pgd) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.set_pgd)), [paravirt_clobber] "i" (((1 << 9) - 1)),"D" ((unsigned long)(pgdp)), "S" ((unsigned long)(
 val)), "d" ((unsigned long)((u64)val >> 32)) : "memory", "cc" , "rax", "r8", "r9", "r10", "r11"); });
	else
		({ unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_mmu_ops.set_pgd ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (648), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_mmu_ops.set_pgd == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_mmu_ops.set_pgd)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" " " ".balign 8" " " "\n" " " ".quad" " " " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : "=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx) : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.set_pgd) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.set_pgd)), [paravirt_clobber] "i" (((1 << 9) - 1)),"D" ((unsigned long)(pgdp)), "S" ((unsigned long)(
 val)) : "memory", "cc" , "rax", "r8", "r9", "r10", "r11"); });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void pgd_clear(pgd_t *pgdp)
{
	set_pgd(pgdp, __pgd(0));
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void pud_clear(pud_t *pudp)
{
	set_pud(pudp, __pud(0));
}
#endif
#endif
#if definedEx(CONFIG_X86_PAE)
/* Special-case pte-setting operations for PAE, which can't update a
   64-bit pte atomically */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void set_pte_atomic(pte_t *ptep, pte_t pte)
{
	({ 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_mmu_ops.set_pte_atomic ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (671), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_mmu_ops.set_pte_atomic == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_mmu_ops.set_pte_atomic)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.set_pte_atomic) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.set_pte_atomic)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(ptep))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(ptep))
#endif
, 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"d" ((unsigned long)(
 pte.pte))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"S" ((unsigned long)(
 pte.pte))
#endif
, 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"c" ((unsigned long)(pte.pte >> 32))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"d" ((unsigned long)(pte.pte >> 32))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "rax", "r8", "r9", "r10", "r11"
#endif
); });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void pte_clear(struct mm_struct *mm, unsigned long addr,
			     pte_t *ptep)
{
	({ 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_mmu_ops.pte_clear ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (677), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_mmu_ops.pte_clear == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_mmu_ops.pte_clear)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.pte_clear) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.pte_clear)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(mm))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(mm))
#endif
, 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"d" ((unsigned long)(addr))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"S" ((unsigned long)(addr))
#endif
, 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"c" ((unsigned long)(ptep))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"d" ((unsigned long)(ptep))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "rax", "r8", "r9", "r10", "r11"
#endif
); });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void pmd_clear(pmd_t *pmdp)
{
	({ 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_mmu_ops.pmd_clear ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (682), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_mmu_ops.pmd_clear == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_mmu_ops.pmd_clear)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.pmd_clear) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.pmd_clear)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(pmdp))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(pmdp))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "rax", "r8", "r9", "r10", "r11"
#endif
); });
}
#endif
#if !definedEx(CONFIG_X86_PAE)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void set_pte_atomic(pte_t *ptep, pte_t pte)
{
	set_pte(ptep, pte);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void pte_clear(struct mm_struct *mm, unsigned long addr,
			     pte_t *ptep)
{
	set_pte_at(mm, addr, ptep, __pte(0));
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void pmd_clear(pmd_t *pmdp)
{
	set_pmd(pmdp, 
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_X86_PAE) && definedEx(CONFIG_PARAVIRT)
((pmd_t) { ((pud_t) { __pgd(0) } ) } )
#endif
#if !definedEx(CONFIG_X86_32) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_X86_PAE) && !definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_X86_PAE)
__pmd(0)
#endif
);
}
#endif
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void arch_start_context_switch(struct task_struct *prev)
{
	({ 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_cpu_ops.start_context_switch ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (705), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_cpu_ops.start_context_switch == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_cpu_ops.start_context_switch)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_cpu_ops.start_context_switch) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_cpu_ops.start_context_switch)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(prev))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(prev))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "rax", "r8", "r9", "r10", "r11"
#endif
); });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void arch_end_context_switch(struct task_struct *next)
{
	({ 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_cpu_ops.end_context_switch ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (710), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_cpu_ops.end_context_switch == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_cpu_ops.end_context_switch)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_cpu_ops.end_context_switch) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_cpu_ops.end_context_switch)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(next))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(next))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "rax", "r8", "r9", "r10", "r11"
#endif
); });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void arch_enter_lazy_mmu_mode(void)
{
	({ 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_mmu_ops.lazy_mode.enter ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (716), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_mmu_ops.lazy_mode.enter == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_mmu_ops.lazy_mode.enter)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.lazy_mode.enter) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.lazy_mode.enter)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
) : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "rax", "r8", "r9", "r10", "r11"
#endif
); });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void arch_leave_lazy_mmu_mode(void)
{
	({ 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_mmu_ops.lazy_mode.leave ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (721), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_mmu_ops.lazy_mode.leave == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_mmu_ops.lazy_mode.leave)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_mmu_ops.lazy_mode.leave) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_mmu_ops.lazy_mode.leave)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
) : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "rax", "r8", "r9", "r10", "r11"
#endif
); });
}
void arch_flush_lazy_mmu_mode(void);
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __set_fixmap(unsigned /* enum fixed_addresses */ idx,
				phys_addr_t phys, pgprot_t flags)
{
	pv_mmu_ops.set_fixmap(idx, phys, flags);
}
#if definedEx(CONFIG_SMP) && definedEx(CONFIG_PARAVIRT_SPINLOCKS)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int arch_spin_is_locked(struct arch_spinlock *lock)
{
	return ({ int __ret; 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_lock_ops.spin_is_locked ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (736), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_lock_ops.spin_is_locked == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_lock_ops.spin_is_locked)
#endif
; if (sizeof(int) > sizeof(unsigned long)) { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
 "=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
, "=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_lock_ops.spin_is_locked) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_lock_ops.spin_is_locked)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(lock))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(lock))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "r8", "r9", "r10", "r11"
#endif
); __ret = (int)((((u64)__edx) << 32) | __eax); } else { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
 "=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
, "=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_lock_ops.spin_is_locked) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_lock_ops.spin_is_locked)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(lock))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(lock))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "r8", "r9", "r10", "r11"
#endif
); __ret = (int)__eax; } __ret; });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int arch_spin_is_contended(struct arch_spinlock *lock)
{
	return ({ int __ret; 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_lock_ops.spin_is_contended ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (741), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_lock_ops.spin_is_contended == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_lock_ops.spin_is_contended)
#endif
; if (sizeof(int) > sizeof(unsigned long)) { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
 "=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
, "=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_lock_ops.spin_is_contended) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_lock_ops.spin_is_contended)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(lock))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(lock))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "r8", "r9", "r10", "r11"
#endif
); __ret = (int)((((u64)__edx) << 32) | __eax); } else { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
 "=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
, "=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_lock_ops.spin_is_contended) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_lock_ops.spin_is_contended)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(lock))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(lock))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "r8", "r9", "r10", "r11"
#endif
); __ret = (int)__eax; } __ret; });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __attribute__((always_inline)) void arch_spin_lock(struct arch_spinlock *lock)
{
	({ 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_lock_ops.spin_lock ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (747), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_lock_ops.spin_lock == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_lock_ops.spin_lock)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_lock_ops.spin_lock) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_lock_ops.spin_lock)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(lock))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(lock))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "rax", "r8", "r9", "r10", "r11"
#endif
); });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __attribute__((always_inline)) void arch_spin_lock_flags(struct arch_spinlock *lock,
						  unsigned long flags)
{
	({ 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_lock_ops.spin_lock_flags ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (753), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_lock_ops.spin_lock_flags == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_lock_ops.spin_lock_flags)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_lock_ops.spin_lock_flags) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_lock_ops.spin_lock_flags)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(lock))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(lock))
#endif
, 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"d" ((unsigned long)(flags))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"S" ((unsigned long)(flags))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "rax", "r8", "r9", "r10", "r11"
#endif
); });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __attribute__((always_inline)) int arch_spin_trylock(struct arch_spinlock *lock)
{
	return ({ int __ret; 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_lock_ops.spin_trylock ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (758), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_lock_ops.spin_trylock == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_lock_ops.spin_trylock)
#endif
; if (sizeof(int) > sizeof(unsigned long)) { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
 "=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
, "=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_lock_ops.spin_trylock) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_lock_ops.spin_trylock)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(lock))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(lock))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "r8", "r9", "r10", "r11"
#endif
); __ret = (int)((((u64)__edx) << 32) | __eax); } else { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
 "=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
, "=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_lock_ops.spin_trylock) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_lock_ops.spin_trylock)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(lock))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(lock))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "r8", "r9", "r10", "r11"
#endif
); __ret = (int)__eax; } __ret; });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __attribute__((always_inline)) void arch_spin_unlock(struct arch_spinlock *lock)
{
	({ 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_lock_ops.spin_unlock ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (763), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_lock_ops.spin_unlock == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_lock_ops.spin_unlock)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx), "=c" (__ecx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=D" (__edi), "=S" (__esi), "=d" (__edx), "=c" (__ecx)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_lock_ops.spin_unlock) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_lock_ops.spin_unlock)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 4) - 1)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 9) - 1)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(lock))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(lock))
#endif
 : "memory", "cc" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
, "rax", "r8", "r9", "r10", "r11"
#endif
); });
}
#endif
#if definedEx(CONFIG_X86_32)
/* save and restore all caller-save registers, except return value */
#endif
#if !definedEx(CONFIG_X86_32)
/* save and restore all caller-save registers, except return value */
/* We save some registers, but all of them, that's too much. We clobber all
 * caller saved registers but the argument parameter */
#endif
/*
 * Generate a thunk around a function which saves all caller-save
 * registers except for the return value.  This allows C functions to
 * be called from assembler code where fewer than normal registers are
 * available.  It may also help code generation around calls from C
 * code if the common case doesn't use many registers.
 *
 * When a callee is wrapped in a thunk, the caller can assume that all
 * arg regs and all scratch registers are preserved across the
 * call. The return value in rax/eax will not be saved, even for void
 * functions.
 */
/* Get a reference to a callee-save function */
/* Promise that "func" already uses the right calling convention */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long __raw_local_save_flags(void)
{
	return ({ unsigned long __ret; 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_irq_ops.save_fl.func ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (843), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_irq_ops.save_fl.func == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_irq_ops.save_fl.func)
#endif
; if (sizeof(unsigned long) > sizeof(unsigned long)) { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_irq_ops.save_fl.func) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_irq_ops.save_fl.func)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 0) |(1 << 2))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
(
(1 << 0)
)
#endif
) : "memory", "cc" ); __ret = (unsigned long)((((u64)__edx) << 32) | __eax); } else { asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_irq_ops.save_fl.func) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_irq_ops.save_fl.func)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 0) |(1 << 2))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
(
(1 << 0)
)
#endif
) : "memory", "cc" ); __ret = (unsigned long)__eax; } __ret; });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void raw_local_irq_restore(unsigned long f)
{
	({ 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_irq_ops.restore_fl.func ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (848), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_irq_ops.restore_fl.func == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_irq_ops.restore_fl.func)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_irq_ops.restore_fl.func) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_irq_ops.restore_fl.func)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 0) |(1 << 2))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
(
(1 << 0)
)
#endif
),
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"a" ((unsigned long)(f))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"D" ((unsigned long)(f))
#endif
 : "memory", "cc" ); });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void raw_local_irq_disable(void)
{
	({ 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_irq_ops.irq_disable.func ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (853), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_irq_ops.irq_disable.func == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_irq_ops.irq_disable.func)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_irq_ops.irq_disable.func) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_irq_ops.irq_disable.func)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 0) |(1 << 2))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
(
(1 << 0)
)
#endif
) : "memory", "cc" ); });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void raw_local_irq_enable(void)
{
	({ 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __eax = __eax, __edx = __edx, __ecx = __ecx
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
unsigned long __edi = __edi, __esi = __esi, __edx = __edx, __ecx = __ecx, __eax = __eax
#endif
; 
#if definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_DEBUG)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(pv_irq_ops.irq_enable.func ==((void *)0)), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h"), "i" (858), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (pv_irq_ops.irq_enable.func == ((void *)0)) ; } while(0)
#endif
#endif
#if definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_DEBUG)
((void)pv_irq_ops.irq_enable.func)
#endif
; asm volatile("" "771:\n\t" "call *%c[paravirt_opptr];" "\n" "772:\n" ".pushsection .parainstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 " 771b\n" "  .byte " "%c[paravirt_typenum]" "\n" "  .byte 772b-771b\n" "  .short " "%c[paravirt_clobber]" "\n" ".popsection\n" "" : 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax), "=d" (__edx)
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
"=a" (__eax)
#endif
 : [paravirt_typenum] "i" ((__builtin_offsetof(struct paravirt_patch_template,pv_irq_ops.irq_enable.func) / sizeof(void *))), [paravirt_opptr] "i" (&(pv_irq_ops.irq_enable.func)), [paravirt_clobber] "i" (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
((1 << 0) |(1 << 2))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_PARAVIRT)
(
(1 << 0)
)
#endif
) : "memory", "cc" ); });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long __raw_local_irq_save(void)
{
	unsigned long f;
	f = __raw_local_save_flags();
	raw_local_irq_disable();
	return f;
}
/* Make sure as little as possible of this mess escapes. */
extern void default_banner(void);
#line 62 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/irqflags.h" 2
#endif
#if !definedEx(CONFIG_PARAVIRT)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long __raw_local_save_flags(void)
{
	return native_save_fl();
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void raw_local_irq_restore(unsigned long flags)
{
	native_restore_fl(flags);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void raw_local_irq_disable(void)
{
	native_irq_disable();
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void raw_local_irq_enable(void)
{
	native_irq_enable();
}
/*
 * Used in the idle loop; sti takes one instruction cycle
 * to complete:
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void raw_safe_halt(void)
{
	native_safe_halt();
}
/*
 * Used when interrupts are already enabled or to
 * shutdown the processor:
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void halt(void)
{
	native_halt();
}
/*
 * For spinlocks, etc:
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long __raw_local_irq_save(void)
{
	unsigned long flags = __raw_local_save_flags();
	raw_local_irq_disable();
	return flags;
}
#endif
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int raw_irqs_disabled_flags(unsigned long flags)
{
	return !(flags & 0x00000200);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int raw_irqs_disabled(void)
{
	unsigned long flags = __raw_local_save_flags();
	return raw_irqs_disabled_flags(flags);
}
#line 59 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/irqflags.h" 2
#line 13 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/system.h" 2
/* entries in ARCH_DLINFO: */
#if !definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_64) && definedEx(CONFIG_IA32_EMULATION)
#endif
#if definedEx(CONFIG_X86_64) && !definedEx(CONFIG_IA32_EMULATION)
#endif
struct task_struct; /* one of the stranger aspects of C forward declarations */
struct task_struct *__switch_to(struct task_struct *prev,
				struct task_struct *next);
struct tss_struct;
void __switch_to_xtra(struct task_struct *prev_p, struct task_struct *next_p,
		      struct tss_struct *tss);
extern void show_regs_common(void);
#if definedEx(CONFIG_X86_32)
#if definedEx(CONFIG_CC_STACKPROTECTOR)
#endif
#if !definedEx(CONFIG_CC_STACKPROTECTOR)
#endif
/*
 * Saving eflags is important. It switches not only IOPL between tasks,
 * it also protects other tasks from NT leaking through sysenter etc.
 */
/*
 * disable hlt during certain critical i/o operations
 */
#endif
#if !definedEx(CONFIG_X86_32)
/* frame pointer must be last for get_wchan */
#if definedEx(CONFIG_CC_STACKPROTECTOR)
#endif
#if !definedEx(CONFIG_CC_STACKPROTECTOR)
#endif
/* Save restore flags to clear handle leaking NT */
#endif
extern void native_load_gs_index(unsigned);
/*
 * Load a segment. Fall back on loading the zero
 * segment if something goes wrong..
 */
/*
 * Save a segment register away
 */
/*
 * x86_32 user gs accessors.
 */
#if definedEx(CONFIG_X86_32)
#if definedEx(CONFIG_X86_32_LAZY_GS)
#endif
#if !definedEx(CONFIG_X86_32_LAZY_GS)
#endif
#endif
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long get_limit(unsigned long segment)
{
	unsigned long __limit;
	asm("lsll %1,%0" : "=r" (__limit) : "r" (segment));
	return __limit + 1;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void native_clts(void)
{
	asm volatile("clts");
}
/*
 * Volatile isn't enough to prevent the compiler from reordering the
 * read/write functions for the control registers and messing everything up.
 * A memory clobber would solve the problem, but would prevent reordering of
 * all loads stores around it, which can hurt performance. Solution is to
 * use a variable and mimic reads and writes to it to enforce serialization
 */
static unsigned long __force_order;
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long native_read_cr0(void)
{
	unsigned long val;
	asm volatile("mov %%cr0,%0\n\t" : "=r" (val), "=m" (__force_order));
	return val;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void native_write_cr0(unsigned long val)
{
	asm volatile("mov %0,%%cr0": : "r" (val), "m" (__force_order));
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long native_read_cr2(void)
{
	unsigned long val;
	asm volatile("mov %%cr2,%0\n\t" : "=r" (val), "=m" (__force_order));
	return val;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void native_write_cr2(unsigned long val)
{
	asm volatile("mov %0,%%cr2": : "r" (val), "m" (__force_order));
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long native_read_cr3(void)
{
	unsigned long val;
	asm volatile("mov %%cr3,%0\n\t" : "=r" (val), "=m" (__force_order));
	return val;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void native_write_cr3(unsigned long val)
{
	asm volatile("mov %0,%%cr3": : "r" (val), "m" (__force_order));
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long native_read_cr4(void)
{
	unsigned long val;
	asm volatile("mov %%cr4,%0\n\t" : "=r" (val), "=m" (__force_order));
	return val;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long native_read_cr4_safe(void)
{
	unsigned long val;
	/* This could fault if %cr4 does not exist. In x86_64, a cr4 always
	 * exists, so it will never fail. */
#if definedEx(CONFIG_X86_32)
	asm volatile("1: mov %%cr4, %0\n"
		     "2:\n"
		     " .section __ex_table,\"a\"\n" " " ".balign 4" " " "\n" " " ".long" " " "1b" "," "2b" "\n" " .previous\n"
		     : "=r" (val), "=m" (__force_order) : "0" (0));
#endif
#if !definedEx(CONFIG_X86_32)
	val = native_read_cr4();
#endif
	return val;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void native_write_cr4(unsigned long val)
{
	asm volatile("mov %0,%%cr4": : "r" (val), "m" (__force_order));
}
#if definedEx(CONFIG_X86_64)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long native_read_cr8(void)
{
	unsigned long cr8;
	asm volatile("movq %%cr8,%0" : "=r" (cr8));
	return cr8;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void native_write_cr8(unsigned long val)
{
	asm volatile("movq %0,%%cr8" :: "r" (val) : "memory");
}
#endif
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void native_wbinvd(void)
{
	asm volatile("wbinvd": : :"memory");
}
#if definedEx(CONFIG_PARAVIRT)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h" 1
#line 308 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/system.h" 2
#endif
#if !definedEx(CONFIG_PARAVIRT)
#if definedEx(CONFIG_X86_64)
#endif
/* Clear the 'TS' bit */
#endif
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void clflush(volatile void *__p)
{
	asm volatile("clflush %0" : "+m" (*(volatile char  *)__p));
}
void disable_hlt(void);
void enable_hlt(void);
void cpu_idle_wait(void);
extern unsigned long arch_align_stack(unsigned long sp);
extern void free_init_pages(char *what, unsigned long begin, unsigned long end);
void default_idle(void);
void stop_this_cpu(void *dummy);
/*
 * Force strict CPU ordering.
 * And yes, this is required on UP too when we're talking
 * to devices.
 */
#if definedEx(CONFIG_X86_32)
/*
 * Some non-Intel clones support out of order store. wmb() ceases to be a
 * nop for these.
 */
#endif
#if !definedEx(CONFIG_X86_32)
#endif
/**
 * read_barrier_depends - Flush all pending reads that subsequents reads
 * depend on.
 *
 * No data-dependent reads from memory-like regions are ever reordered
 * over this barrier.  All reads preceding this primitive are guaranteed
 * to access memory (but not necessarily other CPUs' caches) before any
 * reads following this primitive that depend on the data return by
 * any of the preceding reads.  This primitive is much lighter weight than
 * rmb() on most CPUs, and is never heavier weight than is
 * rmb().
 *
 * These ordering constraints are respected by both the local CPU
 * and the compiler.
 *
 * Ordering is not guaranteed by anything other than these primitives,
 * not even by data dependencies.  See the documentation for
 * memory_barrier() for examples and URLs to more information.
 *
 * For example, the following code would force ordering (the initial
 * value of "a" is zero, "b" is one, and "p" is "&a"):
 *
 * <programlisting>
 *	CPU 0				CPU 1
 *
 *	b = 2;
 *	memory_barrier();
 *	p = &b;				q = p;
 *					read_barrier_depends();
 *					d = *q;
 * </programlisting>
 *
 * because the read of "*q" depends on the read of "p" and these
 * two reads are separated by a read_barrier_depends().  However,
 * the following code, with the same initial values for "a" and "b":
 *
 * <programlisting>
 *	CPU 0				CPU 1
 *
 *	a = 2;
 *	memory_barrier();
 *	b = 3;				y = b;
 *					read_barrier_depends();
 *					x = a;
 * </programlisting>
 *
 * does not enforce ordering, since there is no data dependency between
 * the read of "a" and the read of "b".  Therefore, on some CPUs, such
 * as Alpha, "y" could be set to 3 and "x" to 0.  Use rmb()
 * in cases like this where there are no data dependencies.
 **/
#if definedEx(CONFIG_SMP)
#if definedEx(CONFIG_X86_PPRO_FENCE)
#endif
#if !definedEx(CONFIG_X86_PPRO_FENCE)
#endif
#if definedEx(CONFIG_X86_OOSTORE)
#endif
#if !definedEx(CONFIG_X86_OOSTORE)
#endif
#endif
#if !definedEx(CONFIG_SMP)
#endif
/*
 * Stop RDTSC speculation. This is needed when you need to use RDTSC
 * (or get_cycles or vread that possibly accesses the TSC) in a defined
 * code region.
 *
 * (Could use an alternative three way for this if there was one.)
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void rdtsc_barrier(void)
{
	asm volatile ("661:\n\t" 
#if definedEx(CONFIG_MK7)
".byte 0x8d,0x04,0x20\n"
#endif
#if !definedEx(CONFIG_MK7) && definedEx(CONFIG_X86_P6_NOP)
".byte 0x0f,0x1f,0x00\n"
#endif
#if !definedEx(CONFIG_MK7) && definedEx(CONFIG_X86_64) && !definedEx(CONFIG_X86_P6_NOP)
".byte 0x66,0x66,0x90\n"
#endif
#if !definedEx(CONFIG_MK7) && !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_X86_P6_NOP)
".byte 0x8d,0x76,0x00\n"
#endif
 "\n662:\n" ".section .altinstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 "661b\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 "663f\n" "	 .byte " "(3*32+17)" "\n" "	 .byte 662b-661b\n" "	 .byte 664f-663f\n" "	 .byte 0xff + (664f-663f) - (662b-661b)\n" ".previous\n" ".section .altinstr_replacement, \"ax\"\n" "663:\n\t" "mfence" "\n664:\n" ".previous" : : : "memory");
	asm volatile ("661:\n\t" 
#if definedEx(CONFIG_MK7)
".byte 0x8d,0x04,0x20\n"
#endif
#if !definedEx(CONFIG_MK7) && definedEx(CONFIG_X86_P6_NOP)
".byte 0x0f,0x1f,0x00\n"
#endif
#if !definedEx(CONFIG_MK7) && definedEx(CONFIG_X86_64) && !definedEx(CONFIG_X86_P6_NOP)
".byte 0x66,0x66,0x90\n"
#endif
#if !definedEx(CONFIG_MK7) && !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_X86_P6_NOP)
".byte 0x8d,0x76,0x00\n"
#endif
 "\n662:\n" ".section .altinstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 "661b\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 "663f\n" "	 .byte " "(3*32+18)" "\n" "	 .byte 662b-661b\n" "	 .byte 664f-663f\n" "	 .byte 0xff + (664f-663f) - (662b-661b)\n" ".previous\n" ".section .altinstr_replacement, \"ax\"\n" "663:\n\t" "lfence" "\n664:\n" ".previous" : : : "memory");
}
#line 19 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/processor.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/page.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/page.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/page_types.h" 1
#line 10 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/page.h" 2
#if definedEx(CONFIG_X86_64)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/page_64.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/page_64_types.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/page_64.h" 2
#line 13 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/page.h" 2
#endif
#if !definedEx(CONFIG_X86_64)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/page_32.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/page_32_types.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/page_32.h" 2
#if definedEx(CONFIG_HUGETLB_PAGE)
#endif
#if definedEx(CONFIG_DEBUG_VIRTUAL)
extern unsigned long __phys_addr(unsigned long);
#endif
#if !definedEx(CONFIG_DEBUG_VIRTUAL)
#endif
#if definedEx(CONFIG_FLATMEM)
#endif
#if definedEx(CONFIG_X86_USE_3DNOW)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/mmx.h" 1
#if !definedEx(CONFIG_X86_32) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_X86_USE_3DNOW) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_X86_USE_3DNOW) && !definedEx(CONFIG_PARAVIRT)
/*
 *	MMX 3Dnow! helper operations
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 10 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/mmx.h" 2
extern void *_mmx_memcpy(void *to, const void *from, size_t size);
extern void mmx_clear_page(void *page);
extern void mmx_copy_page(void *to, void *from);
#endif
#line 27 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/page_32.h" 2
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void clear_page(void *page)
{
	mmx_clear_page(page);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void copy_page(void *to, void *from)
{
	mmx_copy_page(to, from);
}
#endif
#if !definedEx(CONFIG_X86_USE_3DNOW)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/string.h" 1
#if !definedEx(CONFIG_PARAVIRT)
/* We don't want strings.h stuff being used by user stuff by accident */
 #line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/compiler.h" 1
#line 12 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/string.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 13 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/string.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/stddef.h" 1
#line 14 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/string.h" 2
#line 1 "systems/redhat/usr/lib/gcc/x86_64-redhat-linux/4.4.4/include/stdarg.h" 1
/* Copyright (C) 1989, 1997, 1998, 1999, 2000, 2009 Free Software Foundation, Inc.
This file is part of GCC.
GCC is free software; you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation; either version 3, or (at your option)
any later version.
GCC is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.
Under Section 7 of GPL version 3, you are granted additional
permissions described in the GCC Runtime Library Exception, version
3.1, as published by the Free Software Foundation.
You should have received a copy of the GNU General Public License and
a copy of the GCC Runtime Library Exception along with this program;
see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see
<http://www.gnu.org/licenses/>.  */
/*
 * ISO C Standard:  7.15  Variable arguments  <stdarg.h>
 */
#line 15 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/string.h" 2
extern char *strndup_user(const char  *, long);
extern void *memdup_user(const void  *, size_t);
/*
 * Include machine specific inline routines
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/string.h" 1
#if definedEx(CONFIG_X86_32)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/string_32.h" 1
/* Let gcc decide whether to inline or use the out of line functions */
extern char *strcpy(char *dest, const char *src);
extern char *strncpy(char *dest, const char *src, size_t count);
extern char *strcat(char *dest, const char *src);
extern char *strncat(char *dest, const char *src, size_t count);
extern int strcmp(const char *cs, const char *ct);
extern int strncmp(const char *cs, const char *ct, size_t count);
extern char *strchr(const char *s, int c);
extern size_t strlen(const char *s);
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __attribute__((always_inline)) void *__memcpy(void *to, const void *from, size_t n)
{
	int d0, d1, d2;
	asm volatile("rep ; movsl\n\t"
		     "movl %4,%%ecx\n\t"
		     "andl $3,%%ecx\n\t"
		     "jz 1f\n\t"
		     "rep ; movsb\n\t"
		     "1:"
		     : "=&c" (d0), "=&D" (d1), "=&S" (d2)
		     : "0" (n / 4), "g" (n), "1" ((long)to), "2" ((long)from)
		     : "memory");
	return to;
}
/*
 * This looks ugly, but the compiler can optimize it totally,
 * as the count is constant.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __attribute__((always_inline)) void *__constant_memcpy(void *to, const void *from,
					       size_t n)
{
	long esi, edi;
	if (!n)
		return to;
	switch (n) {
	case 1:
		*(char *)to = *(char *)from;
		return to;
	case 2:
		*(short *)to = *(short *)from;
		return to;
	case 4:
		*(int *)to = *(int *)from;
		return to;
	case 3:
		*(short *)to = *(short *)from;
		*((char *)to + 2) = *((char *)from + 2);
		return to;
	case 5:
		*(int *)to = *(int *)from;
		*((char *)to + 4) = *((char *)from + 4);
		return to;
	case 6:
		*(int *)to = *(int *)from;
		*((short *)to + 2) = *((short *)from + 2);
		return to;
	case 8:
		*(int *)to = *(int *)from;
		*((int *)to + 1) = *((int *)from + 1);
		return to;
	}
	esi = (long)from;
	edi = (long)to;
	if (n >= 5 * 4) {
		/* large block: use rep prefix */
		int ecx;
		asm volatile("rep ; movsl"
			     : "=&c" (ecx), "=&D" (edi), "=&S" (esi)
			     : "0" (n / 4), "1" (edi), "2" (esi)
			     : "memory"
		);
	} else {
		/* small block: don't clobber ecx + smaller code */
		if (n >= 4 * 4)
			asm volatile("movsl"
				     : "=&D"(edi), "=&S"(esi)
				     : "0"(edi), "1"(esi)
				     : "memory");
		if (n >= 3 * 4)
			asm volatile("movsl"
				     : "=&D"(edi), "=&S"(esi)
				     : "0"(edi), "1"(esi)
				     : "memory");
		if (n >= 2 * 4)
			asm volatile("movsl"
				     : "=&D"(edi), "=&S"(esi)
				     : "0"(edi), "1"(esi)
				     : "memory");
		if (n >= 1 * 4)
			asm volatile("movsl"
				     : "=&D"(edi), "=&S"(esi)
				     : "0"(edi), "1"(esi)
				     : "memory");
	}
	switch (n % 4) {
		/* tail */
	case 0:
		return to;
	case 1:
		asm volatile("movsb"
			     : "=&D"(edi), "=&S"(esi)
			     : "0"(edi), "1"(esi)
			     : "memory");
		return to;
	case 2:
		asm volatile("movsw"
			     : "=&D"(edi), "=&S"(esi)
			     : "0"(edi), "1"(esi)
			     : "memory");
		return to;
	default:
		asm volatile("movsw\n\tmovsb"
			     : "=&D"(edi), "=&S"(esi)
			     : "0"(edi), "1"(esi)
			     : "memory");
		return to;
	}
}
 /*
 *	No 3D Now!
 */
#if !definedEx(CONFIG_KMEMCHECK)
#endif
#if definedEx(CONFIG_KMEMCHECK)
/*
 * kmemcheck becomes very happy if we use the REP instructions unconditionally,
 * because it means that we know both memory operands in advance.
 */
#endif
void *memmove(void *dest, const void *src, size_t n);
extern void *memchr(const void *cs, int c, size_t count);
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void *__memset_generic(void *s, char c, size_t count)
{
	int d0, d1;
	asm volatile("rep\n\t"
		     "stosb"
		     : "=&c" (d0), "=&D" (d1)
		     : "a" (c), "1" (s), "0" (count)
		     : "memory");
	return s;
}
/* we might want to write optimized versions of these later */
/*
 * memset(x, 0, y) is a reasonably common thing to do, so we want to fill
 * things 32 bits at a time even when we don't know the size of the
 * area at compile-time..
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __attribute__((always_inline))
void *__constant_c_memset(void *s, unsigned long c, size_t count)
{
	int d0, d1;
	asm volatile("rep ; stosl\n\t"
		     "testb $2,%b3\n\t"
		     "je 1f\n\t"
		     "stosw\n"
		     "1:\ttestb $1,%b3\n\t"
		     "je 2f\n\t"
		     "stosb\n"
		     "2:"
		     : "=&c" (d0), "=&D" (d1)
		     : "a" (c), "q" (count), "0" (count/4), "1" ((long)s)
		     : "memory");
	return s;
}
/* Added by Gertjan van Wingerde to make minix and sysv module work */
extern size_t strnlen(const char *s, size_t count);
/* end of additional stuff */
extern char *strstr(const char *cs, const char *ct);
/*
 * This looks horribly ugly, but the compiler can optimize it totally,
 * as we by now know that both pattern and count is constant..
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __attribute__((always_inline))
void *__constant_c_and_count_memset(void *s, unsigned long pattern,
				    size_t count)
{
	switch (count) {
	case 0:
		return s;
	case 1:
		*(unsigned char *)s = pattern & 0xff;
		return s;
	case 2:
		*(unsigned short *)s = pattern & 0xffff;
		return s;
	case 3:
		*(unsigned short *)s = pattern & 0xffff;
		*((unsigned char *)s + 2) = pattern & 0xff;
		return s;
	case 4:
		*(unsigned long *)s = pattern;
		return s;
	}
	{
		int d0, d1;
 		unsigned long eax = pattern;
		switch (count % 4) {
		case 0:
			asm volatile("rep ; stosl" "" : "=&c" (d0), "=&D" (d1) : "a" (eax), "0" (count/4), "1" ((long)s) : "memory");
			return s;
		case 1:
			asm volatile("rep ; stosl" "\n\tstosb" : "=&c" (d0), "=&D" (d1) : "a" (eax), "0" (count/4), "1" ((long)s) : "memory");
			return s;
		case 2:
			asm volatile("rep ; stosl" "\n\tstosw" : "=&c" (d0), "=&D" (d1) : "a" (eax), "0" (count/4), "1" ((long)s) : "memory");
			return s;
		default:
			asm volatile("rep ; stosl" "\n\tstosw\n\tstosb" : "=&c" (d0), "=&D" (d1) : "a" (eax), "0" (count/4), "1" ((long)s) : "memory");
			return s;
		}
	}
}
/*
 * find the first occurrence of byte 'c', or 1 past the area if none
 */
extern void *memscan(void *addr, int c, size_t size);
#line 4 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/string.h" 2
#endif
#if !definedEx(CONFIG_X86_32)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/string_64.h" 1
/* Written 2002 by Andi Kleen */
/* Only used for special circumstances. Stolen from i386/string.h */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __attribute__((always_inline)) void *__inline_memcpy(void *to, const void *from, size_t n)
{
	unsigned long d0, d1, d2;
	asm volatile("rep ; movsl\n\t"
		     "testb $2,%b4\n\t"
		     "je 1f\n\t"
		     "movsw\n"
		     "1:\ttestb $1,%b4\n\t"
		     "je 2f\n\t"
		     "movsb\n"
		     "2:"
		     : "=&c" (d0), "=&D" (d1), "=&S" (d2)
		     : "0" (n / 4), "q" (n), "1" ((long)to), "2" ((long)from)
		     : "memory");
	return to;
}
/* Even with __builtin_ the compiler may decide to use the out of line
   function. */
#if !definedEx(CONFIG_KMEMCHECK)
extern void *memcpy(void *to, const void *from, size_t len);
#endif
#if definedEx(CONFIG_KMEMCHECK)
/*
 * kmemcheck becomes very happy if we use the REP instructions unconditionally,
 * because it means that we know both memory operands in advance.
 */
#endif
void *memset(void *s, int c, size_t n);
void *memmove(void *dest, const void *src, size_t count);
int memcmp(const void *cs, const void *ct, size_t count);
size_t strlen(const char *s);
char *strcpy(char *dest, const char *src);
char *strcat(char *dest, const char *src);
int strcmp(const char *cs, const char *ct);
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/string.h" 2
#endif
#line 23 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/string.h" 2
#if !definedEx(CONFIG_X86_32) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_X86_USE_3DNOW) && definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_X86_USE_3DNOW) && !definedEx(CONFIG_PARAVIRT)
extern char * strcpy(char *,const char *);
#endif
#if !definedEx(CONFIG_X86_32) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_X86_USE_3DNOW) && definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_X86_USE_3DNOW) && !definedEx(CONFIG_PARAVIRT)
extern char * strncpy(char *,const char *, __kernel_size_t);
#endif
size_t strlcpy(char *, const char *, size_t);
#if !definedEx(CONFIG_X86_32) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_X86_USE_3DNOW) && definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_X86_USE_3DNOW) && !definedEx(CONFIG_PARAVIRT)
extern char * strcat(char *, const char *);
#endif
#if !definedEx(CONFIG_X86_32) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_X86_USE_3DNOW) && definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_X86_USE_3DNOW) && !definedEx(CONFIG_PARAVIRT)
extern char * strncat(char *, const char *, __kernel_size_t);
#endif
extern size_t strlcat(char *, const char *, __kernel_size_t);
#if !definedEx(CONFIG_X86_32) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_X86_USE_3DNOW) && definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_X86_USE_3DNOW) && !definedEx(CONFIG_PARAVIRT)
extern int strcmp(const char *,const char *);
#endif
#if !definedEx(CONFIG_X86_32) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_X86_USE_3DNOW) && definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_X86_USE_3DNOW) && !definedEx(CONFIG_PARAVIRT)
extern int strncmp(const char *,const char *,__kernel_size_t);
#endif
extern int strnicmp(const char *, const char *, __kernel_size_t);
extern int strcasecmp(const char *s1, const char *s2);
extern int strncasecmp(const char *s1, const char *s2, size_t n);
#if !definedEx(CONFIG_X86_32) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_X86_USE_3DNOW) && definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_X86_USE_3DNOW) && !definedEx(CONFIG_PARAVIRT)
extern char * strchr(const char *,int);
#endif
extern char * strnchr(const char *, size_t, int);
extern char * strrchr(const char *,int);
extern char * 
#if definedEx(CONFIG_ENABLE_MUST_CHECK)
__attribute__((warn_unused_result))
#endif
#if !definedEx(CONFIG_ENABLE_MUST_CHECK)
#endif
 skip_spaces(const char *);
extern char *strim(char *);
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
#if definedEx(CONFIG_ENABLE_MUST_CHECK)
__attribute__((warn_unused_result))
#endif
#if !definedEx(CONFIG_ENABLE_MUST_CHECK)
#endif
 char *strstrip(char *str)
{
	return strim(str);
}
#if !definedEx(CONFIG_X86_32) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_X86_USE_3DNOW) && definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_X86_USE_3DNOW) && !definedEx(CONFIG_PARAVIRT)
extern char * strstr(const char *, const char *);
#endif
extern char * strnstr(const char *, const char *, size_t);
#if !definedEx(CONFIG_X86_32) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_X86_USE_3DNOW) && definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_X86_USE_3DNOW) && !definedEx(CONFIG_PARAVIRT)
extern __kernel_size_t strlen(const char *);
#endif
#if !definedEx(CONFIG_X86_32) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_X86_USE_3DNOW) && definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_X86_USE_3DNOW) && !definedEx(CONFIG_PARAVIRT)
extern __kernel_size_t strnlen(const char *,__kernel_size_t);
#endif
extern char * strpbrk(const char *,const char *);
extern char * strsep(char **,const char *);
extern __kernel_size_t strspn(const char *,const char *);
extern __kernel_size_t strcspn(const char *,const char *);
#if !definedEx(CONFIG_X86_32) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_X86_USE_3DNOW) && definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_X86_USE_3DNOW) && !definedEx(CONFIG_PARAVIRT)
extern void * memscan(void *,int,__kernel_size_t);
#endif
extern int 
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_X86_USE_3DNOW) && !definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_X86_USE_3DNOW) && definedEx(CONFIG_X86_64) && definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_X86_USE_3DNOW) && definedEx(CONFIG_PARAVIRT)
__builtin_memcmp
#endif
#if !definedEx(CONFIG_X86_32) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_X86_USE_3DNOW) && definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_X86_USE_3DNOW) && !definedEx(CONFIG_PARAVIRT)
memcmp
#endif
(const void *,const void *,__kernel_size_t);
#if !definedEx(CONFIG_X86_32) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_X86_USE_3DNOW) && definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_X86_USE_3DNOW) && !definedEx(CONFIG_PARAVIRT)
extern void * memchr(const void *,int,__kernel_size_t);
#endif
extern char *kstrdup(const char *s, gfp_t gfp);
extern char *kstrndup(const char *s, size_t len, gfp_t gfp);
extern void *kmemdup(const void *src, size_t len, gfp_t gfp);
extern char **argv_split(gfp_t gfp, const char *str, int *argcp);
extern void argv_free(char **argv);
extern bool sysfs_streq(const char *s1, const char *s2);
#if definedEx(CONFIG_BINARY_PRINTF)
int vbin_printf(u32 *bin_buf, size_t size, const char *fmt, va_list args);
int bstr_printf(char *buf, size_t size, const char *fmt, const u32 *bin_buf);
int bprintf(u32 *bin_buf, size_t size, const char *fmt, ...) __attribute__((format(printf,3,4)));
#endif
extern ssize_t memory_read_from_buffer(void *to, size_t count, loff_t *ppos,
			const void *from, size_t available);
/**
 * strstarts - does @str start with @prefix?
 * @str: string to examine
 * @prefix: prefix to look for.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 bool strstarts(const char *str, const char *prefix)
{
	return strncmp(str, prefix, strlen(prefix)) == 0;
}
#endif
#line 39 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/page_32.h" 2
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void clear_page(void *page)
{
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_X86_USE_3DNOW) && !definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_X86_USE_3DNOW) && definedEx(CONFIG_X86_64) && definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_X86_USE_3DNOW) && definedEx(CONFIG_PARAVIRT)
__builtin_memset(page, 0, ((1UL) << 12))
#endif
#if !definedEx(CONFIG_X86_32) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_X86_USE_3DNOW) && definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_X86_USE_3DNOW) && !definedEx(CONFIG_PARAVIRT)
memset(page, 0, ((1UL) << 12))
#endif
;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void copy_page(void *to, void *from)
{
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_X86_USE_3DNOW) && !definedEx(CONFIG_KMEMCHECK) && !definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_X86_USE_3DNOW) && !definedEx(CONFIG_KMEMCHECK) && definedEx(CONFIG_X86_64) && definedEx(CONFIG_PARAVIRT)
__builtin_memcpy(to, from, ((1UL) << 12))
#endif
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_X86_USE_3DNOW) && definedEx(CONFIG_KMEMCHECK) && !definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_X86_USE_3DNOW) && definedEx(CONFIG_KMEMCHECK) && definedEx(CONFIG_X86_64) && definedEx(CONFIG_PARAVIRT)
__memcpy((to), (from), (((1UL) << 12)))
#endif
#if !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_X86_USE_3DNOW) && definedEx(CONFIG_KMEMCHECK) && !definedEx(CONFIG_X86_64) || !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_X86_USE_3DNOW) && definedEx(CONFIG_KMEMCHECK) && definedEx(CONFIG_X86_64) && definedEx(CONFIG_PARAVIRT) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_X86_USE_3DNOW) && definedEx(CONFIG_KMEMCHECK) && definedEx(CONFIG_PARAVIRT)
__inline_memcpy((to), (from), (((1UL) << 12)))
#endif
#if !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_X86_USE_3DNOW) && !definedEx(CONFIG_KMEMCHECK) || !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_X86_USE_3DNOW) && definedEx(CONFIG_KMEMCHECK) && definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_X86_USE_3DNOW) && !definedEx(CONFIG_KMEMCHECK) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_X86_USE_3DNOW) && definedEx(CONFIG_KMEMCHECK) && !definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_X86_USE_3DNOW) && definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_X86_USE_3DNOW)
memcpy(to, from, ((1UL) << 12))
#endif
;
}
#endif
#line 15 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/page.h" 2
#endif
struct page;
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void clear_user_page(void *page, unsigned long vaddr,
				   struct page *pg)
{
	clear_page(page);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void copy_user_page(void *to, void *from, unsigned long vaddr,
				  struct page *topage)
{
	copy_page(to, from);
}
/* __pa_symbol should be used for C visible symbols.
   This seems to be the official gcc blessed way to do such arithmetic. */
/*
 * virt_to_page(kaddr) returns a valid pointer if and only if
 * virt_addr_valid(kaddr) returns true.
 */
extern bool __virt_addr_valid(unsigned long kaddr);
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/asm-generic/memory_model.h" 1
#if definedEx(CONFIG_FLATMEM)
#endif
#if !definedEx(CONFIG_FLATMEM) && definedEx(CONFIG_DISCONTIGMEM)
#endif
/*
 * supports 3 memory models.
 */
#if definedEx(CONFIG_FLATMEM)
#endif
#if !definedEx(CONFIG_FLATMEM) && definedEx(CONFIG_DISCONTIGMEM)
#endif
#if !definedEx(CONFIG_FLATMEM) && !definedEx(CONFIG_DISCONTIGMEM) && definedEx(CONFIG_SPARSEMEM_VMEMMAP)
/* memmap is virtually contiguous.  */
#endif
#if !definedEx(CONFIG_FLATMEM) && !definedEx(CONFIG_DISCONTIGMEM) && definedEx(CONFIG_SPARSEMEM) && !definedEx(CONFIG_SPARSEMEM_VMEMMAP)
/*
 * Note: section's mem_map is encorded to reflect its start_pfn.
 * section[i].section_mem_map == mem_map's address - start_pfn;
 */
#endif
#line 60 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/page.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/asm-generic/getorder.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/compiler.h" 1
#line 8 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/asm-generic/getorder.h" 2
/* Pure 2^n version of get_order */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __attribute__((__const__)) int get_order(unsigned long size)
{
	int order;
	size = (size - 1) >> (12 - 1);
	order = -1;
	do {
		size >>= 1;
		order++;
	} while (size);
	return order;
}
#line 61 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/page.h" 2
#line 20 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/processor.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/pgtable_types.h" 1
#if !definedEx(CONFIG_PARAVIRT)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/const.h" 1
/* const.h: Macros for dealing with constants.  */
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/pgtable_types.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/page_types.h" 1
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/pgtable_types.h" 2
/* If _PAGE_BIT_PRESENT is clear, we use these: */
/* - if the user mapped it with PROT_NONE; pte_present gives true */
/* - set: nonlinear file mapping, saved PTE; unset:swap */
#if definedEx(CONFIG_KMEMCHECK)
#endif
#if !definedEx(CONFIG_KMEMCHECK)
#endif
#if !definedEx(CONFIG_X86_PAE) && definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_PAE)
#endif
#if !definedEx(CONFIG_X86_PAE) && !definedEx(CONFIG_X86_64)
#endif
/* Set of bits not changed in pte_modify */
/*         xwr */
/*
 * early identity mapping  pte attrib macros.
 */
#if definedEx(CONFIG_X86_64)
#endif
#if !definedEx(CONFIG_X86_64)
/*
 * For PDE_IDENT_ATTR include USER bit. As the PDE and PTE protection
 * bits are combined, this will alow user to access the high address mapped
 * VDSO in the presence of CONFIG_COMPAT_VDSO
 */
#endif
#if definedEx(CONFIG_X86_32)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/pgtable_32_types.h" 1
/*
 * The Linux x86 paging architecture is 'compile-time dual-mode', it
 * implements both the traditional 2-level x86 page tables and the
 * newer 3-level PAE-mode page tables.
 */
#if definedEx(CONFIG_X86_PAE)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/pgtable-3level_types.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/pgtable-3level_types.h" 2
typedef u64	pteval_t;
typedef u64	pmdval_t;
typedef u64	pudval_t;
typedef u64	pgdval_t;
typedef u64	pgprotval_t;
typedef union {
	struct {
		unsigned long pte_low, pte_high;
	};
	pteval_t pte;
} pte_t;
/*
 * PGDIR_SHIFT determines what a top-level page table entry can map
 */
/*
 * PMD_SHIFT determines the size of the area a middle-level
 * page table can map
 */
/*
 * entries per page directory level
 */
#line 12 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/pgtable_32_types.h" 2
#endif
#if !definedEx(CONFIG_X86_PAE)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/pgtable-2level_types.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/pgtable-2level_types.h" 2
typedef unsigned long	pteval_t;
typedef unsigned long	pmdval_t;
typedef unsigned long	pudval_t;
typedef unsigned long	pgdval_t;
typedef unsigned long	pgprotval_t;
typedef union {
	pteval_t pte;
	pteval_t pte_low;
} pte_t;
/*
 * traditional i386 two-level paging structure:
 */
/*
 * the i386 is two-level, so we don't really have any
 * PMD directory physically.
 */
#line 16 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/pgtable_32_types.h" 2
#endif
/* Just any arbitrary offset to the start of the vmalloc VM area: the
 * current 8MB value just means that there will be a 8MB "hole" after the
 * physical memory until the kernel virtual memory starts.  That means that
 * any out-of-bounds memory accesses will hopefully be caught.
 * The vmalloc() routines leaves a hole of 4kB between each vmalloced
 * area for the same reason. ;)
 */
extern bool __vmalloc_start_set; /* set once high_memory is set */
#if definedEx(CONFIG_X86_PAE)
#endif
#if !definedEx(CONFIG_X86_PAE)
#endif
#if definedEx(CONFIG_HIGHMEM)
#endif
#if !definedEx(CONFIG_HIGHMEM)
#endif
#line 174 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/pgtable_types.h" 2
#endif
#if !definedEx(CONFIG_X86_32)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/pgtable_64_types.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/pgtable_64_types.h" 2
/*
 * These are used to make use of C type-checking..
 */
typedef unsigned long	pteval_t;
typedef unsigned long	pmdval_t;
typedef unsigned long	pudval_t;
typedef unsigned long	pgdval_t;
typedef unsigned long	pgprotval_t;
typedef struct { pteval_t pte; } pte_t;
/*
 * PGDIR_SHIFT determines what a top-level page table entry can map
 */
/*
 * 3rd level page
 */
/*
 * PMD_SHIFT determines the size of the area a middle-level
 * page table can map
 */
/*
 * entries per page directory level
 */
/* See Documentation/x86/x86_64/mm.txt for a description of the memory map. */
#line 176 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/pgtable_types.h" 2
#endif
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 181 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/pgtable_types.h" 2
/* PTE_PFN_MASK extracts the PFN from a (pte|pmd|pud|pgd)val_t */
/* PTE_FLAGS_MASK extracts the flags from a (pte|pmd|pud|pgd)val_t */
typedef struct pgprot { pgprotval_t pgprot; } pgprot_t;
typedef struct { pgdval_t pgd; } pgd_t;
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 pgd_t native_make_pgd(pgdval_t val)
{
	return (pgd_t) { val };
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 pgdval_t native_pgd_val(pgd_t pgd)
{
	return pgd.pgd;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 pgdval_t pgd_flags(pgd_t pgd)
{
	return native_pgd_val(pgd) & (~((pteval_t)(((signed long)(~(((1UL) << 12)-1))) & ((phys_addr_t)(1ULL << 
#if definedEx(CONFIG_X86_64)
46
#endif
#if definedEx(CONFIG_X86_PAE) && !definedEx(CONFIG_X86_64)
44
#endif
#if !definedEx(CONFIG_X86_PAE) && !definedEx(CONFIG_X86_64)
32
#endif
) - 1))));
}
#if !definedEx(CONFIG_X86_32)
typedef struct { pudval_t pud; } pud_t;
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 pud_t native_make_pud(pmdval_t val)
{
	return (pud_t) { val };
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 pudval_t native_pud_val(pud_t pud)
{
	return pud.pud;
}
#endif
#if definedEx(CONFIG_X86_32)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/asm-generic/pgtable-nopud.h" 1
/*
 * Having the pud type consist of a pgd gets the size right, and allows
 * us to conceptually access the pgd entry that this pud is folded into
 * without casting.
 */
typedef struct { pgd_t pgd; } pud_t;
/*
 * The "pgd_xxx()" functions here are trivial for a folded two-level
 * setup: the pud is never bad, and a pud always exists (as it's folded
 * into the pgd entry)
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int pgd_none(pgd_t pgd)		{ return 0; }
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int pgd_bad(pgd_t pgd)		{ return 0; }
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int pgd_present(pgd_t pgd)	{ return 1; }
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void pgd_clear(pgd_t *pgd)	{ }
/*
 * (puds are folded into pgds so this doesn't get actually called,
 * but the define is needed for a generic inline function.)
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 pud_t * pud_offset(pgd_t * pgd, unsigned long address)
{
	return (pud_t *)pgd;
}
/*
 * allocating and freeing a pud is trivial: the 1-entry pud is
 * inside the pgd, so has no extra memory associated with it.
 */
#line 221 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/pgtable_types.h" 2
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 pudval_t native_pud_val(pud_t pud)
{
	return native_pgd_val(pud.pgd);
}
#endif
#if !definedEx(CONFIG_X86_32) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_X86_PAE)
typedef struct { pmdval_t pmd; } pmd_t;
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 pmd_t native_make_pmd(pmdval_t val)
{
	return (pmd_t) { val };
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 pmdval_t native_pmd_val(pmd_t pmd)
{
	return pmd.pmd;
}
#endif
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_X86_PAE)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/asm-generic/pgtable-nopmd.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/asm-generic/pgtable-nopud.h" 1
#line 8 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/asm-generic/pgtable-nopmd.h" 2
struct mm_struct;
/*
 * Having the pmd type consist of a pud gets the size right, and allows
 * us to conceptually access the pud entry that this pmd is folded into
 * without casting.
 */
typedef struct { pud_t pud; } pmd_t;
/*
 * The "pud_xxx()" functions here are trivial for a folded two-level
 * setup: the pmd is never bad, and a pmd always exists (as it's folded
 * into the pud entry)
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int pud_none(pud_t pud)		{ return 0; }
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int pud_bad(pud_t pud)		{ return 0; }
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int pud_present(pud_t pud)	{ return 1; }
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void pud_clear(pud_t *pud)	{ }
/*
 * (pmds are folded into puds so this doesn't get actually called,
 * but the define is needed for a generic inline function.)
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 pmd_t * pmd_offset(pud_t * pud, unsigned long address)
{
	return (pmd_t *)pud;
}
/*
 * allocating and freeing a pmd is trivial: the 1-entry pmd is
 * inside the pud, so has no extra memory associated with it.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void pmd_free(struct mm_struct *mm, pmd_t *pmd)
{
}
#line 242 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/pgtable_types.h" 2
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 pmdval_t native_pmd_val(pmd_t pmd)
{
	return native_pgd_val(pmd.pud.pgd);
}
#endif
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 pudval_t pud_flags(pud_t pud)
{
	return native_pud_val(pud) & (~((pteval_t)(((signed long)(~(((1UL) << 12)-1))) & ((phys_addr_t)(1ULL << 
#if definedEx(CONFIG_X86_64)
46
#endif
#if definedEx(CONFIG_X86_PAE) && !definedEx(CONFIG_X86_64)
44
#endif
#if !definedEx(CONFIG_X86_PAE) && !definedEx(CONFIG_X86_64)
32
#endif
) - 1))));
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 pmdval_t pmd_flags(pmd_t pmd)
{
	return native_pmd_val(pmd) & (~((pteval_t)(((signed long)(~(((1UL) << 12)-1))) & ((phys_addr_t)(1ULL << 
#if definedEx(CONFIG_X86_64)
46
#endif
#if definedEx(CONFIG_X86_PAE) && !definedEx(CONFIG_X86_64)
44
#endif
#if !definedEx(CONFIG_X86_PAE) && !definedEx(CONFIG_X86_64)
32
#endif
) - 1))));
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 pte_t native_make_pte(pteval_t val)
{
	return (pte_t) { .pte = val };
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 pteval_t native_pte_val(pte_t pte)
{
	return pte.pte;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 pteval_t pte_flags(pte_t pte)
{
	return native_pte_val(pte) & (~((pteval_t)(((signed long)(~(((1UL) << 12)-1))) & ((phys_addr_t)(1ULL << 
#if definedEx(CONFIG_X86_64)
46
#endif
#if definedEx(CONFIG_X86_PAE) && !definedEx(CONFIG_X86_64)
44
#endif
#if !definedEx(CONFIG_X86_PAE) && !definedEx(CONFIG_X86_64)
32
#endif
) - 1))));
}
typedef struct page *pgtable_t;
extern pteval_t __supported_pte_mask;
extern void set_nx(void);
extern int nx_enabled;
extern pgprot_t pgprot_writecombine(pgprot_t prot);
/* Indicate that x86 has its own track and untrack pfn vma functions */
struct file;
pgprot_t phys_mem_access_prot(struct file *file, unsigned long pfn,
                              unsigned long size, pgprot_t vma_prot);
int phys_mem_access_prot_allowed(struct file *file, unsigned long pfn,
                              unsigned long size, pgprot_t *vma_prot);
/* Install a pte for a particular vaddr in kernel space. */
void set_pte_vaddr(unsigned long vaddr, pte_t pte);
#if definedEx(CONFIG_X86_32)
extern void native_pagetable_setup_start(pgd_t *base);
extern void native_pagetable_setup_done(pgd_t *base);
#endif
#if !definedEx(CONFIG_X86_32)
#endif
struct seq_file;
extern void arch_report_meminfo(struct seq_file *m);
enum {
	PG_LEVEL_NONE,
	PG_LEVEL_4K,
	PG_LEVEL_2M,
	PG_LEVEL_1G,
	PG_LEVEL_NUM
};
#if definedEx(CONFIG_PROC_FS)
extern void update_page_count(int level, unsigned long pages);
#endif
#if !definedEx(CONFIG_PROC_FS)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void update_page_count(int level, unsigned long pages) { }
#endif
/*
 * Helper function that returns the kernel pagetable entry controlling
 * the virtual address 'address'. NULL means no pagetable entry present.
 * NOTE: the return type is pte_t but if the pmd is PSE then we return it
 * as a pte too.
 */
extern pte_t *lookup_address(unsigned long address, unsigned int *level);
#endif
#line 21 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/processor.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/percpu.h" 1
#line 22 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/processor.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/msr.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/msr-index.h" 1
/* CPU model specific register (MSR) numbers */
/* x86-64 specific MSRs */
/* EFER bits: */
/* Intel MSRs. Some also available on other CPUs */
/* DEBUGCTLMSR bits (others vary by model): */
/* These are consecutive and not in the normal 4er MCE bank block */
/* AMD64 MSRs. Not complete. See the architecture manual for a more
   complete list. */
/* Fam 10h MSRs */
/* K8 MSRs */
/* C1E active bits in int pending message */
/* K7 MSRs */
/* K6 MSRs */
/* Centaur-Hauls/IDT defined MSRs. */
/* VIA Cyrix defined MSRs*/
/* Transmeta defined MSRs */
/* Intel defined MSRs. */
/* MISC_ENABLE bits: architectural */
/* MISC_ENABLE bits: model-specific, meaning may vary from core to core */
/* P4/Xeon+ specific */
/* Pentium IV performance counter MSRs */
/* Intel Core-based CPU performance counters */
/* Geode defined MSRs */
/* Intel VT MSRs */
/* AMD-V MSRs */
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/msr.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 10 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/msr.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/ioctl.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/ioctl.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/asm-generic/ioctl.h" 1
/* ioctl command encoding: 32 bits total, command in lower 16 bits,
 * size of the parameter structure in the lower 14 bits of the
 * upper 16 bits.
 * Encoding the size of the parameter structure in the ioctl request
 * is useful for catching programs compiled with old versions
 * and to avoid overwriting user space outside the user buffer area.
 * The highest 2 bits are reserved for indicating the ``access mode''.
 * NOTE: This limits the max parameter size to 16kB -1 !
 */
/*
 * The following is for compatibility across the various Linux
 * platforms.  The generic ioctl numbering scheme doesn't really enforce
 * a type field.  De facto, however, the top 8 bits of the lower 16
 * bits are indeed used as a type field, so we might just as well make
 * this explicit here.  Please be sure to use the decoding macros
 * below from now on.
 */
/*
 * Let any architecture override either of the following before
 * including this file.
 */
/*
 * Direction bits, which any architecture can choose to override
 * before including this file.
 */
/* provoke compile error for invalid uses of size argument */
extern unsigned int __invalid_size_argument_for_IOC;
/* used to create numbers */
/* used to decode ioctl numbers.. */
/* ...and for the drivers/sound files... */
#line 3 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/ioctl.h" 2
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/ioctl.h" 2
#line 11 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/msr.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/asm.h" 1
#line 18 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/msr.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/errno.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/asm-generic/errno.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/asm-generic/errno-base.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/asm-generic/errno.h" 2
/* for robust mutexes */
#line 3 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/errno.h" 2
#line 19 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/msr.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/cpumask.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/cpumask.h" 1
#if !definedEx(CONFIG_PARAVIRT)
/*
 * Cpumasks provide a bitmap suitable for representing the
 * set of CPU's in a system, one bit position per CPU number.  In general,
 * only nr_cpu_ids (<= NR_CPUS) bits are valid.
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/kernel.h" 1
#line 11 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/cpumask.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/threads.h" 1
#line 12 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/cpumask.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/bitmap.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 8 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/bitmap.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/bitops.h" 1
#line 9 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/bitmap.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/string.h" 1
#if !definedEx(CONFIG_X86_USE_3DNOW) && definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_X86_USE_3DNOW) && !definedEx(CONFIG_PARAVIRT)
/* We don't want strings.h stuff being used by user stuff by accident */
 #line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/compiler.h" 1
#line 12 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/string.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 13 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/string.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/stddef.h" 1
#line 14 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/string.h" 2
#line 1 "systems/redhat/usr/lib/gcc/x86_64-redhat-linux/4.4.4/include/stdarg.h" 1
/* Copyright (C) 1989, 1997, 1998, 1999, 2000, 2009 Free Software Foundation, Inc.
This file is part of GCC.
GCC is free software; you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation; either version 3, or (at your option)
any later version.
GCC is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.
Under Section 7 of GPL version 3, you are granted additional
permissions described in the GCC Runtime Library Exception, version
3.1, as published by the Free Software Foundation.
You should have received a copy of the GNU General Public License and
a copy of the GCC Runtime Library Exception along with this program;
see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see
<http://www.gnu.org/licenses/>.  */
/*
 * ISO C Standard:  7.15  Variable arguments  <stdarg.h>
 */
#line 15 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/string.h" 2
extern char *strndup_user(const char  *, long);
extern void *memdup_user(const void  *, size_t);
/*
 * Include machine specific inline routines
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/string.h" 1
#if definedEx(CONFIG_X86_32)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/string_32.h" 1
/* Let gcc decide whether to inline or use the out of line functions */
extern char *strcpy(char *dest, const char *src);
extern char *strncpy(char *dest, const char *src, size_t count);
extern char *strcat(char *dest, const char *src);
extern char *strncat(char *dest, const char *src, size_t count);
extern int strcmp(const char *cs, const char *ct);
extern int strncmp(const char *cs, const char *ct, size_t count);
extern char *strchr(const char *s, int c);
extern size_t strlen(const char *s);
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __attribute__((always_inline)) void *__memcpy(void *to, const void *from, size_t n)
{
	int d0, d1, d2;
	asm volatile("rep ; movsl\n\t"
		     "movl %4,%%ecx\n\t"
		     "andl $3,%%ecx\n\t"
		     "jz 1f\n\t"
		     "rep ; movsb\n\t"
		     "1:"
		     : "=&c" (d0), "=&D" (d1), "=&S" (d2)
		     : "0" (n / 4), "g" (n), "1" ((long)to), "2" ((long)from)
		     : "memory");
	return to;
}
/*
 * This looks ugly, but the compiler can optimize it totally,
 * as the count is constant.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __attribute__((always_inline)) void *__constant_memcpy(void *to, const void *from,
					       size_t n)
{
	long esi, edi;
	if (!n)
		return to;
	switch (n) {
	case 1:
		*(char *)to = *(char *)from;
		return to;
	case 2:
		*(short *)to = *(short *)from;
		return to;
	case 4:
		*(int *)to = *(int *)from;
		return to;
	case 3:
		*(short *)to = *(short *)from;
		*((char *)to + 2) = *((char *)from + 2);
		return to;
	case 5:
		*(int *)to = *(int *)from;
		*((char *)to + 4) = *((char *)from + 4);
		return to;
	case 6:
		*(int *)to = *(int *)from;
		*((short *)to + 2) = *((short *)from + 2);
		return to;
	case 8:
		*(int *)to = *(int *)from;
		*((int *)to + 1) = *((int *)from + 1);
		return to;
	}
	esi = (long)from;
	edi = (long)to;
	if (n >= 5 * 4) {
		/* large block: use rep prefix */
		int ecx;
		asm volatile("rep ; movsl"
			     : "=&c" (ecx), "=&D" (edi), "=&S" (esi)
			     : "0" (n / 4), "1" (edi), "2" (esi)
			     : "memory"
		);
	} else {
		/* small block: don't clobber ecx + smaller code */
		if (n >= 4 * 4)
			asm volatile("movsl"
				     : "=&D"(edi), "=&S"(esi)
				     : "0"(edi), "1"(esi)
				     : "memory");
		if (n >= 3 * 4)
			asm volatile("movsl"
				     : "=&D"(edi), "=&S"(esi)
				     : "0"(edi), "1"(esi)
				     : "memory");
		if (n >= 2 * 4)
			asm volatile("movsl"
				     : "=&D"(edi), "=&S"(esi)
				     : "0"(edi), "1"(esi)
				     : "memory");
		if (n >= 1 * 4)
			asm volatile("movsl"
				     : "=&D"(edi), "=&S"(esi)
				     : "0"(edi), "1"(esi)
				     : "memory");
	}
	switch (n % 4) {
		/* tail */
	case 0:
		return to;
	case 1:
		asm volatile("movsb"
			     : "=&D"(edi), "=&S"(esi)
			     : "0"(edi), "1"(esi)
			     : "memory");
		return to;
	case 2:
		asm volatile("movsw"
			     : "=&D"(edi), "=&S"(esi)
			     : "0"(edi), "1"(esi)
			     : "memory");
		return to;
	default:
		asm volatile("movsw\n\tmovsb"
			     : "=&D"(edi), "=&S"(esi)
			     : "0"(edi), "1"(esi)
			     : "memory");
		return to;
	}
}
#if definedEx(CONFIG_X86_USE_3DNOW)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/mmx.h" 1
#if !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_X86_USE_3DNOW) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_X86_USE_3DNOW) && definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_X86_USE_3DNOW) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_X86_USE_3DNOW) && definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT)
/*
 *	MMX 3Dnow! helper operations
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 10 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/mmx.h" 2
extern void *_mmx_memcpy(void *to, const void *from, size_t size);
extern void mmx_clear_page(void *page);
extern void mmx_copy_page(void *to, void *from);
#endif
#line 150 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/string_32.h" 2
/*
 *	This CPU favours 3DNow strongly (eg AMD Athlon)
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void *__constant_memcpy3d(void *to, const void *from, size_t len)
{
	if (len < 512)
		return __constant_memcpy(to, from, len);
	return _mmx_memcpy(to, from, len);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void *__memcpy3d(void *to, const void *from, size_t len)
{
	if (len < 512)
		return __memcpy(to, from, len);
	return _mmx_memcpy(to, from, len);
}
#endif
#if !definedEx(CONFIG_X86_USE_3DNOW)
/*
 *	No 3D Now!
 */
#if !definedEx(CONFIG_KMEMCHECK)
#endif
#if definedEx(CONFIG_KMEMCHECK)
/*
 * kmemcheck becomes very happy if we use the REP instructions unconditionally,
 * because it means that we know both memory operands in advance.
 */
#endif
#endif
void *memmove(void *dest, const void *src, size_t n);
extern void *memchr(const void *cs, int c, size_t count);
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void *__memset_generic(void *s, char c, size_t count)
{
	int d0, d1;
	asm volatile("rep\n\t"
		     "stosb"
		     : "=&c" (d0), "=&D" (d1)
		     : "a" (c), "1" (s), "0" (count)
		     : "memory");
	return s;
}
/* we might want to write optimized versions of these later */
/*
 * memset(x, 0, y) is a reasonably common thing to do, so we want to fill
 * things 32 bits at a time even when we don't know the size of the
 * area at compile-time..
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __attribute__((always_inline))
void *__constant_c_memset(void *s, unsigned long c, size_t count)
{
	int d0, d1;
	asm volatile("rep ; stosl\n\t"
		     "testb $2,%b3\n\t"
		     "je 1f\n\t"
		     "stosw\n"
		     "1:\ttestb $1,%b3\n\t"
		     "je 2f\n\t"
		     "stosb\n"
		     "2:"
		     : "=&c" (d0), "=&D" (d1)
		     : "a" (c), "q" (count), "0" (count/4), "1" ((long)s)
		     : "memory");
	return s;
}
/* Added by Gertjan van Wingerde to make minix and sysv module work */
extern size_t strnlen(const char *s, size_t count);
/* end of additional stuff */
extern char *strstr(const char *cs, const char *ct);
/*
 * This looks horribly ugly, but the compiler can optimize it totally,
 * as we by now know that both pattern and count is constant..
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __attribute__((always_inline))
void *__constant_c_and_count_memset(void *s, unsigned long pattern,
				    size_t count)
{
	switch (count) {
	case 0:
		return s;
	case 1:
		*(unsigned char *)s = pattern & 0xff;
		return s;
	case 2:
		*(unsigned short *)s = pattern & 0xffff;
		return s;
	case 3:
		*(unsigned short *)s = pattern & 0xffff;
		*((unsigned char *)s + 2) = pattern & 0xff;
		return s;
	case 4:
		*(unsigned long *)s = pattern;
		return s;
	}
	{
		int d0, d1;
 		unsigned long eax = pattern;
		switch (count % 4) {
		case 0:
			asm volatile("rep ; stosl" "" : "=&c" (d0), "=&D" (d1) : "a" (eax), "0" (count/4), "1" ((long)s) : "memory");
			return s;
		case 1:
			asm volatile("rep ; stosl" "\n\tstosb" : "=&c" (d0), "=&D" (d1) : "a" (eax), "0" (count/4), "1" ((long)s) : "memory");
			return s;
		case 2:
			asm volatile("rep ; stosl" "\n\tstosw" : "=&c" (d0), "=&D" (d1) : "a" (eax), "0" (count/4), "1" ((long)s) : "memory");
			return s;
		default:
			asm volatile("rep ; stosl" "\n\tstosw\n\tstosb" : "=&c" (d0), "=&D" (d1) : "a" (eax), "0" (count/4), "1" ((long)s) : "memory");
			return s;
		}
	}
}
/*
 * find the first occurrence of byte 'c', or 1 past the area if none
 */
extern void *memscan(void *addr, int c, size_t size);
#line 4 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/string.h" 2
#endif
#if !definedEx(CONFIG_X86_32)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/string_64.h" 1
/* Written 2002 by Andi Kleen */
/* Only used for special circumstances. Stolen from i386/string.h */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __attribute__((always_inline)) void *__inline_memcpy(void *to, const void *from, size_t n)
{
	unsigned long d0, d1, d2;
	asm volatile("rep ; movsl\n\t"
		     "testb $2,%b4\n\t"
		     "je 1f\n\t"
		     "movsw\n"
		     "1:\ttestb $1,%b4\n\t"
		     "je 2f\n\t"
		     "movsb\n"
		     "2:"
		     : "=&c" (d0), "=&D" (d1), "=&S" (d2)
		     : "0" (n / 4), "q" (n), "1" ((long)to), "2" ((long)from)
		     : "memory");
	return to;
}
/* Even with __builtin_ the compiler may decide to use the out of line
   function. */
#if !definedEx(CONFIG_KMEMCHECK)
extern void *memcpy(void *to, const void *from, size_t len);
#endif
#if definedEx(CONFIG_KMEMCHECK)
/*
 * kmemcheck becomes very happy if we use the REP instructions unconditionally,
 * because it means that we know both memory operands in advance.
 */
#endif
void *memset(void *s, int c, size_t n);
void *memmove(void *dest, const void *src, size_t count);
int memcmp(const void *cs, const void *ct, size_t count);
size_t strlen(const char *s);
char *strcpy(char *dest, const char *src);
char *strcat(char *dest, const char *src);
int strcmp(const char *cs, const char *ct);
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/string.h" 2
#endif
#line 23 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/string.h" 2
#if !definedEx(CONFIG_X86_32)
extern char * strcpy(char *,const char *);
#endif
#if !definedEx(CONFIG_X86_32)
extern char * strncpy(char *,const char *, __kernel_size_t);
#endif
size_t strlcpy(char *, const char *, size_t);
#if !definedEx(CONFIG_X86_32)
extern char * strcat(char *, const char *);
#endif
#if !definedEx(CONFIG_X86_32)
extern char * strncat(char *, const char *, __kernel_size_t);
#endif
extern size_t strlcat(char *, const char *, __kernel_size_t);
#if !definedEx(CONFIG_X86_32)
extern int strcmp(const char *,const char *);
#endif
#if !definedEx(CONFIG_X86_32)
extern int strncmp(const char *,const char *,__kernel_size_t);
#endif
extern int strnicmp(const char *, const char *, __kernel_size_t);
extern int strcasecmp(const char *s1, const char *s2);
extern int strncasecmp(const char *s1, const char *s2, size_t n);
#if !definedEx(CONFIG_X86_32)
extern char * strchr(const char *,int);
#endif
extern char * strnchr(const char *, size_t, int);
extern char * strrchr(const char *,int);
extern char * 
#if definedEx(CONFIG_ENABLE_MUST_CHECK)
__attribute__((warn_unused_result))
#endif
#if !definedEx(CONFIG_ENABLE_MUST_CHECK)
#endif
 skip_spaces(const char *);
extern char *strim(char *);
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
#if definedEx(CONFIG_ENABLE_MUST_CHECK)
__attribute__((warn_unused_result))
#endif
#if !definedEx(CONFIG_ENABLE_MUST_CHECK)
#endif
 char *strstrip(char *str)
{
	return strim(str);
}
#if !definedEx(CONFIG_X86_32)
extern char * strstr(const char *, const char *);
#endif
extern char * strnstr(const char *, const char *, size_t);
#if !definedEx(CONFIG_X86_32)
extern __kernel_size_t strlen(const char *);
#endif
#if !definedEx(CONFIG_X86_32)
extern __kernel_size_t strnlen(const char *,__kernel_size_t);
#endif
extern char * strpbrk(const char *,const char *);
extern char * strsep(char **,const char *);
extern __kernel_size_t strspn(const char *,const char *);
extern __kernel_size_t strcspn(const char *,const char *);
#if !definedEx(CONFIG_X86_32)
extern void * memscan(void *,int,__kernel_size_t);
#endif
extern int 
#if definedEx(CONFIG_X86_32)
__builtin_memcmp
#endif
#if !definedEx(CONFIG_X86_32)
memcmp
#endif
(const void *,const void *,__kernel_size_t);
#if !definedEx(CONFIG_X86_32)
extern void * memchr(const void *,int,__kernel_size_t);
#endif
extern char *kstrdup(const char *s, gfp_t gfp);
extern char *kstrndup(const char *s, size_t len, gfp_t gfp);
extern void *kmemdup(const void *src, size_t len, gfp_t gfp);
extern char **argv_split(gfp_t gfp, const char *str, int *argcp);
extern void argv_free(char **argv);
extern bool sysfs_streq(const char *s1, const char *s2);
#if definedEx(CONFIG_BINARY_PRINTF)
int vbin_printf(u32 *bin_buf, size_t size, const char *fmt, va_list args);
int bstr_printf(char *buf, size_t size, const char *fmt, const u32 *bin_buf);
int bprintf(u32 *bin_buf, size_t size, const char *fmt, ...) __attribute__((format(printf,3,4)));
#endif
extern ssize_t memory_read_from_buffer(void *to, size_t count, loff_t *ppos,
			const void *from, size_t available);
/**
 * strstarts - does @str start with @prefix?
 * @str: string to examine
 * @prefix: prefix to look for.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 bool strstarts(const char *str, const char *prefix)
{
	return strncmp(str, prefix, strlen(prefix)) == 0;
}
#endif
#line 10 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/bitmap.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/kernel.h" 1
#line 11 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/bitmap.h" 2
/*
 * bitmaps provide bit arrays that consume one or more unsigned
 * longs.  The bitmap interface and available operations are listed
 * here, in bitmap.h
 *
 * Function implementations generic to all architectures are in
 * lib/bitmap.c.  Functions implementations that are architecture
 * specific are in various include/asm-<arch>/bitops.h headers
 * and other arch/<arch> specific files.
 *
 * See lib/bitmap.c for more details.
 */
/*
 * The available bitmap operations and their rough meaning in the
 * case that the bitmap is a single unsigned long are thus:
 *
 * Note that nbits should be always a compile time evaluable constant.
 * Otherwise many inlines will generate horrible code.
 *
 * bitmap_zero(dst, nbits)			*dst = 0UL
 * bitmap_fill(dst, nbits)			*dst = ~0UL
 * bitmap_copy(dst, src, nbits)			*dst = *src
 * bitmap_and(dst, src1, src2, nbits)		*dst = *src1 & *src2
 * bitmap_or(dst, src1, src2, nbits)		*dst = *src1 | *src2
 * bitmap_xor(dst, src1, src2, nbits)		*dst = *src1 ^ *src2
 * bitmap_andnot(dst, src1, src2, nbits)	*dst = *src1 & ~(*src2)
 * bitmap_complement(dst, src, nbits)		*dst = ~(*src)
 * bitmap_equal(src1, src2, nbits)		Are *src1 and *src2 equal?
 * bitmap_intersects(src1, src2, nbits) 	Do *src1 and *src2 overlap?
 * bitmap_subset(src1, src2, nbits)		Is *src1 a subset of *src2?
 * bitmap_empty(src, nbits)			Are all bits zero in *src?
 * bitmap_full(src, nbits)			Are all bits set in *src?
 * bitmap_weight(src, nbits)			Hamming Weight: number set bits
 * bitmap_set(dst, pos, nbits)			Set specified bit area
 * bitmap_clear(dst, pos, nbits)		Clear specified bit area
 * bitmap_find_next_zero_area(buf, len, pos, n, mask)	Find bit free area
 * bitmap_shift_right(dst, src, n, nbits)	*dst = *src >> n
 * bitmap_shift_left(dst, src, n, nbits)	*dst = *src << n
 * bitmap_remap(dst, src, old, new, nbits)	*dst = map(old, new)(src)
 * bitmap_bitremap(oldbit, old, new, nbits)	newbit = map(old, new)(oldbit)
 * bitmap_onto(dst, orig, relmap, nbits)	*dst = orig relative to relmap
 * bitmap_fold(dst, orig, sz, nbits)		dst bits = orig bits mod sz
 * bitmap_scnprintf(buf, len, src, nbits)	Print bitmap src to buf
 * bitmap_parse(buf, buflen, dst, nbits)	Parse bitmap dst from kernel buf
 * bitmap_parse_user(ubuf, ulen, dst, nbits)	Parse bitmap dst from user buf
 * bitmap_scnlistprintf(buf, len, src, nbits)	Print bitmap src as list to buf
 * bitmap_parselist(buf, dst, nbits)		Parse bitmap dst from list
 * bitmap_find_free_region(bitmap, bits, order)	Find and allocate bit region
 * bitmap_release_region(bitmap, pos, order)	Free specified bit region
 * bitmap_allocate_region(bitmap, pos, order)	Allocate specified bit region
 */
/*
 * Also the following operations in asm/bitops.h apply to bitmaps.
 *
 * set_bit(bit, addr)			*addr |= bit
 * clear_bit(bit, addr)			*addr &= ~bit
 * change_bit(bit, addr)		*addr ^= bit
 * test_bit(bit, addr)			Is bit set in *addr?
 * test_and_set_bit(bit, addr)		Set bit and return old value
 * test_and_clear_bit(bit, addr)	Clear bit and return old value
 * test_and_change_bit(bit, addr)	Change bit and return old value
 * find_first_zero_bit(addr, nbits)	Position first zero bit in *addr
 * find_first_bit(addr, nbits)		Position first set bit in *addr
 * find_next_zero_bit(addr, nbits, bit)	Position next zero bit in *addr >= bit
 * find_next_bit(addr, nbits, bit)	Position next set bit in *addr >= bit
 */
/*
 * The DECLARE_BITMAP(name,bits) macro, in linux/types.h, can be used
 * to declare an array named 'name' of just enough unsigned longs to
 * contain all bit positions from 0 to 'bits' - 1.
 */
/*
 * lib/bitmap.c provides these functions:
 */
extern int __bitmap_empty(const unsigned long *bitmap, int bits);
extern int __bitmap_full(const unsigned long *bitmap, int bits);
extern int __bitmap_equal(const unsigned long *bitmap1,
                	const unsigned long *bitmap2, int bits);
extern void __bitmap_complement(unsigned long *dst, const unsigned long *src,
			int bits);
extern void __bitmap_shift_right(unsigned long *dst,
                        const unsigned long *src, int shift, int bits);
extern void __bitmap_shift_left(unsigned long *dst,
                        const unsigned long *src, int shift, int bits);
extern int __bitmap_and(unsigned long *dst, const unsigned long *bitmap1,
			const unsigned long *bitmap2, int bits);
extern void __bitmap_or(unsigned long *dst, const unsigned long *bitmap1,
			const unsigned long *bitmap2, int bits);
extern void __bitmap_xor(unsigned long *dst, const unsigned long *bitmap1,
			const unsigned long *bitmap2, int bits);
extern int __bitmap_andnot(unsigned long *dst, const unsigned long *bitmap1,
			const unsigned long *bitmap2, int bits);
extern int __bitmap_intersects(const unsigned long *bitmap1,
			const unsigned long *bitmap2, int bits);
extern int __bitmap_subset(const unsigned long *bitmap1,
			const unsigned long *bitmap2, int bits);
extern int __bitmap_weight(const unsigned long *bitmap, int bits);
extern void bitmap_set(unsigned long *map, int i, int len);
extern void bitmap_clear(unsigned long *map, int start, int nr);
extern unsigned long bitmap_find_next_zero_area(unsigned long *map,
					 unsigned long size,
					 unsigned long start,
					 unsigned int nr,
					 unsigned long align_mask);
extern int bitmap_scnprintf(char *buf, unsigned int len,
			const unsigned long *src, int nbits);
extern int __bitmap_parse(const char *buf, unsigned int buflen, int is_user,
			unsigned long *dst, int nbits);
extern int bitmap_parse_user(const char  *ubuf, unsigned int ulen,
			unsigned long *dst, int nbits);
extern int bitmap_scnlistprintf(char *buf, unsigned int len,
			const unsigned long *src, int nbits);
extern int bitmap_parselist(const char *buf, unsigned long *maskp,
			int nmaskbits);
extern void bitmap_remap(unsigned long *dst, const unsigned long *src,
		const unsigned long *old, const unsigned long *new, int bits);
extern int bitmap_bitremap(int oldbit,
		const unsigned long *old, const unsigned long *new, int bits);
extern void bitmap_onto(unsigned long *dst, const unsigned long *orig,
		const unsigned long *relmap, int bits);
extern void bitmap_fold(unsigned long *dst, const unsigned long *orig,
		int sz, int bits);
extern int bitmap_find_free_region(unsigned long *bitmap, int bits, int order);
extern void bitmap_release_region(unsigned long *bitmap, int pos, int order);
extern int bitmap_allocate_region(unsigned long *bitmap, int pos, int order);
extern void bitmap_copy_le(void *dst, const unsigned long *src, int nbits);
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void bitmap_zero(unsigned long *dst, int nbits)
{
	if ((__builtin_constant_p(nbits) && (nbits) <= 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
))
		*dst = 0UL;
	else {
		int len = (((nbits) + (8 * sizeof(long)) - 1) / (8 * sizeof(long))) * sizeof(unsigned long);
#if definedEx(CONFIG_X86_32)
__builtin_memset(dst, 0, len)
#endif
#if !definedEx(CONFIG_X86_32)
memset(dst, 0, len)
#endif
;
	}
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void bitmap_fill(unsigned long *dst, int nbits)
{
	size_t nlongs = (((nbits) + (8 * sizeof(long)) - 1) / (8 * sizeof(long)));
	if (!(__builtin_constant_p(nbits) && (nbits) <= 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
)) {
		int len = (nlongs - 1) * sizeof(unsigned long);
#if definedEx(CONFIG_X86_32)
__builtin_memset(dst, 0xff, len)
#endif
#if !definedEx(CONFIG_X86_32)
memset(dst, 0xff,  len)
#endif
;
	}
	dst[nlongs - 1] = ( ((nbits) % 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
) ? (1UL<<((nbits) % 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
))-1 : ~0UL );
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void bitmap_copy(unsigned long *dst, const unsigned long *src,
			int nbits)
{
	if ((__builtin_constant_p(nbits) && (nbits) <= 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
))
		*dst = *src;
	else {
		int len = (((nbits) + (8 * sizeof(long)) - 1) / (8 * sizeof(long))) * sizeof(unsigned long);
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_X86_USE_3DNOW)
(__builtin_constant_p((len)) ? __constant_memcpy3d((dst), (src), (len)) : __memcpy3d((dst), (src), (len)))
#endif
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_X86_USE_3DNOW) && !definedEx(CONFIG_KMEMCHECK)
__builtin_memcpy(dst, src, len)
#endif
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_X86_USE_3DNOW) && definedEx(CONFIG_KMEMCHECK)
__memcpy((dst), (src), (len))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_KMEMCHECK)
__inline_memcpy((dst), (src), (len))
#endif
#if !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_KMEMCHECK)
memcpy(dst, src, len)
#endif
;
	}
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int bitmap_and(unsigned long *dst, const unsigned long *src1,
			const unsigned long *src2, int nbits)
{
	if ((__builtin_constant_p(nbits) && (nbits) <= 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
))
		return (*dst = *src1 & *src2) != 0;
	return __bitmap_and(dst, src1, src2, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void bitmap_or(unsigned long *dst, const unsigned long *src1,
			const unsigned long *src2, int nbits)
{
	if ((__builtin_constant_p(nbits) && (nbits) <= 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
))
		*dst = *src1 | *src2;
	else
		__bitmap_or(dst, src1, src2, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void bitmap_xor(unsigned long *dst, const unsigned long *src1,
			const unsigned long *src2, int nbits)
{
	if ((__builtin_constant_p(nbits) && (nbits) <= 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
))
		*dst = *src1 ^ *src2;
	else
		__bitmap_xor(dst, src1, src2, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int bitmap_andnot(unsigned long *dst, const unsigned long *src1,
			const unsigned long *src2, int nbits)
{
	if ((__builtin_constant_p(nbits) && (nbits) <= 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
))
		return (*dst = *src1 & ~(*src2)) != 0;
	return __bitmap_andnot(dst, src1, src2, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void bitmap_complement(unsigned long *dst, const unsigned long *src,
			int nbits)
{
	if ((__builtin_constant_p(nbits) && (nbits) <= 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
))
		*dst = ~(*src) & ( ((nbits) % 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
) ? (1UL<<((nbits) % 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
))-1 : ~0UL );
	else
		__bitmap_complement(dst, src, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int bitmap_equal(const unsigned long *src1,
			const unsigned long *src2, int nbits)
{
	if ((__builtin_constant_p(nbits) && (nbits) <= 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
))
		return ! ((*src1 ^ *src2) & ( ((nbits) % 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
) ? (1UL<<((nbits) % 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
))-1 : ~0UL ));
	else
		return __bitmap_equal(src1, src2, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int bitmap_intersects(const unsigned long *src1,
			const unsigned long *src2, int nbits)
{
	if ((__builtin_constant_p(nbits) && (nbits) <= 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
))
		return ((*src1 & *src2) & ( ((nbits) % 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
) ? (1UL<<((nbits) % 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
))-1 : ~0UL )) != 0;
	else
		return __bitmap_intersects(src1, src2, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int bitmap_subset(const unsigned long *src1,
			const unsigned long *src2, int nbits)
{
	if ((__builtin_constant_p(nbits) && (nbits) <= 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
))
		return ! ((*src1 & ~(*src2)) & ( ((nbits) % 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
) ? (1UL<<((nbits) % 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
))-1 : ~0UL ));
	else
		return __bitmap_subset(src1, src2, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int bitmap_empty(const unsigned long *src, int nbits)
{
	if ((__builtin_constant_p(nbits) && (nbits) <= 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
))
		return ! (*src & ( ((nbits) % 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
) ? (1UL<<((nbits) % 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
))-1 : ~0UL ));
	else
		return __bitmap_empty(src, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int bitmap_full(const unsigned long *src, int nbits)
{
	if ((__builtin_constant_p(nbits) && (nbits) <= 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
))
		return ! (~(*src) & ( ((nbits) % 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
) ? (1UL<<((nbits) % 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
))-1 : ~0UL ));
	else
		return __bitmap_full(src, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int bitmap_weight(const unsigned long *src, int nbits)
{
	if ((__builtin_constant_p(nbits) && (nbits) <= 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
))
		return hweight_long(*src & ( ((nbits) % 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
) ? (1UL<<((nbits) % 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
))-1 : ~0UL ));
	return __bitmap_weight(src, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void bitmap_shift_right(unsigned long *dst,
			const unsigned long *src, int n, int nbits)
{
	if ((__builtin_constant_p(nbits) && (nbits) <= 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
))
		*dst = *src >> n;
	else
		__bitmap_shift_right(dst, src, n, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void bitmap_shift_left(unsigned long *dst,
			const unsigned long *src, int n, int nbits)
{
	if ((__builtin_constant_p(nbits) && (nbits) <= 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
))
		*dst = (*src << n) & ( ((nbits) % 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
) ? (1UL<<((nbits) % 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
))-1 : ~0UL );
	else
		__bitmap_shift_left(dst, src, n, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int bitmap_parse(const char *buf, unsigned int buflen,
			unsigned long *maskp, int nmaskbits)
{
	return __bitmap_parse(buf, buflen, 0, maskp, nmaskbits);
}
#line 13 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/cpumask.h" 2
typedef struct cpumask { unsigned long bits[(((
#if definedEx(CONFIG_SMP)
8
#endif
#if !definedEx(CONFIG_SMP)
1
#endif
) + (8 * sizeof(long)) - 1) / (8 * sizeof(long)))]; } cpumask_t;
/**
 * cpumask_bits - get the bits in a cpumask
 * @maskp: the struct cpumask *
 *
 * You should only assume nr_cpu_ids bits of this mask are valid.  This is
 * a macro so it's const-correct.
 */
#if !definedEx(CONFIG_SMP)
#endif
#if definedEx(CONFIG_SMP)
extern int nr_cpu_ids;
#endif
#if definedEx(CONFIG_CPUMASK_OFFSTACK)
/* Assuming NR_CPUS is huge, a runtime limit is more efficient.  Also,
 * not all bits may be allocated. */
#endif
#if !definedEx(CONFIG_CPUMASK_OFFSTACK)
#endif
/*
 * The following particular system cpumasks and operations manage
 * possible, present, active and online cpus.
 *
 *     cpu_possible_mask- has bit 'cpu' set iff cpu is populatable
 *     cpu_present_mask - has bit 'cpu' set iff cpu is populated
 *     cpu_online_mask  - has bit 'cpu' set iff cpu available to scheduler
 *     cpu_active_mask  - has bit 'cpu' set iff cpu available to migration
 *
 *  If !CONFIG_HOTPLUG_CPU, present == possible, and active == online.
 *
 *  The cpu_possible_mask is fixed at boot time, as the set of CPU id's
 *  that it is possible might ever be plugged in at anytime during the
 *  life of that system boot.  The cpu_present_mask is dynamic(*),
 *  representing which CPUs are currently plugged in.  And
 *  cpu_online_mask is the dynamic subset of cpu_present_mask,
 *  indicating those CPUs available for scheduling.
 *
 *  If HOTPLUG is enabled, then cpu_possible_mask is forced to have
 *  all NR_CPUS bits set, otherwise it is just the set of CPUs that
 *  ACPI reports present at boot.
 *
 *  If HOTPLUG is enabled, then cpu_present_mask varies dynamically,
 *  depending on what ACPI reports as currently plugged in, otherwise
 *  cpu_present_mask is just a copy of cpu_possible_mask.
 *
 *  (*) Well, cpu_present_mask is dynamic in the hotplug case.  If not
 *      hotplug, it's a copy of cpu_possible_mask, hence fixed at boot.
 *
 * Subtleties:
 * 1) UP arch's (NR_CPUS == 1, CONFIG_SMP not defined) hardcode
 *    assumption that their single CPU is online.  The UP
 *    cpu_{online,possible,present}_masks are placebos.  Changing them
 *    will have no useful affect on the following num_*_cpus()
 *    and cpu_*() macros in the UP case.  This ugliness is a UP
 *    optimization - don't waste any instructions or memory references
 *    asking if you're online or how many CPUs there are if there is
 *    only one CPU.
 */
extern const struct cpumask *const cpu_possible_mask;
extern const struct cpumask *const cpu_online_mask;
extern const struct cpumask *const cpu_present_mask;
extern const struct cpumask *const cpu_active_mask;
#if definedEx(CONFIG_SMP)
#endif
#if !definedEx(CONFIG_SMP)
#endif
/* verify cpu argument to cpumask_* operators */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned int cpumask_check(unsigned int cpu)
{
#if definedEx(CONFIG_DEBUG_PER_CPU_MAPS)
	({ static bool __warned; int __ret_warn_once = !!(cpu >= 
#if definedEx(CONFIG_CPUMASK_OFFSTACK)
#if !definedEx(CONFIG_SMP)
1
#endif
#if definedEx(CONFIG_SMP)
nr_cpu_ids
#endif
#endif
#if !definedEx(CONFIG_CPUMASK_OFFSTACK)
#if definedEx(CONFIG_SMP)
8
#endif
#if !definedEx(CONFIG_SMP)
1
#endif
#endif
); if (__builtin_expect(!!(__ret_warn_once), 0)) if (
#if definedEx(CONFIG_BUG)
({ int __ret_warn_on = !!(!__warned); if (__builtin_expect(!!(__ret_warn_on), 0)) warn_slowpath_null("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/cpumask.h", 107); __builtin_expect(!!(__ret_warn_on), 0); })
#endif
#if !definedEx(CONFIG_BUG)
({ int __ret_warn_on = !!(!__warned); __builtin_expect(!!(__ret_warn_on), 0); })
#endif
) __warned = true; __builtin_expect(!!(__ret_warn_once), 0); });
#endif
	return cpu;
}
#if !definedEx(CONFIG_SMP)
/* Uniprocessor.  Assume all masks are "1". */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned int cpumask_first(const struct cpumask *srcp)
{
	return 0;
}
/* Valid inputs for n are -1 and 0. */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned int cpumask_next(int n, const struct cpumask *srcp)
{
	return n+1;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned int cpumask_next_zero(int n, const struct cpumask *srcp)
{
	return n+1;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned int cpumask_next_and(int n,
					    const struct cpumask *srcp,
					    const struct cpumask *andp)
{
	return n+1;
}
/* cpu must be a valid cpu, ie 0, so there's no other choice. */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned int cpumask_any_but(const struct cpumask *mask,
					   unsigned int cpu)
{
	return 1;
}
#endif
#if definedEx(CONFIG_SMP)
/**
 * cpumask_first - get the first cpu in a cpumask
 * @srcp: the cpumask pointer
 *
 * Returns >= nr_cpu_ids if no cpus set.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned int cpumask_first(const struct cpumask *srcp)
{
	return find_first_bit(((srcp)->bits), 
#if definedEx(CONFIG_CPUMASK_OFFSTACK)
nr_cpu_ids
#endif
#if !definedEx(CONFIG_CPUMASK_OFFSTACK)
8
#endif
);
}
/**
 * cpumask_next - get the next cpu in a cpumask
 * @n: the cpu prior to the place to search (ie. return will be > @n)
 * @srcp: the cpumask pointer
 *
 * Returns >= nr_cpu_ids if no further cpus set.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned int cpumask_next(int n, const struct cpumask *srcp)
{
	/* -1 is a legal arg here. */
	if (n != -1)
		cpumask_check(n);
	return find_next_bit(((srcp)->bits), 
#if definedEx(CONFIG_CPUMASK_OFFSTACK)
nr_cpu_ids
#endif
#if !definedEx(CONFIG_CPUMASK_OFFSTACK)
8
#endif
, n+1);
}
/**
 * cpumask_next_zero - get the next unset cpu in a cpumask
 * @n: the cpu prior to the place to search (ie. return will be > @n)
 * @srcp: the cpumask pointer
 *
 * Returns >= nr_cpu_ids if no further cpus unset.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned int cpumask_next_zero(int n, const struct cpumask *srcp)
{
	/* -1 is a legal arg here. */
	if (n != -1)
		cpumask_check(n);
	return find_next_zero_bit(((srcp)->bits), 
#if definedEx(CONFIG_CPUMASK_OFFSTACK)
nr_cpu_ids
#endif
#if !definedEx(CONFIG_CPUMASK_OFFSTACK)
8
#endif
, n+1);
}
int cpumask_next_and(int n, const struct cpumask *, const struct cpumask *);
int cpumask_any_but(const struct cpumask *mask, unsigned int cpu);
/**
 * for_each_cpu - iterate over every cpu in a mask
 * @cpu: the (optionally unsigned) integer iterator
 * @mask: the cpumask pointer
 *
 * After the loop, cpu is >= nr_cpu_ids.
 */
/**
 * for_each_cpu_and - iterate over every cpu in both masks
 * @cpu: the (optionally unsigned) integer iterator
 * @mask: the first cpumask pointer
 * @and: the second cpumask pointer
 *
 * This saves a temporary CPU mask in many places.  It is equivalent to:
 *	struct cpumask tmp;
 *	cpumask_and(&tmp, &mask, &and);
 *	for_each_cpu(cpu, &tmp)
 *		...
 *
 * After the loop, cpu is >= nr_cpu_ids.
 */
#endif
/**
 * cpumask_set_cpu - set a cpu in a cpumask
 * @cpu: cpu number (< nr_cpu_ids)
 * @dstp: the cpumask pointer
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void cpumask_set_cpu(unsigned int cpu, struct cpumask *dstp)
{
	set_bit(cpumask_check(cpu), ((dstp)->bits));
}
/**
 * cpumask_clear_cpu - clear a cpu in a cpumask
 * @cpu: cpu number (< nr_cpu_ids)
 * @dstp: the cpumask pointer
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void cpumask_clear_cpu(int cpu, struct cpumask *dstp)
{
	clear_bit(cpumask_check(cpu), ((dstp)->bits));
}
/**
 * cpumask_test_cpu - test for a cpu in a cpumask
 * @cpu: cpu number (< nr_cpu_ids)
 * @cpumask: the cpumask pointer
 *
 * No static inline type checking - see Subtlety (1) above.
 */
/**
 * cpumask_test_and_set_cpu - atomically test and set a cpu in a cpumask
 * @cpu: cpu number (< nr_cpu_ids)
 * @cpumask: the cpumask pointer
 *
 * test_and_set_bit wrapper for cpumasks.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int cpumask_test_and_set_cpu(int cpu, struct cpumask *cpumask)
{
	return test_and_set_bit(cpumask_check(cpu), ((cpumask)->bits));
}
/**
 * cpumask_test_and_clear_cpu - atomically test and clear a cpu in a cpumask
 * @cpu: cpu number (< nr_cpu_ids)
 * @cpumask: the cpumask pointer
 *
 * test_and_clear_bit wrapper for cpumasks.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int cpumask_test_and_clear_cpu(int cpu, struct cpumask *cpumask)
{
	return test_and_clear_bit(cpumask_check(cpu), ((cpumask)->bits));
}
/**
 * cpumask_setall - set all cpus (< nr_cpu_ids) in a cpumask
 * @dstp: the cpumask pointer
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void cpumask_setall(struct cpumask *dstp)
{
	bitmap_fill(((dstp)->bits), 
#if definedEx(CONFIG_CPUMASK_OFFSTACK)
#if !definedEx(CONFIG_SMP)
1
#endif
#if definedEx(CONFIG_SMP)
nr_cpu_ids
#endif
#endif
#if !definedEx(CONFIG_CPUMASK_OFFSTACK)
#if definedEx(CONFIG_SMP)
8
#endif
#if !definedEx(CONFIG_SMP)
1
#endif
#endif
);
}
/**
 * cpumask_clear - clear all cpus (< nr_cpu_ids) in a cpumask
 * @dstp: the cpumask pointer
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void cpumask_clear(struct cpumask *dstp)
{
	bitmap_zero(((dstp)->bits), 
#if definedEx(CONFIG_CPUMASK_OFFSTACK)
#if !definedEx(CONFIG_SMP)
1
#endif
#if definedEx(CONFIG_SMP)
nr_cpu_ids
#endif
#endif
#if !definedEx(CONFIG_CPUMASK_OFFSTACK)
#if definedEx(CONFIG_SMP)
8
#endif
#if !definedEx(CONFIG_SMP)
1
#endif
#endif
);
}
/**
 * cpumask_and - *dstp = *src1p & *src2p
 * @dstp: the cpumask result
 * @src1p: the first input
 * @src2p: the second input
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int cpumask_and(struct cpumask *dstp,
			       const struct cpumask *src1p,
			       const struct cpumask *src2p)
{
	return bitmap_and(((dstp)->bits), ((src1p)->bits),
				       ((src2p)->bits), 
#if definedEx(CONFIG_CPUMASK_OFFSTACK)
#if !definedEx(CONFIG_SMP)
1
#endif
#if definedEx(CONFIG_SMP)
nr_cpu_ids
#endif
#endif
#if !definedEx(CONFIG_CPUMASK_OFFSTACK)
#if definedEx(CONFIG_SMP)
8
#endif
#if !definedEx(CONFIG_SMP)
1
#endif
#endif
);
}
/**
 * cpumask_or - *dstp = *src1p | *src2p
 * @dstp: the cpumask result
 * @src1p: the first input
 * @src2p: the second input
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void cpumask_or(struct cpumask *dstp, const struct cpumask *src1p,
			      const struct cpumask *src2p)
{
	bitmap_or(((dstp)->bits), ((src1p)->bits),
				      ((src2p)->bits), 
#if definedEx(CONFIG_CPUMASK_OFFSTACK)
#if !definedEx(CONFIG_SMP)
1
#endif
#if definedEx(CONFIG_SMP)
nr_cpu_ids
#endif
#endif
#if !definedEx(CONFIG_CPUMASK_OFFSTACK)
#if definedEx(CONFIG_SMP)
8
#endif
#if !definedEx(CONFIG_SMP)
1
#endif
#endif
);
}
/**
 * cpumask_xor - *dstp = *src1p ^ *src2p
 * @dstp: the cpumask result
 * @src1p: the first input
 * @src2p: the second input
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void cpumask_xor(struct cpumask *dstp,
			       const struct cpumask *src1p,
			       const struct cpumask *src2p)
{
	bitmap_xor(((dstp)->bits), ((src1p)->bits),
				       ((src2p)->bits), 
#if definedEx(CONFIG_CPUMASK_OFFSTACK)
#if !definedEx(CONFIG_SMP)
1
#endif
#if definedEx(CONFIG_SMP)
nr_cpu_ids
#endif
#endif
#if !definedEx(CONFIG_CPUMASK_OFFSTACK)
#if definedEx(CONFIG_SMP)
8
#endif
#if !definedEx(CONFIG_SMP)
1
#endif
#endif
);
}
/**
 * cpumask_andnot - *dstp = *src1p & ~*src2p
 * @dstp: the cpumask result
 * @src1p: the first input
 * @src2p: the second input
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int cpumask_andnot(struct cpumask *dstp,
				  const struct cpumask *src1p,
				  const struct cpumask *src2p)
{
	return bitmap_andnot(((dstp)->bits), ((src1p)->bits),
					  ((src2p)->bits), 
#if definedEx(CONFIG_CPUMASK_OFFSTACK)
#if !definedEx(CONFIG_SMP)
1
#endif
#if definedEx(CONFIG_SMP)
nr_cpu_ids
#endif
#endif
#if !definedEx(CONFIG_CPUMASK_OFFSTACK)
#if definedEx(CONFIG_SMP)
8
#endif
#if !definedEx(CONFIG_SMP)
1
#endif
#endif
);
}
/**
 * cpumask_complement - *dstp = ~*srcp
 * @dstp: the cpumask result
 * @srcp: the input to invert
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void cpumask_complement(struct cpumask *dstp,
				      const struct cpumask *srcp)
{
	bitmap_complement(((dstp)->bits), ((srcp)->bits),
#if definedEx(CONFIG_CPUMASK_OFFSTACK)
#if !definedEx(CONFIG_SMP)
1
#endif
#if definedEx(CONFIG_SMP)
nr_cpu_ids
#endif
#endif
#if !definedEx(CONFIG_CPUMASK_OFFSTACK)
#if definedEx(CONFIG_SMP)
8
#endif
#if !definedEx(CONFIG_SMP)
1
#endif
#endif
);
}
/**
 * cpumask_equal - *src1p == *src2p
 * @src1p: the first input
 * @src2p: the second input
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 bool cpumask_equal(const struct cpumask *src1p,
				const struct cpumask *src2p)
{
	return bitmap_equal(((src1p)->bits), ((src2p)->bits),
#if definedEx(CONFIG_CPUMASK_OFFSTACK)
#if !definedEx(CONFIG_SMP)
1
#endif
#if definedEx(CONFIG_SMP)
nr_cpu_ids
#endif
#endif
#if !definedEx(CONFIG_CPUMASK_OFFSTACK)
#if definedEx(CONFIG_SMP)
8
#endif
#if !definedEx(CONFIG_SMP)
1
#endif
#endif
);
}
/**
 * cpumask_intersects - (*src1p & *src2p) != 0
 * @src1p: the first input
 * @src2p: the second input
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 bool cpumask_intersects(const struct cpumask *src1p,
				     const struct cpumask *src2p)
{
	return bitmap_intersects(((src1p)->bits), ((src2p)->bits),
#if definedEx(CONFIG_CPUMASK_OFFSTACK)
#if !definedEx(CONFIG_SMP)
1
#endif
#if definedEx(CONFIG_SMP)
nr_cpu_ids
#endif
#endif
#if !definedEx(CONFIG_CPUMASK_OFFSTACK)
#if definedEx(CONFIG_SMP)
8
#endif
#if !definedEx(CONFIG_SMP)
1
#endif
#endif
);
}
/**
 * cpumask_subset - (*src1p & ~*src2p) == 0
 * @src1p: the first input
 * @src2p: the second input
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int cpumask_subset(const struct cpumask *src1p,
				 const struct cpumask *src2p)
{
	return bitmap_subset(((src1p)->bits), ((src2p)->bits),
#if definedEx(CONFIG_CPUMASK_OFFSTACK)
#if !definedEx(CONFIG_SMP)
1
#endif
#if definedEx(CONFIG_SMP)
nr_cpu_ids
#endif
#endif
#if !definedEx(CONFIG_CPUMASK_OFFSTACK)
#if definedEx(CONFIG_SMP)
8
#endif
#if !definedEx(CONFIG_SMP)
1
#endif
#endif
);
}
/**
 * cpumask_empty - *srcp == 0
 * @srcp: the cpumask to that all cpus < nr_cpu_ids are clear.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 bool cpumask_empty(const struct cpumask *srcp)
{
	return bitmap_empty(((srcp)->bits), 
#if definedEx(CONFIG_CPUMASK_OFFSTACK)
#if !definedEx(CONFIG_SMP)
1
#endif
#if definedEx(CONFIG_SMP)
nr_cpu_ids
#endif
#endif
#if !definedEx(CONFIG_CPUMASK_OFFSTACK)
#if definedEx(CONFIG_SMP)
8
#endif
#if !definedEx(CONFIG_SMP)
1
#endif
#endif
);
}
/**
 * cpumask_full - *srcp == 0xFFFFFFFF...
 * @srcp: the cpumask to that all cpus < nr_cpu_ids are set.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 bool cpumask_full(const struct cpumask *srcp)
{
	return bitmap_full(((srcp)->bits), 
#if definedEx(CONFIG_CPUMASK_OFFSTACK)
#if !definedEx(CONFIG_SMP)
1
#endif
#if definedEx(CONFIG_SMP)
nr_cpu_ids
#endif
#endif
#if !definedEx(CONFIG_CPUMASK_OFFSTACK)
#if definedEx(CONFIG_SMP)
8
#endif
#if !definedEx(CONFIG_SMP)
1
#endif
#endif
);
}
/**
 * cpumask_weight - Count of bits in *srcp
 * @srcp: the cpumask to count bits (< nr_cpu_ids) in.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned int cpumask_weight(const struct cpumask *srcp)
{
	return bitmap_weight(((srcp)->bits), 
#if definedEx(CONFIG_CPUMASK_OFFSTACK)
#if !definedEx(CONFIG_SMP)
1
#endif
#if definedEx(CONFIG_SMP)
nr_cpu_ids
#endif
#endif
#if !definedEx(CONFIG_CPUMASK_OFFSTACK)
#if definedEx(CONFIG_SMP)
8
#endif
#if !definedEx(CONFIG_SMP)
1
#endif
#endif
);
}
/**
 * cpumask_shift_right - *dstp = *srcp >> n
 * @dstp: the cpumask result
 * @srcp: the input to shift
 * @n: the number of bits to shift by
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void cpumask_shift_right(struct cpumask *dstp,
				       const struct cpumask *srcp, int n)
{
	bitmap_shift_right(((dstp)->bits), ((srcp)->bits), n,
#if definedEx(CONFIG_CPUMASK_OFFSTACK)
#if !definedEx(CONFIG_SMP)
1
#endif
#if definedEx(CONFIG_SMP)
nr_cpu_ids
#endif
#endif
#if !definedEx(CONFIG_CPUMASK_OFFSTACK)
#if definedEx(CONFIG_SMP)
8
#endif
#if !definedEx(CONFIG_SMP)
1
#endif
#endif
);
}
/**
 * cpumask_shift_left - *dstp = *srcp << n
 * @dstp: the cpumask result
 * @srcp: the input to shift
 * @n: the number of bits to shift by
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void cpumask_shift_left(struct cpumask *dstp,
				      const struct cpumask *srcp, int n)
{
	bitmap_shift_left(((dstp)->bits), ((srcp)->bits), n,
#if definedEx(CONFIG_CPUMASK_OFFSTACK)
#if !definedEx(CONFIG_SMP)
1
#endif
#if definedEx(CONFIG_SMP)
nr_cpu_ids
#endif
#endif
#if !definedEx(CONFIG_CPUMASK_OFFSTACK)
#if definedEx(CONFIG_SMP)
8
#endif
#if !definedEx(CONFIG_SMP)
1
#endif
#endif
);
}
/**
 * cpumask_copy - *dstp = *srcp
 * @dstp: the result
 * @srcp: the input cpumask
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void cpumask_copy(struct cpumask *dstp,
				const struct cpumask *srcp)
{
	bitmap_copy(((dstp)->bits), ((srcp)->bits), 
#if definedEx(CONFIG_CPUMASK_OFFSTACK)
#if !definedEx(CONFIG_SMP)
1
#endif
#if definedEx(CONFIG_SMP)
nr_cpu_ids
#endif
#endif
#if !definedEx(CONFIG_CPUMASK_OFFSTACK)
#if definedEx(CONFIG_SMP)
8
#endif
#if !definedEx(CONFIG_SMP)
1
#endif
#endif
);
}
/**
 * cpumask_any - pick a "random" cpu from *srcp
 * @srcp: the input cpumask
 *
 * Returns >= nr_cpu_ids if no cpus set.
 */
/**
 * cpumask_first_and - return the first cpu from *srcp1 & *srcp2
 * @src1p: the first input
 * @src2p: the second input
 *
 * Returns >= nr_cpu_ids if no cpus set in both.  See also cpumask_next_and().
 */
/**
 * cpumask_any_and - pick a "random" cpu from *mask1 & *mask2
 * @mask1: the first input cpumask
 * @mask2: the second input cpumask
 *
 * Returns >= nr_cpu_ids if no cpus set.
 */
/**
 * cpumask_of - the cpumask containing just a given cpu
 * @cpu: the cpu (<= nr_cpu_ids)
 */
/**
 * cpumask_scnprintf - print a cpumask into a string as comma-separated hex
 * @buf: the buffer to sprintf into
 * @len: the length of the buffer
 * @srcp: the cpumask to print
 *
 * If len is zero, returns zero.  Otherwise returns the length of the
 * (nul-terminated) @buf string.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int cpumask_scnprintf(char *buf, int len,
				    const struct cpumask *srcp)
{
	return bitmap_scnprintf(buf, len, ((srcp)->bits), 
#if definedEx(CONFIG_CPUMASK_OFFSTACK)
#if !definedEx(CONFIG_SMP)
1
#endif
#if definedEx(CONFIG_SMP)
nr_cpu_ids
#endif
#endif
#if !definedEx(CONFIG_CPUMASK_OFFSTACK)
#if definedEx(CONFIG_SMP)
8
#endif
#if !definedEx(CONFIG_SMP)
1
#endif
#endif
);
}
/**
 * cpumask_parse_user - extract a cpumask from a user string
 * @buf: the buffer to extract from
 * @len: the length of the buffer
 * @dstp: the cpumask to set.
 *
 * Returns -errno, or 0 for success.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int cpumask_parse_user(const char  *buf, int len,
				     struct cpumask *dstp)
{
	return bitmap_parse_user(buf, len, ((dstp)->bits), 
#if definedEx(CONFIG_CPUMASK_OFFSTACK)
#if !definedEx(CONFIG_SMP)
1
#endif
#if definedEx(CONFIG_SMP)
nr_cpu_ids
#endif
#endif
#if !definedEx(CONFIG_CPUMASK_OFFSTACK)
#if definedEx(CONFIG_SMP)
8
#endif
#if !definedEx(CONFIG_SMP)
1
#endif
#endif
);
}
/**
 * cpulist_scnprintf - print a cpumask into a string as comma-separated list
 * @buf: the buffer to sprintf into
 * @len: the length of the buffer
 * @srcp: the cpumask to print
 *
 * If len is zero, returns zero.  Otherwise returns the length of the
 * (nul-terminated) @buf string.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int cpulist_scnprintf(char *buf, int len,
				    const struct cpumask *srcp)
{
	return bitmap_scnlistprintf(buf, len, ((srcp)->bits),
#if definedEx(CONFIG_CPUMASK_OFFSTACK)
#if !definedEx(CONFIG_SMP)
1
#endif
#if definedEx(CONFIG_SMP)
nr_cpu_ids
#endif
#endif
#if !definedEx(CONFIG_CPUMASK_OFFSTACK)
#if definedEx(CONFIG_SMP)
8
#endif
#if !definedEx(CONFIG_SMP)
1
#endif
#endif
);
}
/**
 * cpulist_parse_user - extract a cpumask from a user string of ranges
 * @buf: the buffer to extract from
 * @len: the length of the buffer
 * @dstp: the cpumask to set.
 *
 * Returns -errno, or 0 for success.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int cpulist_parse(const char *buf, struct cpumask *dstp)
{
	return bitmap_parselist(buf, ((dstp)->bits), 
#if definedEx(CONFIG_CPUMASK_OFFSTACK)
#if !definedEx(CONFIG_SMP)
1
#endif
#if definedEx(CONFIG_SMP)
nr_cpu_ids
#endif
#endif
#if !definedEx(CONFIG_CPUMASK_OFFSTACK)
#if definedEx(CONFIG_SMP)
8
#endif
#if !definedEx(CONFIG_SMP)
1
#endif
#endif
);
}
/**
 * cpumask_size - size to allocate for a 'struct cpumask' in bytes
 *
 * This will eventually be a runtime variable, depending on nr_cpu_ids.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 size_t cpumask_size(void)
{
	/* FIXME: Once all cpumask assignments are eliminated, this
	 * can be nr_cpumask_bits */
	return (((
#if definedEx(CONFIG_SMP)
8
#endif
#if !definedEx(CONFIG_SMP)
1
#endif
) + (8 * sizeof(long)) - 1) / (8 * sizeof(long))) * sizeof(long);
}
/*
 * cpumask_var_t: struct cpumask for stack usage.
 *
 * Oh, the wicked games we play!  In order to make kernel coding a
 * little more difficult, we typedef cpumask_var_t to an array or a
 * pointer: doing &mask on an array is a noop, so it still works.
 *
 * ie.
 *	cpumask_var_t tmpmask;
 *	if (!alloc_cpumask_var(&tmpmask, GFP_KERNEL))
 *		return -ENOMEM;
 *
 *	  ... use 'tmpmask' like a normal struct cpumask * ...
 *
 *	free_cpumask_var(tmpmask);
 */
#if definedEx(CONFIG_CPUMASK_OFFSTACK)
typedef struct cpumask *cpumask_var_t;
bool alloc_cpumask_var_node(cpumask_var_t *mask, gfp_t flags, int node);
bool alloc_cpumask_var(cpumask_var_t *mask, gfp_t flags);
bool zalloc_cpumask_var_node(cpumask_var_t *mask, gfp_t flags, int node);
bool zalloc_cpumask_var(cpumask_var_t *mask, gfp_t flags);
void alloc_bootmem_cpumask_var(cpumask_var_t *mask);
void free_cpumask_var(cpumask_var_t mask);
void free_bootmem_cpumask_var(cpumask_var_t mask);
#endif
#if !definedEx(CONFIG_CPUMASK_OFFSTACK)
typedef struct cpumask cpumask_var_t[1];
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 bool alloc_cpumask_var(cpumask_var_t *mask, gfp_t flags)
{
	return true;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 bool alloc_cpumask_var_node(cpumask_var_t *mask, gfp_t flags,
					  int node)
{
	return true;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 bool zalloc_cpumask_var(cpumask_var_t *mask, gfp_t flags)
{
	cpumask_clear(*mask);
	return true;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 bool zalloc_cpumask_var_node(cpumask_var_t *mask, gfp_t flags,
					  int node)
{
	cpumask_clear(*mask);
	return true;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void alloc_bootmem_cpumask_var(cpumask_var_t *mask)
{
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void free_cpumask_var(cpumask_var_t mask)
{
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void free_bootmem_cpumask_var(cpumask_var_t mask)
{
}
#endif
/* It's common to want to use cpu_all_mask in struct member initializers,
 * so it has to refer to an address rather than a pointer. */
extern const unsigned long cpu_all_bits[(((
#if definedEx(CONFIG_SMP)
8
#endif
#if !definedEx(CONFIG_SMP)
1
#endif
) + (8 * sizeof(long)) - 1) / (8 * sizeof(long)))];
/* First bits of cpu_bit_bitmap are in fact unset. */
/* Wrappers for arch boot code to manipulate normally-constant masks */
void set_cpu_possible(unsigned int cpu, bool possible);
void set_cpu_present(unsigned int cpu, bool present);
void set_cpu_online(unsigned int cpu, bool online);
void set_cpu_active(unsigned int cpu, bool active);
void init_cpu_present(const struct cpumask *src);
void init_cpu_possible(const struct cpumask *src);
void init_cpu_online(const struct cpumask *src);
/**
 * to_cpumask - convert an NR_CPUS bitmap to a struct cpumask *
 * @bitmap: the bitmap
 *
 * There are a few places where cpumask_var_t isn't appropriate and
 * static cpumasks must be used (eg. very early boot), yet we don't
 * expose the definition of 'struct cpumask'.
 *
 * This does the conversion, and can be used as a constant initializer.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int __check_is_bitmap(const unsigned long *bitmap)
{
	return 1;
}
/*
 * Special-case data structure for "single bit set only" constant CPU masks.
 *
 * We pre-generate all the 64 (or 32) possible bit positions, with enough
 * padding to the left and the right, and return the constant pointer
 * appropriately offset.
 */
extern const unsigned long
	cpu_bit_bitmap[
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
+1][(((
#if definedEx(CONFIG_SMP)
8
#endif
#if !definedEx(CONFIG_SMP)
1
#endif
) + (8 * sizeof(long)) - 1) / (8 * sizeof(long)))];
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 const struct cpumask *get_cpu_mask(unsigned int cpu)
{
	const unsigned long *p = cpu_bit_bitmap[1 + cpu % 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
];
	p -= cpu / 
#if definedEx(CONFIG_64BIT)
64
#endif
#if !definedEx(CONFIG_64BIT)
32
#endif
;
	return ((struct cpumask *)(1 ? (p) : (void *)sizeof(__check_is_bitmap(p))));
}
/*
 *
 * From here down, all obsolete.  Use cpumask_ variants!
 *
 */
/* These strip const, as traditionally they weren't const. */
#if !definedEx(CONFIG_SMP)
#endif
#if definedEx(CONFIG_SMP)
int __first_cpu(const cpumask_t *srcp);
int __next_cpu(int n, const cpumask_t *srcp);
int __any_online_cpu(const cpumask_t *mask);
#endif
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __cpu_set(int cpu, volatile cpumask_t *dstp)
{
	set_bit(cpu, dstp->bits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __cpu_clear(int cpu, volatile cpumask_t *dstp)
{
	clear_bit(cpu, dstp->bits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __cpus_setall(cpumask_t *dstp, int nbits)
{
	bitmap_fill(dstp->bits, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __cpus_clear(cpumask_t *dstp, int nbits)
{
	bitmap_zero(dstp->bits, nbits);
}
/* No static inline type checking - see Subtlety (1) above. */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int __cpu_test_and_set(int cpu, cpumask_t *addr)
{
	return test_and_set_bit(cpu, addr->bits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int __cpus_and(cpumask_t *dstp, const cpumask_t *src1p,
					const cpumask_t *src2p, int nbits)
{
	return bitmap_and(dstp->bits, src1p->bits, src2p->bits, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __cpus_or(cpumask_t *dstp, const cpumask_t *src1p,
					const cpumask_t *src2p, int nbits)
{
	bitmap_or(dstp->bits, src1p->bits, src2p->bits, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __cpus_xor(cpumask_t *dstp, const cpumask_t *src1p,
					const cpumask_t *src2p, int nbits)
{
	bitmap_xor(dstp->bits, src1p->bits, src2p->bits, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int __cpus_andnot(cpumask_t *dstp, const cpumask_t *src1p,
					const cpumask_t *src2p, int nbits)
{
	return bitmap_andnot(dstp->bits, src1p->bits, src2p->bits, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int __cpus_equal(const cpumask_t *src1p,
					const cpumask_t *src2p, int nbits)
{
	return bitmap_equal(src1p->bits, src2p->bits, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int __cpus_intersects(const cpumask_t *src1p,
					const cpumask_t *src2p, int nbits)
{
	return bitmap_intersects(src1p->bits, src2p->bits, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int __cpus_subset(const cpumask_t *src1p,
					const cpumask_t *src2p, int nbits)
{
	return bitmap_subset(src1p->bits, src2p->bits, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int __cpus_empty(const cpumask_t *srcp, int nbits)
{
	return bitmap_empty(srcp->bits, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int __cpus_weight(const cpumask_t *srcp, int nbits)
{
	return bitmap_weight(srcp->bits, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __cpus_shift_left(cpumask_t *dstp,
					const cpumask_t *srcp, int n, int nbits)
{
	bitmap_shift_left(dstp->bits, srcp->bits, n, nbits);
}
#endif
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/cpumask.h" 2
extern cpumask_var_t cpu_callin_mask;
extern cpumask_var_t cpu_callout_mask;
extern cpumask_var_t cpu_initialized_mask;
extern cpumask_var_t cpu_sibling_setup_mask;
extern void setup_cpu_local_masks(void);
#line 20 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/msr.h" 2
struct msr {
	union {
		struct {
			u32 l;
			u32 h;
		};
		u64 q;
	};
};
struct msr_info {
	u32 msr_no;
	struct msr reg;
	struct msr *msrs;
	int err;
};
struct msr_regs_info {
	u32 *regs;
	int err;
};
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long long native_read_tscp(unsigned int *aux)
{
	unsigned long low, high;
	asm volatile(".byte 0x0f,0x01,0xf9"
		     : "=a" (low), "=d" (high), "=c" (*aux));
	return low | ((u64)high << 32);
}
/*
 * both i386 and x86_64 returns 64-bit value in edx:eax, but gcc's "A"
 * constraint has different meanings. For i386, "A" means exactly
 * edx:eax, while for x86_64 it doesn't mean rdx:rax or edx:eax. Instead,
 * it means rax *or* rdx.
 */
#if definedEx(CONFIG_X86_64)
#endif
#if !definedEx(CONFIG_X86_64)
#endif
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long long native_read_msr(unsigned int msr)
{
#if definedEx(CONFIG_X86_64)
unsigned low, high
#endif
#if !definedEx(CONFIG_X86_64)
unsigned long long val
#endif
;
	asm volatile("rdmsr" : 
#if definedEx(CONFIG_X86_64)
"=a" (low), "=d" (high)
#endif
#if !definedEx(CONFIG_X86_64)
"=A" (val)
#endif
 : "c" (msr));
	return 
#if definedEx(CONFIG_X86_64)
((low) | ((u64)(high) << 32))
#endif
#if !definedEx(CONFIG_X86_64)
(val)
#endif
;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long long native_read_msr_safe(unsigned int msr,
						      int *err)
{
#if definedEx(CONFIG_X86_64)
unsigned low, high
#endif
#if !definedEx(CONFIG_X86_64)
unsigned long long val
#endif
;
	asm volatile("2: rdmsr ; xor %[err],%[err]\n"
		     "1:\n\t"
		     ".section .fixup,\"ax\"\n\t"
		     "3:  mov %[fault],%[err] ; jmp 1b\n\t"
		     ".previous\n\t"
		     " .section __ex_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 "2b" "," "3b" "\n" " .previous\n"
		     : [err] "=r" (*err), 
#if definedEx(CONFIG_X86_64)
"=a" (low), "=d" (high)
#endif
#if !definedEx(CONFIG_X86_64)
"=A" (val)
#endif
		     : "c" (msr), [fault] "i" (-5));
	return 
#if definedEx(CONFIG_X86_64)
((low) | ((u64)(high) << 32))
#endif
#if !definedEx(CONFIG_X86_64)
(val)
#endif
;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void native_write_msr(unsigned int msr,
				    unsigned low, unsigned high)
{
	asm volatile("wrmsr" : : "c" (msr), "a"(low), "d" (high) : "memory");
}
/* Can be uninlined because referenced by paravirt */
__attribute__((no_instrument_function)) static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int native_write_msr_safe(unsigned int msr,
					unsigned low, unsigned high)
{
	int err;
	asm volatile("2: wrmsr ; xor %[err],%[err]\n"
		     "1:\n\t"
		     ".section .fixup,\"ax\"\n\t"
		     "3:  mov %[fault],%[err] ; jmp 1b\n\t"
		     ".previous\n\t"
		     " .section __ex_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 "2b" "," "3b" "\n" " .previous\n"
		     : [err] "=a" (err)
		     : "c" (msr), "0" (low), "d" (high),
		       [fault] "i" (-5)
		     : "memory");
	return err;
}
extern unsigned long long native_read_tsc(void);
extern int native_rdmsr_safe_regs(u32 regs[8]);
extern int native_wrmsr_safe_regs(u32 regs[8]);
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __attribute__((always_inline)) unsigned long long __native_read_tsc(void)
{
#if definedEx(CONFIG_X86_64)
unsigned low, high
#endif
#if !definedEx(CONFIG_X86_64)
unsigned long long val
#endif
;
	asm volatile("rdtsc" : 
#if definedEx(CONFIG_X86_64)
"=a" (low), "=d" (high)
#endif
#if !definedEx(CONFIG_X86_64)
"=A" (val)
#endif
);
	return 
#if definedEx(CONFIG_X86_64)
((low) | ((u64)(high) << 32))
#endif
#if !definedEx(CONFIG_X86_64)
(val)
#endif
;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long long native_read_pmc(int counter)
{
#if definedEx(CONFIG_X86_64)
unsigned low, high
#endif
#if !definedEx(CONFIG_X86_64)
unsigned long long val
#endif
;
	asm volatile("rdpmc" : 
#if definedEx(CONFIG_X86_64)
"=a" (low), "=d" (high)
#endif
#if !definedEx(CONFIG_X86_64)
"=A" (val)
#endif
 : "c" (counter));
	return 
#if definedEx(CONFIG_X86_64)
((low) | ((u64)(high) << 32))
#endif
#if !definedEx(CONFIG_X86_64)
(val)
#endif
;
}
#if definedEx(CONFIG_PARAVIRT)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h" 1
#line 141 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/msr.h" 2
#endif
#if !definedEx(CONFIG_PARAVIRT)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/errno.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/errno.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/asm-generic/errno.h" 1
#line 3 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/errno.h" 2
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/errno.h" 2
/*
 * These should never be seen by user programs.  To return one of ERESTART*
 * codes, signal_pending() MUST be set.  Note that ptrace can observe these
 * at syscall exit tracing, but they will never be left for the debugged user
 * process to see.
 */
/* Defined for the NFSv3 protocol */
#line 143 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/msr.h" 2
/*
 * Access to machine-specific registers (available on 586 and better only)
 * Note: the rd* operations modify the parameters directly (without using
 * pointer indirection), this allows gcc to optimize better
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void wrmsr(unsigned msr, unsigned low, unsigned high)
{
	native_write_msr(msr, low, high);
}
/* wrmsr with exception handling */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int wrmsr_safe(unsigned msr, unsigned low, unsigned high)
{
	return native_write_msr_safe(msr, low, high);
}
/* rdmsr with exception handling */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int rdmsrl_safe(unsigned msr, unsigned long long *p)
{
	int err;
	*p = native_read_msr_safe(msr, &err);
	return err;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int rdmsrl_amd_safe(unsigned msr, unsigned long long *p)
{
	u32 gprs[8] = { 0 };
	int err;
	gprs[1] = msr;
	gprs[7] = 0x9c5a203a;
	err = native_rdmsr_safe_regs(gprs);
	*p = gprs[0] | ((u64)gprs[2] << 32);
	return err;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int wrmsrl_amd_safe(unsigned msr, unsigned long long val)
{
	u32 gprs[8] = { 0 };
	gprs[0] = (u32)val;
	gprs[1] = msr;
	gprs[2] = val >> 32;
	gprs[7] = 0x9c5a203a;
	return native_wrmsr_safe_regs(gprs);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int rdmsr_safe_regs(u32 regs[8])
{
	return native_rdmsr_safe_regs(regs);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int wrmsr_safe_regs(u32 regs[8])
{
	return native_wrmsr_safe_regs(regs);
}
#endif
struct msr *msrs_alloc(void);
void msrs_free(struct msr *msrs);
#if definedEx(CONFIG_SMP)
int rdmsr_on_cpu(unsigned int cpu, u32 msr_no, u32 *l, u32 *h);
int wrmsr_on_cpu(unsigned int cpu, u32 msr_no, u32 l, u32 h);
void rdmsr_on_cpus(const struct cpumask *mask, u32 msr_no, struct msr *msrs);
void wrmsr_on_cpus(const struct cpumask *mask, u32 msr_no, struct msr *msrs);
int rdmsr_safe_on_cpu(unsigned int cpu, u32 msr_no, u32 *l, u32 *h);
int wrmsr_safe_on_cpu(unsigned int cpu, u32 msr_no, u32 l, u32 h);
int rdmsr_safe_regs_on_cpu(unsigned int cpu, u32 regs[8]);
int wrmsr_safe_regs_on_cpu(unsigned int cpu, u32 regs[8]);
#endif
#if !definedEx(CONFIG_SMP)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int rdmsr_on_cpu(unsigned int cpu, u32 msr_no, u32 *l, u32 *h)
{
#if definedEx(CONFIG_PARAVIRT)
do { int _err; u64 _l = paravirt_read_msr(msr_no, &_err); *l = (u32)_l; *h = _l >> 32; } while (0)
#endif
#if !definedEx(CONFIG_PARAVIRT)
do { u64 __val = native_read_msr((msr_no)); (*l) = (u32)__val; (*h) = (u32)(__val >> 32); } while (0)
#endif
;
	return 0;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int wrmsr_on_cpu(unsigned int cpu, u32 msr_no, u32 l, u32 h)
{
#if definedEx(CONFIG_PARAVIRT)
do { paravirt_write_msr(msr_no, l, h); } while (0)
#endif
#if !definedEx(CONFIG_PARAVIRT)
wrmsr(msr_no, l, h)
#endif
;
	return 0;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void rdmsr_on_cpus(const struct cpumask *m, u32 msr_no,
				struct msr *msrs)
{
       rdmsr_on_cpu(0, msr_no, &(msrs[0].l), &(msrs[0].h));
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void wrmsr_on_cpus(const struct cpumask *m, u32 msr_no,
				struct msr *msrs)
{
       wrmsr_on_cpu(0, msr_no, msrs[0].l, msrs[0].h);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int rdmsr_safe_on_cpu(unsigned int cpu, u32 msr_no,
				    u32 *l, u32 *h)
{
	return 
#if definedEx(CONFIG_PARAVIRT)
({ int _err; u64 _l = paravirt_read_msr(msr_no, &_err); (*l) = (u32)_l; (*h) = _l >> 32; _err; })
#endif
#if !definedEx(CONFIG_PARAVIRT)
({ int __err; u64 __val = native_read_msr_safe((msr_no), &__err); (*l) = (u32)__val; (*h) = (u32)(__val >> 32); __err; })
#endif
;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int wrmsr_safe_on_cpu(unsigned int cpu, u32 msr_no, u32 l, u32 h)
{
	return 
#if definedEx(CONFIG_PARAVIRT)
paravirt_write_msr(msr_no, l, h)
#endif
#if !definedEx(CONFIG_PARAVIRT)
wrmsr_safe(msr_no, l, h)
#endif
;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int rdmsr_safe_regs_on_cpu(unsigned int cpu, u32 regs[8])
{
	return 
#if definedEx(CONFIG_PARAVIRT)
paravirt_rdmsr_regs(regs)
#endif
#if !definedEx(CONFIG_PARAVIRT)
rdmsr_safe_regs(regs)
#endif
;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int wrmsr_safe_regs_on_cpu(unsigned int cpu, u32 regs[8])
{
	return 
#if definedEx(CONFIG_PARAVIRT)
paravirt_wrmsr_regs(regs)
#endif
#if !definedEx(CONFIG_PARAVIRT)
wrmsr_safe_regs(regs)
#endif
;
}
#endif
#line 23 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/processor.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/desc_defs.h" 1
/* Written 2000 by Andi Kleen */
#if !definedEx(CONFIG_PARAVIRT)
/*
 * Segment descriptor structure definitions, usable from both x86_64 and i386
 * archs.
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 14 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/desc_defs.h" 2
/*
 * FIXME: Accessing the desc_struct through its fields is more elegant,
 * and should be the one valid thing to do. However, a lot of open code
 * still touches the a and b accessors, and doing this allow us to do it
 * incrementally. We keep the signature as a struct, rather than an union,
 * so we can get rid of it transparently in the future -- glommer
 */
/* 8 byte segment descriptor */
struct desc_struct {
	union {
		struct {
			unsigned int a;
			unsigned int b;
		};
		struct {
			u16 limit0;
			u16 base0;
			unsigned base1: 8, type: 4, s: 1, dpl: 2, p: 1;
			unsigned limit: 4, avl: 1, l: 1, d: 1, g: 1, base2: 8;
		};
	};
} __attribute__((packed));
enum {
	GATE_INTERRUPT = 0xE,
	GATE_TRAP = 0xF,
	GATE_CALL = 0xC,
	GATE_TASK = 0x5,
};
/* 16byte gate */
struct gate_struct64 {
	u16 offset_low;
	u16 segment;
	unsigned ist : 3, zero0 : 5, type : 5, dpl : 2, p : 1;
	u16 offset_middle;
	u32 offset_high;
	u32 zero1;
} __attribute__((packed));
enum {
	DESC_TSS = 0x9,
	DESC_LDT = 0x2,
	DESCTYPE_S = 0x10,	/* !system */
};
/* LDT or TSS descriptor in the GDT. 16 bytes. */
struct ldttss_desc64 {
	u16 limit0;
	u16 base0;
	unsigned base1 : 8, type : 5, dpl : 2, p : 1;
	unsigned limit1 : 4, zero0 : 3, g : 1, base2 : 8;
	u32 base3;
	u32 zero1;
} __attribute__((packed));
#if definedEx(CONFIG_X86_64)
typedef struct gate_struct64 gate_desc;
typedef struct ldttss_desc64 ldt_desc;
typedef struct ldttss_desc64 tss_desc;
#endif
#if !definedEx(CONFIG_X86_64)
typedef struct desc_struct gate_desc;
typedef struct desc_struct ldt_desc;
typedef struct desc_struct tss_desc;
#endif
struct desc_ptr {
	unsigned short size;
	unsigned long address;
} __attribute__((packed)) ;
#endif
#line 24 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/processor.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/nops.h" 1
#line 25 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/processor.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/ds.h" 1
/*
 * Debug Store (DS) support
 *
 * This provides a low-level interface to the hardware's Debug Store
 * feature that is used for branch trace store (BTS) and
 * precise-event based sampling (PEBS).
 *
 * It manages:
 * - DS and BTS hardware configuration
 * - buffer overflow handling (to be done)
 * - buffer access
 *
 * It does not do:
 * - security checking (is the caller allowed to trace the task)
 * - buffer allocation (memory accounting)
 *
 *
 * Copyright (C) 2007-2009 Intel Corporation.
 * Markus Metzger <markus.t.metzger@intel.com>, 2007-2009
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 28 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/ds.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/init.h" 1
#line 29 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/ds.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/err.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/compiler.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/err.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/errno.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/asm-generic/errno.h" 1
#line 3 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/errno.h" 2
#line 8 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/err.h" 2
/*
 * Kernel pointers have redundant information, so we can use a
 * scheme where we can return either an error code or a dentry
 * pointer with the same return value.
 *
 * This should be a per-architecture thing, to allow different
 * error and pointer decisions.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void *ERR_PTR(long error)
{
	return (void *) error;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 long PTR_ERR(const void *ptr)
{
	return (long) ptr;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 long IS_ERR(const void *ptr)
{
	return __builtin_expect(!!(((unsigned long)ptr) >=(unsigned long)-4095), 0);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 long IS_ERR_OR_NULL(const void *ptr)
{
	return !ptr || __builtin_expect(!!(((unsigned long)ptr) >=(unsigned long)-4095), 0);
}
/**
 * ERR_CAST - Explicitly cast an error-valued pointer to another pointer type
 * @ptr: The pointer to cast.
 *
 * Explicitly cast an error-valued pointer to another pointer type in such a
 * way as to make it clear that's what's going on.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void *ERR_CAST(const void *ptr)
{
	/* cast away the const */
	return (void *) ptr;
}
#line 30 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/ds.h" 2
 struct cpuinfo_x86;
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __attribute__ ((__section__(".cpuinit.text"))) __attribute__((__cold__)) ds_init_intel(struct cpuinfo_x86 *ignored) {}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void ds_switch_to(struct task_struct *prev,
				struct task_struct *next) {}
#line 26 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/processor.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/personality.h" 1
/*
 * Handling of different ABIs (personalities).
 */
struct exec_domain;
struct pt_regs;
extern int		register_exec_domain(struct exec_domain *);
extern int		unregister_exec_domain(struct exec_domain *);
extern int		__set_personality(unsigned long);
/*
 * Flags for bug emulation.
 *
 * These occupy the top three bytes.
 */
enum {
	ADDR_NO_RANDOMIZE = 	0x0040000,	/* disable randomization of VA space */
	FDPIC_FUNCPTRS =	0x0080000,	/* userspace function ptrs point to descriptors
						 * (signal handling)
						 */
	MMAP_PAGE_ZERO =	0x0100000,
	ADDR_COMPAT_LAYOUT =	0x0200000,
	READ_IMPLIES_EXEC =	0x0400000,
	ADDR_LIMIT_32BIT =	0x0800000,
	SHORT_INODE =		0x1000000,
	WHOLE_SECONDS =		0x2000000,
	STICKY_TIMEOUTS	=	0x4000000,
	ADDR_LIMIT_3GB = 	0x8000000,
};
/*
 * Security-relevant compatibility flags that must be
 * cleared upon setuid or setgid exec:
 */
/*
 * Personality types.
 *
 * These go in the low byte.  Avoid using the top bit, it will
 * conflict with error returns.
 */
enum {
	PER_LINUX =		0x0000,
	PER_LINUX_32BIT =	0x0000 | ADDR_LIMIT_32BIT,
	PER_LINUX_FDPIC =	0x0000 | FDPIC_FUNCPTRS,
	PER_SVR4 =		0x0001 | STICKY_TIMEOUTS | MMAP_PAGE_ZERO,
	PER_SVR3 =		0x0002 | STICKY_TIMEOUTS | SHORT_INODE,
	PER_SCOSVR3 =		0x0003 | STICKY_TIMEOUTS |
					 WHOLE_SECONDS | SHORT_INODE,
	PER_OSR5 =		0x0003 | STICKY_TIMEOUTS | WHOLE_SECONDS,
	PER_WYSEV386 =		0x0004 | STICKY_TIMEOUTS | SHORT_INODE,
	PER_ISCR4 =		0x0005 | STICKY_TIMEOUTS,
	PER_BSD =		0x0006,
	PER_SUNOS =		0x0006 | STICKY_TIMEOUTS,
	PER_XENIX =		0x0007 | STICKY_TIMEOUTS | SHORT_INODE,
	PER_LINUX32 =		0x0008,
	PER_LINUX32_3GB =	0x0008 | ADDR_LIMIT_3GB,
	PER_IRIX32 =		0x0009 | STICKY_TIMEOUTS,/* IRIX5 32-bit */
	PER_IRIXN32 =		0x000a | STICKY_TIMEOUTS,/* IRIX6 new 32-bit */
	PER_IRIX64 =		0x000b | STICKY_TIMEOUTS,/* IRIX6 64-bit */
	PER_RISCOS =		0x000c,
	PER_SOLARIS =		0x000d | STICKY_TIMEOUTS,
	PER_UW7 =		0x000e | STICKY_TIMEOUTS | MMAP_PAGE_ZERO,
	PER_OSF4 =		0x000f,			 /* OSF/1 v4 */
	PER_HPUX =		0x0010,
	PER_MASK =		0x00ff,
};
/*
 * Description of an execution domain.
 * 
 * The first two members are refernced from assembly source
 * and should stay where they are unless explicitly needed.
 */
typedef void (*handler_t)(int, struct pt_regs *);
struct exec_domain {
	const char		*name;		/* name of the execdomain */
	handler_t		handler;	/* handler for syscalls */
	unsigned char		pers_low;	/* lowest personality */
	unsigned char		pers_high;	/* highest personality */
	unsigned long		*signal_map;	/* signal mapping */
	unsigned long		*signal_invmap;	/* reverse signal mapping */
	struct map_segment	*err_map;	/* error mapping */
	struct map_segment	*socktype_map;	/* socket type mapping */
	struct map_segment	*sockopt_map;	/* socket option mapping */
	struct map_segment	*af_map;	/* address family mapping */
	struct module		*module;	/* module context of the ed. */
	struct exec_domain	*next;		/* linked list (internal) */
};
/*
 * Return the base personality without flags.
 */
/*
 * Change personality of the currently running process.
 */
#line 28 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/processor.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/cpumask.h" 1
#line 29 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/processor.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/cache.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/kernel.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/cache.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/cache.h" 1
#if definedEx(CONFIG_X86_32)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/linkage.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/cache.h" 2
/* L1 cache line size */
#if definedEx(CONFIG_X86_VSMP)
#if definedEx(CONFIG_SMP)
#endif
#endif
#endif
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/cache.h" 2
#if definedEx(CONFIG_SMP)
#endif
#if !definedEx(CONFIG_SMP)
#endif
#if !definedEx(CONFIG_SMP) || definedEx(CONFIG_SMP) && !definedEx(CONFIG_X86_VSMP)
#if definedEx(CONFIG_SMP)
#endif
#if !definedEx(CONFIG_SMP)
#endif
#endif
/*
 * The maximum alignment needed for some critical structures
 * These could be inter-node cacheline sizes/L3 cacheline
 * size etc.  Define this in asm/cache.h for your arch
 */
#if definedEx(CONFIG_SMP)
#endif
#if !definedEx(CONFIG_SMP)
#endif
#line 30 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/processor.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/threads.h" 1
#line 31 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/processor.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/math64.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/math64.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/div64.h" 1
#if !definedEx(CONFIG_LBDAF)
#if definedEx(CONFIG_X86_32)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 8 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/div64.h" 2
/*
 * do_div() is NOT a C function. It wants to return
 * two values (the quotient and the remainder), but
 * since that doesn't work very well in C, what it
 * does is:
 *
 * - modifies the 64-bit dividend _in_place_
 * - returns the 32-bit remainder
 *
 * This ends up being the most efficient "calling
 * convention" on x86.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 u64 div_u64_rem(u64 dividend, u32 divisor, u32 *remainder)
{
	union {
		u64 v64;
		u32 v32[2];
	} d = { dividend };
	u32 upper;
	upper = d.v32[1];
	d.v32[1] = 0;
	if (upper >= divisor) {
		d.v32[1] = upper / divisor;
		upper %= divisor;
	}
	asm ("divl %2" : "=a" (d.v32[0]), "=d" (*remainder) :
		"rm" (divisor), "0" (d.v32[0]), "1" (upper));
	return d.v64;
}
#endif
#if !definedEx(CONFIG_X86_32)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/asm-generic/div64.h" 1
/*
 * Copyright (C) 2003 Bernardo Innocenti <bernie@develer.com>
 * Based on former asm-ppc/div64.h and asm-m68knommu/div64.h
 *
 * The semantics of do_div() are:
 *
 * uint32_t do_div(uint64_t *n, uint32_t base)
 * {
 * 	uint32_t remainder = *n % base;
 * 	*n = *n / base;
 * 	return remainder;
 * }
 *
 * NOTE: macro parameter n is evaluated multiple times,
 *       beware of side effects!
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 22 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/asm-generic/div64.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/compiler.h" 1
#line 23 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/asm-generic/div64.h" 2
#line 59 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/div64.h" 2
#endif
#endif
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/math64.h" 2
#if definedEx(CONFIG_64BIT)
/**
 * div_u64_rem - unsigned 64bit divide with 32bit divisor with remainder
 *
 * This is commonly provided by 32bit archs to provide an optimized 64bit
 * divide.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 u64 div_u64_rem(u64 dividend, u32 divisor, u32 *remainder)
{
	*remainder = dividend % divisor;
	return dividend / divisor;
}
/**
 * div_s64_rem - signed 64bit divide with 32bit divisor with remainder
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 s64 div_s64_rem(s64 dividend, s32 divisor, s32 *remainder)
{
	*remainder = dividend % divisor;
	return dividend / divisor;
}
/**
 * div64_u64 - unsigned 64bit divide with 64bit divisor
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 u64 div64_u64(u64 dividend, u64 divisor)
{
	return dividend / divisor;
}
#endif
#if !definedEx(CONFIG_64BIT)
extern s64 div_s64_rem(s64 dividend, s32 divisor, s32 *remainder);
extern u64 div64_u64(u64 dividend, u64 divisor);
#endif
/**
 * div_u64 - unsigned 64bit divide with 32bit divisor
 *
 * This is the most common 64bit divide and should be used if possible,
 * as many 32bit archs can optimize this variant better than a full 64bit
 * divide.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 u64 div_u64(u64 dividend, u32 divisor)
{
	u32 remainder;
	return 
#if definedEx(CONFIG_X86_32)
div_u64_rem
#endif
#if !definedEx(CONFIG_X86_32)
div_u64_rem
#endif
(dividend, divisor, &remainder);
}
/**
 * div_s64 - signed 64bit divide with 32bit divisor
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 s64 div_s64(s64 dividend, s32 divisor)
{
	s32 remainder;
	return div_s64_rem(dividend, divisor, &remainder);
}
u32 iter_div_u64_rem(u64 dividend, u32 divisor, u64 *remainder);
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __attribute__((always_inline)) u32
__iter_div_u64_rem(u64 dividend, u32 divisor, u64 *remainder)
{
	u32 ret = 0;
	while (dividend >= divisor) {
		/* The following asm() prevents the compiler from
		   optimising this loop into a modulo operation.  */
		asm("" : "+rm"(dividend));
		dividend -= divisor;
		ret++;
	}
	*remainder = dividend;
	return ret;
}
#line 32 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/processor.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/init.h" 1
#line 33 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/processor.h" 2
/*
 * Default implementation of macro that returns current
 * instruction pointer ("program counter").
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void *current_text_addr(void)
{
	void *pc;
	asm volatile("mov $1f, %0; 1:":"=r" (pc));
	return pc;
}
#if definedEx(CONFIG_X86_VSMP)
#endif
#if !definedEx(CONFIG_X86_VSMP)
#endif
/*
 *  CPU type and hardware bug flags. Kept separately for each CPU.
 *  Members of this structure are referenced in head.S, so think twice
 *  before touching them. [mj]
 */
struct cpuinfo_x86 {
	__u8			x86;		/* CPU family */
	__u8			x86_vendor;	/* CPU vendor */
	__u8			x86_model;
	__u8			x86_mask;
#if definedEx(CONFIG_X86_32)
	char			wp_works_ok;	/* It doesn't on 386's */
	/* Problems on some 486Dx4's and old 386's: */
	char			hlt_works_ok;
	char			hard_math;
	char			rfu;
	char			fdiv_bug;
	char			f00f_bug;
	char			coma_bug;
	char			pad0;
#endif
#if !definedEx(CONFIG_X86_32)
	/* Number of 4K pages in DTLB/ITLB combined(in pages): */
	int			x86_tlbsize;
#endif
	__u8			x86_virt_bits;
	__u8			x86_phys_bits;
	/* CPUID returned core id bits: */
	__u8			x86_coreid_bits;
	/* Max extended CPUID function supported: */
	__u32			extended_cpuid_level;
	/* Maximum supported CPUID level, -1=no CPUID: */
	int			cpuid_level;
	__u32			x86_capability[9];
	char			x86_vendor_id[16];
	char			x86_model_id[64];
	/* in KB - valid for CPUS which support this call: */
	int			x86_cache_size;
	int			x86_cache_alignment;	/* In bytes */
	int			x86_power;
	unsigned long		loops_per_jiffy;
#if definedEx(CONFIG_SMP)
	/* cpus sharing the last level cache: */
	cpumask_var_t		llc_shared_map;
#endif
	/* cpuid returned max cores value: */
	u16			 x86_max_cores;
	u16			apicid;
	u16			initial_apicid;
	u16			x86_clflush_size;
#if definedEx(CONFIG_SMP)
	/* number of cores as seen by the OS: */
	u16			booted_cores;
	/* Physical processor id: */
	u16			phys_proc_id;
	/* Core id: */
	u16			cpu_core_id;
	/* Index into per_cpu list: */
	u16			cpu_index;
#endif
	unsigned int		x86_hyper_vendor;
} __attribute__((__aligned__((1 << (5)))));
/*
 * capabilities of CPUs
 */
extern struct cpuinfo_x86	boot_cpu_data;
extern struct cpuinfo_x86	new_cpu_data;
extern struct tss_struct	doublefault_tss;
extern __u32			cpu_caps_cleared[9];
extern __u32			cpu_caps_set[9];
#if definedEx(CONFIG_SMP)
#if definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(".discard"), unused)) char __pcpu_scope_cpu_info; extern __attribute__((section(".data.percpu" ".shared_aligned")))  __typeof__(struct cpuinfo_x86) per_cpu__cpu_info
#endif
#if !definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(".data.percpu" ".shared_aligned")))  __typeof__(struct cpuinfo_x86) per_cpu__cpu_info
#endif
 __attribute__((__aligned__((1 << (5)))));
#endif
#if !definedEx(CONFIG_SMP)
#endif
extern const struct seq_operations cpuinfo_op;
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int hlt_works(int cpu)
{
#if definedEx(CONFIG_X86_32)
	return 
#if definedEx(CONFIG_SMP)
(*({ unsigned long __ptr; __asm__ ("" : "=r"(__ptr) : "0"((&per_cpu__cpu_info))); (typeof((&per_cpu__cpu_info))) (__ptr + (((__per_cpu_offset[cpu])))); }))
#endif
#if !definedEx(CONFIG_SMP)
boot_cpu_data
#endif
.hlt_works_ok;
#endif
#if !definedEx(CONFIG_X86_32)
	return 1;
#endif
}
extern void cpu_detect(struct cpuinfo_x86 *c);
extern struct pt_regs *idle_regs(struct pt_regs *);
extern void early_cpu_init(void);
extern void identify_boot_cpu(void);
extern void identify_secondary_cpu(struct cpuinfo_x86 *);
extern void print_cpu_info(struct cpuinfo_x86 *);
extern void init_scattered_cpuid_features(struct cpuinfo_x86 *c);
extern unsigned int init_intel_cacheinfo(struct cpuinfo_x86 *c);
extern unsigned short num_cache_leaves;
extern void detect_extended_topology(struct cpuinfo_x86 *c);
extern void detect_ht(struct cpuinfo_x86 *c);
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void native_cpuid(unsigned int *eax, unsigned int *ebx,
				unsigned int *ecx, unsigned int *edx)
{
	/* ecx is often an input as well as an output. */
	asm volatile("cpuid"
	    : "=a" (*eax),
	      "=b" (*ebx),
	      "=c" (*ecx),
	      "=d" (*edx)
	    : "0" (*eax), "2" (*ecx));
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void load_cr3(pgd_t *pgdir)
{
#if !definedEx(CONFIG_PARAVIRT)
(native_write_cr3(
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_DEBUG_VIRTUAL)
(((unsigned long)(pgdir)) - ((unsigned long)
 (0xC0000000UL)
))
#endif
#if !definedEx(CONFIG_X86_64) && definedEx(CONFIG_DEBUG_VIRTUAL) || definedEx(CONFIG_X86_64)
__phys_addr((unsigned long)(pgdir))
#endif
))
#endif
#if definedEx(CONFIG_PARAVIRT)
write_cr3(
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_DEBUG_VIRTUAL)
(((unsigned long)(pgdir)) - ((unsigned long)(0xC0000000UL)))
#endif
#if !definedEx(CONFIG_X86_64) && definedEx(CONFIG_DEBUG_VIRTUAL) || definedEx(CONFIG_X86_64)
__phys_addr((unsigned long)(pgdir))
#endif
)
#endif
;
}
#if definedEx(CONFIG_X86_32)
/* This is the TSS defined by the hardware. */
struct x86_hw_tss {
	unsigned short		back_link, __blh;
	unsigned long		sp0;
	unsigned short		ss0, __ss0h;
	unsigned long		sp1;
	/* ss1 caches MSR_IA32_SYSENTER_CS: */
	unsigned short		ss1, __ss1h;
	unsigned long		sp2;
	unsigned short		ss2, __ss2h;
	unsigned long		__cr3;
	unsigned long		ip;
	unsigned long		flags;
	unsigned long		ax;
	unsigned long		cx;
	unsigned long		dx;
	unsigned long		bx;
	unsigned long		sp;
	unsigned long		bp;
	unsigned long		si;
	unsigned long		di;
	unsigned short		es, __esh;
	unsigned short		cs, __csh;
	unsigned short		ss, __ssh;
	unsigned short		ds, __dsh;
	unsigned short		fs, __fsh;
	unsigned short		gs, __gsh;
	unsigned short		ldt, __ldth;
	unsigned short		trace;
	unsigned short		io_bitmap_base;
} __attribute__((packed));
#endif
#if !definedEx(CONFIG_X86_32)
struct x86_hw_tss {
	u32			reserved1;
	u64			sp0;
	u64			sp1;
	u64			sp2;
	u64			reserved2;
	u64			ist[7];
	u32			reserved3;
	u32			reserved4;
	u16			reserved5;
	u16			io_bitmap_base;
} __attribute__((packed)) __attribute__((__aligned__((1 << (5)))));
#endif
/*
 * IO-bitmap sizes:
 */
struct tss_struct {
	/*
	 * The hardware state:
	 */
	struct x86_hw_tss	x86_tss;
	/*
	 * The extra 1 is there because the CPU will access an
	 * additional byte beyond the end of the IO permission
	 * bitmap. The extra byte must be all 1 bits, and must
	 * be within the limit.
	 */
	unsigned long		io_bitmap[((65536/8)/sizeof(long)) + 1];
	/*
	 * .. and then another 0x100 bytes for the emergency kernel stack:
	 */
	unsigned long		stack[64];
} __attribute__((__aligned__((1 << (5)))));
#if definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(".discard"), unused)) char __pcpu_scope_init_tss; extern __attribute__((section(
#if definedEx(CONFIG_SMP)
".data.percpu"
#endif
#if !definedEx(CONFIG_SMP)
".data"
#endif
#if definedEx(CONFIG_SMP)
".shared_aligned"
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
)))  __typeof__(struct tss_struct) per_cpu__init_tss
#endif
#if !definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(
#if definedEx(CONFIG_SMP)
".data.percpu"
#endif
#if !definedEx(CONFIG_SMP)
".data"
#endif
#if definedEx(CONFIG_SMP)
".shared_aligned"
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
)))  __typeof__(struct tss_struct) per_cpu__init_tss
#endif
#if definedEx(CONFIG_SMP)
__attribute__((__aligned__((1 << (5)))))
#endif
#if !definedEx(CONFIG_SMP)
#endif
;
/*
 * Save the original ist values for checking stack pointers during debugging
 */
struct orig_ist {
	unsigned long		ist[7];
};
struct i387_fsave_struct {
	u32			cwd;	/* FPU Control Word		*/
	u32			swd;	/* FPU Status Word		*/
	u32			twd;	/* FPU Tag Word			*/
	u32			fip;	/* FPU IP Offset		*/
	u32			fcs;	/* FPU IP Selector		*/
	u32			foo;	/* FPU Operand Pointer Offset	*/
	u32			fos;	/* FPU Operand Pointer Selector	*/
	/* 8*10 bytes for each FP-reg = 80 bytes:			*/
	u32			st_space[20];
	/* Software status information [not touched by FSAVE ]:		*/
	u32			status;
};
struct i387_fxsave_struct {
	u16			cwd; /* Control Word			*/
	u16			swd; /* Status Word			*/
	u16			twd; /* Tag Word			*/
	u16			fop; /* Last Instruction Opcode		*/
	union {
		struct {
			u64	rip; /* Instruction Pointer		*/
			u64	rdp; /* Data Pointer			*/
		};
		struct {
			u32	fip; /* FPU IP Offset			*/
			u32	fcs; /* FPU IP Selector			*/
			u32	foo; /* FPU Operand Offset		*/
			u32	fos; /* FPU Operand Selector		*/
		};
	};
	u32			mxcsr;		/* MXCSR Register State */
	u32			mxcsr_mask;	/* MXCSR Mask		*/
	/* 8*16 bytes for each FP-reg = 128 bytes:			*/
	u32			st_space[32];
	/* 16*16 bytes for each XMM-reg = 256 bytes:			*/
	u32			xmm_space[64];
	u32			padding[12];
	union {
		u32		padding1[12];
		u32		sw_reserved[12];
	};
} __attribute__((aligned(16)));
struct i387_soft_struct {
	u32			cwd;
	u32			swd;
	u32			twd;
	u32			fip;
	u32			fcs;
	u32			foo;
	u32			fos;
	/* 8*10 bytes for each FP-reg = 80 bytes: */
	u32			st_space[20];
	u8			ftop;
	u8			changed;
	u8			lookahead;
	u8			no_update;
	u8			rm;
	u8			alimit;
	struct math_emu_info	*info;
	u32			entry_eip;
};
struct ymmh_struct {
	/* 16 * 16 bytes for each YMMH-reg = 256 bytes */
	u32 ymmh_space[64];
};
struct xsave_hdr_struct {
	u64 xstate_bv;
	u64 reserved1[2];
	u64 reserved2[5];
} __attribute__((packed));
struct xsave_struct {
	struct i387_fxsave_struct i387;
	struct xsave_hdr_struct xsave_hdr;
	struct ymmh_struct ymmh;
	/* new processor state extensions will go here */
} __attribute__ ((packed, aligned (64)));
union thread_xstate {
	struct i387_fsave_struct	fsave;
	struct i387_fxsave_struct	fxsave;
	struct i387_soft_struct		soft;
	struct xsave_struct		xsave;
};
#if definedEx(CONFIG_X86_64)
#if definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(".discard"), unused)) char __pcpu_scope_orig_ist; extern __attribute__((section(
#if definedEx(CONFIG_SMP)
".data.percpu"
#endif
#if !definedEx(CONFIG_SMP)
".data"
#endif
 "")))  __typeof__(struct orig_ist) per_cpu__orig_ist
#endif
#if !definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(
#if definedEx(CONFIG_SMP)
".data.percpu"
#endif
#if !definedEx(CONFIG_SMP)
".data"
#endif
 "")))  __typeof__(struct orig_ist) per_cpu__orig_ist
#endif
;
union irq_stack_union {
	char irq_stack[(((1UL) << 12) << 2)];
	/*
	 * GCC hardcodes the stack canary as %gs:40.  Since the
	 * irq_stack is the object at %gs:0, we reserve the bottom
	 * 48 bytes of the irq stack for the canary.
	 */
	struct {
		char gs_base[40];
		unsigned long stack_canary;
	};
};
#if definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(".discard"), unused)) char __pcpu_scope_irq_stack_union; extern __attribute__((section(
#if definedEx(CONFIG_SMP)
".data.percpu"
#endif
#if !definedEx(CONFIG_SMP)
".data"
#endif
#if definedEx(CONFIG_SMP)
".first"
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
)))  __typeof__(union irq_stack_union) per_cpu__irq_stack_union
#endif
#if !definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(
#if definedEx(CONFIG_SMP)
".data.percpu"
#endif
#if !definedEx(CONFIG_SMP)
".data"
#endif
#if definedEx(CONFIG_SMP)
".first"
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
)))  __typeof__(union irq_stack_union) per_cpu__irq_stack_union
#endif
;
extern typeof(per_cpu__irq_stack_union) 
#if definedEx(CONFIG_X86_64_SMP)
init_per_cpu__irq_stack_union
#endif
#if !definedEx(CONFIG_X86_64_SMP)
per_cpu__irq_stack_union
#endif
;
#if definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(".discard"), unused)) char __pcpu_scope_irq_stack_ptr; extern __attribute__((section(
#if definedEx(CONFIG_SMP)
".data.percpu"
#endif
#if !definedEx(CONFIG_SMP)
".data"
#endif
 "")))  __typeof__(char *) per_cpu__irq_stack_ptr
#endif
#if !definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(
#if definedEx(CONFIG_SMP)
".data.percpu"
#endif
#if !definedEx(CONFIG_SMP)
".data"
#endif
 "")))  __typeof__(char *) per_cpu__irq_stack_ptr
#endif
;
#if definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(".discard"), unused)) char __pcpu_scope_irq_count; extern __attribute__((section(
#if definedEx(CONFIG_SMP)
".data.percpu"
#endif
#if !definedEx(CONFIG_SMP)
".data"
#endif
 "")))  __typeof__(unsigned int) per_cpu__irq_count
#endif
#if !definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(
#if definedEx(CONFIG_SMP)
".data.percpu"
#endif
#if !definedEx(CONFIG_SMP)
".data"
#endif
 "")))  __typeof__(unsigned int) per_cpu__irq_count
#endif
;
extern unsigned long kernel_eflags;
extern 
#if !definedEx(CONFIG_X86_32)
#endif
#if definedEx(CONFIG_X86_32)
 __attribute__((regparm(0)))
#endif
 void ignore_sysret(void);
#endif
#if !definedEx(CONFIG_X86_64)
#if definedEx(CONFIG_CC_STACKPROTECTOR)
/*
 * Make sure stack canary segment base is cached-aligned:
 *   "For Intel Atom processors, avoid non zero segment base address
 *    that is not aligned to cache line boundary at all cost."
 * (Optim Ref Manual Assembly/Compiler Coding Rule 15.)
 */
struct stack_canary {
	char __pad[20];		/* canary at %gs:20 */
	unsigned long canary;
};
#if definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(".discard"), unused)) char __pcpu_scope_stack_canary; extern __attribute__((section(
#if definedEx(CONFIG_SMP)
".data.percpu"
#endif
#if !definedEx(CONFIG_SMP)
".data"
#endif
 ".shared_aligned")))  __typeof__(struct stack_canary) per_cpu__stack_canary
#endif
#if !definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(
#if definedEx(CONFIG_SMP)
".data.percpu"
#endif
#if !definedEx(CONFIG_SMP)
".data"
#endif
 ".shared_aligned")))  __typeof__(struct stack_canary) per_cpu__stack_canary
#endif
 __attribute__((__aligned__((1 << (5)))));
#endif
#endif
extern unsigned int xstate_size;
extern void free_thread_xstate(struct task_struct *);
extern struct kmem_cache *task_xstate_cachep;
struct perf_event;
struct thread_struct {
	/* Cached TLS descriptors: */
	struct desc_struct	tls_array[3];
	unsigned long		sp0;
	unsigned long		sp;
#if definedEx(CONFIG_X86_32)
	unsigned long		sysenter_cs;
#endif
#if !definedEx(CONFIG_X86_32)
	unsigned long		usersp;	/* Copy from PDA */
	unsigned short		es;
	unsigned short		ds;
	unsigned short		fsindex;
	unsigned short		gsindex;
#endif
#if definedEx(CONFIG_X86_32)
	unsigned long		ip;
#endif
#if definedEx(CONFIG_X86_64)
	unsigned long		fs;
#endif
	unsigned long		gs;
	/* Save middle states of ptrace breakpoints */
	struct perf_event	*ptrace_bps[4];
	/* Debug status used for traps, single steps, etc... */
	unsigned long           debugreg6;
	/* Keep track of the exact dr7 value set by the user */
	unsigned long           ptrace_dr7;
	/* Fault info: */
	unsigned long		cr2;
	unsigned long		trap_no;
	unsigned long		error_code;
	/* floating point and extended processor state */
	union thread_xstate	*xstate;
#if definedEx(CONFIG_X86_32)
	/* Virtual 86 mode info */
	struct vm86_struct  *vm86_info;
	unsigned long		screen_bitmap;
	unsigned long		v86flags;
	unsigned long		v86mask;
	unsigned long		saved_sp0;
	unsigned int		saved_fs;
	unsigned int		saved_gs;
#endif
	/* IO permissions: */
	unsigned long		*io_bitmap_ptr;
	unsigned long		iopl;
	/* Max allowed port in the bitmap, in bytes: */
	unsigned		io_bitmap_max;
/* MSR_IA32_DEBUGCTLMSR value to switch in if TIF_DEBUGCTLMSR is set.  */
	unsigned long	debugctlmsr;
	/* Debug Store context; see asm/ds.h */
	struct ds_context	*ds_ctx;
};
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long native_get_debugreg(int regno)
{
	unsigned long val = 0;	/* Damn you, gcc! */
	switch (regno) {
	case 0:
		asm("mov %%db0, %0" :"=r" (val));
		break;
	case 1:
		asm("mov %%db1, %0" :"=r" (val));
		break;
	case 2:
		asm("mov %%db2, %0" :"=r" (val));
		break;
	case 3:
		asm("mov %%db3, %0" :"=r" (val));
		break;
	case 6:
		asm("mov %%db6, %0" :"=r" (val));
		break;
	case 7:
		asm("mov %%db7, %0" :"=r" (val));
		break;
	default:
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/processor.h"), "i" (506), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
#if !definedEx(CONFIG_BUG)
do {} while(0)
#endif
;
	}
	return val;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void native_set_debugreg(int regno, unsigned long value)
{
	switch (regno) {
	case 0:
		asm("mov %0, %%db0"	::"r" (value));
		break;
	case 1:
		asm("mov %0, %%db1"	::"r" (value));
		break;
	case 2:
		asm("mov %0, %%db2"	::"r" (value));
		break;
	case 3:
		asm("mov %0, %%db3"	::"r" (value));
		break;
	case 6:
		asm("mov %0, %%db6"	::"r" (value));
		break;
	case 7:
		asm("mov %0, %%db7"	::"r" (value));
		break;
	default:
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/processor.h"), "i" (533), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
#if !definedEx(CONFIG_BUG)
do {} while(0)
#endif
;
	}
}
/*
 * Set IOPL bits in EFLAGS from given mask
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void native_set_iopl_mask(unsigned mask)
{
#if definedEx(CONFIG_X86_32)
	unsigned int reg;
	asm volatile ("pushfl;"
		      "popl %0;"
		      "andl %1, %0;"
		      "orl %2, %0;"
		      "pushl %0;"
		      "popfl"
		      : "=&r" (reg)
		      : "i" (~0x00003000), "r" (mask));
#endif
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void
native_load_sp0(struct tss_struct *tss, struct thread_struct *thread)
{
	tss->x86_tss.sp0 = thread->sp0;
#if definedEx(CONFIG_X86_32)
	/* Only happens when SEP is enabled, no need to test "SEP"arately: */
	if (__builtin_expect(!!(tss->x86_tss.ss1 != thread->sysenter_cs), 0)) {
		tss->x86_tss.ss1 = thread->sysenter_cs;
#if definedEx(CONFIG_PARAVIRT)
do { paravirt_write_msr(0x00000174, thread->sysenter_cs, 0); } while (0)
#endif
#if !definedEx(CONFIG_PARAVIRT)
wrmsr(0x00000174, thread->sysenter_cs, 0)
#endif
;
	}
#endif
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void native_swapgs(void)
{
#if definedEx(CONFIG_X86_64)
	asm volatile("swapgs" ::: "memory");
#endif
}
#if definedEx(CONFIG_PARAVIRT)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h" 1
#line 579 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/processor.h" 2
#endif
#if !definedEx(CONFIG_PARAVIRT)
/*
 * These special macros can be used to get or set a debugging register
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void load_sp0(struct tss_struct *tss,
			    struct thread_struct *thread)
{
	native_load_sp0(tss, thread);
}
#endif
/*
 * Save the cr4 feature set we're using (ie
 * Pentium 4MB enable and PPro Global page
 * enable), so that any CPU's that boot up
 * after us can get the correct flags.
 */
extern unsigned long		mmu_cr4_features;
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void set_in_cr4(unsigned long mask)
{
	unsigned cr4;
	mmu_cr4_features |= mask;
	cr4 = 
#if !definedEx(CONFIG_PARAVIRT)
(native_read_cr4())
#endif
#if definedEx(CONFIG_PARAVIRT)
read_cr4()
#endif
;
	cr4 |= mask;
#if !definedEx(CONFIG_PARAVIRT)
(native_write_cr4(cr4))
#endif
#if definedEx(CONFIG_PARAVIRT)
write_cr4(cr4)
#endif
;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void clear_in_cr4(unsigned long mask)
{
	unsigned cr4;
	mmu_cr4_features &= ~mask;
	cr4 = 
#if !definedEx(CONFIG_PARAVIRT)
(native_read_cr4())
#endif
#if definedEx(CONFIG_PARAVIRT)
read_cr4()
#endif
;
	cr4 &= ~mask;
#if !definedEx(CONFIG_PARAVIRT)
(native_write_cr4(cr4))
#endif
#if definedEx(CONFIG_PARAVIRT)
write_cr4(cr4)
#endif
;
}
typedef struct {
	unsigned long		seg;
} mm_segment_t;
/*
 * create a kernel thread without removing it from tasklists
 */
extern int kernel_thread(int (*fn)(void *), void *arg, unsigned long flags);
/* Free all resources held by a thread. */
extern void release_thread(struct task_struct *);
/* Prepare to copy thread state - unlazy all lazy state */
extern void prepare_to_copy(struct task_struct *tsk);
unsigned long get_wchan(struct task_struct *p);
/*
 * Generic CPUID function
 * clear %ecx since some cpus (Cyrix MII) do not set or clear %ecx
 * resulting in stale register contents being returned.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void cpuid(unsigned int op,
			 unsigned int *eax, unsigned int *ebx,
			 unsigned int *ecx, unsigned int *edx)
{
	*eax = op;
	*ecx = 0;
#if !definedEx(CONFIG_PARAVIRT)
native_cpuid
#endif
#if definedEx(CONFIG_PARAVIRT)
__cpuid
#endif
(eax, ebx, ecx, edx);
}
/* Some CPUID calls want 'count' to be placed in ecx */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void cpuid_count(unsigned int op, int count,
			       unsigned int *eax, unsigned int *ebx,
			       unsigned int *ecx, unsigned int *edx)
{
	*eax = op;
	*ecx = count;
#if !definedEx(CONFIG_PARAVIRT)
native_cpuid
#endif
#if definedEx(CONFIG_PARAVIRT)
__cpuid
#endif
(eax, ebx, ecx, edx);
}
/*
 * CPUID functions returning a single datum
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned int cpuid_eax(unsigned int op)
{
	unsigned int eax, ebx, ecx, edx;
	cpuid(op, &eax, &ebx, &ecx, &edx);
	return eax;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned int cpuid_ebx(unsigned int op)
{
	unsigned int eax, ebx, ecx, edx;
	cpuid(op, &eax, &ebx, &ecx, &edx);
	return ebx;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned int cpuid_ecx(unsigned int op)
{
	unsigned int eax, ebx, ecx, edx;
	cpuid(op, &eax, &ebx, &ecx, &edx);
	return ecx;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned int cpuid_edx(unsigned int op)
{
	unsigned int eax, ebx, ecx, edx;
	cpuid(op, &eax, &ebx, &ecx, &edx);
	return edx;
}
/* REP NOP (PAUSE) is a good thing to insert into busy-wait loops. */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void rep_nop(void)
{
	asm volatile("rep; nop" ::: "memory");
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void cpu_relax(void)
{
	rep_nop();
}
/* Stop speculative execution and prefetching of modified code. */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void sync_core(void)
{
	int tmp;
#if !definedEx(CONFIG_M386) && definedEx(CONFIG_M486) || definedEx(CONFIG_M386)
	if (boot_cpu_data.x86 < 5)
		/* There is no speculative execution.
		 * jmp is a barrier to prefetching. */
		asm volatile("jmp 1f\n1:\n" ::: "memory");
	else
#endif
		/* cpuid is a barrier to speculative execution.
		 * Prefetched instructions are automatically
		 * invalidated when modified. */
		asm volatile("cpuid" : "=a" (tmp) : "0" (1)
			     : "ebx", "ecx", "edx", "memory");
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __monitor(const void *eax, unsigned long ecx,
			     unsigned long edx)
{
	/* "monitor %eax, %ecx, %edx;" */
	asm volatile(".byte 0x0f, 0x01, 0xc8;"
		     :: "a" (eax), "c" (ecx), "d"(edx));
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __mwait(unsigned long eax, unsigned long ecx)
{
	/* "mwait %eax, %ecx;" */
	asm volatile(".byte 0x0f, 0x01, 0xc9;"
		     :: "a" (eax), "c" (ecx));
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __sti_mwait(unsigned long eax, unsigned long ecx)
{
#if !definedEx(CONFIG_TRACE_IRQFLAGS)
do { } while (0)
#endif
#if definedEx(CONFIG_TRACE_IRQFLAGS)
trace_hardirqs_on()
#endif
;
	/* "mwait %eax, %ecx;" */
	asm volatile("sti; .byte 0x0f, 0x01, 0xc9;"
		     :: "a" (eax), "c" (ecx));
}
extern void mwait_idle_with_hints(unsigned long eax, unsigned long ecx);
extern void select_idle_routine(const struct cpuinfo_x86 *c);
extern void init_c1e_mask(void);
extern unsigned long		boot_option_idle_override;
extern unsigned long		idle_halt;
extern unsigned long		idle_nomwait;
/*
 * on systems with caches, caches must be flashed as the absolute
 * last instruction before going into a suspended halt.  Otherwise,
 * dirty data can linger in the cache and become stale on resume,
 * leading to strange errors.
 *
 * perform a variety of operations to guarantee that the compiler
 * will not reorder instructions.  wbinvd itself is serializing
 * so the processor will not reorder.
 *
 * Systems without cache can just go into halt.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void wbinvd_halt(void)
{
#if definedEx(CONFIG_X86_32)
asm volatile ("661:\n\t" "lock; addl $0,0(%%esp)" "\n662:\n" ".section .altinstructions,\"a\"\n" " " ".balign 4" " " "\n" " " ".long" " " "661b\n" " " ".long" " " "663f\n" "	 .byte " "(0*32+26)" "\n" "	 .byte 662b-661b\n" "	 .byte 664f-663f\n" "	 .byte 0xff + (664f-663f) - (662b-661b)\n" ".previous\n" ".section .altinstr_replacement, \"ax\"\n" "663:\n\t" "mfence" "\n664:\n" ".previous" : : : "memory")
#endif
#if !definedEx(CONFIG_X86_32)
asm volatile("mfence":::"memory")
#endif
;
	/* check for clflush to determine if wbinvd is legal */
	if ((__builtin_constant_p((0*32+19)) && ( ((((0*32+19))>>5)==0 && (1UL<<(((0*32+19))&31) & (
#if !definedEx(CONFIG_MATH_EMULATION)
(1<<((0*32+ 0) & 31))
#endif
#if definedEx(CONFIG_MATH_EMULATION)
0
#endif
|
#if !definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_64) && definedEx(CONFIG_PARAVIRT)
0
#endif
#if definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT)
(1<<((0*32+ 3)) & 31)
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+ 5) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if !definedEx(CONFIG_X86_PAE) && definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_PAE)
(1<<((0*32+ 6) & 31))
#endif
#if !definedEx(CONFIG_X86_PAE) && !definedEx(CONFIG_X86_64)
0
#endif
| 
#if definedEx(CONFIG_X86_CMPXCHG64)
(1<<((0*32+ 8) & 31))
#endif
#if !definedEx(CONFIG_X86_CMPXCHG64)
0
#endif
|
#if !definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_64) && definedEx(CONFIG_PARAVIRT)
0
#endif
#if definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT)
(1<<((0*32+13)) & 31)
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+24) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if !definedEx(CONFIG_X86_64) && definedEx(CONFIG_X86_CMOV) || definedEx(CONFIG_X86_64)
(1<<((0*32+15) & 31))
#endif
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_X86_CMOV)
0
#endif
| 
#if definedEx(CONFIG_X86_64)
(1<<((0*32+25) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+26) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
))) || ((((0*32+19))>>5)==1 && (1UL<<(((0*32+19))&31) & (
#if definedEx(CONFIG_X86_64)
(1<<((1*32+29) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if definedEx(CONFIG_X86_USE_3DNOW)
(1<<((1*32+31) & 31))
#endif
#if !definedEx(CONFIG_X86_USE_3DNOW)
0
#endif
))) || ((((0*32+19))>>5)==2 && (1UL<<(((0*32+19))&31) & 0)) || ((((0*32+19))>>5)==3 && (1UL<<(((0*32+19))&31) & (
#if !definedEx(CONFIG_X86_64) && definedEx(CONFIG_X86_P6_NOP) || definedEx(CONFIG_X86_64)
(1<<((3*32+20) & 31))
#endif
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_X86_P6_NOP)
0
#endif
))) || ((((0*32+19))>>5)==4 && (1UL<<(((0*32+19))&31) & 0)) || ((((0*32+19))>>5)==5 && (1UL<<(((0*32+19))&31) & 0)) || ((((0*32+19))>>5)==6 && (1UL<<(((0*32+19))&31) & 0)) || ((((0*32+19))>>5)==7 && (1UL<<(((0*32+19))&31) & 0)) ) ? 1 : (__builtin_constant_p(((0*32+19))) ? constant_test_bit(((0*32+19)), ((unsigned long *)((&boot_cpu_data)->x86_capability))) : variable_test_bit(((0*32+19)), ((unsigned long *)((&boot_cpu_data)->x86_capability))))))
		asm volatile("cli; wbinvd; 1: hlt; jmp 1b" : : : "memory");
	else
		while (1)
			halt();
}
extern void enable_sep_cpu(void);
extern int sysenter_setup(void);
/* Defined in head.S */
extern struct desc_ptr		early_gdt_descr;
extern void cpu_set_gdt(int);
extern void switch_to_new_gdt(int);
extern void load_percpu_segment(int);
extern void cpu_init(void);
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long get_debugctlmsr(void)
{
    unsigned long debugctlmsr = 0;
#if !definedEx(CONFIG_X86_DEBUGCTLMSR)
	if (boot_cpu_data.x86 < 6)
		return 0;
#endif
#if definedEx(CONFIG_PARAVIRT)
do { int _err; debugctlmsr = paravirt_read_msr(0x000001d9, &_err); } while (0)
#endif
#if !definedEx(CONFIG_PARAVIRT)
((debugctlmsr) = native_read_msr((0x000001d9)))
#endif
;
    return debugctlmsr;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long get_debugctlmsr_on_cpu(int cpu)
{
	u64 debugctlmsr = 0;
	u32 val1, val2;
#if !definedEx(CONFIG_X86_DEBUGCTLMSR)
	if (boot_cpu_data.x86 < 6)
		return 0;
#endif
	rdmsr_on_cpu(cpu, 0x000001d9, &val1, &val2);
	debugctlmsr = val1 | ((u64)val2 << 32);
	return debugctlmsr;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void update_debugctlmsr(unsigned long debugctlmsr)
{
#if !definedEx(CONFIG_X86_DEBUGCTLMSR)
	if (boot_cpu_data.x86 < 6)
		return;
#endif
#if definedEx(CONFIG_PARAVIRT)
do { paravirt_write_msr(0x000001d9, (u32)((u64)(debugctlmsr)), ((u64)(debugctlmsr))>>32); } while (0)
#endif
#if !definedEx(CONFIG_PARAVIRT)
native_write_msr((0x000001d9), (u32)((u64)(debugctlmsr)), (u32)((u64)(debugctlmsr) >> 32))
#endif
;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void update_debugctlmsr_on_cpu(int cpu,
					     unsigned long debugctlmsr)
{
#if !definedEx(CONFIG_X86_DEBUGCTLMSR)
	if (boot_cpu_data.x86 < 6)
		return;
#endif
	wrmsr_on_cpu(cpu, 0x000001d9,
		     (u32)((u64)debugctlmsr),
		     (u32)((u64)debugctlmsr >> 32));
}
/*
 * from system description table in BIOS. Mostly for MCA use, but
 * others may find it useful:
 */
extern unsigned int		machine_id;
extern unsigned int		machine_submodel_id;
extern unsigned int		BIOS_revision;
/* Boot loader type from the setup header: */
extern int			bootloader_type;
extern int			bootloader_version;
extern char			ignore_fpu_irq;
#if definedEx(CONFIG_X86_32)
#endif
#if !definedEx(CONFIG_X86_32)
#endif
/*
 * Prefetch instructions for Pentium III (+) and AMD Athlon (+)
 *
 * It's not worth to care about 3dnow prefetches for the K6
 * because they are microcoded there and very slow.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void prefetch(const void *x)
{
	asm volatile ("661:\n\t" 
#if definedEx(CONFIG_X86_32)
#if definedEx(CONFIG_MK7)
".byte 0x8d,0x44,0x20,0x00\n"
#endif
#if !definedEx(CONFIG_MK7) && definedEx(CONFIG_X86_P6_NOP)
".byte 0x0f,0x1f,0x40,0\n"
#endif
#if !definedEx(CONFIG_MK7) && definedEx(CONFIG_X86_64) && !definedEx(CONFIG_X86_P6_NOP)
".byte 0x66,0x66,0x66,0x90\n"
#endif
#if !definedEx(CONFIG_MK7) && !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_X86_P6_NOP)
".byte 0x8d,0x74,0x26,0x00\n"
#endif
#endif
#if !definedEx(CONFIG_X86_32)
"prefetcht0 (%1)"
#endif
 "\n662:\n" ".section .altinstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 "661b\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 "663f\n" "	 .byte " "(0*32+25)" "\n" "	 .byte 662b-661b\n" "	 .byte 664f-663f\n" "	 .byte 0xff + (664f-663f) - (662b-661b)\n" ".previous\n" ".section .altinstr_replacement, \"ax\"\n" "663:\n\t" 
 "prefetchnta (%1)" "\n664:\n" ".previous" : : "i" (0),"r"(x));
}
/*
 * 3dnow prefetch to get an exclusive cache line.
 * Useful for spinlocks to avoid one state transition in the
 * cache coherency protocol:
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void prefetchw(const void *x)
{
	asm volatile ("661:\n\t" 
#if definedEx(CONFIG_X86_32)
#if definedEx(CONFIG_MK7)
".byte 0x8d,0x44,0x20,0x00\n"
#endif
#if !definedEx(CONFIG_MK7) && definedEx(CONFIG_X86_P6_NOP)
".byte 0x0f,0x1f,0x40,0\n"
#endif
#if !definedEx(CONFIG_MK7) && definedEx(CONFIG_X86_64) && !definedEx(CONFIG_X86_P6_NOP)
".byte 0x66,0x66,0x66,0x90\n"
#endif
#if !definedEx(CONFIG_MK7) && !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_X86_P6_NOP)
".byte 0x8d,0x74,0x26,0x00\n"
#endif
#endif
#if !definedEx(CONFIG_X86_32)
"prefetcht0 (%1)"
#endif
 "\n662:\n" ".section .altinstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 "661b\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 "663f\n" "	 .byte " "(1*32+31)" "\n" "	 .byte 662b-661b\n" "	 .byte 664f-663f\n" "	 .byte 0xff + (664f-663f) - (662b-661b)\n" ".previous\n" ".section .altinstr_replacement, \"ax\"\n" "663:\n\t" 
 "prefetchw (%1)" "\n664:\n" ".previous" : : "i" (0),"r"(x));
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void spin_lock_prefetch(const void *x)
{
	prefetchw(x);
}
#if definedEx(CONFIG_X86_32)
/*
 * User space process size: 3GB (default).
 */
/*
 * Note that the .io_bitmap member must be extra-big. This is because
 * the CPU will access an additional byte beyond the end of the IO
 * permission bitmap. The extra byte must be all 1 bits, and must
 * be within the limit.
 */
extern unsigned long thread_saved_pc(struct task_struct *tsk);
/*
 * The below -8 is to reserve 8 bytes on top of the ring0 stack.
 * This is necessary to guarantee that the entire "struct pt_regs"
 * is accessable even if the CPU haven't stored the SS/ESP registers
 * on the stack (interrupt gate does not save these registers
 * when switching to the same priv ring).
 * Therefore beware: accessing the ss/esp fields of the
 * "struct pt_regs" is possible, but they may contain the
 * completely wrong values.
 */
#endif
#if !definedEx(CONFIG_X86_32)
/*
 * User space process size. 47bits minus one guard page.
 */
/* This decides where the kernel will search for a free chunk of vm
 * space during mmap's.
 */
/*
 * Return saved PC of a blocked thread.
 * What is this good for? it will be always the scheduler or ret_from_fork.
 */
extern unsigned long KSTK_ESP(struct task_struct *task);
#endif
extern void start_thread(struct pt_regs *regs, unsigned long new_ip,
					       unsigned long new_sp);
/*
 * This decides where the kernel will search for a free chunk of vm
 * space during mmap's.
 */
/* Get/set a process' ability to use the timestamp counter instruction */
extern int get_tsc_mode(unsigned long adr);
extern int set_tsc_mode(unsigned int val);
extern int amd_get_nb_id(int cpu);
struct aperfmperf {
	u64 aperf, mperf;
};
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void get_aperfmperf(struct aperfmperf *am)
{
	({ static bool __warned; int __ret_warn_once = !!(!(__builtin_constant_p((3*32+28)) && ( ((((3*32+28))>>5)==0 && (1UL<<(((3*32+28))&31) & (
#if !definedEx(CONFIG_MATH_EMULATION)
(1<<((0*32+ 0) & 31))
#endif
#if definedEx(CONFIG_MATH_EMULATION)
0
#endif
|
#if !definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_64) && definedEx(CONFIG_PARAVIRT)
0
#endif
#if definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT)
(1<<((0*32+ 3)) & 31)
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+ 5) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if !definedEx(CONFIG_X86_PAE) && definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_PAE)
(1<<((0*32+ 6) & 31))
#endif
#if !definedEx(CONFIG_X86_PAE) && !definedEx(CONFIG_X86_64)
0
#endif
| 
#if definedEx(CONFIG_X86_CMPXCHG64)
(1<<((0*32+ 8) & 31))
#endif
#if !definedEx(CONFIG_X86_CMPXCHG64)
0
#endif
|
#if !definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_64) && definedEx(CONFIG_PARAVIRT)
0
#endif
#if definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT)
(1<<((0*32+13)) & 31)
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+24) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if !definedEx(CONFIG_X86_64) && definedEx(CONFIG_X86_CMOV) || definedEx(CONFIG_X86_64)
(1<<((0*32+15) & 31))
#endif
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_X86_CMOV)
0
#endif
| 
#if definedEx(CONFIG_X86_64)
(1<<((0*32+25) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+26) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
))) || ((((3*32+28))>>5)==1 && (1UL<<(((3*32+28))&31) & (
#if definedEx(CONFIG_X86_64)
(1<<((1*32+29) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if definedEx(CONFIG_X86_USE_3DNOW)
(1<<((1*32+31) & 31))
#endif
#if !definedEx(CONFIG_X86_USE_3DNOW)
0
#endif
))) || ((((3*32+28))>>5)==2 && (1UL<<(((3*32+28))&31) & 0)) || ((((3*32+28))>>5)==3 && (1UL<<(((3*32+28))&31) & (
#if !definedEx(CONFIG_X86_64) && definedEx(CONFIG_X86_P6_NOP) || definedEx(CONFIG_X86_64)
(1<<((3*32+20) & 31))
#endif
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_X86_P6_NOP)
0
#endif
))) || ((((3*32+28))>>5)==4 && (1UL<<(((3*32+28))&31) & 0)) || ((((3*32+28))>>5)==5 && (1UL<<(((3*32+28))&31) & 0)) || ((((3*32+28))>>5)==6 && (1UL<<(((3*32+28))&31) & 0)) || ((((3*32+28))>>5)==7 && (1UL<<(((3*32+28))&31) & 0)) ) ? 1 : (__builtin_constant_p(((3*32+28))) ? constant_test_bit(((3*32+28)), ((unsigned long *)((&boot_cpu_data)->x86_capability))) : variable_test_bit(((3*32+28)), ((unsigned long *)((&boot_cpu_data)->x86_capability)))))); if (__builtin_expect(!!(__ret_warn_once), 0)) if (
#if definedEx(CONFIG_BUG)
({ int __ret_warn_on = !!(!__warned); if (__builtin_expect(!!(__ret_warn_on), 0)) warn_slowpath_null("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/processor.h", 1034); __builtin_expect(!!(__ret_warn_on), 0); })
#endif
#if !definedEx(CONFIG_BUG)
({ int __ret_warn_on = !!(!__warned); __builtin_expect(!!(__ret_warn_on), 0); })
#endif
) __warned = true; __builtin_expect(!!(__ret_warn_once), 0); });
#if definedEx(CONFIG_PARAVIRT)
do { int _err; am->aperf = paravirt_read_msr(0x000000e8, &_err); } while (0)
#endif
#if !definedEx(CONFIG_PARAVIRT)
((am->aperf) = native_read_msr((0x000000e8)))
#endif
;
#if definedEx(CONFIG_PARAVIRT)
do { int _err; am->mperf = paravirt_read_msr(0x000000e7, &_err); } while (0)
#endif
#if !definedEx(CONFIG_PARAVIRT)
((am->mperf) = native_read_msr((0x000000e7)))
#endif
;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
unsigned long calc_aperfmperf_ratio(struct aperfmperf *old,
				    struct aperfmperf *new)
{
	u64 aperf = new->aperf - old->aperf;
	u64 mperf = new->mperf - old->mperf;
	unsigned long ratio = aperf;
	mperf >>= 10;
	if (mperf)
		ratio = div64_u64(aperf, mperf);
	return ratio;
}
#line 16 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/prefetch.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/cache.h" 1
#line 17 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/prefetch.h" 2
/*
	prefetch(x) attempts to pre-emptively get the memory pointed to
	by address "x" into the CPU L1 cache. 
	prefetch(x) should not cause any kind of exception, prefetch(0) is
	specifically ok.
	prefetch() should be defined by the architecture, if not, the 
	#define below provides a no-op define.	
	There are 3 prefetch() macros:
	prefetch(x)  	- prefetches the cacheline at "x" for read
	prefetchw(x)	- prefetches the cacheline at "x" for write
	spin_lock_prefetch(x) - prefetches the spinlock *x for taking
	there is also PREFETCH_STRIDE which is the architecure-prefered 
	"lookahead" size for prefetching streamed operations.
*/
#if !definedEx(CONFIG_X86_32)
#endif
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void prefetch_range(void *addr, size_t len)
{
#if definedEx(CONFIG_X86_32)
	char *cp;
	char *end = addr + len;
	for (cp = addr; cp < end; cp += (4*(1 << (5))))
		prefetch(cp);
#endif
}
#line 8 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/list.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/system.h" 1
#line 9 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/list.h" 2
/*
 * Simple doubly linked list implementation.
 *
 * Some of the internal functions ("__xxx") are useful when
 * manipulating whole lists rather than single entries, as
 * sometimes we already know the next/prev entries and we can
 * generate better code by using them directly rather than
 * using the generic single-entry routines.
 */
struct list_head {
	struct list_head *next, *prev;
};
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void INIT_LIST_HEAD(struct list_head *list)
{
	list->next = list;
	list->prev = list;
}
/*
 * Insert a new entry between two known consecutive entries.
 *
 * This is only for internal list manipulation where we know
 * the prev/next entries already!
 */
#if !definedEx(CONFIG_DEBUG_LIST)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __list_add(struct list_head *new,
			      struct list_head *prev,
			      struct list_head *next)
{
	next->prev = new;
	new->next = next;
	new->prev = prev;
	prev->next = new;
}
#endif
#if definedEx(CONFIG_DEBUG_LIST)
extern void __list_add(struct list_head *new,
			      struct list_head *prev,
			      struct list_head *next);
#endif
/**
 * list_add - add a new entry
 * @new: new entry to be added
 * @head: list head to add it after
 *
 * Insert a new entry after the specified head.
 * This is good for implementing stacks.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void list_add(struct list_head *new, struct list_head *head)
{
	__list_add(new, head, head->next);
}
/**
 * list_add_tail - add a new entry
 * @new: new entry to be added
 * @head: list head to add it before
 *
 * Insert a new entry before the specified head.
 * This is useful for implementing queues.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void list_add_tail(struct list_head *new, struct list_head *head)
{
	__list_add(new, head->prev, head);
}
/*
 * Delete a list entry by making the prev/next entries
 * point to each other.
 *
 * This is only for internal list manipulation where we know
 * the prev/next entries already!
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __list_del(struct list_head * prev, struct list_head * next)
{
	next->prev = prev;
	prev->next = next;
}
/**
 * list_del - deletes entry from list.
 * @entry: the element to delete from the list.
 * Note: list_empty() on entry does not return true after this, the entry is
 * in an undefined state.
 */
#if !definedEx(CONFIG_DEBUG_LIST)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void list_del(struct list_head *entry)
{
	__list_del(entry->prev, entry->next);
	entry->next = ((void *) 0x00100100 + (0x0UL));
	entry->prev = ((void *) 0x00200200 + (0x0UL));
}
#endif
#if definedEx(CONFIG_DEBUG_LIST)
extern void list_del(struct list_head *entry);
#endif
/**
 * list_replace - replace old entry by new one
 * @old : the element to be replaced
 * @new : the new element to insert
 *
 * If @old was empty, it will be overwritten.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void list_replace(struct list_head *old,
				struct list_head *new)
{
	new->next = old->next;
	new->next->prev = new;
	new->prev = old->prev;
	new->prev->next = new;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void list_replace_init(struct list_head *old,
					struct list_head *new)
{
	list_replace(old, new);
	INIT_LIST_HEAD(old);
}
/**
 * list_del_init - deletes entry from list and reinitialize it.
 * @entry: the element to delete from the list.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void list_del_init(struct list_head *entry)
{
	__list_del(entry->prev, entry->next);
	INIT_LIST_HEAD(entry);
}
/**
 * list_move - delete from one list and add as another's head
 * @list: the entry to move
 * @head: the head that will precede our entry
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void list_move(struct list_head *list, struct list_head *head)
{
	__list_del(list->prev, list->next);
	list_add(list, head);
}
/**
 * list_move_tail - delete from one list and add as another's tail
 * @list: the entry to move
 * @head: the head that will follow our entry
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void list_move_tail(struct list_head *list,
				  struct list_head *head)
{
	__list_del(list->prev, list->next);
	list_add_tail(list, head);
}
/**
 * list_is_last - tests whether @list is the last entry in list @head
 * @list: the entry to test
 * @head: the head of the list
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int list_is_last(const struct list_head *list,
				const struct list_head *head)
{
	return list->next == head;
}
/**
 * list_empty - tests whether a list is empty
 * @head: the list to test.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int list_empty(const struct list_head *head)
{
	return head->next == head;
}
/**
 * list_empty_careful - tests whether a list is empty and not being modified
 * @head: the list to test
 *
 * Description:
 * tests whether a list is empty _and_ checks that no other CPU might be
 * in the process of modifying either member (next or prev)
 *
 * NOTE: using list_empty_careful() without synchronization
 * can only be safe if the only activity that can happen
 * to the list entry is list_del_init(). Eg. it cannot be used
 * if another CPU could re-list_add() it.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int list_empty_careful(const struct list_head *head)
{
	struct list_head *next = head->next;
	return (next == head) && (next == head->prev);
}
/**
 * list_is_singular - tests whether a list has just one entry.
 * @head: the list to test.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int list_is_singular(const struct list_head *head)
{
	return !list_empty(head) && (head->next == head->prev);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __list_cut_position(struct list_head *list,
		struct list_head *head, struct list_head *entry)
{
	struct list_head *new_first = entry->next;
	list->next = head->next;
	list->next->prev = list;
	list->prev = entry;
	entry->next = list;
	head->next = new_first;
	new_first->prev = head;
}
/**
 * list_cut_position - cut a list into two
 * @list: a new list to add all removed entries
 * @head: a list with entries
 * @entry: an entry within head, could be the head itself
 *	and if so we won't cut the list
 *
 * This helper moves the initial part of @head, up to and
 * including @entry, from @head to @list. You should
 * pass on @entry an element you know is on @head. @list
 * should be an empty list or a list you do not care about
 * losing its data.
 *
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void list_cut_position(struct list_head *list,
		struct list_head *head, struct list_head *entry)
{
	if (list_empty(head))
		return;
	if (list_is_singular(head) &&
		(head->next != entry && head != entry))
		return;
	if (entry == head)
		INIT_LIST_HEAD(list);
	else
		__list_cut_position(list, head, entry);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __list_splice(const struct list_head *list,
				 struct list_head *prev,
				 struct list_head *next)
{
	struct list_head *first = list->next;
	struct list_head *last = list->prev;
	first->prev = prev;
	prev->next = first;
	last->next = next;
	next->prev = last;
}
/**
 * list_splice - join two lists, this is designed for stacks
 * @list: the new list to add.
 * @head: the place to add it in the first list.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void list_splice(const struct list_head *list,
				struct list_head *head)
{
	if (!list_empty(list))
		__list_splice(list, head, head->next);
}
/**
 * list_splice_tail - join two lists, each list being a queue
 * @list: the new list to add.
 * @head: the place to add it in the first list.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void list_splice_tail(struct list_head *list,
				struct list_head *head)
{
	if (!list_empty(list))
		__list_splice(list, head->prev, head);
}
/**
 * list_splice_init - join two lists and reinitialise the emptied list.
 * @list: the new list to add.
 * @head: the place to add it in the first list.
 *
 * The list at @list is reinitialised
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void list_splice_init(struct list_head *list,
				    struct list_head *head)
{
	if (!list_empty(list)) {
		__list_splice(list, head, head->next);
		INIT_LIST_HEAD(list);
	}
}
/**
 * list_splice_tail_init - join two lists and reinitialise the emptied list
 * @list: the new list to add.
 * @head: the place to add it in the first list.
 *
 * Each of the lists is a queue.
 * The list at @list is reinitialised
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void list_splice_tail_init(struct list_head *list,
					 struct list_head *head)
{
	if (!list_empty(list)) {
		__list_splice(list, head->prev, head);
		INIT_LIST_HEAD(list);
	}
}
/**
 * list_entry - get the struct for this entry
 * @ptr:	the &struct list_head pointer.
 * @type:	the type of the struct this is embedded in.
 * @member:	the name of the list_struct within the struct.
 */
/**
 * list_first_entry - get the first element from a list
 * @ptr:	the list head to take the element from.
 * @type:	the type of the struct this is embedded in.
 * @member:	the name of the list_struct within the struct.
 *
 * Note, that list is expected to be not empty.
 */
/**
 * list_for_each	-	iterate over a list
 * @pos:	the &struct list_head to use as a loop cursor.
 * @head:	the head for your list.
 */
/**
 * __list_for_each	-	iterate over a list
 * @pos:	the &struct list_head to use as a loop cursor.
 * @head:	the head for your list.
 *
 * This variant differs from list_for_each() in that it's the
 * simplest possible list iteration code, no prefetching is done.
 * Use this for code that knows the list to be very short (empty
 * or 1 entry) most of the time.
 */
/**
 * list_for_each_prev	-	iterate over a list backwards
 * @pos:	the &struct list_head to use as a loop cursor.
 * @head:	the head for your list.
 */
/**
 * list_for_each_safe - iterate over a list safe against removal of list entry
 * @pos:	the &struct list_head to use as a loop cursor.
 * @n:		another &struct list_head to use as temporary storage
 * @head:	the head for your list.
 */
/**
 * list_for_each_prev_safe - iterate over a list backwards safe against removal of list entry
 * @pos:	the &struct list_head to use as a loop cursor.
 * @n:		another &struct list_head to use as temporary storage
 * @head:	the head for your list.
 */
/**
 * list_for_each_entry	-	iterate over list of given type
 * @pos:	the type * to use as a loop cursor.
 * @head:	the head for your list.
 * @member:	the name of the list_struct within the struct.
 */
/**
 * list_for_each_entry_reverse - iterate backwards over list of given type.
 * @pos:	the type * to use as a loop cursor.
 * @head:	the head for your list.
 * @member:	the name of the list_struct within the struct.
 */
/**
 * list_prepare_entry - prepare a pos entry for use in list_for_each_entry_continue()
 * @pos:	the type * to use as a start point
 * @head:	the head of the list
 * @member:	the name of the list_struct within the struct.
 *
 * Prepares a pos entry for use as a start point in list_for_each_entry_continue().
 */
/**
 * list_for_each_entry_continue - continue iteration over list of given type
 * @pos:	the type * to use as a loop cursor.
 * @head:	the head for your list.
 * @member:	the name of the list_struct within the struct.
 *
 * Continue to iterate over list of given type, continuing after
 * the current position.
 */
/**
 * list_for_each_entry_continue_reverse - iterate backwards from the given point
 * @pos:	the type * to use as a loop cursor.
 * @head:	the head for your list.
 * @member:	the name of the list_struct within the struct.
 *
 * Start to iterate over list of given type backwards, continuing after
 * the current position.
 */
/**
 * list_for_each_entry_from - iterate over list of given type from the current point
 * @pos:	the type * to use as a loop cursor.
 * @head:	the head for your list.
 * @member:	the name of the list_struct within the struct.
 *
 * Iterate over list of given type, continuing from current position.
 */
/**
 * list_for_each_entry_safe - iterate over list of given type safe against removal of list entry
 * @pos:	the type * to use as a loop cursor.
 * @n:		another type * to use as temporary storage
 * @head:	the head for your list.
 * @member:	the name of the list_struct within the struct.
 */
/**
 * list_for_each_entry_safe_continue
 * @pos:	the type * to use as a loop cursor.
 * @n:		another type * to use as temporary storage
 * @head:	the head for your list.
 * @member:	the name of the list_struct within the struct.
 *
 * Iterate over list of given type, continuing after current point,
 * safe against removal of list entry.
 */
/**
 * list_for_each_entry_safe_from
 * @pos:	the type * to use as a loop cursor.
 * @n:		another type * to use as temporary storage
 * @head:	the head for your list.
 * @member:	the name of the list_struct within the struct.
 *
 * Iterate over list of given type from current point, safe against
 * removal of list entry.
 */
/**
 * list_for_each_entry_safe_reverse
 * @pos:	the type * to use as a loop cursor.
 * @n:		another type * to use as temporary storage
 * @head:	the head for your list.
 * @member:	the name of the list_struct within the struct.
 *
 * Iterate backwards over list of given type, safe against removal
 * of list entry.
 */
/*
 * Double linked lists with a single pointer list head.
 * Mostly useful for hash tables where the two pointer list head is
 * too wasteful.
 * You lose the ability to access the tail in O(1).
 */
struct hlist_head {
	struct hlist_node *first;
};
struct hlist_node {
	struct hlist_node *next, **pprev;
};
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void INIT_HLIST_NODE(struct hlist_node *h)
{
	h->next = ((void *)0);
	h->pprev = ((void *)0);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int hlist_unhashed(const struct hlist_node *h)
{
	return !h->pprev;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int hlist_empty(const struct hlist_head *h)
{
	return !h->first;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __hlist_del(struct hlist_node *n)
{
	struct hlist_node *next = n->next;
	struct hlist_node **pprev = n->pprev;
	*pprev = next;
	if (next)
		next->pprev = pprev;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void hlist_del(struct hlist_node *n)
{
	__hlist_del(n);
	n->next = ((void *) 0x00100100 + (0x0UL));
	n->pprev = ((void *) 0x00200200 + (0x0UL));
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void hlist_del_init(struct hlist_node *n)
{
	if (!hlist_unhashed(n)) {
		__hlist_del(n);
		INIT_HLIST_NODE(n);
	}
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void hlist_add_head(struct hlist_node *n, struct hlist_head *h)
{
	struct hlist_node *first = h->first;
	n->next = first;
	if (first)
		first->pprev = &n->next;
	h->first = n;
	n->pprev = &h->first;
}
/* next must be != NULL */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void hlist_add_before(struct hlist_node *n,
					struct hlist_node *next)
{
	n->pprev = next->pprev;
	n->next = next;
	next->pprev = &n->next;
	*(n->pprev) = n;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void hlist_add_after(struct hlist_node *n,
					struct hlist_node *next)
{
	next->next = n->next;
	n->next = next;
	next->pprev = &n->next;
	if(next->next)
		next->next->pprev  = &next->next;
}
/*
 * Move a list from one list head to another. Fixup the pprev
 * reference of the first entry if it exists.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void hlist_move_list(struct hlist_head *old,
				   struct hlist_head *new)
{
	new->first = old->first;
	if (new->first)
		new->first->pprev = &new->first;
	old->first = ((void *)0);
}
/**
 * hlist_for_each_entry	- iterate over list of given type
 * @tpos:	the type * to use as a loop cursor.
 * @pos:	the &struct hlist_node to use as a loop cursor.
 * @head:	the head for your list.
 * @member:	the name of the hlist_node within the struct.
 */
/**
 * hlist_for_each_entry_continue - iterate over a hlist continuing after current point
 * @tpos:	the type * to use as a loop cursor.
 * @pos:	the &struct hlist_node to use as a loop cursor.
 * @member:	the name of the hlist_node within the struct.
 */
/**
 * hlist_for_each_entry_from - iterate over a hlist continuing from current point
 * @tpos:	the type * to use as a loop cursor.
 * @pos:	the &struct hlist_node to use as a loop cursor.
 * @member:	the name of the hlist_node within the struct.
 */
/**
 * hlist_for_each_entry_safe - iterate over list of given type safe against removal of list entry
 * @tpos:	the type * to use as a loop cursor.
 * @pos:	the &struct hlist_node to use as a loop cursor.
 * @n:		another &struct hlist_node to use as temporary storage
 * @head:	the head for your list.
 * @member:	the name of the hlist_node within the struct.
 */
#line 11 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/module.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/stat.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/stat.h" 1
#if definedEx(CONFIG_X86_32)
struct stat {
	unsigned long  st_dev;
	unsigned long  st_ino;
	unsigned short st_mode;
	unsigned short st_nlink;
	unsigned short st_uid;
	unsigned short st_gid;
	unsigned long  st_rdev;
	unsigned long  st_size;
	unsigned long  st_blksize;
	unsigned long  st_blocks;
	unsigned long  st_atime;
	unsigned long  st_atime_nsec;
	unsigned long  st_mtime;
	unsigned long  st_mtime_nsec;
	unsigned long  st_ctime;
	unsigned long  st_ctime_nsec;
	unsigned long  __unused4;
	unsigned long  __unused5;
};
/* This matches struct stat64 in glibc2.1, hence the absolutely
 * insane amounts of padding around dev_t's.
 */
struct stat64 {
	unsigned long long	st_dev;
	unsigned char	__pad0[4];
	unsigned long	__st_ino;
	unsigned int	st_mode;
	unsigned int	st_nlink;
	unsigned long	st_uid;
	unsigned long	st_gid;
	unsigned long long	st_rdev;
	unsigned char	__pad3[4];
	long long	st_size;
	unsigned long	st_blksize;
	/* Number 512-byte blocks allocated. */
	unsigned long long	st_blocks;
	unsigned long	st_atime;
	unsigned long	st_atime_nsec;
	unsigned long	st_mtime;
	unsigned int	st_mtime_nsec;
	unsigned long	st_ctime;
	unsigned long	st_ctime_nsec;
	unsigned long long	st_ino;
};
#endif
#if !definedEx(CONFIG_X86_32)
struct stat {
	unsigned long	st_dev;
	unsigned long	st_ino;
	unsigned long	st_nlink;
	unsigned int	st_mode;
	unsigned int	st_uid;
	unsigned int	st_gid;
	unsigned int	__pad0;
	unsigned long	st_rdev;
	long		st_size;
	long		st_blksize;
	long		st_blocks;	/* Number 512-byte blocks allocated. */
	unsigned long	st_atime;
	unsigned long	st_atime_nsec;
	unsigned long	st_mtime;
	unsigned long	st_mtime_nsec;
	unsigned long	st_ctime;
	unsigned long   st_ctime_nsec;
	long		__unused[3];
};
#endif
/* for 32bit emulation and 32 bit kernels */
struct __old_kernel_stat {
	unsigned short st_dev;
	unsigned short st_ino;
	unsigned short st_mode;
	unsigned short st_nlink;
	unsigned short st_uid;
	unsigned short st_gid;
	unsigned short st_rdev;
#if definedEx(CONFIG_X86_32)
	unsigned long  st_size;
	unsigned long  st_atime;
	unsigned long  st_mtime;
	unsigned long  st_ctime;
#endif
#if !definedEx(CONFIG_X86_32)
	unsigned int  st_size;
	unsigned int  st_atime;
	unsigned int  st_mtime;
	unsigned int  st_ctime;
#endif
};
#line 8 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/stat.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 61 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/stat.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/time.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/time.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/cache.h" 1
#line 9 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/time.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/seqlock.h" 1
/*
 * Reader/writer consistent mechanism without starving writers. This type of
 * lock for data where the reader wants a consistent set of information
 * and is willing to retry if the information changes.  Readers never
 * block but they may have to retry if a writer is in
 * progress. Writers do not wait for readers. 
 *
 * This is not as cache friendly as brlock. Also, this will not work
 * for data that contains pointers, because any writer could
 * invalidate a pointer that a reader was following.
 *
 * Expected reader usage:
 * 	do {
 *	    seq = read_seqbegin(&foo);
 * 	...
 *      } while (read_seqretry(&foo, seq));
 *
 *
 * On non-SMP the spin locks disappear but the writer still needs
 * to increment the sequence variables because an interrupt routine could
 * change the state of the data.
 *
 * Based on x86_64 vsyscall gettimeofday 
 * by Keith Owens and Andrea Arcangeli
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/spinlock.h" 1
/*
 * include/linux/spinlock.h - generic spinlock/rwlock declarations
 *
 * here's the role of the various spinlock/rwlock related include files:
 *
 * on SMP builds:
 *
 *  asm/spinlock_types.h: contains the arch_spinlock_t/arch_rwlock_t and the
 *                        initializers
 *
 *  linux/spinlock_types.h:
 *                        defines the generic type and initializers
 *
 *  asm/spinlock.h:       contains the arch_spin_*()/etc. lowlevel
 *                        implementations, mostly inline assembly code
 *
 *   (also included on UP-debug builds:)
 *
 *  linux/spinlock_api_smp.h:
 *                        contains the prototypes for the _spin_*() APIs.
 *
 *  linux/spinlock.h:     builds the final spin_*() APIs.
 *
 * on UP builds:
 *
 *  linux/spinlock_type_up.h:
 *                        contains the generic, simplified UP spinlock type.
 *                        (which is an empty structure on non-debug builds)
 *
 *  linux/spinlock_types.h:
 *                        defines the generic type and initializers
 *
 *  linux/spinlock_up.h:
 *                        contains the arch_spin_*()/etc. version of UP
 *                        builds. (which are NOPs on non-debug, non-preempt
 *                        builds)
 *
 *   (included on UP-non-debug builds:)
 *
 *  linux/spinlock_api_up.h:
 *                        builds the _spin_*() APIs.
 *
 *  linux/spinlock.h:     builds the final spin_*() APIs.
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/typecheck.h" 1
#line 51 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/spinlock.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/preempt.h" 1
/*
 * include/linux/preempt.h - macros for accessing and manipulating
 * preempt_count (used for kernel preemption, interrupt count, etc.)
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/thread_info.h" 1
/* thread_info.h: common low-level thread information accessors
 *
 * Copyright (C) 2002  David Howells (dhowells@redhat.com)
 * - Incorporating suggestions made by Linus Torvalds
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 12 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/thread_info.h" 2
struct timespec;
struct compat_timespec;
/*
 * System call restart block.
 */
struct restart_block {
	long (*fn)(struct restart_block *);
	union {
		struct {
			unsigned long arg0, arg1, arg2, arg3;
		};
		/* For futex_wait and futex_wait_requeue_pi */
		struct {
			u32 *uaddr;
			u32 val;
			u32 flags;
			u32 bitset;
			u64 time;
			u32 *uaddr2;
		} futex;
		/* For nanosleep */
		struct {
			clockid_t index;
			struct timespec  *rmtp;
#if definedEx(CONFIG_COMPAT)
			struct compat_timespec  *compat_rmtp;
#endif
			u64 expires;
		} nanosleep;
		/* For poll */
		struct {
			struct pollfd  *ufds;
			int nfds;
			int has_timeout;
			unsigned long tv_sec;
			unsigned long tv_nsec;
		} poll;
	};
};
extern long do_no_restart_syscall(struct restart_block *parm);
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/bitops.h" 1
#line 57 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/thread_info.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/thread_info.h" 1
/* thread_info.h: low-level thread information
 *
 * Copyright (C) 2002  David Howells (dhowells@redhat.com)
 * - Incorporating suggestions made by Linus Torvalds and Dave Miller
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/compiler.h" 1
#line 12 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/thread_info.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/page.h" 1
#line 13 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/thread_info.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/types.h" 1
#line 14 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/thread_info.h" 2
/*
 * low level task data that entry.S needs immediate access to
 * - this struct should fit entirely inside of one cache line
 * - this struct shares the supervisor stack pages
 */
struct task_struct;
struct exec_domain;
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/processor.h" 1
#line 24 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/thread_info.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/ftrace.h" 1
#if definedEx(CONFIG_FUNCTION_TRACER)
extern void mcount(void);
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long ftrace_call_adjust(unsigned long addr)
{
	/*
	 * call mcount is "e8 <4 byte offset>"
	 * The addr points to the 4 byte offset and the caller of this
	 * function wants the pointer to e8. Simply subtract one.
	 */
	return addr - 1;
}
#if definedEx(CONFIG_DYNAMIC_FTRACE)
struct dyn_arch_ftrace {
	/* No extra data needed for x86 */
};
#endif
#endif
#line 25 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/thread_info.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic.h" 1
#if definedEx(CONFIG_X86_32)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic_32.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/compiler.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic_32.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic_32.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/processor.h" 1
#line 8 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic_32.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/cmpxchg.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/cmpxchg_32.h" 1
#line 4 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/cmpxchg.h" 2
#line 9 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic_32.h" 2
/*
 * Atomic operations that C can't guarantee us.  Useful for
 * resource counting etc..
 */
/**
 * atomic_read - read atomic variable
 * @v: pointer of type atomic_t
 *
 * Atomically reads the value of @v.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int atomic_read(const atomic_t *v)
{
	return v->counter;
}
/**
 * atomic_set - set atomic variable
 * @v: pointer of type atomic_t
 * @i: required value
 *
 * Atomically sets the value of @v to @i.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void atomic_set(atomic_t *v, int i)
{
	v->counter = i;
}
/**
 * atomic_add - add integer to atomic variable
 * @i: integer value to add
 * @v: pointer of type atomic_t
 *
 * Atomically adds @i to @v.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void atomic_add(int i, atomic_t *v)
{
	asm volatile(
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" " " ".balign 4" " " "\n" " " ".long" " " "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "addl %1,%0"
		     : "+m" (v->counter)
		     : "ir" (i));
}
/**
 * atomic_sub - subtract integer from atomic variable
 * @i: integer value to subtract
 * @v: pointer of type atomic_t
 *
 * Atomically subtracts @i from @v.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void atomic_sub(int i, atomic_t *v)
{
	asm volatile(
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" " " ".balign 4" " " "\n" " " ".long" " " "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "subl %1,%0"
		     : "+m" (v->counter)
		     : "ir" (i));
}
/**
 * atomic_sub_and_test - subtract value from variable and test result
 * @i: integer value to subtract
 * @v: pointer of type atomic_t
 *
 * Atomically subtracts @i from @v and returns
 * true if the result is zero, or false for all
 * other cases.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int atomic_sub_and_test(int i, atomic_t *v)
{
	unsigned char c;
	asm volatile(
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" " " ".balign 4" " " "\n" " " ".long" " " "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "subl %2,%0; sete %1"
		     : "+m" (v->counter), "=qm" (c)
		     : "ir" (i) : "memory");
	return c;
}
/**
 * atomic_inc - increment atomic variable
 * @v: pointer of type atomic_t
 *
 * Atomically increments @v by 1.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void atomic_inc(atomic_t *v)
{
	asm volatile(
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" " " ".balign 4" " " "\n" " " ".long" " " "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "incl %0"
		     : "+m" (v->counter));
}
/**
 * atomic_dec - decrement atomic variable
 * @v: pointer of type atomic_t
 *
 * Atomically decrements @v by 1.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void atomic_dec(atomic_t *v)
{
	asm volatile(
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" " " ".balign 4" " " "\n" " " ".long" " " "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "decl %0"
		     : "+m" (v->counter));
}
/**
 * atomic_dec_and_test - decrement and test
 * @v: pointer of type atomic_t
 *
 * Atomically decrements @v by 1 and
 * returns true if the result is 0, or false for all other
 * cases.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int atomic_dec_and_test(atomic_t *v)
{
	unsigned char c;
	asm volatile(
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" " " ".balign 4" " " "\n" " " ".long" " " "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "decl %0; sete %1"
		     : "+m" (v->counter), "=qm" (c)
		     : : "memory");
	return c != 0;
}
/**
 * atomic_inc_and_test - increment and test
 * @v: pointer of type atomic_t
 *
 * Atomically increments @v by 1
 * and returns true if the result is zero, or false for all
 * other cases.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int atomic_inc_and_test(atomic_t *v)
{
	unsigned char c;
	asm volatile(
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" " " ".balign 4" " " "\n" " " ".long" " " "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "incl %0; sete %1"
		     : "+m" (v->counter), "=qm" (c)
		     : : "memory");
	return c != 0;
}
/**
 * atomic_add_negative - add and test if negative
 * @v: pointer of type atomic_t
 * @i: integer value to add
 *
 * Atomically adds @i to @v and returns true
 * if the result is negative, or false when
 * result is greater than or equal to zero.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int atomic_add_negative(int i, atomic_t *v)
{
	unsigned char c;
	asm volatile(
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" " " ".balign 4" " " "\n" " " ".long" " " "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "addl %2,%0; sets %1"
		     : "+m" (v->counter), "=qm" (c)
		     : "ir" (i) : "memory");
	return c;
}
/**
 * atomic_add_return - add integer and return
 * @v: pointer of type atomic_t
 * @i: integer value to add
 *
 * Atomically adds @i to @v and returns @i + @v
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int atomic_add_return(int i, atomic_t *v)
{
	int __i;
#if definedEx(CONFIG_M386)
	unsigned long flags;
	if (__builtin_expect(!!(boot_cpu_data.x86 <= 3), 0))
		goto no_xadd;
#endif
	/* Modern 486+ processor */
	__i = i;
	asm volatile(
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" " " ".balign 4" " " "\n" " " ".long" " " "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "xaddl %0, %1"
		     : "+r" (i), "+m" (v->counter)
		     : : "memory");
	return i + __i;
#if definedEx(CONFIG_M386)
no_xadd: /* Legacy 386 processor */
	do { ({ unsigned long __dummy; typeof(flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); do { (flags) = __raw_local_irq_save(); } while (0); 
#if !definedEx(CONFIG_TRACE_IRQFLAGS)
do { } while (0)
#endif
#if definedEx(CONFIG_TRACE_IRQFLAGS)
trace_hardirqs_off()
#endif
; } while (0);
	__i = atomic_read(v);
	atomic_set(v, i + __i);
	do { ({ unsigned long __dummy; typeof(flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); if (raw_irqs_disabled_flags(flags)) { raw_local_irq_restore(flags); 
#if !definedEx(CONFIG_TRACE_IRQFLAGS)
do { } while (0)
#endif
#if definedEx(CONFIG_TRACE_IRQFLAGS)
trace_hardirqs_off()
#endif
; } else { 
#if !definedEx(CONFIG_TRACE_IRQFLAGS)
do { } while (0)
#endif
#if definedEx(CONFIG_TRACE_IRQFLAGS)
trace_hardirqs_on()
#endif
; raw_local_irq_restore(flags); } } while (0);
	return i + __i;
#endif
}
/**
 * atomic_sub_return - subtract integer and return
 * @v: pointer of type atomic_t
 * @i: integer value to subtract
 *
 * Atomically subtracts @i from @v and returns @v - @i
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int atomic_sub_return(int i, atomic_t *v)
{
	return atomic_add_return(-i, v);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int atomic_cmpxchg(atomic_t *v, int old, int new)
{
	return 
#if !definedEx(CONFIG_X86_32) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_X86_CMPXCHG)
({ __typeof__(*(((&v->counter)))) __ret; __typeof__(*(((&v->counter)))) __old = (((old))); __typeof__(*(((&v->counter)))) __new = (((new))); switch ((sizeof(*&v->counter))) { case 1: asm volatile(
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" " " ".balign 4" " " "\n" " " ".long" " " "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "cmpxchgb %b1,%2" : "=a"(__ret) : "q"(__new), "m"(*((struct __xchg_dummy *)(((&v->counter))))), "0"(__old) : "memory"); break; case 2: asm volatile(
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" " " ".balign 4" " " "\n" " " ".long" " " "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "cmpxchgw %w1,%2" : "=a"(__ret) : "r"(__new), "m"(*((struct __xchg_dummy *)(((&v->counter))))), "0"(__old) : "memory"); break; case 4: asm volatile(
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" " " ".balign 4" " " "\n" " " ".long" " " "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "cmpxchgl %1,%2" : "=a"(__ret) : "r"(__new), "m"(*((struct __xchg_dummy *)(((&v->counter))))), "0"(__old) : "memory"); break; default: __cmpxchg_wrong_size(); } __ret; })
#endif
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_X86_CMPXCHG)
({ __typeof__(*(&v->counter)) __ret; if (__builtin_expect(!!(boot_cpu_data.x86 > 3), 1)) __ret = (__typeof__(*(&v->counter)))({ __typeof__(*(((&v->counter)))) __ret; __typeof__(*(((&v->counter)))) __old = (((unsigned long)(old))); __typeof__(*(((&v->counter)))) __new = (((unsigned long)(new))); switch ((sizeof(*(&v->counter)))) { case 1: asm volatile(
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" " " ".balign 4" " " "\n" " " ".long" " " "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "cmpxchgb %b1,%2" : "=a"(__ret) : "q"(__new), "m"(*((struct __xchg_dummy *)(((&v->counter))))), "0"(__old) : "memory"); break; case 2: asm volatile(
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" " " ".balign 4" " " "\n" " " ".long" " " "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "cmpxchgw %w1,%2" : "=a"(__ret) : "r"(__new), "m"(*((struct __xchg_dummy *)(((&v->counter))))), "0"(__old) : "memory"); break; case 4: asm volatile(
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" " " ".balign 4" " " "\n" " " ".long" " " "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "cmpxchgl %1,%2" : "=a"(__ret) : "r"(__new), "m"(*((struct __xchg_dummy *)(((&v->counter))))), "0"(__old) : "memory"); break; default: __cmpxchg_wrong_size(); } __ret; }); else __ret = (__typeof__(*(&v->counter)))cmpxchg_386((&v->counter), (unsigned long)(old), (unsigned long)(new), sizeof(*(&v->counter))); __ret; })
#endif
;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int atomic_xchg(atomic_t *v, int new)
{
	return ({ __typeof(*((&v->counter))) __x = ((new)); switch (sizeof(*&v->counter)) { case 1: asm volatile("xchgb %b0,%1" : "=q" (__x) : "m" (*((struct __xchg_dummy *)((&v->counter)))), "0" (__x) : "memory"); break; case 2: asm volatile("xchgw %w0,%1" : "=r" (__x) : "m" (*((struct __xchg_dummy *)((&v->counter)))), "0" (__x) : "memory"); break; case 4: asm volatile("xchgl %0,%1" : "=r" (__x) : "m" (*((struct __xchg_dummy *)((&v->counter)))), "0" (__x) : "memory"); break; default: __xchg_wrong_size(); } __x; });
}
/**
 * atomic_add_unless - add unless the number is already a given value
 * @v: pointer of type atomic_t
 * @a: the amount to add to v...
 * @u: ...unless v is equal to u.
 *
 * Atomically adds @a to @v, so long as @v was not already @u.
 * Returns non-zero if @v was not @u, and zero otherwise.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int atomic_add_unless(atomic_t *v, int a, int u)
{
	int c, old;
	c = atomic_read(v);
	for (;;) {
		if (__builtin_expect(!!(c ==(u)), 0))
			break;
		old = atomic_cmpxchg((v), c, c + (a));
		if (__builtin_expect(!!(old == c), 1))
			break;
		c = old;
	}
	return c != (u);
}
/* These are x86-specific, used by some header files */
/* Atomic operations are already serializing on x86 */
/* An 64bit atomic type */
typedef struct {
	u64 __attribute__((aligned(8))) counter;
} atomic64_t;
extern u64 atomic64_cmpxchg(atomic64_t *ptr, u64 old_val, u64 new_val);
/**
 * atomic64_xchg - xchg atomic64 variable
 * @ptr:      pointer to type atomic64_t
 * @new_val:  value to assign
 *
 * Atomically xchgs the value of @ptr to @new_val and returns
 * the old value.
 */
extern u64 atomic64_xchg(atomic64_t *ptr, u64 new_val);
/**
 * atomic64_set - set atomic64 variable
 * @ptr:      pointer to type atomic64_t
 * @new_val:  value to assign
 *
 * Atomically sets the value of @ptr to @new_val.
 */
extern void atomic64_set(atomic64_t *ptr, u64 new_val);
/**
 * atomic64_read - read atomic64 variable
 * @ptr:      pointer to type atomic64_t
 *
 * Atomically reads the value of @ptr and returns it.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 u64 atomic64_read(atomic64_t *ptr)
{
	u64 res;
	/*
	 * Note, we inline this atomic64_t primitive because
	 * it only clobbers EAX/EDX and leaves the others
	 * untouched. We also (somewhat subtly) rely on the
	 * fact that cmpxchg8b returns the current 64-bit value
	 * of the memory location we are touching:
	 */
	asm volatile(
		"mov %%ebx, %%eax\n\t"
		"mov %%ecx, %%edx\n\t"
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" " " ".balign 4" " " "\n" " " ".long" " " "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "cmpxchg8b %1\n"
			: "=&A" (res)
			: "m" (*ptr)
		);
	return res;
}
extern u64 atomic64_read(atomic64_t *ptr);
/**
 * atomic64_add_return - add and return
 * @delta: integer value to add
 * @ptr:   pointer to type atomic64_t
 *
 * Atomically adds @delta to @ptr and returns @delta + *@ptr
 */
extern u64 atomic64_add_return(u64 delta, atomic64_t *ptr);
/*
 * Other variants with different arithmetic operators:
 */
extern u64 atomic64_sub_return(u64 delta, atomic64_t *ptr);
extern u64 atomic64_inc_return(atomic64_t *ptr);
extern u64 atomic64_dec_return(atomic64_t *ptr);
/**
 * atomic64_add - add integer to atomic64 variable
 * @delta: integer value to add
 * @ptr:   pointer to type atomic64_t
 *
 * Atomically adds @delta to @ptr.
 */
extern void atomic64_add(u64 delta, atomic64_t *ptr);
/**
 * atomic64_sub - subtract the atomic64 variable
 * @delta: integer value to subtract
 * @ptr:   pointer to type atomic64_t
 *
 * Atomically subtracts @delta from @ptr.
 */
extern void atomic64_sub(u64 delta, atomic64_t *ptr);
/**
 * atomic64_sub_and_test - subtract value from variable and test result
 * @delta: integer value to subtract
 * @ptr:   pointer to type atomic64_t
 *
 * Atomically subtracts @delta from @ptr and returns
 * true if the result is zero, or false for all
 * other cases.
 */
extern int atomic64_sub_and_test(u64 delta, atomic64_t *ptr);
/**
 * atomic64_inc - increment atomic64 variable
 * @ptr: pointer to type atomic64_t
 *
 * Atomically increments @ptr by 1.
 */
extern void atomic64_inc(atomic64_t *ptr);
/**
 * atomic64_dec - decrement atomic64 variable
 * @ptr: pointer to type atomic64_t
 *
 * Atomically decrements @ptr by 1.
 */
extern void atomic64_dec(atomic64_t *ptr);
/**
 * atomic64_dec_and_test - decrement and test
 * @ptr: pointer to type atomic64_t
 *
 * Atomically decrements @ptr by 1 and
 * returns true if the result is 0, or false for all other
 * cases.
 */
extern int atomic64_dec_and_test(atomic64_t *ptr);
/**
 * atomic64_inc_and_test - increment and test
 * @ptr: pointer to type atomic64_t
 *
 * Atomically increments @ptr by 1
 * and returns true if the result is zero, or false for all
 * other cases.
 */
extern int atomic64_inc_and_test(atomic64_t *ptr);
/**
 * atomic64_add_negative - add and test if negative
 * @delta: integer value to add
 * @ptr:   pointer to type atomic64_t
 *
 * Atomically adds @delta to @ptr and returns true
 * if the result is negative, or false when
 * result is greater than or equal to zero.
 */
extern int atomic64_add_negative(u64 delta, atomic64_t *ptr);
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/asm-generic/atomic-long.h" 1
/*
 * Copyright (C) 2005 Silicon Graphics, Inc.
 *	Christoph Lameter
 *
 * Allows to provide arch independent atomic definitions without the need to
 * edit all arch specific atomic.h files.
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/types.h" 1
#line 13 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/asm-generic/atomic-long.h" 2
/*
 * Suppport for atomic_long_t
 *
 * Casts for parameters are avoided for existing atomic functions in order to
 * avoid issues with cast-as-lval under gcc 4.x and other limitations that the
 * macros of a platform may have.
 */
 typedef atomic_t atomic_long_t;
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 long atomic_long_read(atomic_long_t *l)
{
	atomic_t *v = (atomic_t *)l;
	return (long)atomic_read(v);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void atomic_long_set(atomic_long_t *l, long i)
{
	atomic_t *v = (atomic_t *)l;
	atomic_set(v, i);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void atomic_long_inc(atomic_long_t *l)
{
	atomic_t *v = (atomic_t *)l;
	atomic_inc(v);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void atomic_long_dec(atomic_long_t *l)
{
	atomic_t *v = (atomic_t *)l;
	atomic_dec(v);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void atomic_long_add(long i, atomic_long_t *l)
{
	atomic_t *v = (atomic_t *)l;
	atomic_add(i, v);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void atomic_long_sub(long i, atomic_long_t *l)
{
	atomic_t *v = (atomic_t *)l;
	atomic_sub(i, v);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int atomic_long_sub_and_test(long i, atomic_long_t *l)
{
	atomic_t *v = (atomic_t *)l;
	return atomic_sub_and_test(i, v);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int atomic_long_dec_and_test(atomic_long_t *l)
{
	atomic_t *v = (atomic_t *)l;
	return atomic_dec_and_test(v);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int atomic_long_inc_and_test(atomic_long_t *l)
{
	atomic_t *v = (atomic_t *)l;
	return atomic_inc_and_test(v);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int atomic_long_add_negative(long i, atomic_long_t *l)
{
	atomic_t *v = (atomic_t *)l;
	return atomic_add_negative(i, v);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 long atomic_long_add_return(long i, atomic_long_t *l)
{
	atomic_t *v = (atomic_t *)l;
	return (long)atomic_add_return(i, v);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 long atomic_long_sub_return(long i, atomic_long_t *l)
{
	atomic_t *v = (atomic_t *)l;
	return (long)atomic_sub_return(i, v);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 long atomic_long_inc_return(atomic_long_t *l)
{
	atomic_t *v = (atomic_t *)l;
	return (long)(atomic_add_return(1, v));
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 long atomic_long_dec_return(atomic_long_t *l)
{
	atomic_t *v = (atomic_t *)l;
	return (long)(atomic_sub_return(1, v));
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 long atomic_long_add_unless(atomic_long_t *l, long a, long u)
{
	atomic_t *v = (atomic_t *)l;
	return (long)atomic_add_unless(v, a, u);
}
#line 416 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic_32.h" 2
#line 4 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic.h" 2
#endif
#if !definedEx(CONFIG_X86_32)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic_64.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic_64.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/alternative.h" 1
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic_64.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/cmpxchg.h" 1
 #line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/cmpxchg_64.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/cmpxchg.h" 2
#line 8 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic_64.h" 2
/*
 * Atomic operations that C can't guarantee us.  Useful for
 * resource counting etc..
 */
/**
 * atomic_read - read atomic variable
 * @v: pointer of type atomic_t
 *
 * Atomically reads the value of @v.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int atomic_read(const atomic_t *v)
{
	return v->counter;
}
/**
 * atomic_set - set atomic variable
 * @v: pointer of type atomic_t
 * @i: required value
 *
 * Atomically sets the value of @v to @i.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void atomic_set(atomic_t *v, int i)
{
	v->counter = i;
}
/**
 * atomic_add - add integer to atomic variable
 * @i: integer value to add
 * @v: pointer of type atomic_t
 *
 * Atomically adds @i to @v.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void atomic_add(int i, atomic_t *v)
{
	asm volatile(
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" " " ".balign 8" " " "\n" " " ".quad" " " "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "addl %1,%0"
		     : "=m" (v->counter)
		     : "ir" (i), "m" (v->counter));
}
/**
 * atomic_sub - subtract the atomic variable
 * @i: integer value to subtract
 * @v: pointer of type atomic_t
 *
 * Atomically subtracts @i from @v.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void atomic_sub(int i, atomic_t *v)
{
	asm volatile(
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" " " ".balign 8" " " "\n" " " ".quad" " " "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "subl %1,%0"
		     : "=m" (v->counter)
		     : "ir" (i), "m" (v->counter));
}
/**
 * atomic_sub_and_test - subtract value from variable and test result
 * @i: integer value to subtract
 * @v: pointer of type atomic_t
 *
 * Atomically subtracts @i from @v and returns
 * true if the result is zero, or false for all
 * other cases.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int atomic_sub_and_test(int i, atomic_t *v)
{
	unsigned char c;
	asm volatile(
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" " " ".balign 8" " " "\n" " " ".quad" " " "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "subl %2,%0; sete %1"
		     : "=m" (v->counter), "=qm" (c)
		     : "ir" (i), "m" (v->counter) : "memory");
	return c;
}
/**
 * atomic_inc - increment atomic variable
 * @v: pointer of type atomic_t
 *
 * Atomically increments @v by 1.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void atomic_inc(atomic_t *v)
{
	asm volatile(
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" " " ".balign 8" " " "\n" " " ".quad" " " "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "incl %0"
		     : "=m" (v->counter)
		     : "m" (v->counter));
}
/**
 * atomic_dec - decrement atomic variable
 * @v: pointer of type atomic_t
 *
 * Atomically decrements @v by 1.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void atomic_dec(atomic_t *v)
{
	asm volatile(
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" " " ".balign 8" " " "\n" " " ".quad" " " "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "decl %0"
		     : "=m" (v->counter)
		     : "m" (v->counter));
}
/**
 * atomic_dec_and_test - decrement and test
 * @v: pointer of type atomic_t
 *
 * Atomically decrements @v by 1 and
 * returns true if the result is 0, or false for all other
 * cases.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int atomic_dec_and_test(atomic_t *v)
{
	unsigned char c;
	asm volatile(
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" " " ".balign 8" " " "\n" " " ".quad" " " "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "decl %0; sete %1"
		     : "=m" (v->counter), "=qm" (c)
		     : "m" (v->counter) : "memory");
	return c != 0;
}
/**
 * atomic_inc_and_test - increment and test
 * @v: pointer of type atomic_t
 *
 * Atomically increments @v by 1
 * and returns true if the result is zero, or false for all
 * other cases.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int atomic_inc_and_test(atomic_t *v)
{
	unsigned char c;
	asm volatile(
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" " " ".balign 8" " " "\n" " " ".quad" " " "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "incl %0; sete %1"
		     : "=m" (v->counter), "=qm" (c)
		     : "m" (v->counter) : "memory");
	return c != 0;
}
/**
 * atomic_add_negative - add and test if negative
 * @i: integer value to add
 * @v: pointer of type atomic_t
 *
 * Atomically adds @i to @v and returns true
 * if the result is negative, or false when
 * result is greater than or equal to zero.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int atomic_add_negative(int i, atomic_t *v)
{
	unsigned char c;
	asm volatile(
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" " " ".balign 8" " " "\n" " " ".quad" " " "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "addl %2,%0; sets %1"
		     : "=m" (v->counter), "=qm" (c)
		     : "ir" (i), "m" (v->counter) : "memory");
	return c;
}
/**
 * atomic_add_return - add and return
 * @i: integer value to add
 * @v: pointer of type atomic_t
 *
 * Atomically adds @i to @v and returns @i + @v
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int atomic_add_return(int i, atomic_t *v)
{
	int __i = i;
	asm volatile(
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" " " ".balign 8" " " "\n" " " ".quad" " " "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "xaddl %0, %1"
		     : "+r" (i), "+m" (v->counter)
		     : : "memory");
	return i + __i;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int atomic_sub_return(int i, atomic_t *v)
{
	return atomic_add_return(-i, v);
}
/* The 64-bit atomic type */
/**
 * atomic64_read - read atomic64 variable
 * @v: pointer of type atomic64_t
 *
 * Atomically reads the value of @v.
 * Doesn't imply a read memory barrier.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 long atomic64_read(const atomic64_t *v)
{
	return v->counter;
}
/**
 * atomic64_set - set atomic64 variable
 * @v: pointer to type atomic64_t
 * @i: required value
 *
 * Atomically sets the value of @v to @i.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void atomic64_set(atomic64_t *v, long i)
{
	v->counter = i;
}
/**
 * atomic64_add - add integer to atomic64 variable
 * @i: integer value to add
 * @v: pointer to type atomic64_t
 *
 * Atomically adds @i to @v.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void atomic64_add(long i, atomic64_t *v)
{
	asm volatile(
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" " " ".balign 8" " " "\n" " " ".quad" " " "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "addq %1,%0"
		     : "=m" (v->counter)
		     : "er" (i), "m" (v->counter));
}
/**
 * atomic64_sub - subtract the atomic64 variable
 * @i: integer value to subtract
 * @v: pointer to type atomic64_t
 *
 * Atomically subtracts @i from @v.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void atomic64_sub(long i, atomic64_t *v)
{
	asm volatile(
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" " " ".balign 8" " " "\n" " " ".quad" " " "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "subq %1,%0"
		     : "=m" (v->counter)
		     : "er" (i), "m" (v->counter));
}
/**
 * atomic64_sub_and_test - subtract value from variable and test result
 * @i: integer value to subtract
 * @v: pointer to type atomic64_t
 *
 * Atomically subtracts @i from @v and returns
 * true if the result is zero, or false for all
 * other cases.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int atomic64_sub_and_test(long i, atomic64_t *v)
{
	unsigned char c;
	asm volatile(
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" " " ".balign 8" " " "\n" " " ".quad" " " "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "subq %2,%0; sete %1"
		     : "=m" (v->counter), "=qm" (c)
		     : "er" (i), "m" (v->counter) : "memory");
	return c;
}
/**
 * atomic64_inc - increment atomic64 variable
 * @v: pointer to type atomic64_t
 *
 * Atomically increments @v by 1.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void atomic64_inc(atomic64_t *v)
{
	asm volatile(
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" " " ".balign 8" " " "\n" " " ".quad" " " "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "incq %0"
		     : "=m" (v->counter)
		     : "m" (v->counter));
}
/**
 * atomic64_dec - decrement atomic64 variable
 * @v: pointer to type atomic64_t
 *
 * Atomically decrements @v by 1.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void atomic64_dec(atomic64_t *v)
{
	asm volatile(
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" " " ".balign 8" " " "\n" " " ".quad" " " "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "decq %0"
		     : "=m" (v->counter)
		     : "m" (v->counter));
}
/**
 * atomic64_dec_and_test - decrement and test
 * @v: pointer to type atomic64_t
 *
 * Atomically decrements @v by 1 and
 * returns true if the result is 0, or false for all other
 * cases.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int atomic64_dec_and_test(atomic64_t *v)
{
	unsigned char c;
	asm volatile(
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" " " ".balign 8" " " "\n" " " ".quad" " " "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "decq %0; sete %1"
		     : "=m" (v->counter), "=qm" (c)
		     : "m" (v->counter) : "memory");
	return c != 0;
}
/**
 * atomic64_inc_and_test - increment and test
 * @v: pointer to type atomic64_t
 *
 * Atomically increments @v by 1
 * and returns true if the result is zero, or false for all
 * other cases.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int atomic64_inc_and_test(atomic64_t *v)
{
	unsigned char c;
	asm volatile(
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" " " ".balign 8" " " "\n" " " ".quad" " " "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "incq %0; sete %1"
		     : "=m" (v->counter), "=qm" (c)
		     : "m" (v->counter) : "memory");
	return c != 0;
}
/**
 * atomic64_add_negative - add and test if negative
 * @i: integer value to add
 * @v: pointer to type atomic64_t
 *
 * Atomically adds @i to @v and returns true
 * if the result is negative, or false when
 * result is greater than or equal to zero.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int atomic64_add_negative(long i, atomic64_t *v)
{
	unsigned char c;
	asm volatile(
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" " " ".balign 8" " " "\n" " " ".quad" " " "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "addq %2,%0; sets %1"
		     : "=m" (v->counter), "=qm" (c)
		     : "er" (i), "m" (v->counter) : "memory");
	return c;
}
/**
 * atomic64_add_return - add and return
 * @i: integer value to add
 * @v: pointer to type atomic64_t
 *
 * Atomically adds @i to @v and returns @i + @v
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 long atomic64_add_return(long i, atomic64_t *v)
{
	long __i = i;
	asm volatile(
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" " " ".balign 8" " " "\n" " " ".quad" " " "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "xaddq %0, %1;"
		     : "+r" (i), "+m" (v->counter)
		     : : "memory");
	return i + __i;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 long atomic64_sub_return(long i, atomic64_t *v)
{
	return atomic64_add_return(-i, v);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 long atomic64_cmpxchg(atomic64_t *v, long old, long new)
{
	return ({ __typeof__(*(((&v->counter)))) __ret; __typeof__(*(((&v->counter)))) __old = (((old))); __typeof__(*(((&v->counter)))) __new = (((new))); switch ((sizeof(*&v->counter))) { case 1: asm volatile(
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" " " ".balign 8" " " "\n" " " ".quad" " " "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "cmpxchgb %b1,%2" : "=a"(__ret) : "q"(__new), "m"(*((volatile long *)(((&v->counter))))), "0"(__old) : "memory"); break; case 2: asm volatile(
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" " " ".balign 8" " " "\n" " " ".quad" " " "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "cmpxchgw %w1,%2" : "=a"(__ret) : "r"(__new), "m"(*((volatile long *)(((&v->counter))))), "0"(__old) : "memory"); break; case 4: asm volatile(
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" " " ".balign 8" " " "\n" " " ".quad" " " "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "cmpxchgl %k1,%2" : "=a"(__ret) : "r"(__new), "m"(*((volatile long *)(((&v->counter))))), "0"(__old) : "memory"); break; case 8: asm volatile(
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" " " ".balign 8" " " "\n" " " ".quad" " " "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "cmpxchgq %1,%2" : "=a"(__ret) : "r"(__new), "m"(*((volatile long *)(((&v->counter))))), "0"(__old) : "memory"); break; default: __cmpxchg_wrong_size(); } __ret; });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 long atomic64_xchg(atomic64_t *v, long new)
{
	return ({ __typeof(*((&v->counter))) __x = ((new)); switch (sizeof(*&v->counter)) { case 1: asm volatile("xchgb %b0,%1" : "=q" (__x) : "m" (*((volatile long *)((&v->counter)))), "0" (__x) : "memory"); break; case 2: asm volatile("xchgw %w0,%1" : "=r" (__x) : "m" (*((volatile long *)((&v->counter)))), "0" (__x) : "memory"); break; case 4: asm volatile("xchgl %k0,%1" : "=r" (__x) : "m" (*((volatile long *)((&v->counter)))), "0" (__x) : "memory"); break; case 8: asm volatile("xchgq %0,%1" : "=r" (__x) : "m" (*((volatile long *)((&v->counter)))), "0" (__x) : "memory"); break; default: __xchg_wrong_size(); } __x; });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 long atomic_cmpxchg(atomic_t *v, int old, int new)
{
	return ({ __typeof__(*(((&v->counter)))) __ret; __typeof__(*(((&v->counter)))) __old = (((old))); __typeof__(*(((&v->counter)))) __new = (((new))); switch ((sizeof(*&v->counter))) { case 1: asm volatile(
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" " " ".balign 8" " " "\n" " " ".quad" " " "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "cmpxchgb %b1,%2" : "=a"(__ret) : "q"(__new), "m"(*((volatile long *)(((&v->counter))))), "0"(__old) : "memory"); break; case 2: asm volatile(
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" " " ".balign 8" " " "\n" " " ".quad" " " "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "cmpxchgw %w1,%2" : "=a"(__ret) : "r"(__new), "m"(*((volatile long *)(((&v->counter))))), "0"(__old) : "memory"); break; case 4: asm volatile(
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" " " ".balign 8" " " "\n" " " ".quad" " " "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "cmpxchgl %k1,%2" : "=a"(__ret) : "r"(__new), "m"(*((volatile long *)(((&v->counter))))), "0"(__old) : "memory"); break; case 8: asm volatile(
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" " " ".balign 8" " " "\n" " " ".quad" " " "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "cmpxchgq %1,%2" : "=a"(__ret) : "r"(__new), "m"(*((volatile long *)(((&v->counter))))), "0"(__old) : "memory"); break; default: __cmpxchg_wrong_size(); } __ret; });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 long atomic_xchg(atomic_t *v, int new)
{
	return ({ __typeof(*((&v->counter))) __x = ((new)); switch (sizeof(*&v->counter)) { case 1: asm volatile("xchgb %b0,%1" : "=q" (__x) : "m" (*((volatile long *)((&v->counter)))), "0" (__x) : "memory"); break; case 2: asm volatile("xchgw %w0,%1" : "=r" (__x) : "m" (*((volatile long *)((&v->counter)))), "0" (__x) : "memory"); break; case 4: asm volatile("xchgl %k0,%1" : "=r" (__x) : "m" (*((volatile long *)((&v->counter)))), "0" (__x) : "memory"); break; case 8: asm volatile("xchgq %0,%1" : "=r" (__x) : "m" (*((volatile long *)((&v->counter)))), "0" (__x) : "memory"); break; default: __xchg_wrong_size(); } __x; });
}
/**
 * atomic_add_unless - add unless the number is a given value
 * @v: pointer of type atomic_t
 * @a: the amount to add to v...
 * @u: ...unless v is equal to u.
 *
 * Atomically adds @a to @v, so long as it was not @u.
 * Returns non-zero if @v was not @u, and zero otherwise.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int atomic_add_unless(atomic_t *v, int a, int u)
{
	int c, old;
	c = atomic_read(v);
	for (;;) {
		if (__builtin_expect(!!(c ==(u)), 0))
			break;
		old = atomic_cmpxchg((v), c, c + (a));
		if (__builtin_expect(!!(old == c), 1))
			break;
		c = old;
	}
	return c != (u);
}
/**
 * atomic64_add_unless - add unless the number is a given value
 * @v: pointer of type atomic64_t
 * @a: the amount to add to v...
 * @u: ...unless v is equal to u.
 *
 * Atomically adds @a to @v, so long as it was not @u.
 * Returns non-zero if @v was not @u, and zero otherwise.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int atomic64_add_unless(atomic64_t *v, long a, long u)
{
	long c, old;
	c = atomic64_read(v);
	for (;;) {
		if (__builtin_expect(!!(c ==(u)), 0))
			break;
		old = atomic64_cmpxchg((v), c, c + (a));
		if (__builtin_expect(!!(old == c), 1))
			break;
		c = old;
	}
	return c != (u);
}
/**
 * atomic_inc_short - increment of a short integer
 * @v: pointer to type int
 *
 * Atomically adds 1 to @v
 * Returns the new value of @u
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 short int atomic_inc_short(short int *v)
{
	asm(
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" " " ".balign 8" " " "\n" " " ".quad" " " "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "addw $1, %0" : "+m" (*v));
	return *v;
}
/**
 * atomic_or_long - OR of two long integers
 * @v1: pointer to type unsigned long
 * @v2: pointer to type unsigned long
 *
 * Atomically ORs @v1 and @v2
 * Returns the result of the OR
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void atomic_or_long(unsigned long *v1, unsigned long v2)
{
	asm(
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" " " ".balign 8" " " "\n" " " ".quad" " " "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "orq %1, %0" : "+m" (*v1) : "r" (v2));
}
/* These are x86-specific, used by some header files */
/* Atomic operations are already serializing on x86 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/asm-generic/atomic-long.h" 1
/*
 * Copyright (C) 2005 Silicon Graphics, Inc.
 *	Christoph Lameter
 *
 * Allows to provide arch independent atomic definitions without the need to
 * edit all arch specific atomic.h files.
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/types.h" 1
#line 13 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/asm-generic/atomic-long.h" 2
/*
 * Suppport for atomic_long_t
 *
 * Casts for parameters are avoided for existing atomic functions in order to
 * avoid issues with cast-as-lval under gcc 4.x and other limitations that the
 * macros of a platform may have.
 */
typedef atomic64_t atomic_long_t;
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 long atomic_long_read(atomic_long_t *l)
{
	atomic64_t *v = (atomic64_t *)l;
	return (long)atomic64_read(v);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void atomic_long_set(atomic_long_t *l, long i)
{
	atomic64_t *v = (atomic64_t *)l;
	atomic64_set(v, i);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void atomic_long_inc(atomic_long_t *l)
{
	atomic64_t *v = (atomic64_t *)l;
	atomic64_inc(v);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void atomic_long_dec(atomic_long_t *l)
{
	atomic64_t *v = (atomic64_t *)l;
	atomic64_dec(v);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void atomic_long_add(long i, atomic_long_t *l)
{
	atomic64_t *v = (atomic64_t *)l;
	atomic64_add(i, v);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void atomic_long_sub(long i, atomic_long_t *l)
{
	atomic64_t *v = (atomic64_t *)l;
	atomic64_sub(i, v);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int atomic_long_sub_and_test(long i, atomic_long_t *l)
{
	atomic64_t *v = (atomic64_t *)l;
	return atomic64_sub_and_test(i, v);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int atomic_long_dec_and_test(atomic_long_t *l)
{
	atomic64_t *v = (atomic64_t *)l;
	return atomic64_dec_and_test(v);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int atomic_long_inc_and_test(atomic_long_t *l)
{
	atomic64_t *v = (atomic64_t *)l;
	return atomic64_inc_and_test(v);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int atomic_long_add_negative(long i, atomic_long_t *l)
{
	atomic64_t *v = (atomic64_t *)l;
	return atomic64_add_negative(i, v);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 long atomic_long_add_return(long i, atomic_long_t *l)
{
	atomic64_t *v = (atomic64_t *)l;
	return (long)atomic64_add_return(i, v);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 long atomic_long_sub_return(long i, atomic_long_t *l)
{
	atomic64_t *v = (atomic64_t *)l;
	return (long)atomic64_sub_return(i, v);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 long atomic_long_inc_return(atomic_long_t *l)
{
	atomic64_t *v = (atomic64_t *)l;
	return (long)(atomic64_add_return(1, (v)));
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 long atomic_long_dec_return(atomic_long_t *l)
{
	atomic64_t *v = (atomic64_t *)l;
	return (long)(atomic64_sub_return(1, (v)));
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 long atomic_long_add_unless(atomic_long_t *l, long a, long u)
{
	atomic64_t *v = (atomic64_t *)l;
	return (long)atomic64_add_unless(v, a, u);
}
#line 486 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic_64.h" 2
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic.h" 2
#endif
#line 26 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/thread_info.h" 2
struct thread_info {
	struct task_struct	*task;		/* main task structure */
	struct exec_domain	*exec_domain;	/* execution domain */
	__u32			flags;		/* low level flags */
	__u32			status;		/* thread synchronous flags */
	__u32			cpu;		/* current CPU */
	int			preempt_count;	/* 0 => preemptable,
						   <0 => BUG */
	mm_segment_t		addr_limit;
	struct restart_block    restart_block;
	void 		*sysenter_return;
#if definedEx(CONFIG_X86_32)
	unsigned long           previous_esp;   /* ESP of the previous stack in
						   case of nested (IRQ) stacks
						*/
	__u8			supervisor_stack[0];
#endif
	int			uaccess_err;
};
/*
 * thread information flags
 * - these are process state flags that various assembly files
 *   may need to access
 * - pending work-to-be-done flags are in LSW
 * - other flags in MSW
 * Warning: layout of LSW is hardcoded in entry.S
 */
/* work to do in syscall_trace_enter() */
/* work to do in syscall_trace_leave() */
/* work to do on interrupt/exception return */
/* work to do on any return to user space */
/* Only used for 64 bit */
/* flags to check in __switch_to() */
/* thread information allocation */
#if definedEx(CONFIG_DEBUG_STACK_USAGE)
#endif
#if !definedEx(CONFIG_DEBUG_STACK_USAGE)
#endif
#if definedEx(CONFIG_X86_32)
/*
 * macros/functions for gaining access to the thread information structure
 *
 * preempt_count needs to be 1 initially, until the scheduler is functional.
 */
/* how to get the current stack pointer from C */
register unsigned long current_stack_pointer asm("esp") __attribute__((__used__));
/* how to get the thread information struct from C */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 struct thread_info *current_thread_info(void)
{
	return (struct thread_info *)
		(current_stack_pointer & ~((((1UL) << 12) << 
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_4KSTACKS) || definedEx(CONFIG_X86_64)
1
#endif
#if !definedEx(CONFIG_X86_64) && definedEx(CONFIG_4KSTACKS)
0
#endif
) - 1));
}
#endif
#if !definedEx(CONFIG_X86_32)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/percpu.h" 1
#line 207 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/thread_info.h" 2
/*
 * macros/functions for gaining access to the thread information structure
 * preempt_count needs to be 1 initially, until the scheduler is functional.
 */
#if definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(".discard"), unused)) char __pcpu_scope_kernel_stack; extern __attribute__((section(
#if definedEx(CONFIG_SMP)
".data.percpu"
#endif
#if !definedEx(CONFIG_SMP)
".data"
#endif
 "")))  __typeof__(unsigned long) per_cpu__kernel_stack
#endif
#if !definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(
#if definedEx(CONFIG_SMP)
".data.percpu"
#endif
#if !definedEx(CONFIG_SMP)
".data"
#endif
 "")))  __typeof__(unsigned long) per_cpu__kernel_stack
#endif
;
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 struct thread_info *current_thread_info(void)
{
	struct thread_info *ti;
	ti = (void *)(({ typeof(per_cpu__kernel_stack) pfo_ret__; switch (sizeof(per_cpu__kernel_stack)) { case 1: asm("mov" "b "
#if definedEx(CONFIG_SMP)
"%%""#if definedEx(CONFIG_X86_64)\ngs\n#elif !definedEx(CONFIG_X86_64)\nfs\n#endif\n"":%P" "1"
#endif
#if !definedEx(CONFIG_SMP)
"%P" "1"
#endif
",%0" : "=q" (pfo_ret__) : "p"(&per_cpu__kernel_stack)); break; case 2: asm("mov" "w "
#if definedEx(CONFIG_SMP)
"%%""#if definedEx(CONFIG_X86_64)\ngs\n#elif !definedEx(CONFIG_X86_64)\nfs\n#endif\n"":%P" "1"
#endif
#if !definedEx(CONFIG_SMP)
"%P" "1"
#endif
",%0" : "=r" (pfo_ret__) : "p"(&per_cpu__kernel_stack)); break; case 4: asm("mov" "l "
#if definedEx(CONFIG_SMP)
"%%""#if definedEx(CONFIG_X86_64)\ngs\n#elif !definedEx(CONFIG_X86_64)\nfs\n#endif\n"":%P" "1"
#endif
#if !definedEx(CONFIG_SMP)
"%P" "1"
#endif
",%0" : "=r" (pfo_ret__) : "p"(&per_cpu__kernel_stack)); break; case 8: asm("mov" "q "
#if definedEx(CONFIG_SMP)
"%%""#if definedEx(CONFIG_X86_64)\ngs\n#elif !definedEx(CONFIG_X86_64)\nfs\n#endif\n"":%P" "1"
#endif
#if !definedEx(CONFIG_SMP)
"%P" "1"
#endif
",%0" : "=r" (pfo_ret__) : "p"(&per_cpu__kernel_stack)); break; default: __bad_percpu_size(); } pfo_ret__; }) +
		      (5*8) - (((1UL) << 12) << 
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_4KSTACKS) || definedEx(CONFIG_X86_64)
1
#endif
#if !definedEx(CONFIG_X86_64) && definedEx(CONFIG_4KSTACKS)
0
#endif
));
	return ti;
}
#endif
/*
 * Thread-synchronous status.
 *
 * This is different from the flags in that nobody else
 * ever touches our thread-synchronous status, so we don't
 * have to worry about atomic accesses.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void set_restore_sigmask(void)
{
	struct thread_info *ti = current_thread_info();
	ti->status |= 0x0008;
	set_bit(2, (unsigned long *)&ti->flags);
}
extern void arch_task_cache_init(void);
extern void free_thread_info(struct thread_info *ti);
extern int arch_dup_task_struct(struct task_struct *dst, struct task_struct *src);
#line 58 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/thread_info.h" 2
/*
 * flag set/clear/test wrappers
 * - pass TIF_xxxx constants to these functions
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void set_ti_thread_flag(struct thread_info *ti, int flag)
{
	set_bit(flag, (unsigned long *)&ti->flags);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void clear_ti_thread_flag(struct thread_info *ti, int flag)
{
	clear_bit(flag, (unsigned long *)&ti->flags);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int test_and_set_ti_thread_flag(struct thread_info *ti, int flag)
{
	return test_and_set_bit(flag, (unsigned long *)&ti->flags);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int test_and_clear_ti_thread_flag(struct thread_info *ti, int flag)
{
	return test_and_clear_bit(flag, (unsigned long *)&ti->flags);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int test_ti_thread_flag(struct thread_info *ti, int flag)
{
	return (__builtin_constant_p((flag)) ? constant_test_bit((flag), ((unsigned long *)&ti->flags)) : variable_test_bit((flag), ((unsigned long *)&ti->flags)));
}
#line 11 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/preempt.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/linkage.h" 1
#line 12 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/preempt.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/list.h" 1
#line 13 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/preempt.h" 2
#if !definedEx(CONFIG_DEBUG_PREEMPT) && definedEx(CONFIG_PREEMPT_TRACER) || definedEx(CONFIG_DEBUG_PREEMPT)
  extern void add_preempt_count(int val);
  extern void sub_preempt_count(int val);
#endif
#if !definedEx(CONFIG_DEBUG_PREEMPT) && !definedEx(CONFIG_PREEMPT_TRACER)
#endif
#if definedEx(CONFIG_PREEMPT)
#if !definedEx(CONFIG_X86_32)
#endif
#if definedEx(CONFIG_X86_32)
 __attribute__((regparm(0)))
#endif
 void preempt_schedule(void);
/* For debugging and tracer internals only! */
/* preempt_check_resched is OK to trace */
#endif
#if !definedEx(CONFIG_PREEMPT)
#endif
#if definedEx(CONFIG_PREEMPT_NOTIFIERS)
struct preempt_notifier;
/**
 * preempt_ops - notifiers called when a task is preempted and rescheduled
 * @sched_in: we're about to be rescheduled:
 *    notifier: struct preempt_notifier for the task being scheduled
 *    cpu:  cpu we're scheduled on
 * @sched_out: we've just been preempted
 *    notifier: struct preempt_notifier for the task being preempted
 *    next: the task that's kicking us out
 *
 * Please note that sched_in and out are called under different
 * contexts.  sched_out is called with rq lock held and irq disabled
 * while sched_in is called without rq lock and irq enabled.  This
 * difference is intentional and depended upon by its users.
 */
struct preempt_ops {
	void (*sched_in)(struct preempt_notifier *notifier, int cpu);
	void (*sched_out)(struct preempt_notifier *notifier,
			  struct task_struct *next);
};
/**
 * preempt_notifier - key for installing preemption notifiers
 * @link: internal use
 * @ops: defines the notifier functions to be called
 *
 * Usually used in conjunction with container_of().
 */
struct preempt_notifier {
	struct hlist_node link;
	struct preempt_ops *ops;
};
void preempt_notifier_register(struct preempt_notifier *notifier);
void preempt_notifier_unregister(struct preempt_notifier *notifier);
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void preempt_notifier_init(struct preempt_notifier *notifier,
				     struct preempt_ops *ops)
{
	INIT_HLIST_NODE(&notifier->link);
	notifier->ops = ops;
}
#endif
#line 52 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/spinlock.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/linkage.h" 1
#line 53 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/spinlock.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/compiler.h" 1
#line 54 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/spinlock.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/thread_info.h" 1
/* thread_info.h: common low-level thread information accessors
 *
 * Copyright (C) 2002  David Howells (dhowells@redhat.com)
 * - Incorporating suggestions made by Linus Torvalds
 */
#line 55 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/spinlock.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/kernel.h" 1
#line 56 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/spinlock.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/stringify.h" 1
#line 57 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/spinlock.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/bottom_half.h" 1
extern void local_bh_disable(void);
extern void _local_bh_enable(void);
extern void local_bh_enable(void);
extern void local_bh_enable_ip(unsigned long ip);
#line 58 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/spinlock.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/system.h" 1
#line 60 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/spinlock.h" 2
/*
 * Must define these before including other files, inline functions need them
 */
/*
 * Pull the arch_spinlock_t and arch_rwlock_t definitions:
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/spinlock_types.h" 1
/*
 * include/linux/spinlock_types.h - generic spinlock type definitions
 *                                  and initializers
 *
 * portions Copyright 2005, Red Hat, Inc., Ingo Molnar
 * Released under the General Public License (GPL).
 */
#if definedEx(CONFIG_SMP)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/spinlock_types.h" 1
typedef struct arch_spinlock {
	unsigned int slock;
} arch_spinlock_t;
typedef struct {
	unsigned int lock;
} arch_rwlock_t;
#line 15 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/spinlock_types.h" 2
#endif
#if !definedEx(CONFIG_SMP)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/spinlock_types_up.h" 1
/*
 * include/linux/spinlock_types_up.h - spinlock type definitions for UP
 *
 * portions Copyright 2005, Red Hat, Inc., Ingo Molnar
 * Released under the General Public License (GPL).
 */
#if definedEx(CONFIG_DEBUG_SPINLOCK)
typedef struct {
	volatile unsigned int slock;
} arch_spinlock_t;
#endif
#if !definedEx(CONFIG_DEBUG_SPINLOCK)
typedef struct { } arch_spinlock_t;
#endif
typedef struct {
	/* no debug version on UP */
} arch_rwlock_t;
#line 17 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/spinlock_types.h" 2
#endif
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/lockdep.h" 1
/*
 * Runtime locking correctness validator
 *
 *  Copyright (C) 2006,2007 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>
 *  Copyright (C) 2007 Red Hat, Inc., Peter Zijlstra <pzijlstr@redhat.com>
 *
 * see Documentation/lockdep-design.txt for more details.
 */
struct task_struct;
struct lockdep_map;
#if definedEx(CONFIG_LOCKDEP)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/linkage.h" 1
#line 19 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/lockdep.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/list.h" 1
#line 20 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/lockdep.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/debug_locks.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/kernel.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/debug_locks.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic.h" 1
#if definedEx(CONFIG_X86_32)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic_32.h" 1
#line 4 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic.h" 2
#endif
#if !definedEx(CONFIG_X86_32)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic_64.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic.h" 2
#endif
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/debug_locks.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/system.h" 1
#line 8 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/debug_locks.h" 2
struct task_struct;
extern int debug_locks;
extern int debug_locks_silent;
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int __debug_locks_off(void)
{
	return 
#if definedEx(CONFIG_X86_32)
({ __typeof(*((&debug_locks))) __x = ((0)); switch (sizeof(*&debug_locks)) { case 1: asm volatile("xchgb %b0,%1" : "=q" (__x) : "m" (*((struct __xchg_dummy *)((&debug_locks)))), "0" (__x) : "memory"); break; case 2: asm volatile("xchgw %w0,%1" : "=r" (__x) : "m" (*((struct __xchg_dummy *)((&debug_locks)))), "0" (__x) : "memory"); break; case 4: asm volatile("xchgl %0,%1" : "=r" (__x) : "m" (*((struct __xchg_dummy *)((&debug_locks)))), "0" (__x) : "memory"); break; default: __xchg_wrong_size(); } __x; })
#endif
#if !definedEx(CONFIG_X86_32)
({ __typeof(*((&debug_locks))) __x = ((0)); switch (sizeof(*&debug_locks)) { case 1: asm volatile("xchgb %b0,%1" : "=q" (__x) : "m" (*((volatile long *)((&debug_locks)))), "0" (__x) : "memory"); break; case 2: asm volatile("xchgw %w0,%1" : "=r" (__x) : "m" (*((volatile long *)((&debug_locks)))), "0" (__x) : "memory"); break; case 4: asm volatile("xchgl %k0,%1" : "=r" (__x) : "m" (*((volatile long *)((&debug_locks)))), "0" (__x) : "memory"); break; case 8: asm volatile("xchgq %0,%1" : "=r" (__x) : "m" (*((volatile long *)((&debug_locks)))), "0" (__x) : "memory"); break; default: __xchg_wrong_size(); } __x; })
#endif
;
}
/*
 * Generic 'turn off all lock debugging' function:
 */
extern int debug_locks_off(void);
#if definedEx(CONFIG_SMP)
#endif
#if !definedEx(CONFIG_SMP)
#endif
#if definedEx(CONFIG_DEBUG_LOCKING_API_SELFTESTS)
  extern void locking_selftest(void);
#endif
#if !definedEx(CONFIG_DEBUG_LOCKING_API_SELFTESTS)
#endif
struct task_struct;
extern void debug_show_all_locks(void);
extern void __debug_show_held_locks(struct task_struct *task);
extern void debug_show_held_locks(struct task_struct *task);
extern void debug_check_no_locks_freed(const void *from, unsigned long len);
extern void debug_check_no_locks_held(struct task_struct *task);
#line 21 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/lockdep.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/stacktrace.h" 1
struct task_struct;
#if definedEx(CONFIG_STACKTRACE)
struct task_struct;
struct stack_trace {
	unsigned int nr_entries, max_entries;
	unsigned long *entries;
	int skip;	/* input argument: How many entries to skip */
};
extern void save_stack_trace(struct stack_trace *trace);
extern void save_stack_trace_bp(struct stack_trace *trace, unsigned long bp);
extern void save_stack_trace_tsk(struct task_struct *tsk,
				struct stack_trace *trace);
extern void print_stack_trace(struct stack_trace *trace, int spaces);
extern void save_stack_trace_user(struct stack_trace *trace);
#endif
#if !definedEx(CONFIG_STACKTRACE)
#endif
#line 22 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/lockdep.h" 2
/*
 * We'd rather not expose kernel/lockdep_states.h this wide, but we do need
 * the total number of states... :-(
 */
/*
 * Lock-classes are keyed via unique addresses, by embedding the
 * lockclass-key into the kernel (or module) .data section. (For
 * static locks we use the lock address itself as the key.)
 */
struct lockdep_subclass_key {
	char __one_byte;
} __attribute__ ((__packed__));
struct lock_class_key {
	struct lockdep_subclass_key	subkeys[8UL];
};
/*
 * The lock-class itself:
 */
struct lock_class {
	/*
	 * class-hash:
	 */
	struct list_head		hash_entry;
	/*
	 * global list of all lock-classes:
	 */
	struct list_head		lock_entry;
	struct lockdep_subclass_key	*key;
	unsigned int			subclass;
	unsigned int			dep_gen_id;
	/*
	 * IRQ/softirq usage tracking bits:
	 */
	unsigned long			usage_mask;
	struct stack_trace		usage_traces[(1+3*4)];
	/*
	 * These fields represent a directed graph of lock dependencies,
	 * to every node we attach a list of "forward" and a list of
	 * "backward" graph nodes.
	 */
	struct list_head		locks_after, locks_before;
	/*
	 * Generation counter, when doing certain classes of graph walking,
	 * to ensure that we check one node only once:
	 */
	unsigned int			version;
	/*
	 * Statistics counter:
	 */
	unsigned long			ops;
	const char			*name;
	int				name_version;
#if definedEx(CONFIG_LOCK_STAT)
	unsigned long			contention_point[4];
	unsigned long			contending_point[4];
#endif
};
#if definedEx(CONFIG_LOCK_STAT)
struct lock_time {
	s64				min;
	s64				max;
	s64				total;
	unsigned long			nr;
};
enum bounce_type {
	bounce_acquired_write,
	bounce_acquired_read,
	bounce_contended_write,
	bounce_contended_read,
	nr_bounce_types,
	bounce_acquired = bounce_acquired_write,
	bounce_contended = bounce_contended_write,
};
struct lock_class_stats {
	unsigned long			contention_point[4];
	unsigned long			contending_point[4];
	struct lock_time		read_waittime;
	struct lock_time		write_waittime;
	struct lock_time		read_holdtime;
	struct lock_time		write_holdtime;
	unsigned long			bounces[nr_bounce_types];
};
struct lock_class_stats lock_stats(struct lock_class *class);
void clear_lock_stats(struct lock_class *class);
#endif
/*
 * Map the lock object (the lock instance) to the lock-class object.
 * This is embedded into specific lock instances:
 */
struct lockdep_map {
	struct lock_class_key		*key;
	struct lock_class		*class_cache;
	const char			*name;
#if definedEx(CONFIG_LOCK_STAT)
	int				cpu;
	unsigned long			ip;
#endif
};
/*
 * Every lock has a list of other locks that were taken after it.
 * We only grow the list, never remove from it:
 */
struct lock_list {
	struct list_head		entry;
	struct lock_class		*class;
	struct stack_trace		trace;
	int				distance;
	/*
	 * The parent field is used to implement breadth-first search, and the
	 * bit 0 is reused to indicate if the lock has been accessed in BFS.
	 */
	struct lock_list		*parent;
};
/*
 * We record lock dependency chains, so that we can cache them:
 */
struct lock_chain {
	u8				irq_context;
	u8				depth;
	u16				base;
	struct list_head		entry;
	u64				chain_key;
};
/*
 * Subtract one because we offset hlock->class_idx by 1 in order
 * to make 0 mean no class. This avoids overflowing the class_idx
 * bitfield and hitting the BUG in hlock_class().
 */
struct held_lock {
	/*
	 * One-way hash of the dependency chain up to this point. We
	 * hash the hashes step by step as the dependency chain grows.
	 *
	 * We use it for dependency-caching and we skip detection
	 * passes and dependency-updates if there is a cache-hit, so
	 * it is absolutely critical for 100% coverage of the validator
	 * to have a unique key value for every unique dependency path
	 * that can occur in the system, to make a unique hash value
	 * as likely as possible - hence the 64-bit width.
	 *
	 * The task struct holds the current hash value (initialized
	 * with zero), here we store the previous hash value:
	 */
	u64				prev_chain_key;
	unsigned long			acquire_ip;
	struct lockdep_map		*instance;
	struct lockdep_map		*nest_lock;
#if definedEx(CONFIG_LOCK_STAT)
	u64 				waittime_stamp;
	u64				holdtime_stamp;
#endif
	unsigned int			class_idx:13;
	/*
	 * The lock-stack is unified in that the lock chains of interrupt
	 * contexts nest ontop of process context chains, but we 'separate'
	 * the hashes by starting with 0 if we cross into an interrupt
	 * context, and we also keep do not add cross-context lock
	 * dependencies - the lock usage graph walking covers that area
	 * anyway, and we'd just unnecessarily increase the number of
	 * dependencies otherwise. [Note: hardirq and softirq contexts
	 * are separated from each other too.]
	 *
	 * The following field is used to detect when we cross into an
	 * interrupt context:
	 */
	unsigned int irq_context:2; /* bit 0 - soft, bit 1 - hard */
	unsigned int trylock:1;						/* 16 bits */
	unsigned int read:2;        /* see lock_acquire() comment */
	unsigned int check:2;       /* see lock_acquire() comment */
	unsigned int hardirqs_off:1;
	unsigned int references:11;					/* 32 bits */
};
/*
 * Initialization, self-test and debugging-output methods:
 */
extern void lockdep_init(void);
extern void lockdep_info(void);
extern void lockdep_reset(void);
extern void lockdep_reset_lock(struct lockdep_map *lock);
extern void lockdep_free_key_range(void *start, unsigned long size);
extern void lockdep_sys_exit(void);
extern void lockdep_off(void);
extern void lockdep_on(void);
/*
 * These methods are used by specific locking variants (spinlocks,
 * rwlocks, mutexes and rwsems) to pass init/acquire/release events
 * to lockdep:
 */
extern void lockdep_init_map(struct lockdep_map *lock, const char *name,
			     struct lock_class_key *key, int subclass);
/*
 * To initialize a lockdep_map statically use this macro.
 * Note that _name must not be NULL.
 */
/*
 * Reinitialize a lock key - for cases where there is special locking or
 * special initialization of locks so that the validator gets the scope
 * of dependencies wrong: they are either too broad (they need a class-split)
 * or they are too narrow (they suffer from a false class-split):
 */
/*
 * Compare locking classes
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int lockdep_match_key(struct lockdep_map *lock,
				    struct lock_class_key *key)
{
	return lock->key == key;
}
/*
 * Acquire a lock.
 *
 * Values for "read":
 *
 *   0: exclusive (write) acquire
 *   1: read-acquire (no recursion allowed)
 *   2: read-acquire with same-instance recursion allowed
 *
 * Values for check:
 *
 *   0: disabled
 *   1: simple checks (freeing, held-at-exit-time, etc.)
 *   2: full validation
 */
extern void lock_acquire(struct lockdep_map *lock, unsigned int subclass,
			 int trylock, int read, int check,
			 struct lockdep_map *nest_lock, unsigned long ip);
extern void lock_release(struct lockdep_map *lock, int nested,
			 unsigned long ip);
extern int lock_is_held(struct lockdep_map *lock);
extern void lock_set_class(struct lockdep_map *lock, const char *name,
			   struct lock_class_key *key, unsigned int subclass,
			   unsigned long ip);
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void lock_set_subclass(struct lockdep_map *lock,
		unsigned int subclass, unsigned long ip)
{
	lock_set_class(lock, lock->name, lock->key, subclass, ip);
}
extern void lockdep_set_current_reclaim_state(gfp_t gfp_mask);
extern void lockdep_clear_current_reclaim_state(void);
extern void lockdep_trace_alloc(gfp_t mask);
#endif
#if !definedEx(CONFIG_LOCKDEP)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void lockdep_off(void)
{
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void lockdep_on(void)
{
}
/*
 * We don't define lockdep_match_class() and lockdep_match_key() for !LOCKDEP
 * case since the result is not well defined and the caller should rather
 * #ifdef the call himself.
 */
/*
 * The class key takes no space if lockdep is disabled:
 */
struct lock_class_key { };
#endif
#if definedEx(CONFIG_LOCK_STAT)
extern void lock_contended(struct lockdep_map *lock, unsigned long ip);
extern void lock_acquired(struct lockdep_map *lock, unsigned long ip);
#endif
#if !definedEx(CONFIG_LOCK_STAT)
#endif
#if definedEx(CONFIG_LOCKDEP)
/*
 * On lockdep we dont want the hand-coded irq-enable of
 * _raw_*_lock_flags() code, because lockdep assumes
 * that interrupts are not re-enabled during lock-acquire:
 */
#endif
#if !definedEx(CONFIG_LOCKDEP)
#endif
extern void early_init_irq_lock_class(void);
#if definedEx(CONFIG_TRACE_IRQFLAGS)
extern void early_boot_irqs_off(void);
extern void early_boot_irqs_on(void);
extern void print_irqtrace_events(struct task_struct *curr);
#endif
#if !definedEx(CONFIG_TRACE_IRQFLAGS)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void early_boot_irqs_off(void)
{
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void early_boot_irqs_on(void)
{
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void print_irqtrace_events(struct task_struct *curr)
{
}
#endif
/*
 * For trivial one-depth nesting of a lock-class, the following
 * global define can be used. (Subsystems with multiple levels
 * of nesting should define their own lock-nesting subclasses.)
 */
/*
 * Map the dependency ops to NOP or to real lockdep ops, depending
 * on the per lock-class debug mode:
 */
#if definedEx(CONFIG_DEBUG_LOCK_ALLOC)
#if definedEx(CONFIG_PROVE_LOCKING)
#endif
#if !definedEx(CONFIG_PROVE_LOCKING)
#endif
#endif
#if !definedEx(CONFIG_DEBUG_LOCK_ALLOC)
#endif
#if definedEx(CONFIG_DEBUG_LOCK_ALLOC)
#if definedEx(CONFIG_PROVE_LOCKING)
#endif
#if !definedEx(CONFIG_PROVE_LOCKING)
#endif
#endif
#if !definedEx(CONFIG_DEBUG_LOCK_ALLOC)
#endif
#if definedEx(CONFIG_DEBUG_LOCK_ALLOC)
#if definedEx(CONFIG_PROVE_LOCKING)
#endif
#if !definedEx(CONFIG_PROVE_LOCKING)
#endif
#endif
#if !definedEx(CONFIG_DEBUG_LOCK_ALLOC)
#endif
#if definedEx(CONFIG_DEBUG_LOCK_ALLOC)
#if definedEx(CONFIG_PROVE_LOCKING)
#endif
#if !definedEx(CONFIG_PROVE_LOCKING)
#endif
#endif
#if !definedEx(CONFIG_DEBUG_LOCK_ALLOC)
#endif
#if definedEx(CONFIG_DEBUG_LOCK_ALLOC)
#if definedEx(CONFIG_PROVE_LOCKING)
#endif
#if !definedEx(CONFIG_PROVE_LOCKING)
#endif
#endif
#if !definedEx(CONFIG_DEBUG_LOCK_ALLOC)
#endif
#if definedEx(CONFIG_PROVE_LOCKING)
#endif
#if !definedEx(CONFIG_PROVE_LOCKING)
#endif
#line 20 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/spinlock_types.h" 2
typedef struct raw_spinlock {
	arch_spinlock_t raw_lock;
#if definedEx(CONFIG_DEBUG_SPINLOCK)
	unsigned int magic, owner_cpu;
	void *owner;
#endif
#if definedEx(CONFIG_DEBUG_LOCK_ALLOC)
	struct lockdep_map dep_map;
#endif
} raw_spinlock_t;
#if definedEx(CONFIG_DEBUG_LOCK_ALLOC)
#endif
#if !definedEx(CONFIG_DEBUG_LOCK_ALLOC)
#endif
#if definedEx(CONFIG_DEBUG_SPINLOCK)
#endif
#if !definedEx(CONFIG_DEBUG_SPINLOCK)
#endif
typedef struct spinlock {
	union {
		struct raw_spinlock rlock;
#if definedEx(CONFIG_DEBUG_LOCK_ALLOC)
		struct {
			u8 __padding[(__builtin_offsetof(struct raw_spinlock,dep_map))];
			struct lockdep_map dep_map;
		};
#endif
	};
} spinlock_t;
/*
 * SPIN_LOCK_UNLOCKED defeats lockdep state tracking and is hence
 * deprecated.
 * Please use DEFINE_SPINLOCK() or __SPIN_LOCK_UNLOCKED() as
 * appropriate.
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/rwlock_types.h" 1
/*
 * include/linux/rwlock_types.h - generic rwlock type definitions
 *				  and initializers
 *
 * portions Copyright 2005, Red Hat, Inc., Ingo Molnar
 * Released under the General Public License (GPL).
 */
typedef struct {
	arch_rwlock_t raw_lock;
#if definedEx(CONFIG_DEBUG_SPINLOCK)
	unsigned int magic, owner_cpu;
	void *owner;
#endif
#if definedEx(CONFIG_DEBUG_LOCK_ALLOC)
	struct lockdep_map dep_map;
#endif
} rwlock_t;
#if definedEx(CONFIG_DEBUG_LOCK_ALLOC)
#endif
#if !definedEx(CONFIG_DEBUG_LOCK_ALLOC)
#endif
#if definedEx(CONFIG_DEBUG_SPINLOCK)
#endif
#if !definedEx(CONFIG_DEBUG_SPINLOCK)
#endif
/*
 * RW_LOCK_UNLOCKED defeat lockdep state tracking and is hence
 * deprecated.
 *
 * Please use DEFINE_RWLOCK() or __RW_LOCK_UNLOCKED() as appropriate.
 */
#line 96 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/spinlock_types.h" 2
#line 82 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/spinlock.h" 2
/*
 * Pull the arch_spin*() functions/declarations (UP-nondebug doesnt need them):
 */
#if definedEx(CONFIG_SMP)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/spinlock.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic.h" 1
#if definedEx(CONFIG_X86_32)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic_32.h" 1
#line 4 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic.h" 2
#endif
#if !definedEx(CONFIG_X86_32)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic_64.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic.h" 2
#endif
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/spinlock.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/rwlock.h" 1
/* Actual code is in asm/spinlock.h or in arch/x86/lib/rwlock.S */
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/spinlock.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/page.h" 1
#line 8 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/spinlock.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/processor.h" 1
#line 9 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/spinlock.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/compiler.h" 1
#line 10 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/spinlock.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h" 1
#if !definedEx(CONFIG_PARAVIRT)
/* Various instructions on x86 need to be replaced for
 * para-virtualization: those hooks are defined here. */
#endif
#line 11 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/spinlock.h" 2
/*
 * Your basic SMP spinlocks, allowing only a single CPU anywhere
 *
 * Simple spin lock operations.  There are two variants, one clears IRQ's
 * on the local processor, one does not.
 *
 * These are fair FIFO ticket locks, which are currently limited to 256
 * CPUs.
 *
 * (the type definitions are in asm/spinlock_types.h)
 */
#if definedEx(CONFIG_X86_32)
#endif
#if !definedEx(CONFIG_X86_32)
#endif
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_X86_PPRO_FENCE) && definedEx(CONFIG_X86_OOSTORE) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_X86_PPRO_FENCE)
/*
 * On PPro SMP or if we are using OOSTORE, we use a locked operation to unlock
 * (PPro errata 66, 92)
 */
#endif
#if !definedEx(CONFIG_X86_32) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_X86_PPRO_FENCE) && !definedEx(CONFIG_X86_OOSTORE)
#endif
/*
 * Ticket locks are conceptually two parts, one indicating the current head of
 * the queue, and the other indicating the current tail. The lock is acquired
 * by atomically noting the tail and incrementing it by one (thus adding
 * ourself to the queue and noting our position), then waiting until the head
 * becomes equal to the the initial value of the tail.
 *
 * We use an xadd covering *both* parts of the lock, to increment the tail and
 * also load the position of the head, which takes care of memory ordering
 * issues and should be optimal for the uncontended case. Note the tail must be
 * in the high part, because a wide xadd increment of the low part would carry
 * up and contaminate the high part.
 *
 * With fewer than 2^8 possible CPUs, we can use x86's partial registers to
 * save some instructions and make the code more elegant. There really isn't
 * much between them in performance though, especially as locks are out of line.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __attribute__((always_inline)) void __ticket_spin_lock(arch_spinlock_t *lock)
{
	short inc = 0x0100;
	asm volatile (
		".section .smp_locks,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 "661f\n" ".previous\n" "661:\n\tlock; " "xaddw %w0, %1\n"
		"1:\t"
		"cmpb %h0, %b0\n\t"
		"je 2f\n\t"
		"rep ; nop\n\t"
		"movb %1, %b0\n\t"
		/* don't need lfence here, because loads are in-order */
		"jmp 1b\n"
		"2:"
		: "+Q" (inc), "+m" (lock->slock)
		:
		: "memory", "cc");
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __attribute__((always_inline)) int __ticket_spin_trylock(arch_spinlock_t *lock)
{
	int tmp, new;
	asm volatile("movzwl %2, %0\n\t"
		     "cmpb %h0,%b0\n\t"
		     "leal 0x100(%" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_SMP)
"k"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_SMP)
"q"
#endif
 "0), %1\n\t"
		     "jne 1f\n\t"
		     ".section .smp_locks,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 "661f\n" ".previous\n" "661:\n\tlock; " "cmpxchgw %w1,%2\n\t"
		     "1:"
		     "sete %b1\n\t"
		     "movzbl %b1,%0\n\t"
		     : "=&a" (tmp), "=&q" (new), "+m" (lock->slock)
		     :
		     : "memory", "cc");
	return tmp;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __attribute__((always_inline)) void __ticket_spin_unlock(arch_spinlock_t *lock)
{
	asm volatile(
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_X86_PPRO_FENCE) && definedEx(CONFIG_X86_OOSTORE) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_PPRO_FENCE)
".section .smp_locks,\"a\"\n" " " ".balign 4" " " "\n" " " ".long" " " "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_SMP) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_X86_PPRO_FENCE) && !definedEx(CONFIG_X86_OOSTORE)
#endif
 "incb %0"
		     : "+m" (lock->slock)
		     :
		     : "memory", "cc");
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int __ticket_spin_is_locked(arch_spinlock_t *lock)
{
	int tmp = (*(volatile typeof(lock->slock) *)&(lock->slock));
	return !!(((tmp >> 8) ^ tmp) & ((1 << 8) - 1));
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int __ticket_spin_is_contended(arch_spinlock_t *lock)
{
	int tmp = (*(volatile typeof(lock->slock) *)&(lock->slock));
	return (((tmp >> 8) - tmp) & ((1 << 8) - 1)) > 1;
}
#if !definedEx(CONFIG_PARAVIRT_SPINLOCKS)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int arch_spin_is_locked(arch_spinlock_t *lock)
{
	return __ticket_spin_is_locked(lock);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int arch_spin_is_contended(arch_spinlock_t *lock)
{
	return __ticket_spin_is_contended(lock);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __attribute__((always_inline)) void arch_spin_lock(arch_spinlock_t *lock)
{
	__ticket_spin_lock(lock);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __attribute__((always_inline)) int arch_spin_trylock(arch_spinlock_t *lock)
{
	return __ticket_spin_trylock(lock);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __attribute__((always_inline)) void arch_spin_unlock(arch_spinlock_t *lock)
{
	__ticket_spin_unlock(lock);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __attribute__((always_inline)) void arch_spin_lock_flags(arch_spinlock_t *lock,
						  unsigned long flags)
{
	arch_spin_lock(lock);
}
#endif
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void arch_spin_unlock_wait(arch_spinlock_t *lock)
{
	while (arch_spin_is_locked(lock))
		cpu_relax();
}
/*
 * Read-write spinlocks, allowing multiple readers
 * but only one writer.
 *
 * NOTE! it is quite common to have readers in interrupts
 * but no interrupt writers. For those circumstances we
 * can "mix" irq-safe locks - any writer needs to get a
 * irq-safe write-lock, but readers can get non-irqsafe
 * read-locks.
 *
 * On x86, we implement read-write locks as a 32-bit counter
 * with the high bit (sign) being the "contended" bit.
 */
/**
 * read_can_lock - would read_trylock() succeed?
 * @lock: the rwlock in question.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int arch_read_can_lock(arch_rwlock_t *lock)
{
	return (int)(lock)->lock > 0;
}
/**
 * write_can_lock - would write_trylock() succeed?
 * @lock: the rwlock in question.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int arch_write_can_lock(arch_rwlock_t *lock)
{
	return (lock)->lock == 0x01000000;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void arch_read_lock(arch_rwlock_t *rw)
{
	asm volatile(".section .smp_locks,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 "661f\n" ".previous\n" "661:\n\tlock; " " subl $1,(%0)\n\t"
		     "jns 1f\n"
		     "call __read_lock_failed\n\t"
		     "1:\n"
		     ::
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_SMP)
"a"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_SMP)
"D"
#endif
 (rw) : "memory");
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void arch_write_lock(arch_rwlock_t *rw)
{
	asm volatile(".section .smp_locks,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 "661f\n" ".previous\n" "661:\n\tlock; " " subl %1,(%0)\n\t"
		     "jz 1f\n"
		     "call __write_lock_failed\n\t"
		     "1:\n"
		     ::
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_SMP)
"a"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_SMP)
"D"
#endif
 (rw), "i" (0x01000000) : "memory");
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int arch_read_trylock(arch_rwlock_t *lock)
{
	atomic_t *count = (atomic_t *)lock;
	if ((atomic_sub_return(1, count)) >= 0)
		return 1;
	atomic_inc(count);
	return 0;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int arch_write_trylock(arch_rwlock_t *lock)
{
	atomic_t *count = (atomic_t *)lock;
	if (atomic_sub_and_test(0x01000000, count))
		return 1;
	atomic_add(0x01000000, count);
	return 0;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void arch_read_unlock(arch_rwlock_t *rw)
{
	asm volatile(".section .smp_locks,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 "661f\n" ".previous\n" "661:\n\tlock; " "incl %0" :"+m" (rw->lock) : : "memory");
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void arch_write_unlock(arch_rwlock_t *rw)
{
	asm volatile(".section .smp_locks,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 "661f\n" ".previous\n" "661:\n\tlock; " "addl %1, %0"
		     : "+m" (rw->lock) : "i" (0x01000000) : "memory");
}
/* The {read|write|spin}_lock() on x86 are full memory barriers. */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void smp_mb__after_lock(void) { }
#line 88 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/spinlock.h" 2
#endif
#if !definedEx(CONFIG_SMP)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/spinlock_up.h" 1
/*
 * include/linux/spinlock_up.h - UP-debug version of spinlocks.
 *
 * portions Copyright 2005, Red Hat, Inc., Ingo Molnar
 * Released under the General Public License (GPL).
 *
 * In the debug case, 1 means unlocked, 0 means locked. (the values
 * are inverted, to catch initialization bugs)
 *
 * No atomicity anywhere, we are on UP.
 */
#if definedEx(CONFIG_DEBUG_SPINLOCK)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void arch_spin_lock(arch_spinlock_t *lock)
{
	lock->slock = 0;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void
arch_spin_lock_flags(arch_spinlock_t *lock, unsigned long flags)
{
	do { ({ unsigned long __dummy; typeof(flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); do { (flags) = __raw_local_irq_save(); } while (0); 
#if !definedEx(CONFIG_TRACE_IRQFLAGS)
do { } while (0)
#endif
#if definedEx(CONFIG_TRACE_IRQFLAGS)
trace_hardirqs_off()
#endif
; } while (0);
	lock->slock = 0;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int arch_spin_trylock(arch_spinlock_t *lock)
{
	char oldval = lock->slock;
	lock->slock = 0;
	return oldval > 0;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void arch_spin_unlock(arch_spinlock_t *lock)
{
	lock->slock = 1;
}
/*
 * Read-write spinlocks. No debug version.
 */
#endif
#if !definedEx(CONFIG_DEBUG_SPINLOCK)
/* for sched.c and kernel_lock.c: */
#endif
#line 90 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/spinlock.h" 2
#endif
#if definedEx(CONFIG_DEBUG_SPINLOCK)
  extern void __raw_spin_lock_init(raw_spinlock_t *lock, const char *name,
				   struct lock_class_key *key);
#endif
#if !definedEx(CONFIG_DEBUG_SPINLOCK)
#endif
 #if !definedEx(CONFIG_SMP) || definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_SPINLOCKS) || definedEx(CONFIG_SMP) && definedEx(CONFIG_PARAVIRT)
#endif
#if definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_SPINLOCKS)
#endif
/* The lock does not imply full memory barrier. */
#if !definedEx(CONFIG_SMP)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void smp_mb__after_lock(void) { __asm__ __volatile__("": : :"memory"); }
#endif
/**
 * raw_spin_unlock_wait - wait until the spinlock gets unlocked
 * @lock: the spinlock in question.
 */
#if definedEx(CONFIG_DEBUG_SPINLOCK)
 extern void do_raw_spin_lock(raw_spinlock_t *lock);
 extern int do_raw_spin_trylock(raw_spinlock_t *lock);
 extern void do_raw_spin_unlock(raw_spinlock_t *lock);
#endif
#if !definedEx(CONFIG_DEBUG_SPINLOCK)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void do_raw_spin_lock(raw_spinlock_t *lock)
{
#if !definedEx(CONFIG_SMP) && !definedEx(CONFIG_DEBUG_SPINLOCK)
do { (void)(&lock->raw_lock); } while (0)
#endif
#if !definedEx(CONFIG_SMP) && definedEx(CONFIG_DEBUG_SPINLOCK) || definedEx(CONFIG_SMP)
arch_spin_lock(&lock->raw_lock)
#endif
;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void
do_raw_spin_lock_flags(raw_spinlock_t *lock, unsigned long *flags)
{
#if !definedEx(CONFIG_SMP) && !definedEx(CONFIG_DEBUG_SPINLOCK)
do { (void)(&lock->raw_lock); } while (0)
#endif
#if !definedEx(CONFIG_SMP) && definedEx(CONFIG_DEBUG_SPINLOCK) || definedEx(CONFIG_SMP)
arch_spin_lock_flags(&lock->raw_lock, *flags)
#endif
;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int do_raw_spin_trylock(raw_spinlock_t *lock)
{
	return 
#if !definedEx(CONFIG_SMP) && !definedEx(CONFIG_DEBUG_SPINLOCK)
({ (void)(&(lock)->raw_lock); 1; })
#endif
#if !definedEx(CONFIG_SMP) && definedEx(CONFIG_DEBUG_SPINLOCK) || definedEx(CONFIG_SMP)
arch_spin_trylock(&(lock)->raw_lock)
#endif
;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void do_raw_spin_unlock(raw_spinlock_t *lock)
{
#if !definedEx(CONFIG_SMP) && !definedEx(CONFIG_DEBUG_SPINLOCK)
do { (void)(&lock->raw_lock); } while (0)
#endif
#if !definedEx(CONFIG_SMP) && definedEx(CONFIG_DEBUG_SPINLOCK) || definedEx(CONFIG_SMP)
arch_spin_unlock(&lock->raw_lock)
#endif
;
}
#endif
/*
 * Define the various spin_lock methods.  Note we define these
 * regardless of whether CONFIG_SMP or CONFIG_PREEMPT are set. The
 * various methods are defined as nops in the case they are not
 * required.
 */
#if definedEx(CONFIG_DEBUG_LOCK_ALLOC)
#endif
#if !definedEx(CONFIG_DEBUG_LOCK_ALLOC)
#endif
#if !definedEx(CONFIG_SMP) && definedEx(CONFIG_DEBUG_SPINLOCK) || definedEx(CONFIG_SMP)
#if definedEx(CONFIG_DEBUG_LOCK_ALLOC)
#endif
#if !definedEx(CONFIG_DEBUG_LOCK_ALLOC)
#endif
#endif
#if !definedEx(CONFIG_SMP) && !definedEx(CONFIG_DEBUG_SPINLOCK)
#endif
/**
 * raw_spin_can_lock - would raw_spin_trylock() succeed?
 * @lock: the spinlock in question.
 */
/* Include rwlock functions */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/rwlock.h" 1
/*
 * rwlock related methods
 *
 * split out from spinlock.h
 *
 * portions Copyright 2005, Red Hat, Inc., Ingo Molnar
 * Released under the General Public License (GPL).
 */
#if definedEx(CONFIG_DEBUG_SPINLOCK)
  extern void __rwlock_init(rwlock_t *lock, const char *name,
			    struct lock_class_key *key);
#endif
#if !definedEx(CONFIG_DEBUG_SPINLOCK)
#endif
#if definedEx(CONFIG_DEBUG_SPINLOCK)
 extern void do_raw_read_lock(rwlock_t *lock);
 extern int do_raw_read_trylock(rwlock_t *lock);
 extern void do_raw_read_unlock(rwlock_t *lock);
 extern void do_raw_write_lock(rwlock_t *lock);
 extern int do_raw_write_trylock(rwlock_t *lock);
 extern void do_raw_write_unlock(rwlock_t *lock);
#endif
#if !definedEx(CONFIG_DEBUG_SPINLOCK)
#endif
/*
 * Define the various rw_lock methods.  Note we define these
 * regardless of whether CONFIG_SMP or CONFIG_PREEMPT are set. The various
 * methods are defined as nops in the case they are not required.
 */
#if !definedEx(CONFIG_SMP) && definedEx(CONFIG_DEBUG_SPINLOCK) || definedEx(CONFIG_SMP)
#endif
#if !definedEx(CONFIG_SMP) && !definedEx(CONFIG_DEBUG_SPINLOCK)
#endif
#line 255 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/spinlock.h" 2
/*
 * Pull the _spin_*()/_read_*()/_write_*() functions/declarations:
 */
#if !definedEx(CONFIG_SMP) && definedEx(CONFIG_DEBUG_SPINLOCK) || definedEx(CONFIG_SMP)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/spinlock_api_smp.h" 1
/*
 * include/linux/spinlock_api_smp.h
 *
 * spinlock API declarations on SMP (and debug)
 * (implemented in kernel/spinlock.c)
 *
 * portions Copyright 2005, Red Hat, Inc., Ingo Molnar
 * Released under the General Public License (GPL).
 */
int in_lock_functions(unsigned long addr);
void __attribute__((section(".spinlock.text"))) _raw_spin_lock(raw_spinlock_t *lock)		;
void __attribute__((section(".spinlock.text"))) _raw_spin_lock_nested(raw_spinlock_t *lock, int subclass)
								;
void __attribute__((section(".spinlock.text")))
_raw_spin_lock_nest_lock(raw_spinlock_t *lock, struct lockdep_map *map)
								;
void __attribute__((section(".spinlock.text"))) _raw_spin_lock_bh(raw_spinlock_t *lock)		;
void __attribute__((section(".spinlock.text"))) _raw_spin_lock_irq(raw_spinlock_t *lock)
								;
unsigned long __attribute__((section(".spinlock.text"))) _raw_spin_lock_irqsave(raw_spinlock_t *lock)
								;
unsigned long __attribute__((section(".spinlock.text")))
_raw_spin_lock_irqsave_nested(raw_spinlock_t *lock, int subclass)
								;
int __attribute__((section(".spinlock.text"))) _raw_spin_trylock(raw_spinlock_t *lock);
int __attribute__((section(".spinlock.text"))) _raw_spin_trylock_bh(raw_spinlock_t *lock);
void __attribute__((section(".spinlock.text"))) _raw_spin_unlock(raw_spinlock_t *lock)		;
void __attribute__((section(".spinlock.text"))) _raw_spin_unlock_bh(raw_spinlock_t *lock)	;
void __attribute__((section(".spinlock.text"))) _raw_spin_unlock_irq(raw_spinlock_t *lock)	;
void __attribute__((section(".spinlock.text")))
_raw_spin_unlock_irqrestore(raw_spinlock_t *lock, unsigned long flags)
								;
#if definedEx(CONFIG_INLINE_SPIN_UNLOCK)
#endif
#if definedEx(CONFIG_INLINE_SPIN_UNLOCK_IRQ)
#endif
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int __raw_spin_trylock(raw_spinlock_t *lock)
{
#if definedEx(CONFIG_PREEMPT)
do { 
#if !definedEx(CONFIG_DEBUG_PREEMPT) && !definedEx(CONFIG_PREEMPT_TRACER)
do { (current_thread_info()->preempt_count) += (1); } while (0)
#endif
#if !definedEx(CONFIG_DEBUG_PREEMPT) && definedEx(CONFIG_PREEMPT_TRACER) || definedEx(CONFIG_DEBUG_PREEMPT)
add_preempt_count(1)
#endif
; __asm__ __volatile__("": : :"memory"); } while (0)
#endif
#if !definedEx(CONFIG_PREEMPT)
do { } while (0)
#endif
;
	if (do_raw_spin_trylock(lock)) {
#if definedEx(CONFIG_PROVE_LOCKING) && definedEx(CONFIG_DEBUG_LOCK_ALLOC)
lock_acquire(&lock->dep_map, 0, 1, 0, 2, ((void *)0), (unsigned long)__builtin_return_address(0))
#endif
#if !definedEx(CONFIG_PROVE_LOCKING) && definedEx(CONFIG_DEBUG_LOCK_ALLOC)
lock_acquire(&lock->dep_map, 0, 1, 0, 1, ((void *)0), (unsigned long)__builtin_return_address(0))
#endif
#if !definedEx(CONFIG_DEBUG_LOCK_ALLOC)
do { } while (0)
#endif
;
		return 1;
	}
#if definedEx(CONFIG_PREEMPT)
do { do { __asm__ __volatile__("": : :"memory"); 
#if !definedEx(CONFIG_DEBUG_PREEMPT) && !definedEx(CONFIG_PREEMPT_TRACER)
do { (current_thread_info()->preempt_count) -= (1); } while (0)
#endif
#if !definedEx(CONFIG_DEBUG_PREEMPT) && definedEx(CONFIG_PREEMPT_TRACER) || definedEx(CONFIG_DEBUG_PREEMPT)
sub_preempt_count(1)
#endif
; } while (0); __asm__ __volatile__("": : :"memory"); do { if (__builtin_expect(!!(test_ti_thread_flag(current_thread_info(), 3)), 0)) preempt_schedule(); } while (0); } while (0)
#endif
#if !definedEx(CONFIG_PREEMPT)
do { } while (0)
#endif
;
	return 0;
}
/*
 * If lockdep is enabled then we use the non-preemption spin-ops
 * even on CONFIG_PREEMPT, because lockdep assumes that interrupts are
 * not re-enabled during lock-acquire (which the preempt-spin-ops do):
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long __raw_spin_lock_irqsave(raw_spinlock_t *lock)
{
	unsigned long flags;
	do { ({ unsigned long __dummy; typeof(flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); do { (flags) = __raw_local_irq_save(); } while (0); 
#if !definedEx(CONFIG_TRACE_IRQFLAGS)
do { } while (0)
#endif
#if definedEx(CONFIG_TRACE_IRQFLAGS)
trace_hardirqs_off()
#endif
; } while (0);
#if definedEx(CONFIG_PREEMPT)
do { 
#if !definedEx(CONFIG_DEBUG_PREEMPT) && !definedEx(CONFIG_PREEMPT_TRACER)
do { (current_thread_info()->preempt_count) += (1); } while (0)
#endif
#if !definedEx(CONFIG_DEBUG_PREEMPT) && definedEx(CONFIG_PREEMPT_TRACER) || definedEx(CONFIG_DEBUG_PREEMPT)
add_preempt_count(1)
#endif
; __asm__ __volatile__("": : :"memory"); } while (0)
#endif
#if !definedEx(CONFIG_PREEMPT)
do { } while (0)
#endif
;
#if definedEx(CONFIG_PROVE_LOCKING) && definedEx(CONFIG_DEBUG_LOCK_ALLOC)
lock_acquire(&lock->dep_map, 0, 0, 0, 2, ((void *)0), (unsigned long)__builtin_return_address(0))
#endif
#if !definedEx(CONFIG_PROVE_LOCKING) && definedEx(CONFIG_DEBUG_LOCK_ALLOC)
lock_acquire(&lock->dep_map, 0, 0, 0, 1, ((void *)0), (unsigned long)__builtin_return_address(0))
#endif
#if !definedEx(CONFIG_DEBUG_LOCK_ALLOC)
do { } while (0)
#endif
;
	/*
	 * On lockdep we dont want the hand-coded irq-enable of
	 * do_raw_spin_lock_flags() code, because lockdep assumes
	 * that interrupts are not re-enabled during lock-acquire:
	 */
#if definedEx(CONFIG_LOCKDEP)
#if definedEx(CONFIG_LOCK_STAT)
do { if (!do_raw_spin_trylock(lock)) { lock_contended(&(lock)->dep_map, (unsigned long)__builtin_return_address(0)); do_raw_spin_lock(lock); } lock_acquired(&(lock)->dep_map, (unsigned long)__builtin_return_address(0)); } while (0)
#endif
#if !definedEx(CONFIG_LOCK_STAT)
do_raw_spin_lock(lock)
#endif
;
#endif
#if !definedEx(CONFIG_LOCKDEP)
#if definedEx(CONFIG_DEBUG_SPINLOCK)
do_raw_spin_lock(lock)
#endif
#if !definedEx(CONFIG_DEBUG_SPINLOCK)
do_raw_spin_lock_flags(lock, &flags)
#endif
;
#endif
	return flags;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __raw_spin_lock_irq(raw_spinlock_t *lock)
{
	do { raw_local_irq_disable(); 
#if !definedEx(CONFIG_TRACE_IRQFLAGS)
do { } while (0)
#endif
#if definedEx(CONFIG_TRACE_IRQFLAGS)
trace_hardirqs_off()
#endif
; } while (0);
#if definedEx(CONFIG_PREEMPT)
do { 
#if !definedEx(CONFIG_DEBUG_PREEMPT) && !definedEx(CONFIG_PREEMPT_TRACER)
do { (current_thread_info()->preempt_count) += (1); } while (0)
#endif
#if !definedEx(CONFIG_DEBUG_PREEMPT) && definedEx(CONFIG_PREEMPT_TRACER) || definedEx(CONFIG_DEBUG_PREEMPT)
add_preempt_count(1)
#endif
; __asm__ __volatile__("": : :"memory"); } while (0)
#endif
#if !definedEx(CONFIG_PREEMPT)
do { } while (0)
#endif
;
#if definedEx(CONFIG_PROVE_LOCKING) && definedEx(CONFIG_DEBUG_LOCK_ALLOC)
lock_acquire(&lock->dep_map, 0, 0, 0, 2, ((void *)0), (unsigned long)__builtin_return_address(0))
#endif
#if !definedEx(CONFIG_PROVE_LOCKING) && definedEx(CONFIG_DEBUG_LOCK_ALLOC)
lock_acquire(&lock->dep_map, 0, 0, 0, 1, ((void *)0), (unsigned long)__builtin_return_address(0))
#endif
#if !definedEx(CONFIG_DEBUG_LOCK_ALLOC)
do { } while (0)
#endif
;
#if definedEx(CONFIG_LOCK_STAT)
do { if (!do_raw_spin_trylock(lock)) { lock_contended(&(lock)->dep_map, (unsigned long)__builtin_return_address(0)); do_raw_spin_lock(lock); } lock_acquired(&(lock)->dep_map, (unsigned long)__builtin_return_address(0)); } while (0)
#endif
#if !definedEx(CONFIG_LOCK_STAT)
do_raw_spin_lock(lock)
#endif
;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __raw_spin_lock_bh(raw_spinlock_t *lock)
{
	local_bh_disable();
#if definedEx(CONFIG_PREEMPT)
do { 
#if !definedEx(CONFIG_DEBUG_PREEMPT) && !definedEx(CONFIG_PREEMPT_TRACER)
do { (current_thread_info()->preempt_count) += (1); } while (0)
#endif
#if !definedEx(CONFIG_DEBUG_PREEMPT) && definedEx(CONFIG_PREEMPT_TRACER) || definedEx(CONFIG_DEBUG_PREEMPT)
add_preempt_count(1)
#endif
; __asm__ __volatile__("": : :"memory"); } while (0)
#endif
#if !definedEx(CONFIG_PREEMPT)
do { } while (0)
#endif
;
#if definedEx(CONFIG_PROVE_LOCKING) && definedEx(CONFIG_DEBUG_LOCK_ALLOC)
lock_acquire(&lock->dep_map, 0, 0, 0, 2, ((void *)0), (unsigned long)__builtin_return_address(0))
#endif
#if !definedEx(CONFIG_PROVE_LOCKING) && definedEx(CONFIG_DEBUG_LOCK_ALLOC)
lock_acquire(&lock->dep_map, 0, 0, 0, 1, ((void *)0), (unsigned long)__builtin_return_address(0))
#endif
#if !definedEx(CONFIG_DEBUG_LOCK_ALLOC)
do { } while (0)
#endif
;
#if definedEx(CONFIG_LOCK_STAT)
do { if (!do_raw_spin_trylock(lock)) { lock_contended(&(lock)->dep_map, (unsigned long)__builtin_return_address(0)); do_raw_spin_lock(lock); } lock_acquired(&(lock)->dep_map, (unsigned long)__builtin_return_address(0)); } while (0)
#endif
#if !definedEx(CONFIG_LOCK_STAT)
do_raw_spin_lock(lock)
#endif
;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __raw_spin_lock(raw_spinlock_t *lock)
{
#if definedEx(CONFIG_PREEMPT)
do { 
#if !definedEx(CONFIG_DEBUG_PREEMPT) && !definedEx(CONFIG_PREEMPT_TRACER)
do { (current_thread_info()->preempt_count) += (1); } while (0)
#endif
#if !definedEx(CONFIG_DEBUG_PREEMPT) && definedEx(CONFIG_PREEMPT_TRACER) || definedEx(CONFIG_DEBUG_PREEMPT)
add_preempt_count(1)
#endif
; __asm__ __volatile__("": : :"memory"); } while (0)
#endif
#if !definedEx(CONFIG_PREEMPT)
do { } while (0)
#endif
;
#if definedEx(CONFIG_PROVE_LOCKING) && definedEx(CONFIG_DEBUG_LOCK_ALLOC)
lock_acquire(&lock->dep_map, 0, 0, 0, 2, ((void *)0), (unsigned long)__builtin_return_address(0))
#endif
#if !definedEx(CONFIG_PROVE_LOCKING) && definedEx(CONFIG_DEBUG_LOCK_ALLOC)
lock_acquire(&lock->dep_map, 0, 0, 0, 1, ((void *)0), (unsigned long)__builtin_return_address(0))
#endif
#if !definedEx(CONFIG_DEBUG_LOCK_ALLOC)
do { } while (0)
#endif
;
#if definedEx(CONFIG_LOCK_STAT)
do { if (!do_raw_spin_trylock(lock)) { lock_contended(&(lock)->dep_map, (unsigned long)__builtin_return_address(0)); do_raw_spin_lock(lock); } lock_acquired(&(lock)->dep_map, (unsigned long)__builtin_return_address(0)); } while (0)
#endif
#if !definedEx(CONFIG_LOCK_STAT)
do_raw_spin_lock(lock)
#endif
;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __raw_spin_unlock(raw_spinlock_t *lock)
{
#if definedEx(CONFIG_DEBUG_LOCK_ALLOC)
lock_release(&lock->dep_map, 1, (unsigned long)__builtin_return_address(0))
#endif
#if !definedEx(CONFIG_DEBUG_LOCK_ALLOC)
do { } while (0)
#endif
;
	do_raw_spin_unlock(lock);
#if definedEx(CONFIG_PREEMPT)
do { do { __asm__ __volatile__("": : :"memory"); 
#if !definedEx(CONFIG_DEBUG_PREEMPT) && !definedEx(CONFIG_PREEMPT_TRACER)
do { (current_thread_info()->preempt_count) -= (1); } while (0)
#endif
#if !definedEx(CONFIG_DEBUG_PREEMPT) && definedEx(CONFIG_PREEMPT_TRACER) || definedEx(CONFIG_DEBUG_PREEMPT)
sub_preempt_count(1)
#endif
; } while (0); __asm__ __volatile__("": : :"memory"); do { if (__builtin_expect(!!(test_ti_thread_flag(current_thread_info(), 3)), 0)) preempt_schedule(); } while (0); } while (0)
#endif
#if !definedEx(CONFIG_PREEMPT)
do { } while (0)
#endif
;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __raw_spin_unlock_irqrestore(raw_spinlock_t *lock,
					    unsigned long flags)
{
#if definedEx(CONFIG_DEBUG_LOCK_ALLOC)
lock_release(&lock->dep_map, 1, (unsigned long)__builtin_return_address(0))
#endif
#if !definedEx(CONFIG_DEBUG_LOCK_ALLOC)
do { } while (0)
#endif
;
	do_raw_spin_unlock(lock);
	do { ({ unsigned long __dummy; typeof(flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); if (raw_irqs_disabled_flags(flags)) { raw_local_irq_restore(flags); 
#if !definedEx(CONFIG_TRACE_IRQFLAGS)
do { } while (0)
#endif
#if definedEx(CONFIG_TRACE_IRQFLAGS)
trace_hardirqs_off()
#endif
; } else { 
#if !definedEx(CONFIG_TRACE_IRQFLAGS)
do { } while (0)
#endif
#if definedEx(CONFIG_TRACE_IRQFLAGS)
trace_hardirqs_on()
#endif
; raw_local_irq_restore(flags); } } while (0);
#if definedEx(CONFIG_PREEMPT)
do { do { __asm__ __volatile__("": : :"memory"); 
#if !definedEx(CONFIG_DEBUG_PREEMPT) && !definedEx(CONFIG_PREEMPT_TRACER)
do { (current_thread_info()->preempt_count) -= (1); } while (0)
#endif
#if !definedEx(CONFIG_DEBUG_PREEMPT) && definedEx(CONFIG_PREEMPT_TRACER) || definedEx(CONFIG_DEBUG_PREEMPT)
sub_preempt_count(1)
#endif
; } while (0); __asm__ __volatile__("": : :"memory"); do { if (__builtin_expect(!!(test_ti_thread_flag(current_thread_info(), 3)), 0)) preempt_schedule(); } while (0); } while (0)
#endif
#if !definedEx(CONFIG_PREEMPT)
do { } while (0)
#endif
;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __raw_spin_unlock_irq(raw_spinlock_t *lock)
{
#if definedEx(CONFIG_DEBUG_LOCK_ALLOC)
lock_release(&lock->dep_map, 1, (unsigned long)__builtin_return_address(0))
#endif
#if !definedEx(CONFIG_DEBUG_LOCK_ALLOC)
do { } while (0)
#endif
;
	do_raw_spin_unlock(lock);
	do { 
#if !definedEx(CONFIG_TRACE_IRQFLAGS)
do { } while (0)
#endif
#if definedEx(CONFIG_TRACE_IRQFLAGS)
trace_hardirqs_on()
#endif
; raw_local_irq_enable(); } while (0);
#if definedEx(CONFIG_PREEMPT)
do { do { __asm__ __volatile__("": : :"memory"); 
#if !definedEx(CONFIG_DEBUG_PREEMPT) && !definedEx(CONFIG_PREEMPT_TRACER)
do { (current_thread_info()->preempt_count) -= (1); } while (0)
#endif
#if !definedEx(CONFIG_DEBUG_PREEMPT) && definedEx(CONFIG_PREEMPT_TRACER) || definedEx(CONFIG_DEBUG_PREEMPT)
sub_preempt_count(1)
#endif
; } while (0); __asm__ __volatile__("": : :"memory"); do { if (__builtin_expect(!!(test_ti_thread_flag(current_thread_info(), 3)), 0)) preempt_schedule(); } while (0); } while (0)
#endif
#if !definedEx(CONFIG_PREEMPT)
do { } while (0)
#endif
;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __raw_spin_unlock_bh(raw_spinlock_t *lock)
{
#if definedEx(CONFIG_DEBUG_LOCK_ALLOC)
lock_release(&lock->dep_map, 1, (unsigned long)__builtin_return_address(0))
#endif
#if !definedEx(CONFIG_DEBUG_LOCK_ALLOC)
do { } while (0)
#endif
;
	do_raw_spin_unlock(lock);
#if definedEx(CONFIG_PREEMPT)
do { __asm__ __volatile__("": : :"memory"); 
#if !definedEx(CONFIG_DEBUG_PREEMPT) && !definedEx(CONFIG_PREEMPT_TRACER)
do { (current_thread_info()->preempt_count) -= (1); } while (0)
#endif
#if !definedEx(CONFIG_DEBUG_PREEMPT) && definedEx(CONFIG_PREEMPT_TRACER) || definedEx(CONFIG_DEBUG_PREEMPT)
sub_preempt_count(1)
#endif
; } while (0)
#endif
#if !definedEx(CONFIG_PREEMPT)
do { } while (0)
#endif
;
	local_bh_enable_ip((unsigned long)__builtin_return_address(0));
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int __raw_spin_trylock_bh(raw_spinlock_t *lock)
{
	local_bh_disable();
#if definedEx(CONFIG_PREEMPT)
do { 
#if !definedEx(CONFIG_DEBUG_PREEMPT) && !definedEx(CONFIG_PREEMPT_TRACER)
do { (current_thread_info()->preempt_count) += (1); } while (0)
#endif
#if !definedEx(CONFIG_DEBUG_PREEMPT) && definedEx(CONFIG_PREEMPT_TRACER) || definedEx(CONFIG_DEBUG_PREEMPT)
add_preempt_count(1)
#endif
; __asm__ __volatile__("": : :"memory"); } while (0)
#endif
#if !definedEx(CONFIG_PREEMPT)
do { } while (0)
#endif
;
	if (do_raw_spin_trylock(lock)) {
#if definedEx(CONFIG_PROVE_LOCKING) && definedEx(CONFIG_DEBUG_LOCK_ALLOC)
lock_acquire(&lock->dep_map, 0, 1, 0, 2, ((void *)0), (unsigned long)__builtin_return_address(0))
#endif
#if !definedEx(CONFIG_PROVE_LOCKING) && definedEx(CONFIG_DEBUG_LOCK_ALLOC)
lock_acquire(&lock->dep_map, 0, 1, 0, 1, ((void *)0), (unsigned long)__builtin_return_address(0))
#endif
#if !definedEx(CONFIG_DEBUG_LOCK_ALLOC)
do { } while (0)
#endif
;
		return 1;
	}
#if definedEx(CONFIG_PREEMPT)
do { __asm__ __volatile__("": : :"memory"); 
#if !definedEx(CONFIG_DEBUG_PREEMPT) && !definedEx(CONFIG_PREEMPT_TRACER)
do { (current_thread_info()->preempt_count) -= (1); } while (0)
#endif
#if !definedEx(CONFIG_DEBUG_PREEMPT) && definedEx(CONFIG_PREEMPT_TRACER) || definedEx(CONFIG_DEBUG_PREEMPT)
sub_preempt_count(1)
#endif
; } while (0)
#endif
#if !definedEx(CONFIG_PREEMPT)
do { } while (0)
#endif
;
	local_bh_enable_ip((unsigned long)__builtin_return_address(0));
	return 0;
}
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/rwlock_api_smp.h" 1
/*
 * include/linux/rwlock_api_smp.h
 *
 * spinlock API declarations on SMP (and debug)
 * (implemented in kernel/spinlock.c)
 *
 * portions Copyright 2005, Red Hat, Inc., Ingo Molnar
 * Released under the General Public License (GPL).
 */
void __attribute__((section(".spinlock.text"))) _raw_read_lock(rwlock_t *lock)		;
void __attribute__((section(".spinlock.text"))) _raw_write_lock(rwlock_t *lock)		;
void __attribute__((section(".spinlock.text"))) _raw_read_lock_bh(rwlock_t *lock)	;
void __attribute__((section(".spinlock.text"))) _raw_write_lock_bh(rwlock_t *lock)	;
void __attribute__((section(".spinlock.text"))) _raw_read_lock_irq(rwlock_t *lock)	;
void __attribute__((section(".spinlock.text"))) _raw_write_lock_irq(rwlock_t *lock)	;
unsigned long __attribute__((section(".spinlock.text"))) _raw_read_lock_irqsave(rwlock_t *lock)
							;
unsigned long __attribute__((section(".spinlock.text"))) _raw_write_lock_irqsave(rwlock_t *lock)
							;
int __attribute__((section(".spinlock.text"))) _raw_read_trylock(rwlock_t *lock);
int __attribute__((section(".spinlock.text"))) _raw_write_trylock(rwlock_t *lock);
void __attribute__((section(".spinlock.text"))) _raw_read_unlock(rwlock_t *lock)	;
void __attribute__((section(".spinlock.text"))) _raw_write_unlock(rwlock_t *lock)	;
void __attribute__((section(".spinlock.text"))) _raw_read_unlock_bh(rwlock_t *lock)	;
void __attribute__((section(".spinlock.text"))) _raw_write_unlock_bh(rwlock_t *lock)	;
void __attribute__((section(".spinlock.text"))) _raw_read_unlock_irq(rwlock_t *lock)	;
void __attribute__((section(".spinlock.text"))) _raw_write_unlock_irq(rwlock_t *lock)	;
void __attribute__((section(".spinlock.text")))
_raw_read_unlock_irqrestore(rwlock_t *lock, unsigned long flags)
							;
void __attribute__((section(".spinlock.text")))
_raw_write_unlock_irqrestore(rwlock_t *lock, unsigned long flags)
							;
#if definedEx(CONFIG_INLINE_READ_UNLOCK)
#endif
#if definedEx(CONFIG_INLINE_WRITE_UNLOCK)
#endif
#if definedEx(CONFIG_INLINE_READ_UNLOCK_IRQ)
#endif
#if definedEx(CONFIG_INLINE_WRITE_UNLOCK_IRQ)
#endif
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int __raw_read_trylock(rwlock_t *lock)
{
#if definedEx(CONFIG_PREEMPT)
do { 
#if !definedEx(CONFIG_DEBUG_PREEMPT) && !definedEx(CONFIG_PREEMPT_TRACER)
do { (current_thread_info()->preempt_count) += (1); } while (0)
#endif
#if !definedEx(CONFIG_DEBUG_PREEMPT) && definedEx(CONFIG_PREEMPT_TRACER) || definedEx(CONFIG_DEBUG_PREEMPT)
add_preempt_count(1)
#endif
; __asm__ __volatile__("": : :"memory"); } while (0)
#endif
#if !definedEx(CONFIG_PREEMPT)
do { } while (0)
#endif
;
	if (
#if !definedEx(CONFIG_DEBUG_SPINLOCK)
arch_read_trylock(&(lock)->raw_lock)
#endif
#if definedEx(CONFIG_DEBUG_SPINLOCK)
do_raw_read_trylock(lock)
#endif
) {
#if definedEx(CONFIG_PROVE_LOCKING) && definedEx(CONFIG_DEBUG_LOCK_ALLOC)
lock_acquire(&lock->dep_map, 0, 1, 2, 2, ((void *)0), (unsigned long)__builtin_return_address(0))
#endif
#if !definedEx(CONFIG_PROVE_LOCKING) && definedEx(CONFIG_DEBUG_LOCK_ALLOC)
lock_acquire(&lock->dep_map, 0, 1, 2, 1, ((void *)0), (unsigned long)__builtin_return_address(0))
#endif
#if !definedEx(CONFIG_DEBUG_LOCK_ALLOC)
do { } while (0)
#endif
;
		return 1;
	}
#if definedEx(CONFIG_PREEMPT)
do { do { __asm__ __volatile__("": : :"memory"); 
#if !definedEx(CONFIG_DEBUG_PREEMPT) && !definedEx(CONFIG_PREEMPT_TRACER)
do { (current_thread_info()->preempt_count) -= (1); } while (0)
#endif
#if !definedEx(CONFIG_DEBUG_PREEMPT) && definedEx(CONFIG_PREEMPT_TRACER) || definedEx(CONFIG_DEBUG_PREEMPT)
sub_preempt_count(1)
#endif
; } while (0); __asm__ __volatile__("": : :"memory"); do { if (__builtin_expect(!!(test_ti_thread_flag(current_thread_info(), 3)), 0)) preempt_schedule(); } while (0); } while (0)
#endif
#if !definedEx(CONFIG_PREEMPT)
do { } while (0)
#endif
;
	return 0;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int __raw_write_trylock(rwlock_t *lock)
{
#if definedEx(CONFIG_PREEMPT)
do { 
#if !definedEx(CONFIG_DEBUG_PREEMPT) && !definedEx(CONFIG_PREEMPT_TRACER)
do { (current_thread_info()->preempt_count) += (1); } while (0)
#endif
#if !definedEx(CONFIG_DEBUG_PREEMPT) && definedEx(CONFIG_PREEMPT_TRACER) || definedEx(CONFIG_DEBUG_PREEMPT)
add_preempt_count(1)
#endif
; __asm__ __volatile__("": : :"memory"); } while (0)
#endif
#if !definedEx(CONFIG_PREEMPT)
do { } while (0)
#endif
;
	if (
#if !definedEx(CONFIG_DEBUG_SPINLOCK)
arch_write_trylock(&(lock)->raw_lock)
#endif
#if definedEx(CONFIG_DEBUG_SPINLOCK)
do_raw_write_trylock(lock)
#endif
) {
#if definedEx(CONFIG_PROVE_LOCKING) && definedEx(CONFIG_DEBUG_LOCK_ALLOC)
lock_acquire(&lock->dep_map, 0, 1, 0, 2, ((void *)0), (unsigned long)__builtin_return_address(0))
#endif
#if !definedEx(CONFIG_PROVE_LOCKING) && definedEx(CONFIG_DEBUG_LOCK_ALLOC)
lock_acquire(&lock->dep_map, 0, 1, 0, 1, ((void *)0), (unsigned long)__builtin_return_address(0))
#endif
#if !definedEx(CONFIG_DEBUG_LOCK_ALLOC)
do { } while (0)
#endif
;
		return 1;
	}
#if definedEx(CONFIG_PREEMPT)
do { do { __asm__ __volatile__("": : :"memory"); 
#if !definedEx(CONFIG_DEBUG_PREEMPT) && !definedEx(CONFIG_PREEMPT_TRACER)
do { (current_thread_info()->preempt_count) -= (1); } while (0)
#endif
#if !definedEx(CONFIG_DEBUG_PREEMPT) && definedEx(CONFIG_PREEMPT_TRACER) || definedEx(CONFIG_DEBUG_PREEMPT)
sub_preempt_count(1)
#endif
; } while (0); __asm__ __volatile__("": : :"memory"); do { if (__builtin_expect(!!(test_ti_thread_flag(current_thread_info(), 3)), 0)) preempt_schedule(); } while (0); } while (0)
#endif
#if !definedEx(CONFIG_PREEMPT)
do { } while (0)
#endif
;
	return 0;
}
/*
 * If lockdep is enabled then we use the non-preemption spin-ops
 * even on CONFIG_PREEMPT, because lockdep assumes that interrupts are
 * not re-enabled during lock-acquire (which the preempt-spin-ops do):
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __raw_read_lock(rwlock_t *lock)
{
#if definedEx(CONFIG_PREEMPT)
do { 
#if !definedEx(CONFIG_DEBUG_PREEMPT) && !definedEx(CONFIG_PREEMPT_TRACER)
do { (current_thread_info()->preempt_count) += (1); } while (0)
#endif
#if !definedEx(CONFIG_DEBUG_PREEMPT) && definedEx(CONFIG_PREEMPT_TRACER) || definedEx(CONFIG_DEBUG_PREEMPT)
add_preempt_count(1)
#endif
; __asm__ __volatile__("": : :"memory"); } while (0)
#endif
#if !definedEx(CONFIG_PREEMPT)
do { } while (0)
#endif
;
#if definedEx(CONFIG_PROVE_LOCKING) && definedEx(CONFIG_DEBUG_LOCK_ALLOC)
lock_acquire(&lock->dep_map, 0, 0, 2, 2, ((void *)0), (unsigned long)__builtin_return_address(0))
#endif
#if !definedEx(CONFIG_PROVE_LOCKING) && definedEx(CONFIG_DEBUG_LOCK_ALLOC)
lock_acquire(&lock->dep_map, 0, 0, 2, 1, ((void *)0), (unsigned long)__builtin_return_address(0))
#endif
#if !definedEx(CONFIG_DEBUG_LOCK_ALLOC)
do { } while (0)
#endif
;
#if definedEx(CONFIG_LOCK_STAT)
do { if (!
#if !definedEx(CONFIG_DEBUG_SPINLOCK)
arch_read_trylock(&(lock)->raw_lock)
#endif
#if definedEx(CONFIG_DEBUG_SPINLOCK)
do_raw_read_trylock(lock)
#endif
) { lock_contended(&(lock)->dep_map, (unsigned long)__builtin_return_address(0)); 
#if !definedEx(CONFIG_DEBUG_SPINLOCK)
arch_read_lock(&(lock)->raw_lock)
#endif
#if definedEx(CONFIG_DEBUG_SPINLOCK)
do_raw_read_lock(lock)
#endif
; } lock_acquired(&(lock)->dep_map, (unsigned long)__builtin_return_address(0)); } while (0)
#endif
#if !definedEx(CONFIG_LOCK_STAT)
#if !definedEx(CONFIG_DEBUG_SPINLOCK)
arch_read_lock(&(lock)->raw_lock)
#endif
#if definedEx(CONFIG_DEBUG_SPINLOCK)
do_raw_read_lock(lock)
#endif
#endif
;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long __raw_read_lock_irqsave(rwlock_t *lock)
{
	unsigned long flags;
	do { ({ unsigned long __dummy; typeof(flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); do { (flags) = __raw_local_irq_save(); } while (0); 
#if !definedEx(CONFIG_TRACE_IRQFLAGS)
do { } while (0)
#endif
#if definedEx(CONFIG_TRACE_IRQFLAGS)
trace_hardirqs_off()
#endif
; } while (0);
#if definedEx(CONFIG_PREEMPT)
do { 
#if !definedEx(CONFIG_DEBUG_PREEMPT) && !definedEx(CONFIG_PREEMPT_TRACER)
do { (current_thread_info()->preempt_count) += (1); } while (0)
#endif
#if !definedEx(CONFIG_DEBUG_PREEMPT) && definedEx(CONFIG_PREEMPT_TRACER) || definedEx(CONFIG_DEBUG_PREEMPT)
add_preempt_count(1)
#endif
; __asm__ __volatile__("": : :"memory"); } while (0)
#endif
#if !definedEx(CONFIG_PREEMPT)
do { } while (0)
#endif
;
#if definedEx(CONFIG_PROVE_LOCKING) && definedEx(CONFIG_DEBUG_LOCK_ALLOC)
lock_acquire(&lock->dep_map, 0, 0, 2, 2, ((void *)0), (unsigned long)__builtin_return_address(0))
#endif
#if !definedEx(CONFIG_PROVE_LOCKING) && definedEx(CONFIG_DEBUG_LOCK_ALLOC)
lock_acquire(&lock->dep_map, 0, 0, 2, 1, ((void *)0), (unsigned long)__builtin_return_address(0))
#endif
#if !definedEx(CONFIG_DEBUG_LOCK_ALLOC)
do { } while (0)
#endif
;
#if definedEx(CONFIG_LOCKDEP)
#if definedEx(CONFIG_LOCK_STAT)
do { if (!(do_raw_read_trylock)((lock))) { lock_contended(&((lock))->dep_map, (unsigned long)__builtin_return_address(0)); (do_raw_read_lock)((lock)); } lock_acquired(&((lock))->dep_map, (unsigned long)__builtin_return_address(0)); } while (0)
#endif
#if !definedEx(CONFIG_LOCK_STAT)
(do_raw_read_lock)((lock))
#endif
#endif
#if !definedEx(CONFIG_LOCKDEP)
#if definedEx(CONFIG_DEBUG_SPINLOCK)
do_raw_read_lock((lock))
#endif
#if !definedEx(CONFIG_DEBUG_SPINLOCK)
arch_read_lock(&((lock))->raw_lock)
#endif
#endif
;
	return flags;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __raw_read_lock_irq(rwlock_t *lock)
{
	do { raw_local_irq_disable(); 
#if !definedEx(CONFIG_TRACE_IRQFLAGS)
do { } while (0)
#endif
#if definedEx(CONFIG_TRACE_IRQFLAGS)
trace_hardirqs_off()
#endif
; } while (0);
#if definedEx(CONFIG_PREEMPT)
do { 
#if !definedEx(CONFIG_DEBUG_PREEMPT) && !definedEx(CONFIG_PREEMPT_TRACER)
do { (current_thread_info()->preempt_count) += (1); } while (0)
#endif
#if !definedEx(CONFIG_DEBUG_PREEMPT) && definedEx(CONFIG_PREEMPT_TRACER) || definedEx(CONFIG_DEBUG_PREEMPT)
add_preempt_count(1)
#endif
; __asm__ __volatile__("": : :"memory"); } while (0)
#endif
#if !definedEx(CONFIG_PREEMPT)
do { } while (0)
#endif
;
#if definedEx(CONFIG_PROVE_LOCKING) && definedEx(CONFIG_DEBUG_LOCK_ALLOC)
lock_acquire(&lock->dep_map, 0, 0, 2, 2, ((void *)0), (unsigned long)__builtin_return_address(0))
#endif
#if !definedEx(CONFIG_PROVE_LOCKING) && definedEx(CONFIG_DEBUG_LOCK_ALLOC)
lock_acquire(&lock->dep_map, 0, 0, 2, 1, ((void *)0), (unsigned long)__builtin_return_address(0))
#endif
#if !definedEx(CONFIG_DEBUG_LOCK_ALLOC)
do { } while (0)
#endif
;
#if definedEx(CONFIG_LOCK_STAT)
do { if (!
#if !definedEx(CONFIG_DEBUG_SPINLOCK)
arch_read_trylock(&(lock)->raw_lock)
#endif
#if definedEx(CONFIG_DEBUG_SPINLOCK)
do_raw_read_trylock(lock)
#endif
) { lock_contended(&(lock)->dep_map, (unsigned long)__builtin_return_address(0)); 
#if !definedEx(CONFIG_DEBUG_SPINLOCK)
arch_read_lock(&(lock)->raw_lock)
#endif
#if definedEx(CONFIG_DEBUG_SPINLOCK)
do_raw_read_lock(lock)
#endif
; } lock_acquired(&(lock)->dep_map, (unsigned long)__builtin_return_address(0)); } while (0)
#endif
#if !definedEx(CONFIG_LOCK_STAT)
#if !definedEx(CONFIG_DEBUG_SPINLOCK)
arch_read_lock(&(lock)->raw_lock)
#endif
#if definedEx(CONFIG_DEBUG_SPINLOCK)
do_raw_read_lock(lock)
#endif
#endif
;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __raw_read_lock_bh(rwlock_t *lock)
{
	local_bh_disable();
#if definedEx(CONFIG_PREEMPT)
do { 
#if !definedEx(CONFIG_DEBUG_PREEMPT) && !definedEx(CONFIG_PREEMPT_TRACER)
do { (current_thread_info()->preempt_count) += (1); } while (0)
#endif
#if !definedEx(CONFIG_DEBUG_PREEMPT) && definedEx(CONFIG_PREEMPT_TRACER) || definedEx(CONFIG_DEBUG_PREEMPT)
add_preempt_count(1)
#endif
; __asm__ __volatile__("": : :"memory"); } while (0)
#endif
#if !definedEx(CONFIG_PREEMPT)
do { } while (0)
#endif
;
#if definedEx(CONFIG_PROVE_LOCKING) && definedEx(CONFIG_DEBUG_LOCK_ALLOC)
lock_acquire(&lock->dep_map, 0, 0, 2, 2, ((void *)0), (unsigned long)__builtin_return_address(0))
#endif
#if !definedEx(CONFIG_PROVE_LOCKING) && definedEx(CONFIG_DEBUG_LOCK_ALLOC)
lock_acquire(&lock->dep_map, 0, 0, 2, 1, ((void *)0), (unsigned long)__builtin_return_address(0))
#endif
#if !definedEx(CONFIG_DEBUG_LOCK_ALLOC)
do { } while (0)
#endif
;
#if definedEx(CONFIG_LOCK_STAT)
do { if (!
#if !definedEx(CONFIG_DEBUG_SPINLOCK)
arch_read_trylock(&(lock)->raw_lock)
#endif
#if definedEx(CONFIG_DEBUG_SPINLOCK)
do_raw_read_trylock(lock)
#endif
) { lock_contended(&(lock)->dep_map, (unsigned long)__builtin_return_address(0)); 
#if !definedEx(CONFIG_DEBUG_SPINLOCK)
arch_read_lock(&(lock)->raw_lock)
#endif
#if definedEx(CONFIG_DEBUG_SPINLOCK)
do_raw_read_lock(lock)
#endif
; } lock_acquired(&(lock)->dep_map, (unsigned long)__builtin_return_address(0)); } while (0)
#endif
#if !definedEx(CONFIG_LOCK_STAT)
#if !definedEx(CONFIG_DEBUG_SPINLOCK)
arch_read_lock(&(lock)->raw_lock)
#endif
#if definedEx(CONFIG_DEBUG_SPINLOCK)
do_raw_read_lock(lock)
#endif
#endif
;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long __raw_write_lock_irqsave(rwlock_t *lock)
{
	unsigned long flags;
	do { ({ unsigned long __dummy; typeof(flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); do { (flags) = __raw_local_irq_save(); } while (0); 
#if !definedEx(CONFIG_TRACE_IRQFLAGS)
do { } while (0)
#endif
#if definedEx(CONFIG_TRACE_IRQFLAGS)
trace_hardirqs_off()
#endif
; } while (0);
#if definedEx(CONFIG_PREEMPT)
do { 
#if !definedEx(CONFIG_DEBUG_PREEMPT) && !definedEx(CONFIG_PREEMPT_TRACER)
do { (current_thread_info()->preempt_count) += (1); } while (0)
#endif
#if !definedEx(CONFIG_DEBUG_PREEMPT) && definedEx(CONFIG_PREEMPT_TRACER) || definedEx(CONFIG_DEBUG_PREEMPT)
add_preempt_count(1)
#endif
; __asm__ __volatile__("": : :"memory"); } while (0)
#endif
#if !definedEx(CONFIG_PREEMPT)
do { } while (0)
#endif
;
#if definedEx(CONFIG_PROVE_LOCKING) && definedEx(CONFIG_DEBUG_LOCK_ALLOC)
lock_acquire(&lock->dep_map, 0, 0, 0, 2, ((void *)0), (unsigned long)__builtin_return_address(0))
#endif
#if !definedEx(CONFIG_PROVE_LOCKING) && definedEx(CONFIG_DEBUG_LOCK_ALLOC)
lock_acquire(&lock->dep_map, 0, 0, 0, 1, ((void *)0), (unsigned long)__builtin_return_address(0))
#endif
#if !definedEx(CONFIG_DEBUG_LOCK_ALLOC)
do { } while (0)
#endif
;
#if definedEx(CONFIG_LOCKDEP)
#if definedEx(CONFIG_LOCK_STAT)
do { if (!(do_raw_write_trylock)((lock))) { lock_contended(&((lock))->dep_map, (unsigned long)__builtin_return_address(0)); (do_raw_write_lock)((lock)); } lock_acquired(&((lock))->dep_map, (unsigned long)__builtin_return_address(0)); } while (0)
#endif
#if !definedEx(CONFIG_LOCK_STAT)
(do_raw_write_lock)((lock))
#endif
#endif
#if !definedEx(CONFIG_LOCKDEP)
#if definedEx(CONFIG_DEBUG_SPINLOCK)
do_raw_write_lock((lock))
#endif
#if !definedEx(CONFIG_DEBUG_SPINLOCK)
arch_write_lock(&((lock))->raw_lock)
#endif
#endif
;
	return flags;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __raw_write_lock_irq(rwlock_t *lock)
{
	do { raw_local_irq_disable(); 
#if !definedEx(CONFIG_TRACE_IRQFLAGS)
do { } while (0)
#endif
#if definedEx(CONFIG_TRACE_IRQFLAGS)
trace_hardirqs_off()
#endif
; } while (0);
#if definedEx(CONFIG_PREEMPT)
do { 
#if !definedEx(CONFIG_DEBUG_PREEMPT) && !definedEx(CONFIG_PREEMPT_TRACER)
do { (current_thread_info()->preempt_count) += (1); } while (0)
#endif
#if !definedEx(CONFIG_DEBUG_PREEMPT) && definedEx(CONFIG_PREEMPT_TRACER) || definedEx(CONFIG_DEBUG_PREEMPT)
add_preempt_count(1)
#endif
; __asm__ __volatile__("": : :"memory"); } while (0)
#endif
#if !definedEx(CONFIG_PREEMPT)
do { } while (0)
#endif
;
#if definedEx(CONFIG_PROVE_LOCKING) && definedEx(CONFIG_DEBUG_LOCK_ALLOC)
lock_acquire(&lock->dep_map, 0, 0, 0, 2, ((void *)0), (unsigned long)__builtin_return_address(0))
#endif
#if !definedEx(CONFIG_PROVE_LOCKING) && definedEx(CONFIG_DEBUG_LOCK_ALLOC)
lock_acquire(&lock->dep_map, 0, 0, 0, 1, ((void *)0), (unsigned long)__builtin_return_address(0))
#endif
#if !definedEx(CONFIG_DEBUG_LOCK_ALLOC)
do { } while (0)
#endif
;
#if definedEx(CONFIG_LOCK_STAT)
do { if (!
#if !definedEx(CONFIG_DEBUG_SPINLOCK)
arch_write_trylock(&(lock)->raw_lock)
#endif
#if definedEx(CONFIG_DEBUG_SPINLOCK)
do_raw_write_trylock(lock)
#endif
) { lock_contended(&(lock)->dep_map, (unsigned long)__builtin_return_address(0)); 
#if !definedEx(CONFIG_DEBUG_SPINLOCK)
arch_write_lock(&(lock)->raw_lock)
#endif
#if definedEx(CONFIG_DEBUG_SPINLOCK)
do_raw_write_lock(lock)
#endif
; } lock_acquired(&(lock)->dep_map, (unsigned long)__builtin_return_address(0)); } while (0)
#endif
#if !definedEx(CONFIG_LOCK_STAT)
#if !definedEx(CONFIG_DEBUG_SPINLOCK)
arch_write_lock(&(lock)->raw_lock)
#endif
#if definedEx(CONFIG_DEBUG_SPINLOCK)
do_raw_write_lock(lock)
#endif
#endif
;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __raw_write_lock_bh(rwlock_t *lock)
{
	local_bh_disable();
#if definedEx(CONFIG_PREEMPT)
do { 
#if !definedEx(CONFIG_DEBUG_PREEMPT) && !definedEx(CONFIG_PREEMPT_TRACER)
do { (current_thread_info()->preempt_count) += (1); } while (0)
#endif
#if !definedEx(CONFIG_DEBUG_PREEMPT) && definedEx(CONFIG_PREEMPT_TRACER) || definedEx(CONFIG_DEBUG_PREEMPT)
add_preempt_count(1)
#endif
; __asm__ __volatile__("": : :"memory"); } while (0)
#endif
#if !definedEx(CONFIG_PREEMPT)
do { } while (0)
#endif
;
#if definedEx(CONFIG_PROVE_LOCKING) && definedEx(CONFIG_DEBUG_LOCK_ALLOC)
lock_acquire(&lock->dep_map, 0, 0, 0, 2, ((void *)0), (unsigned long)__builtin_return_address(0))
#endif
#if !definedEx(CONFIG_PROVE_LOCKING) && definedEx(CONFIG_DEBUG_LOCK_ALLOC)
lock_acquire(&lock->dep_map, 0, 0, 0, 1, ((void *)0), (unsigned long)__builtin_return_address(0))
#endif
#if !definedEx(CONFIG_DEBUG_LOCK_ALLOC)
do { } while (0)
#endif
;
#if definedEx(CONFIG_LOCK_STAT)
do { if (!
#if !definedEx(CONFIG_DEBUG_SPINLOCK)
arch_write_trylock(&(lock)->raw_lock)
#endif
#if definedEx(CONFIG_DEBUG_SPINLOCK)
do_raw_write_trylock(lock)
#endif
) { lock_contended(&(lock)->dep_map, (unsigned long)__builtin_return_address(0)); 
#if !definedEx(CONFIG_DEBUG_SPINLOCK)
arch_write_lock(&(lock)->raw_lock)
#endif
#if definedEx(CONFIG_DEBUG_SPINLOCK)
do_raw_write_lock(lock)
#endif
; } lock_acquired(&(lock)->dep_map, (unsigned long)__builtin_return_address(0)); } while (0)
#endif
#if !definedEx(CONFIG_LOCK_STAT)
#if !definedEx(CONFIG_DEBUG_SPINLOCK)
arch_write_lock(&(lock)->raw_lock)
#endif
#if definedEx(CONFIG_DEBUG_SPINLOCK)
do_raw_write_lock(lock)
#endif
#endif
;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __raw_write_lock(rwlock_t *lock)
{
#if definedEx(CONFIG_PREEMPT)
do { 
#if !definedEx(CONFIG_DEBUG_PREEMPT) && !definedEx(CONFIG_PREEMPT_TRACER)
do { (current_thread_info()->preempt_count) += (1); } while (0)
#endif
#if !definedEx(CONFIG_DEBUG_PREEMPT) && definedEx(CONFIG_PREEMPT_TRACER) || definedEx(CONFIG_DEBUG_PREEMPT)
add_preempt_count(1)
#endif
; __asm__ __volatile__("": : :"memory"); } while (0)
#endif
#if !definedEx(CONFIG_PREEMPT)
do { } while (0)
#endif
;
#if definedEx(CONFIG_PROVE_LOCKING) && definedEx(CONFIG_DEBUG_LOCK_ALLOC)
lock_acquire(&lock->dep_map, 0, 0, 0, 2, ((void *)0), (unsigned long)__builtin_return_address(0))
#endif
#if !definedEx(CONFIG_PROVE_LOCKING) && definedEx(CONFIG_DEBUG_LOCK_ALLOC)
lock_acquire(&lock->dep_map, 0, 0, 0, 1, ((void *)0), (unsigned long)__builtin_return_address(0))
#endif
#if !definedEx(CONFIG_DEBUG_LOCK_ALLOC)
do { } while (0)
#endif
;
#if definedEx(CONFIG_LOCK_STAT)
do { if (!
#if !definedEx(CONFIG_DEBUG_SPINLOCK)
arch_write_trylock(&(lock)->raw_lock)
#endif
#if definedEx(CONFIG_DEBUG_SPINLOCK)
do_raw_write_trylock(lock)
#endif
) { lock_contended(&(lock)->dep_map, (unsigned long)__builtin_return_address(0)); 
#if !definedEx(CONFIG_DEBUG_SPINLOCK)
arch_write_lock(&(lock)->raw_lock)
#endif
#if definedEx(CONFIG_DEBUG_SPINLOCK)
do_raw_write_lock(lock)
#endif
; } lock_acquired(&(lock)->dep_map, (unsigned long)__builtin_return_address(0)); } while (0)
#endif
#if !definedEx(CONFIG_LOCK_STAT)
#if !definedEx(CONFIG_DEBUG_SPINLOCK)
arch_write_lock(&(lock)->raw_lock)
#endif
#if definedEx(CONFIG_DEBUG_SPINLOCK)
do_raw_write_lock(lock)
#endif
#endif
;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __raw_write_unlock(rwlock_t *lock)
{
#if definedEx(CONFIG_DEBUG_LOCK_ALLOC)
lock_release(&lock->dep_map, 1, (unsigned long)__builtin_return_address(0))
#endif
#if !definedEx(CONFIG_DEBUG_LOCK_ALLOC)
do { } while (0)
#endif
;
#if !definedEx(CONFIG_DEBUG_SPINLOCK)
arch_write_unlock(&(lock)->raw_lock)
#endif
#if definedEx(CONFIG_DEBUG_SPINLOCK)
do_raw_write_unlock(lock)
#endif
;
#if definedEx(CONFIG_PREEMPT)
do { do { __asm__ __volatile__("": : :"memory"); 
#if !definedEx(CONFIG_DEBUG_PREEMPT) && !definedEx(CONFIG_PREEMPT_TRACER)
do { (current_thread_info()->preempt_count) -= (1); } while (0)
#endif
#if !definedEx(CONFIG_DEBUG_PREEMPT) && definedEx(CONFIG_PREEMPT_TRACER) || definedEx(CONFIG_DEBUG_PREEMPT)
sub_preempt_count(1)
#endif
; } while (0); __asm__ __volatile__("": : :"memory"); do { if (__builtin_expect(!!(test_ti_thread_flag(current_thread_info(), 3)), 0)) preempt_schedule(); } while (0); } while (0)
#endif
#if !definedEx(CONFIG_PREEMPT)
do { } while (0)
#endif
;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __raw_read_unlock(rwlock_t *lock)
{
#if definedEx(CONFIG_DEBUG_LOCK_ALLOC)
lock_release(&lock->dep_map, 1, (unsigned long)__builtin_return_address(0))
#endif
#if !definedEx(CONFIG_DEBUG_LOCK_ALLOC)
do { } while (0)
#endif
;
#if !definedEx(CONFIG_DEBUG_SPINLOCK)
arch_read_unlock(&(lock)->raw_lock)
#endif
#if definedEx(CONFIG_DEBUG_SPINLOCK)
do_raw_read_unlock(lock)
#endif
;
#if definedEx(CONFIG_PREEMPT)
do { do { __asm__ __volatile__("": : :"memory"); 
#if !definedEx(CONFIG_DEBUG_PREEMPT) && !definedEx(CONFIG_PREEMPT_TRACER)
do { (current_thread_info()->preempt_count) -= (1); } while (0)
#endif
#if !definedEx(CONFIG_DEBUG_PREEMPT) && definedEx(CONFIG_PREEMPT_TRACER) || definedEx(CONFIG_DEBUG_PREEMPT)
sub_preempt_count(1)
#endif
; } while (0); __asm__ __volatile__("": : :"memory"); do { if (__builtin_expect(!!(test_ti_thread_flag(current_thread_info(), 3)), 0)) preempt_schedule(); } while (0); } while (0)
#endif
#if !definedEx(CONFIG_PREEMPT)
do { } while (0)
#endif
;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void
__raw_read_unlock_irqrestore(rwlock_t *lock, unsigned long flags)
{
#if definedEx(CONFIG_DEBUG_LOCK_ALLOC)
lock_release(&lock->dep_map, 1, (unsigned long)__builtin_return_address(0))
#endif
#if !definedEx(CONFIG_DEBUG_LOCK_ALLOC)
do { } while (0)
#endif
;
#if !definedEx(CONFIG_DEBUG_SPINLOCK)
arch_read_unlock(&(lock)->raw_lock)
#endif
#if definedEx(CONFIG_DEBUG_SPINLOCK)
do_raw_read_unlock(lock)
#endif
;
	do { ({ unsigned long __dummy; typeof(flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); if (raw_irqs_disabled_flags(flags)) { raw_local_irq_restore(flags); 
#if !definedEx(CONFIG_TRACE_IRQFLAGS)
do { } while (0)
#endif
#if definedEx(CONFIG_TRACE_IRQFLAGS)
trace_hardirqs_off()
#endif
; } else { 
#if !definedEx(CONFIG_TRACE_IRQFLAGS)
do { } while (0)
#endif
#if definedEx(CONFIG_TRACE_IRQFLAGS)
trace_hardirqs_on()
#endif
; raw_local_irq_restore(flags); } } while (0);
#if definedEx(CONFIG_PREEMPT)
do { do { __asm__ __volatile__("": : :"memory"); 
#if !definedEx(CONFIG_DEBUG_PREEMPT) && !definedEx(CONFIG_PREEMPT_TRACER)
do { (current_thread_info()->preempt_count) -= (1); } while (0)
#endif
#if !definedEx(CONFIG_DEBUG_PREEMPT) && definedEx(CONFIG_PREEMPT_TRACER) || definedEx(CONFIG_DEBUG_PREEMPT)
sub_preempt_count(1)
#endif
; } while (0); __asm__ __volatile__("": : :"memory"); do { if (__builtin_expect(!!(test_ti_thread_flag(current_thread_info(), 3)), 0)) preempt_schedule(); } while (0); } while (0)
#endif
#if !definedEx(CONFIG_PREEMPT)
do { } while (0)
#endif
;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __raw_read_unlock_irq(rwlock_t *lock)
{
#if definedEx(CONFIG_DEBUG_LOCK_ALLOC)
lock_release(&lock->dep_map, 1, (unsigned long)__builtin_return_address(0))
#endif
#if !definedEx(CONFIG_DEBUG_LOCK_ALLOC)
do { } while (0)
#endif
;
#if !definedEx(CONFIG_DEBUG_SPINLOCK)
arch_read_unlock(&(lock)->raw_lock)
#endif
#if definedEx(CONFIG_DEBUG_SPINLOCK)
do_raw_read_unlock(lock)
#endif
;
	do { 
#if !definedEx(CONFIG_TRACE_IRQFLAGS)
do { } while (0)
#endif
#if definedEx(CONFIG_TRACE_IRQFLAGS)
trace_hardirqs_on()
#endif
; raw_local_irq_enable(); } while (0);
#if definedEx(CONFIG_PREEMPT)
do { do { __asm__ __volatile__("": : :"memory"); 
#if !definedEx(CONFIG_DEBUG_PREEMPT) && !definedEx(CONFIG_PREEMPT_TRACER)
do { (current_thread_info()->preempt_count) -= (1); } while (0)
#endif
#if !definedEx(CONFIG_DEBUG_PREEMPT) && definedEx(CONFIG_PREEMPT_TRACER) || definedEx(CONFIG_DEBUG_PREEMPT)
sub_preempt_count(1)
#endif
; } while (0); __asm__ __volatile__("": : :"memory"); do { if (__builtin_expect(!!(test_ti_thread_flag(current_thread_info(), 3)), 0)) preempt_schedule(); } while (0); } while (0)
#endif
#if !definedEx(CONFIG_PREEMPT)
do { } while (0)
#endif
;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __raw_read_unlock_bh(rwlock_t *lock)
{
#if definedEx(CONFIG_DEBUG_LOCK_ALLOC)
lock_release(&lock->dep_map, 1, (unsigned long)__builtin_return_address(0))
#endif
#if !definedEx(CONFIG_DEBUG_LOCK_ALLOC)
do { } while (0)
#endif
;
#if !definedEx(CONFIG_DEBUG_SPINLOCK)
arch_read_unlock(&(lock)->raw_lock)
#endif
#if definedEx(CONFIG_DEBUG_SPINLOCK)
do_raw_read_unlock(lock)
#endif
;
#if definedEx(CONFIG_PREEMPT)
do { __asm__ __volatile__("": : :"memory"); 
#if !definedEx(CONFIG_DEBUG_PREEMPT) && !definedEx(CONFIG_PREEMPT_TRACER)
do { (current_thread_info()->preempt_count) -= (1); } while (0)
#endif
#if !definedEx(CONFIG_DEBUG_PREEMPT) && definedEx(CONFIG_PREEMPT_TRACER) || definedEx(CONFIG_DEBUG_PREEMPT)
sub_preempt_count(1)
#endif
; } while (0)
#endif
#if !definedEx(CONFIG_PREEMPT)
do { } while (0)
#endif
;
	local_bh_enable_ip((unsigned long)__builtin_return_address(0));
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __raw_write_unlock_irqrestore(rwlock_t *lock,
					     unsigned long flags)
{
#if definedEx(CONFIG_DEBUG_LOCK_ALLOC)
lock_release(&lock->dep_map, 1, (unsigned long)__builtin_return_address(0))
#endif
#if !definedEx(CONFIG_DEBUG_LOCK_ALLOC)
do { } while (0)
#endif
;
#if !definedEx(CONFIG_DEBUG_SPINLOCK)
arch_write_unlock(&(lock)->raw_lock)
#endif
#if definedEx(CONFIG_DEBUG_SPINLOCK)
do_raw_write_unlock(lock)
#endif
;
	do { ({ unsigned long __dummy; typeof(flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); if (raw_irqs_disabled_flags(flags)) { raw_local_irq_restore(flags); 
#if !definedEx(CONFIG_TRACE_IRQFLAGS)
do { } while (0)
#endif
#if definedEx(CONFIG_TRACE_IRQFLAGS)
trace_hardirqs_off()
#endif
; } else { 
#if !definedEx(CONFIG_TRACE_IRQFLAGS)
do { } while (0)
#endif
#if definedEx(CONFIG_TRACE_IRQFLAGS)
trace_hardirqs_on()
#endif
; raw_local_irq_restore(flags); } } while (0);
#if definedEx(CONFIG_PREEMPT)
do { do { __asm__ __volatile__("": : :"memory"); 
#if !definedEx(CONFIG_DEBUG_PREEMPT) && !definedEx(CONFIG_PREEMPT_TRACER)
do { (current_thread_info()->preempt_count) -= (1); } while (0)
#endif
#if !definedEx(CONFIG_DEBUG_PREEMPT) && definedEx(CONFIG_PREEMPT_TRACER) || definedEx(CONFIG_DEBUG_PREEMPT)
sub_preempt_count(1)
#endif
; } while (0); __asm__ __volatile__("": : :"memory"); do { if (__builtin_expect(!!(test_ti_thread_flag(current_thread_info(), 3)), 0)) preempt_schedule(); } while (0); } while (0)
#endif
#if !definedEx(CONFIG_PREEMPT)
do { } while (0)
#endif
;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __raw_write_unlock_irq(rwlock_t *lock)
{
#if definedEx(CONFIG_DEBUG_LOCK_ALLOC)
lock_release(&lock->dep_map, 1, (unsigned long)__builtin_return_address(0))
#endif
#if !definedEx(CONFIG_DEBUG_LOCK_ALLOC)
do { } while (0)
#endif
;
#if !definedEx(CONFIG_DEBUG_SPINLOCK)
arch_write_unlock(&(lock)->raw_lock)
#endif
#if definedEx(CONFIG_DEBUG_SPINLOCK)
do_raw_write_unlock(lock)
#endif
;
	do { 
#if !definedEx(CONFIG_TRACE_IRQFLAGS)
do { } while (0)
#endif
#if definedEx(CONFIG_TRACE_IRQFLAGS)
trace_hardirqs_on()
#endif
; raw_local_irq_enable(); } while (0);
#if definedEx(CONFIG_PREEMPT)
do { do { __asm__ __volatile__("": : :"memory"); 
#if !definedEx(CONFIG_DEBUG_PREEMPT) && !definedEx(CONFIG_PREEMPT_TRACER)
do { (current_thread_info()->preempt_count) -= (1); } while (0)
#endif
#if !definedEx(CONFIG_DEBUG_PREEMPT) && definedEx(CONFIG_PREEMPT_TRACER) || definedEx(CONFIG_DEBUG_PREEMPT)
sub_preempt_count(1)
#endif
; } while (0); __asm__ __volatile__("": : :"memory"); do { if (__builtin_expect(!!(test_ti_thread_flag(current_thread_info(), 3)), 0)) preempt_schedule(); } while (0); } while (0)
#endif
#if !definedEx(CONFIG_PREEMPT)
do { } while (0)
#endif
;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __raw_write_unlock_bh(rwlock_t *lock)
{
#if definedEx(CONFIG_DEBUG_LOCK_ALLOC)
lock_release(&lock->dep_map, 1, (unsigned long)__builtin_return_address(0))
#endif
#if !definedEx(CONFIG_DEBUG_LOCK_ALLOC)
do { } while (0)
#endif
;
#if !definedEx(CONFIG_DEBUG_SPINLOCK)
arch_write_unlock(&(lock)->raw_lock)
#endif
#if definedEx(CONFIG_DEBUG_SPINLOCK)
do_raw_write_unlock(lock)
#endif
;
#if definedEx(CONFIG_PREEMPT)
do { __asm__ __volatile__("": : :"memory"); 
#if !definedEx(CONFIG_DEBUG_PREEMPT) && !definedEx(CONFIG_PREEMPT_TRACER)
do { (current_thread_info()->preempt_count) -= (1); } while (0)
#endif
#if !definedEx(CONFIG_DEBUG_PREEMPT) && definedEx(CONFIG_PREEMPT_TRACER) || definedEx(CONFIG_DEBUG_PREEMPT)
sub_preempt_count(1)
#endif
; } while (0)
#endif
#if !definedEx(CONFIG_PREEMPT)
do { } while (0)
#endif
;
	local_bh_enable_ip((unsigned long)__builtin_return_address(0));
}
#line 196 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/spinlock_api_smp.h" 2
#line 261 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/spinlock.h" 2
#endif
#if !definedEx(CONFIG_SMP) && !definedEx(CONFIG_DEBUG_SPINLOCK)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/spinlock_api_up.h" 1
/*
 * include/linux/spinlock_api_up.h
 *
 * spinlock API implementation on UP-nondebug (inlined implementation)
 *
 * portions Copyright 2005, Red Hat, Inc., Ingo Molnar
 * Released under the General Public License (GPL).
 */
/*
 * In the UP-nondebug case there's no real locking going on, so the
 * only thing we have to do is to keep the preempt counts and irq
 * flags straight, to suppress compiler warnings of unused lock
 * variables, and to add the proper checker annotations:
 */
#line 263 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/spinlock.h" 2
#endif
/*
 * Map the spin_lock functions to the raw variants for PREEMPT_RT=n
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 raw_spinlock_t *spinlock_check(spinlock_t *lock)
{
	return &lock->rlock;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void spin_lock(spinlock_t *lock)
{
#if !definedEx(CONFIG_SMP) && !definedEx(CONFIG_DEBUG_SPINLOCK)
do { 
#if definedEx(CONFIG_PREEMPT)
do { 
#if !definedEx(CONFIG_DEBUG_PREEMPT) && !definedEx(CONFIG_PREEMPT_TRACER)
do { (current_thread_info()->preempt_count) += (1); } while (0)
#endif
#if !definedEx(CONFIG_DEBUG_PREEMPT) && definedEx(CONFIG_PREEMPT_TRACER) || definedEx(CONFIG_DEBUG_PREEMPT)
add_preempt_count(1)
#endif
; __asm__ __volatile__("": : :"memory"); } while (0)
#endif
#if !definedEx(CONFIG_PREEMPT)
do { } while (0)
#endif
; (void)0; (void)(&lock->rlock); } while (0)
#endif
#if !definedEx(CONFIG_SMP) && definedEx(CONFIG_DEBUG_SPINLOCK) || definedEx(CONFIG_SMP)
_raw_spin_lock(&lock->rlock)
#endif
;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void spin_lock_bh(spinlock_t *lock)
{
#if !definedEx(CONFIG_SMP) && !definedEx(CONFIG_DEBUG_SPINLOCK)
do { local_bh_disable(); do { 
#if definedEx(CONFIG_PREEMPT)
do { 
#if !definedEx(CONFIG_DEBUG_PREEMPT) && !definedEx(CONFIG_PREEMPT_TRACER)
do { (current_thread_info()->preempt_count) += (1); } while (0)
#endif
#if !definedEx(CONFIG_DEBUG_PREEMPT) && definedEx(CONFIG_PREEMPT_TRACER) || definedEx(CONFIG_DEBUG_PREEMPT)
add_preempt_count(1)
#endif
; __asm__ __volatile__("": : :"memory"); } while (0)
#endif
#if !definedEx(CONFIG_PREEMPT)
do { } while (0)
#endif
; (void)0; (void)(&lock->rlock); } while (0); } while (0)
#endif
#if !definedEx(CONFIG_SMP) && definedEx(CONFIG_DEBUG_SPINLOCK) || definedEx(CONFIG_SMP)
_raw_spin_lock_bh(&lock->rlock)
#endif
;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int spin_trylock(spinlock_t *lock)
{
	return (
#if !definedEx(CONFIG_SMP) && !definedEx(CONFIG_DEBUG_SPINLOCK)
({ 
do { 
#if definedEx(CONFIG_PREEMPT)
do { 
#if !definedEx(CONFIG_DEBUG_PREEMPT) && !definedEx(CONFIG_PREEMPT_TRACER)
do { (current_thread_info()->preempt_count) += (1); } while (0)
#endif
#if !definedEx(CONFIG_DEBUG_PREEMPT) && definedEx(CONFIG_PREEMPT_TRACER) || definedEx(CONFIG_DEBUG_PREEMPT)
add_preempt_count(1)
#endif
; __asm__ __volatile__("": : :"memory"); } while (0)
#endif
#if !definedEx(CONFIG_PREEMPT)
do { } while (0)
#endif
; (void)0; (void)(&lock->rlock); } while (0)
; 1; })
#endif
#if !definedEx(CONFIG_SMP) && definedEx(CONFIG_DEBUG_SPINLOCK) || definedEx(CONFIG_SMP)
_raw_spin_trylock(&lock->rlock)
#endif
);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void spin_lock_irq(spinlock_t *lock)
{
#if !definedEx(CONFIG_SMP) && !definedEx(CONFIG_DEBUG_SPINLOCK)
do { do { raw_local_irq_disable(); 
#if !definedEx(CONFIG_TRACE_IRQFLAGS)
do { } while (0)
#endif
#if definedEx(CONFIG_TRACE_IRQFLAGS)
trace_hardirqs_off()
#endif
; } while (0); do { 
#if definedEx(CONFIG_PREEMPT)
do { 
#if !definedEx(CONFIG_DEBUG_PREEMPT) && !definedEx(CONFIG_PREEMPT_TRACER)
do { (current_thread_info()->preempt_count) += (1); } while (0)
#endif
#if !definedEx(CONFIG_DEBUG_PREEMPT) && definedEx(CONFIG_PREEMPT_TRACER) || definedEx(CONFIG_DEBUG_PREEMPT)
add_preempt_count(1)
#endif
; __asm__ __volatile__("": : :"memory"); } while (0)
#endif
#if !definedEx(CONFIG_PREEMPT)
do { } while (0)
#endif
; (void)0; (void)(&lock->rlock); } while (0); } while (0)
#endif
#if !definedEx(CONFIG_SMP) && definedEx(CONFIG_DEBUG_SPINLOCK) || definedEx(CONFIG_SMP)
_raw_spin_lock_irq(&lock->rlock)
#endif
;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void spin_unlock(spinlock_t *lock)
{
#if !definedEx(CONFIG_SMP) && definedEx(CONFIG_DEBUG_SPINLOCK) && definedEx(CONFIG_INLINE_SPIN_UNLOCK) || definedEx(CONFIG_SMP) && definedEx(CONFIG_INLINE_SPIN_UNLOCK)
__raw_spin_unlock(&lock->rlock)
#endif
#if !definedEx(CONFIG_SMP) && !definedEx(CONFIG_DEBUG_SPINLOCK)
do { 
#if definedEx(CONFIG_PREEMPT)
do { do { __asm__ __volatile__("": : :"memory"); 
#if !definedEx(CONFIG_DEBUG_PREEMPT) && !definedEx(CONFIG_PREEMPT_TRACER)
do { (current_thread_info()->preempt_count) -= (1); } while (0)
#endif
#if !definedEx(CONFIG_DEBUG_PREEMPT) && definedEx(CONFIG_PREEMPT_TRACER) || definedEx(CONFIG_DEBUG_PREEMPT)
sub_preempt_count(1)
#endif
; } while (0); __asm__ __volatile__("": : :"memory"); do { if (__builtin_expect(!!(test_ti_thread_flag(current_thread_info(), 3)), 0)) preempt_schedule(); } while (0); } while (0)
#endif
#if !definedEx(CONFIG_PREEMPT)
do { } while (0)
#endif
; (void)0; (void)(&lock->rlock); } while (0)
#endif
#if !definedEx(CONFIG_SMP) && definedEx(CONFIG_DEBUG_SPINLOCK) && !definedEx(CONFIG_INLINE_SPIN_UNLOCK) || definedEx(CONFIG_SMP) && !definedEx(CONFIG_INLINE_SPIN_UNLOCK)
_raw_spin_unlock(&lock->rlock)
#endif
;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void spin_unlock_bh(spinlock_t *lock)
{
#if !definedEx(CONFIG_SMP) && !definedEx(CONFIG_DEBUG_SPINLOCK)
do { 
#if definedEx(CONFIG_PREEMPT)
do { __asm__ __volatile__("": : :"memory"); 
#if !definedEx(CONFIG_DEBUG_PREEMPT) && !definedEx(CONFIG_PREEMPT_TRACER)
do { (current_thread_info()->preempt_count) -= (1); } while (0)
#endif
#if !definedEx(CONFIG_DEBUG_PREEMPT) && definedEx(CONFIG_PREEMPT_TRACER) || definedEx(CONFIG_DEBUG_PREEMPT)
sub_preempt_count(1)
#endif
; } while (0)
#endif
#if !definedEx(CONFIG_PREEMPT)
do { } while (0)
#endif
; local_bh_enable(); (void)0; (void)(&lock->rlock); } while (0)
#endif
#if !definedEx(CONFIG_SMP) && definedEx(CONFIG_DEBUG_SPINLOCK) || definedEx(CONFIG_SMP)
_raw_spin_unlock_bh(&lock->rlock)
#endif
;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void spin_unlock_irq(spinlock_t *lock)
{
#if !definedEx(CONFIG_SMP) && definedEx(CONFIG_DEBUG_SPINLOCK) && definedEx(CONFIG_INLINE_SPIN_UNLOCK_IRQ) || definedEx(CONFIG_SMP) && definedEx(CONFIG_INLINE_SPIN_UNLOCK_IRQ)
__raw_spin_unlock_irq(&lock->rlock)
#endif
#if !definedEx(CONFIG_SMP) && !definedEx(CONFIG_DEBUG_SPINLOCK)
do { do { 
#if !definedEx(CONFIG_TRACE_IRQFLAGS)
do { } while (0)
#endif
#if definedEx(CONFIG_TRACE_IRQFLAGS)
trace_hardirqs_on()
#endif
; raw_local_irq_enable(); } while (0); do { 
#if definedEx(CONFIG_PREEMPT)
do { do { __asm__ __volatile__("": : :"memory"); 
#if !definedEx(CONFIG_DEBUG_PREEMPT) && !definedEx(CONFIG_PREEMPT_TRACER)
do { (current_thread_info()->preempt_count) -= (1); } while (0)
#endif
#if !definedEx(CONFIG_DEBUG_PREEMPT) && definedEx(CONFIG_PREEMPT_TRACER) || definedEx(CONFIG_DEBUG_PREEMPT)
sub_preempt_count(1)
#endif
; } while (0); __asm__ __volatile__("": : :"memory"); do { if (__builtin_expect(!!(test_ti_thread_flag(current_thread_info(), 3)), 0)) preempt_schedule(); } while (0); } while (0)
#endif
#if !definedEx(CONFIG_PREEMPT)
do { } while (0)
#endif
; (void)0; (void)(&lock->rlock); } while (0); } while (0)
#endif
#if !definedEx(CONFIG_SMP) && definedEx(CONFIG_DEBUG_SPINLOCK) && !definedEx(CONFIG_INLINE_SPIN_UNLOCK_IRQ) || definedEx(CONFIG_SMP) && !definedEx(CONFIG_INLINE_SPIN_UNLOCK_IRQ)
_raw_spin_unlock_irq(&lock->rlock)
#endif
;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void spin_unlock_irqrestore(spinlock_t *lock, unsigned long flags)
{
	do { ({ unsigned long __dummy; typeof(flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); 
#if !definedEx(CONFIG_SMP) && !definedEx(CONFIG_DEBUG_SPINLOCK)
do { do { ({ unsigned long __dummy; typeof(flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); if (raw_irqs_disabled_flags(flags)) { raw_local_irq_restore(flags); 
#if !definedEx(CONFIG_TRACE_IRQFLAGS)
do { } while (0)
#endif
#if definedEx(CONFIG_TRACE_IRQFLAGS)
trace_hardirqs_off()
#endif
; } else { 
#if !definedEx(CONFIG_TRACE_IRQFLAGS)
do { } while (0)
#endif
#if definedEx(CONFIG_TRACE_IRQFLAGS)
trace_hardirqs_on()
#endif
; raw_local_irq_restore(flags); } } while (0); do { 
#if definedEx(CONFIG_PREEMPT)
do { do { __asm__ __volatile__("": : :"memory"); 
#if !definedEx(CONFIG_DEBUG_PREEMPT) && !definedEx(CONFIG_PREEMPT_TRACER)
do { (current_thread_info()->preempt_count) -= (1); } while (0)
#endif
#if !definedEx(CONFIG_DEBUG_PREEMPT) && definedEx(CONFIG_PREEMPT_TRACER) || definedEx(CONFIG_DEBUG_PREEMPT)
sub_preempt_count(1)
#endif
; } while (0); __asm__ __volatile__("": : :"memory"); do { if (__builtin_expect(!!(test_ti_thread_flag(current_thread_info(), 3)), 0)) preempt_schedule(); } while (0); } while (0)
#endif
#if !definedEx(CONFIG_PREEMPT)
do { } while (0)
#endif
; (void)0; (void)(&lock->rlock); } while (0); } while (0)
#endif
#if !definedEx(CONFIG_SMP) && definedEx(CONFIG_DEBUG_SPINLOCK) || definedEx(CONFIG_SMP)
_raw_spin_unlock_irqrestore(&lock->rlock, flags)
#endif
; } while (0);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int spin_trylock_bh(spinlock_t *lock)
{
	return (
#if !definedEx(CONFIG_SMP) && !definedEx(CONFIG_DEBUG_SPINLOCK)
({ 
do { local_bh_disable(); 
do { 
#if definedEx(CONFIG_PREEMPT)
do { 
#if !definedEx(CONFIG_DEBUG_PREEMPT) && !definedEx(CONFIG_PREEMPT_TRACER)
do { (current_thread_info()->preempt_count) += (1); } while (0)
#endif
#if !definedEx(CONFIG_DEBUG_PREEMPT) && definedEx(CONFIG_PREEMPT_TRACER) || definedEx(CONFIG_DEBUG_PREEMPT)
add_preempt_count(1)
#endif
; __asm__ __volatile__("": : :"memory"); } while (0)
#endif
#if !definedEx(CONFIG_PREEMPT)
do { } while (0)
#endif
; (void)0; (void)(&lock->rlock); } while (0)
; } while (0)
; 1; })
#endif
#if !definedEx(CONFIG_SMP) && definedEx(CONFIG_DEBUG_SPINLOCK) || definedEx(CONFIG_SMP)
_raw_spin_trylock_bh(&lock->rlock)
#endif
);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int spin_trylock_irq(spinlock_t *lock)
{
	return ({ do { raw_local_irq_disable(); 
#if !definedEx(CONFIG_TRACE_IRQFLAGS)
do { } while (0)
#endif
#if definedEx(CONFIG_TRACE_IRQFLAGS)
trace_hardirqs_off()
#endif
; } while (0); (
#if !definedEx(CONFIG_SMP) && !definedEx(CONFIG_DEBUG_SPINLOCK)
({ 
do { 
#if definedEx(CONFIG_PREEMPT)
do { 
#if !definedEx(CONFIG_DEBUG_PREEMPT) && !definedEx(CONFIG_PREEMPT_TRACER)
do { (current_thread_info()->preempt_count) += (1); } while (0)
#endif
#if !definedEx(CONFIG_DEBUG_PREEMPT) && definedEx(CONFIG_PREEMPT_TRACER) || definedEx(CONFIG_DEBUG_PREEMPT)
add_preempt_count(1)
#endif
; __asm__ __volatile__("": : :"memory"); } while (0)
#endif
#if !definedEx(CONFIG_PREEMPT)
do { } while (0)
#endif
; (void)0; (void)(&lock->rlock); } while (0)
; 1; })
#endif
#if !definedEx(CONFIG_SMP) && definedEx(CONFIG_DEBUG_SPINLOCK) || definedEx(CONFIG_SMP)
_raw_spin_trylock(&lock->rlock)
#endif
) ? 1 : ({ do { 
#if !definedEx(CONFIG_TRACE_IRQFLAGS)
do { } while (0)
#endif
#if definedEx(CONFIG_TRACE_IRQFLAGS)
trace_hardirqs_on()
#endif
; raw_local_irq_enable(); } while (0); 0; }); });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void spin_unlock_wait(spinlock_t *lock)
{
#if !definedEx(CONFIG_SMP)
do { cpu_relax(); } while (
#if !definedEx(CONFIG_SMP) && definedEx(CONFIG_DEBUG_SPINLOCK)
((&(&lock->rlock)->raw_lock)->slock == 0)
#endif
#if !definedEx(CONFIG_SMP) && !definedEx(CONFIG_DEBUG_SPINLOCK)
((void)(&(&lock->rlock)->raw_lock), 0)
#endif
)
#endif
#if definedEx(CONFIG_SMP)
arch_spin_unlock_wait(&(&lock->rlock)->raw_lock)
#endif
;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int spin_is_locked(spinlock_t *lock)
{
	return 
#if !definedEx(CONFIG_SMP) && definedEx(CONFIG_DEBUG_SPINLOCK)
((&(&lock->rlock)->raw_lock)->slock == 0)
#endif
#if !definedEx(CONFIG_SMP) && !definedEx(CONFIG_DEBUG_SPINLOCK)
((void)(&(&lock->rlock)->raw_lock), 0)
#endif
#if definedEx(CONFIG_SMP)
arch_spin_is_locked(&(&lock->rlock)->raw_lock)
#endif
;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int spin_is_contended(spinlock_t *lock)
{
	return 
#if !definedEx(CONFIG_SMP) || definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_SPINLOCKS) || definedEx(CONFIG_SMP) && definedEx(CONFIG_PARAVIRT)
#if definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_PARAVIRT_SPINLOCKS) || definedEx(CONFIG_SMP) && definedEx(CONFIG_PARAVIRT)
arch_spin_is_contended(&(&lock->rlock)->raw_lock)
#endif
#if !definedEx(CONFIG_SMP)
(((void)(&(&lock->rlock)->raw_lock), 0))
#endif
#endif
#if definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_PARAVIRT_SPINLOCKS)
(((void)(&lock->rlock), 0))
#endif
;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int spin_can_lock(spinlock_t *lock)
{
	return (!
#if !definedEx(CONFIG_SMP) && definedEx(CONFIG_DEBUG_SPINLOCK)
((&(&lock->rlock)->raw_lock)->slock == 0)
#endif
#if !definedEx(CONFIG_SMP) && !definedEx(CONFIG_DEBUG_SPINLOCK)
((void)(&(&lock->rlock)->raw_lock), 0)
#endif
#if definedEx(CONFIG_SMP)
arch_spin_is_locked(&(&lock->rlock)->raw_lock)
#endif
);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void assert_spin_locked(spinlock_t *lock)
{
#if !definedEx(CONFIG_SMP) && definedEx(CONFIG_DEBUG_SPINLOCK) || definedEx(CONFIG_SMP)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(!
#if !definedEx(CONFIG_SMP) && definedEx(CONFIG_DEBUG_SPINLOCK)
((&(&lock->rlock)->raw_lock)->slock == 0)
#endif
#if !definedEx(CONFIG_SMP) && !definedEx(CONFIG_DEBUG_SPINLOCK) || definedEx(CONFIG_SMP)
 arch_spin_is_locked(&(&lock->rlock)->raw_lock)
#endif
), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/spinlock.h"), "i" (376), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (!
#if !definedEx(CONFIG_SMP) && definedEx(CONFIG_DEBUG_SPINLOCK)
((&(&lock->rlock)->raw_lock)->slock == 0)
#endif
#if !definedEx(CONFIG_SMP) && !definedEx(CONFIG_DEBUG_SPINLOCK) || definedEx(CONFIG_SMP)
arch_spin_is_locked(&(&lock->rlock)->raw_lock)
#endif
) ; } while(0)
#endif
#endif
#if !definedEx(CONFIG_SMP) && !definedEx(CONFIG_DEBUG_SPINLOCK)
do { (void)(&lock->rlock); } while (0)
#endif
;
}
/*
 * Pull the atomic_t declaration:
 * (asm-mips/atomic.h needs above definitions)
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic.h" 1
#if definedEx(CONFIG_X86_32)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic_32.h" 1
#line 4 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic.h" 2
#endif
#if !definedEx(CONFIG_X86_32)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic_64.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic.h" 2
#endif
#line 385 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/spinlock.h" 2
/**
 * atomic_dec_and_lock - lock on reaching reference count zero
 * @atomic: the atomic counter
 * @lock: the spinlock in question
 *
 * Decrements @atomic by 1.  If the result is 0, returns true and locks
 * @lock.  Returns false for all other cases.
 */
extern int _atomic_dec_and_lock(atomic_t *atomic, spinlock_t *lock);
#line 31 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/seqlock.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/preempt.h" 1
#line 32 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/seqlock.h" 2
typedef struct {
	unsigned sequence;
	spinlock_t lock;
} seqlock_t;
/*
 * These macros triggered gcc-3.x compile-time problems.  We think these are
 * OK now.  Be cautious.
 */
/* Lock out other writers and update the count.
 * Acts like a normal spin_lock/unlock.
 * Don't need preempt_disable() because that is in the spin_lock already.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void write_seqlock(seqlock_t *sl)
{
	spin_lock(&sl->lock);
	++sl->sequence;
#if definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_OOSTORE)
#if definedEx(CONFIG_X86_32)
asm volatile ("661:\n\t" "lock; addl $0,0(%%esp)" "\n662:\n" ".section .altinstructions,\"a\"\n" " " ".balign 4" " " "\n" " " ".long" " " "661b\n" " " ".long" " " "663f\n" "	 .byte " "(0*32+25)" "\n" "	 .byte 662b-661b\n" "	 .byte 664f-663f\n" "	 .byte 0xff + (664f-663f) - (662b-661b)\n" ".previous\n" ".section .altinstr_replacement, \"ax\"\n" "663:\n\t" "sfence" "\n664:\n" ".previous" : : : "memory")
#endif
#if !definedEx(CONFIG_X86_32)
asm volatile("sfence" ::: "memory")
#endif
#endif
#if !definedEx(CONFIG_SMP) || definedEx(CONFIG_SMP) && !definedEx(CONFIG_X86_OOSTORE)
__asm__ __volatile__("": : :"memory")
#endif
;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void write_sequnlock(seqlock_t *sl)
{
#if definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_OOSTORE)
#if definedEx(CONFIG_X86_32)
asm volatile ("661:\n\t" "lock; addl $0,0(%%esp)" "\n662:\n" ".section .altinstructions,\"a\"\n" " " ".balign 4" " " "\n" " " ".long" " " "661b\n" " " ".long" " " "663f\n" "	 .byte " "(0*32+25)" "\n" "	 .byte 662b-661b\n" "	 .byte 664f-663f\n" "	 .byte 0xff + (664f-663f) - (662b-661b)\n" ".previous\n" ".section .altinstr_replacement, \"ax\"\n" "663:\n\t" "sfence" "\n664:\n" ".previous" : : : "memory")
#endif
#if !definedEx(CONFIG_X86_32)
asm volatile("sfence" ::: "memory")
#endif
#endif
#if !definedEx(CONFIG_SMP) || definedEx(CONFIG_SMP) && !definedEx(CONFIG_X86_OOSTORE)
__asm__ __volatile__("": : :"memory")
#endif
;
	sl->sequence++;
	spin_unlock(&sl->lock);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int write_tryseqlock(seqlock_t *sl)
{
	int ret = spin_trylock(&sl->lock);
	if (ret) {
		++sl->sequence;
#if definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_OOSTORE)
#if definedEx(CONFIG_X86_32)
asm volatile ("661:\n\t" "lock; addl $0,0(%%esp)" "\n662:\n" ".section .altinstructions,\"a\"\n" " " ".balign 4" " " "\n" " " ".long" " " "661b\n" " " ".long" " " "663f\n" "	 .byte " "(0*32+25)" "\n" "	 .byte 662b-661b\n" "	 .byte 664f-663f\n" "	 .byte 0xff + (664f-663f) - (662b-661b)\n" ".previous\n" ".section .altinstr_replacement, \"ax\"\n" "663:\n\t" "sfence" "\n664:\n" ".previous" : : : "memory")
#endif
#if !definedEx(CONFIG_X86_32)
asm volatile("sfence" ::: "memory")
#endif
#endif
#if !definedEx(CONFIG_SMP) || definedEx(CONFIG_SMP) && !definedEx(CONFIG_X86_OOSTORE)
__asm__ __volatile__("": : :"memory")
#endif
;
	}
	return ret;
}
/* Start of read calculation -- fetch last complete writer token */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __attribute__((always_inline)) unsigned read_seqbegin(const seqlock_t *sl)
{
	unsigned ret;
repeat:
	ret = sl->sequence;
#if definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_PPRO_FENCE)
#if definedEx(CONFIG_X86_32)
asm volatile ("661:\n\t" "lock; addl $0,0(%%esp)" "\n662:\n" ".section .altinstructions,\"a\"\n" " " ".balign 4" " " "\n" " " ".long" " " "661b\n" " " ".long" " " "663f\n" "	 .byte " "(0*32+26)" "\n" "	 .byte 662b-661b\n" "	 .byte 664f-663f\n" "	 .byte 0xff + (664f-663f) - (662b-661b)\n" ".previous\n" ".section .altinstr_replacement, \"ax\"\n" "663:\n\t" "lfence" "\n664:\n" ".previous" : : : "memory")
#endif
#if !definedEx(CONFIG_X86_32)
asm volatile("lfence":::"memory")
#endif
#endif
#if !definedEx(CONFIG_SMP) || definedEx(CONFIG_SMP) && !definedEx(CONFIG_X86_PPRO_FENCE)
__asm__ __volatile__("": : :"memory")
#endif
;
	if (__builtin_expect(!!(ret & 1), 0)) {
		cpu_relax();
		goto repeat;
	}
	return ret;
}
/*
 * Test if reader processed invalid data.
 *
 * If sequence value changed then writer changed data while in section.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __attribute__((always_inline)) int read_seqretry(const seqlock_t *sl, unsigned start)
{
#if definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_PPRO_FENCE)
#if definedEx(CONFIG_X86_32)
asm volatile ("661:\n\t" "lock; addl $0,0(%%esp)" "\n662:\n" ".section .altinstructions,\"a\"\n" " " ".balign 4" " " "\n" " " ".long" " " "661b\n" " " ".long" " " "663f\n" "	 .byte " "(0*32+26)" "\n" "	 .byte 662b-661b\n" "	 .byte 664f-663f\n" "	 .byte 0xff + (664f-663f) - (662b-661b)\n" ".previous\n" ".section .altinstr_replacement, \"ax\"\n" "663:\n\t" "lfence" "\n664:\n" ".previous" : : : "memory")
#endif
#if !definedEx(CONFIG_X86_32)
asm volatile("lfence":::"memory")
#endif
#endif
#if !definedEx(CONFIG_SMP) || definedEx(CONFIG_SMP) && !definedEx(CONFIG_X86_PPRO_FENCE)
__asm__ __volatile__("": : :"memory")
#endif
;
	return (sl->sequence != start);
}
/*
 * Version using sequence counter only.
 * This can be used when code has its own mutex protecting the
 * updating starting before the write_seqcountbeqin() and ending
 * after the write_seqcount_end().
 */
typedef struct seqcount {
	unsigned sequence;
} seqcount_t;
/* Start of read using pointer to a sequence counter only.  */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned read_seqcount_begin(const seqcount_t *s)
{
	unsigned ret;
repeat:
	ret = s->sequence;
#if definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_PPRO_FENCE)
#if definedEx(CONFIG_X86_32)
asm volatile ("661:\n\t" "lock; addl $0,0(%%esp)" "\n662:\n" ".section .altinstructions,\"a\"\n" " " ".balign 4" " " "\n" " " ".long" " " "661b\n" " " ".long" " " "663f\n" "	 .byte " "(0*32+26)" "\n" "	 .byte 662b-661b\n" "	 .byte 664f-663f\n" "	 .byte 0xff + (664f-663f) - (662b-661b)\n" ".previous\n" ".section .altinstr_replacement, \"ax\"\n" "663:\n\t" "lfence" "\n664:\n" ".previous" : : : "memory")
#endif
#if !definedEx(CONFIG_X86_32)
asm volatile("lfence":::"memory")
#endif
#endif
#if !definedEx(CONFIG_SMP) || definedEx(CONFIG_SMP) && !definedEx(CONFIG_X86_PPRO_FENCE)
__asm__ __volatile__("": : :"memory")
#endif
;
	if (__builtin_expect(!!(ret & 1), 0)) {
		cpu_relax();
		goto repeat;
	}
	return ret;
}
/*
 * Test if reader processed invalid data because sequence number has changed.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int read_seqcount_retry(const seqcount_t *s, unsigned start)
{
#if definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_PPRO_FENCE)
#if definedEx(CONFIG_X86_32)
asm volatile ("661:\n\t" "lock; addl $0,0(%%esp)" "\n662:\n" ".section .altinstructions,\"a\"\n" " " ".balign 4" " " "\n" " " ".long" " " "661b\n" " " ".long" " " "663f\n" "	 .byte " "(0*32+26)" "\n" "	 .byte 662b-661b\n" "	 .byte 664f-663f\n" "	 .byte 0xff + (664f-663f) - (662b-661b)\n" ".previous\n" ".section .altinstr_replacement, \"ax\"\n" "663:\n\t" "lfence" "\n664:\n" ".previous" : : : "memory")
#endif
#if !definedEx(CONFIG_X86_32)
asm volatile("lfence":::"memory")
#endif
#endif
#if !definedEx(CONFIG_SMP) || definedEx(CONFIG_SMP) && !definedEx(CONFIG_X86_PPRO_FENCE)
__asm__ __volatile__("": : :"memory")
#endif
;
	return s->sequence != start;
}
/*
 * Sequence counter only version assumes that callers are using their
 * own mutexing.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void write_seqcount_begin(seqcount_t *s)
{
	s->sequence++;
#if definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_OOSTORE)
#if definedEx(CONFIG_X86_32)
asm volatile ("661:\n\t" "lock; addl $0,0(%%esp)" "\n662:\n" ".section .altinstructions,\"a\"\n" " " ".balign 4" " " "\n" " " ".long" " " "661b\n" " " ".long" " " "663f\n" "	 .byte " "(0*32+25)" "\n" "	 .byte 662b-661b\n" "	 .byte 664f-663f\n" "	 .byte 0xff + (664f-663f) - (662b-661b)\n" ".previous\n" ".section .altinstr_replacement, \"ax\"\n" "663:\n\t" "sfence" "\n664:\n" ".previous" : : : "memory")
#endif
#if !definedEx(CONFIG_X86_32)
asm volatile("sfence" ::: "memory")
#endif
#endif
#if !definedEx(CONFIG_SMP) || definedEx(CONFIG_SMP) && !definedEx(CONFIG_X86_OOSTORE)
__asm__ __volatile__("": : :"memory")
#endif
;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void write_seqcount_end(seqcount_t *s)
{
#if definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_OOSTORE)
#if definedEx(CONFIG_X86_32)
asm volatile ("661:\n\t" "lock; addl $0,0(%%esp)" "\n662:\n" ".section .altinstructions,\"a\"\n" " " ".balign 4" " " "\n" " " ".long" " " "661b\n" " " ".long" " " "663f\n" "	 .byte " "(0*32+25)" "\n" "	 .byte 662b-661b\n" "	 .byte 664f-663f\n" "	 .byte 0xff + (664f-663f) - (662b-661b)\n" ".previous\n" ".section .altinstr_replacement, \"ax\"\n" "663:\n\t" "sfence" "\n664:\n" ".previous" : : : "memory")
#endif
#if !definedEx(CONFIG_X86_32)
asm volatile("sfence" ::: "memory")
#endif
#endif
#if !definedEx(CONFIG_SMP) || definedEx(CONFIG_SMP) && !definedEx(CONFIG_X86_OOSTORE)
__asm__ __volatile__("": : :"memory")
#endif
;
	s->sequence++;
}
/*
 * Possible sw/hw IRQ protected versions of the interfaces.
 */
#line 10 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/time.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/math64.h" 1
#line 11 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/time.h" 2
struct timespec {
	__kernel_time_t	tv_sec;			/* seconds */
	long		tv_nsec;		/* nanoseconds */
};
struct timeval {
	__kernel_time_t		tv_sec;		/* seconds */
	__kernel_suseconds_t	tv_usec;	/* microseconds */
};
struct timezone {
	int	tz_minuteswest;	/* minutes west of Greenwich */
	int	tz_dsttime;	/* type of dst correction */
};
extern struct timezone sys_tz;
/* Parameters used to convert the timespec values: */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int timespec_equal(const struct timespec *a,
                                 const struct timespec *b)
{
	return (a->tv_sec == b->tv_sec) && (a->tv_nsec == b->tv_nsec);
}
/*
 * lhs < rhs:  return <0
 * lhs == rhs: return 0
 * lhs > rhs:  return >0
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int timespec_compare(const struct timespec *lhs, const struct timespec *rhs)
{
	if (lhs->tv_sec < rhs->tv_sec)
		return -1;
	if (lhs->tv_sec > rhs->tv_sec)
		return 1;
	return lhs->tv_nsec - rhs->tv_nsec;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int timeval_compare(const struct timeval *lhs, const struct timeval *rhs)
{
	if (lhs->tv_sec < rhs->tv_sec)
		return -1;
	if (lhs->tv_sec > rhs->tv_sec)
		return 1;
	return lhs->tv_usec - rhs->tv_usec;
}
extern unsigned long mktime(const unsigned int year, const unsigned int mon,
			    const unsigned int day, const unsigned int hour,
			    const unsigned int min, const unsigned int sec);
extern void set_normalized_timespec(struct timespec *ts, time_t sec, s64 nsec);
extern struct timespec timespec_add_safe(const struct timespec lhs,
					 const struct timespec rhs);
/*
 * sub = lhs - rhs, in normalized form
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 struct timespec timespec_sub(struct timespec lhs,
						struct timespec rhs)
{
	struct timespec ts_delta;
	set_normalized_timespec(&ts_delta, lhs.tv_sec - rhs.tv_sec,
				lhs.tv_nsec - rhs.tv_nsec);
	return ts_delta;
}
/*
 * Returns true if the timespec is norm, false if denorm:
 */
extern struct timespec xtime;
extern struct timespec wall_to_monotonic;
extern seqlock_t xtime_lock;
extern void read_persistent_clock(struct timespec *ts);
extern void read_boot_clock(struct timespec *ts);
extern int update_persistent_clock(struct timespec now);
extern int no_sync_cmos_clock __attribute__((__section__(".data.read_mostly")));
void timekeeping_init(void);
extern int timekeeping_suspended;
unsigned long get_seconds(void);
struct timespec current_kernel_time(void);
struct timespec __current_kernel_time(void); /* does not hold xtime_lock */
struct timespec get_monotonic_coarse(void);
/* Some architectures do not supply their own clocksource.
 * This is mainly the case in architectures that get their
 * inter-tick times by reading the counter on their interval
 * timer. Since these timers wrap every tick, they're not really
 * useful as clocksources. Wrapping them to act like one is possible
 * but not very efficient. So we provide a callout these arches
 * can implement for use with the jiffies clocksource to provide
 * finer then tick granular time.
 */
 static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 u32 arch_gettimeoffset(void) { return 0; }
extern void do_gettimeofday(struct timeval *tv);
extern int do_settimeofday(struct timespec *tv);
extern int do_sys_settimeofday(struct timespec *tv, struct timezone *tz);
extern long do_utimes(int dfd, char  *filename, struct timespec *times, int flags);
struct itimerval;
extern int do_setitimer(int which, struct itimerval *value,
			struct itimerval *ovalue);
extern unsigned int alarm_setitimer(unsigned int seconds);
extern int do_getitimer(int which, struct itimerval *value);
extern void getnstimeofday(struct timespec *tv);
extern void getrawmonotonic(struct timespec *ts);
extern void getboottime(struct timespec *ts);
extern void monotonic_to_bootbased(struct timespec *ts);
extern struct timespec timespec_trunc(struct timespec t, unsigned gran);
extern int timekeeping_valid_for_hres(void);
extern u64 timekeeping_max_deferment(void);
extern void update_wall_time(void);
extern void update_xtime_cache(u64 nsec);
extern void timekeeping_leap_insert(int leapsecond);
struct tms;
extern void do_sys_times(struct tms *);
/*
 * Similar to the struct tm in userspace <time.h>, but it needs to be here so
 * that the kernel source is self contained.
 */
struct tm {
	/*
	 * the number of seconds after the minute, normally in the range
	 * 0 to 59, but can be up to 60 to allow for leap seconds
	 */
	int tm_sec;
	/* the number of minutes after the hour, in the range 0 to 59*/
	int tm_min;
	/* the number of hours past midnight, in the range 0 to 23 */
	int tm_hour;
	/* the day of the month, in the range 1 to 31 */
	int tm_mday;
	/* the number of months since January, in the range 0 to 11 */
	int tm_mon;
	/* the number of years since 1900 */
	long tm_year;
	/* the number of days since Sunday, in the range 0 to 6 */
	int tm_wday;
	/* the number of days since January 1, in the range 0 to 365 */
	int tm_yday;
};
void time_to_tm(time_t totalsecs, int offset, struct tm *result);
/**
 * timespec_to_ns - Convert timespec to nanoseconds
 * @ts:		pointer to the timespec variable to be converted
 *
 * Returns the scalar nanosecond representation of the timespec
 * parameter.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 s64 timespec_to_ns(const struct timespec *ts)
{
	return ((s64) ts->tv_sec * 1000000000L) + ts->tv_nsec;
}
/**
 * timeval_to_ns - Convert timeval to nanoseconds
 * @ts:		pointer to the timeval variable to be converted
 *
 * Returns the scalar nanosecond representation of the timeval
 * parameter.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 s64 timeval_to_ns(const struct timeval *tv)
{
	return ((s64) tv->tv_sec * 1000000000L) +
		tv->tv_usec * 1000L;
}
/**
 * ns_to_timespec - Convert nanoseconds to timespec
 * @nsec:	the nanoseconds value to be converted
 *
 * Returns the timespec representation of the nsec parameter.
 */
extern struct timespec ns_to_timespec(const s64 nsec);
/**
 * ns_to_timeval - Convert nanoseconds to timeval
 * @nsec:	the nanoseconds value to be converted
 *
 * Returns the timeval representation of the nsec parameter.
 */
extern struct timeval ns_to_timeval(const s64 nsec);
/**
 * timespec_add_ns - Adds nanoseconds to a timespec
 * @a:		pointer to timespec to be incremented
 * @ns:		unsigned nanoseconds value to be added
 *
 * This must always be inlined because its used from the x86-64 vdso,
 * which cannot call other kernel functions.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __attribute__((always_inline)) void timespec_add_ns(struct timespec *a, u64 ns)
{
	a->tv_sec += __iter_div_u64_rem(a->tv_nsec + ns, 1000000000L, &ns);
	a->tv_nsec = ns;
}
/*
 * Names of the interval timers, and structure
 * defining a timer setting:
 */
struct itimerspec {
	struct timespec it_interval;	/* timer period */
	struct timespec it_value;	/* timer expiration */
};
struct itimerval {
	struct timeval it_interval;	/* timer interval */
	struct timeval it_value;	/* current value */
};
/*
 * The IDs of the various system clocks (for POSIX.1b interval timers):
 */
/*
 * The IDs of various hardware clocks:
 */
/*
 * The various flags for setting POSIX.1b interval timers:
 */
#line 62 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/stat.h" 2
struct kstat {
	u64		ino;
	dev_t		dev;
	umode_t		mode;
	unsigned int	nlink;
	uid_t		uid;
	gid_t		gid;
	dev_t		rdev;
	loff_t		size;
	struct timespec  atime;
	struct timespec	mtime;
	struct timespec	ctime;
	unsigned long	blksize;
	unsigned long long	blocks;
};
#line 12 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/module.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/compiler.h" 1
#line 13 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/module.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/cache.h" 1
#line 14 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/module.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/kmod.h" 1
/*
 *	include/linux/kmod.h
 *
 *      This program is free software; you can redistribute it and/or modify
 *      it under the terms of the GNU General Public License as published by
 *      the Free Software Foundation; either version 2 of the License, or
 *      (at your option) any later version.
 *
 *      This program is distributed in the hope that it will be useful,
 *      but WITHOUT ANY WARRANTY; without even the implied warranty of
 *      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *      GNU General Public License for more details.
 *
 *      You should have received a copy of the GNU General Public License
 *      along with this program; if not, write to the Free Software
 *      Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/gfp.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/mmzone.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/spinlock.h" 1
#line 9 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/mmzone.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/list.h" 1
#line 10 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/mmzone.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/wait.h" 1
/* First argument to waitid: */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/list.h" 1
#line 24 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/wait.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/stddef.h" 1
#line 25 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/wait.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/spinlock.h" 1
#line 26 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/wait.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/system.h" 1
#line 27 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/wait.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/current.h" 1
#line 28 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/wait.h" 2
typedef struct __wait_queue wait_queue_t;
typedef int (*wait_queue_func_t)(wait_queue_t *wait, unsigned mode, int flags, void *key);
int default_wake_function(wait_queue_t *wait, unsigned mode, int flags, void *key);
struct __wait_queue {
	unsigned int flags;
	void *private;
	wait_queue_func_t func;
	struct list_head task_list;
};
struct wait_bit_key {
	void *flags;
	int bit_nr;
};
struct wait_bit_queue {
	struct wait_bit_key key;
	wait_queue_t wait;
};
struct __wait_queue_head {
	spinlock_t lock;
	struct list_head task_list;
};
typedef struct __wait_queue_head wait_queue_head_t;
struct task_struct;
/*
 * Macros for declaration and initialisaton of the datatypes
 */
extern void __init_waitqueue_head(wait_queue_head_t *q, struct lock_class_key *);
#if definedEx(CONFIG_LOCKDEP)
#endif
#if !definedEx(CONFIG_LOCKDEP)
#endif
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void init_waitqueue_entry(wait_queue_t *q, struct task_struct *p)
{
	q->flags = 0;
	q->private = p;
	q->func = default_wake_function;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void init_waitqueue_func_entry(wait_queue_t *q,
					wait_queue_func_t func)
{
	q->flags = 0;
	q->private = ((void *)0);
	q->func = func;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int waitqueue_active(wait_queue_head_t *q)
{
	return !list_empty(&q->task_list);
}
extern void add_wait_queue(wait_queue_head_t *q, wait_queue_t *wait);
extern void add_wait_queue_exclusive(wait_queue_head_t *q, wait_queue_t *wait);
extern void remove_wait_queue(wait_queue_head_t *q, wait_queue_t *wait);
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __add_wait_queue(wait_queue_head_t *head, wait_queue_t *new)
{
	list_add(&new->task_list, &head->task_list);
}
/*
 * Used for wake-one threads:
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __add_wait_queue_tail(wait_queue_head_t *head,
						wait_queue_t *new)
{
	list_add_tail(&new->task_list, &head->task_list);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __remove_wait_queue(wait_queue_head_t *head,
							wait_queue_t *old)
{
	list_del(&old->task_list);
}
void __wake_up(wait_queue_head_t *q, unsigned int mode, int nr, void *key);
void __wake_up_locked_key(wait_queue_head_t *q, unsigned int mode, void *key);
void __wake_up_sync_key(wait_queue_head_t *q, unsigned int mode, int nr,
			void *key);
void __wake_up_locked(wait_queue_head_t *q, unsigned int mode);
void __wake_up_sync(wait_queue_head_t *q, unsigned int mode, int nr);
void __wake_up_bit(wait_queue_head_t *, void *, int);
int __wait_on_bit(wait_queue_head_t *, struct wait_bit_queue *, int (*)(void *), unsigned);
int __wait_on_bit_lock(wait_queue_head_t *, struct wait_bit_queue *, int (*)(void *), unsigned);
void wake_up_bit(void *, int);
int out_of_line_wait_on_bit(void *, int, int (*)(void *), unsigned);
int out_of_line_wait_on_bit_lock(void *, int, int (*)(void *), unsigned);
wait_queue_head_t *bit_waitqueue(void *, int);
/*
 * Wakeup macros to be used to report events to the targets.
 */
/**
 * wait_event - sleep until a condition gets true
 * @wq: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 *
 * The process is put to sleep (TASK_UNINTERRUPTIBLE) until the
 * @condition evaluates to true. The @condition is checked each time
 * the waitqueue @wq is woken up.
 *
 * wake_up() has to be called after changing any variable that could
 * change the result of the wait condition.
 */
/**
 * wait_event_timeout - sleep until a condition gets true or a timeout elapses
 * @wq: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 * @timeout: timeout, in jiffies
 *
 * The process is put to sleep (TASK_UNINTERRUPTIBLE) until the
 * @condition evaluates to true. The @condition is checked each time
 * the waitqueue @wq is woken up.
 *
 * wake_up() has to be called after changing any variable that could
 * change the result of the wait condition.
 *
 * The function returns 0 if the @timeout elapsed, and the remaining
 * jiffies if the condition evaluated to true before the timeout elapsed.
 */
/**
 * wait_event_interruptible - sleep until a condition gets true
 * @wq: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 *
 * The process is put to sleep (TASK_INTERRUPTIBLE) until the
 * @condition evaluates to true or a signal is received.
 * The @condition is checked each time the waitqueue @wq is woken up.
 *
 * wake_up() has to be called after changing any variable that could
 * change the result of the wait condition.
 *
 * The function will return -ERESTARTSYS if it was interrupted by a
 * signal and 0 if @condition evaluated to true.
 */
/**
 * wait_event_interruptible_timeout - sleep until a condition gets true or a timeout elapses
 * @wq: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 * @timeout: timeout, in jiffies
 *
 * The process is put to sleep (TASK_INTERRUPTIBLE) until the
 * @condition evaluates to true or a signal is received.
 * The @condition is checked each time the waitqueue @wq is woken up.
 *
 * wake_up() has to be called after changing any variable that could
 * change the result of the wait condition.
 *
 * The function returns 0 if the @timeout elapsed, -ERESTARTSYS if it
 * was interrupted by a signal, and the remaining jiffies otherwise
 * if the condition evaluated to true before the timeout elapsed.
 */
/**
 * wait_event_killable - sleep until a condition gets true
 * @wq: the waitqueue to wait on
 * @condition: a C expression for the event to wait for
 *
 * The process is put to sleep (TASK_KILLABLE) until the
 * @condition evaluates to true or a signal is received.
 * The @condition is checked each time the waitqueue @wq is woken up.
 *
 * wake_up() has to be called after changing any variable that could
 * change the result of the wait condition.
 *
 * The function will return -ERESTARTSYS if it was interrupted by a
 * signal and 0 if @condition evaluated to true.
 */
/*
 * Must be called with the spinlock in the wait_queue_head_t held.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void add_wait_queue_exclusive_locked(wait_queue_head_t *q,
						   wait_queue_t * wait)
{
	wait->flags |= 0x01;
	__add_wait_queue_tail(q,  wait);
}
/*
 * Must be called with the spinlock in the wait_queue_head_t held.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void remove_wait_queue_locked(wait_queue_head_t *q,
					    wait_queue_t * wait)
{
	__remove_wait_queue(q,  wait);
}
/*
 * These are the old interfaces to sleep waiting for an event.
 * They are racy.  DO NOT use them, use the wait_event* interfaces above.
 * We plan to remove these interfaces.
 */
extern void sleep_on(wait_queue_head_t *q);
extern long sleep_on_timeout(wait_queue_head_t *q,
				      signed long timeout);
extern void interruptible_sleep_on(wait_queue_head_t *q);
extern long interruptible_sleep_on_timeout(wait_queue_head_t *q,
					   signed long timeout);
/*
 * Waitqueues which are removed from the waitqueue_head at wakeup time
 */
void prepare_to_wait(wait_queue_head_t *q, wait_queue_t *wait, int state);
void prepare_to_wait_exclusive(wait_queue_head_t *q, wait_queue_t *wait, int state);
void finish_wait(wait_queue_head_t *q, wait_queue_t *wait);
void abort_exclusive_wait(wait_queue_head_t *q, wait_queue_t *wait,
			unsigned int mode, void *key);
int autoremove_wake_function(wait_queue_t *wait, unsigned mode, int sync, void *key);
int wake_bit_function(wait_queue_t *wait, unsigned mode, int sync, void *key);
/**
 * wait_on_bit - wait for a bit to be cleared
 * @word: the word being waited on, a kernel virtual address
 * @bit: the bit of the word being waited on
 * @action: the function used to sleep, which may take special actions
 * @mode: the task state to sleep in
 *
 * There is a standard hashed waitqueue table for generic use. This
 * is the part of the hashtable's accessor API that waits on a bit.
 * For instance, if one were to have waiters on a bitflag, one would
 * call wait_on_bit() in threads waiting for the bit to clear.
 * One uses wait_on_bit() where one is waiting for the bit to clear,
 * but has no intention of setting it.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int wait_on_bit(void *word, int bit,
				int (*action)(void *), unsigned mode)
{
	if (!(__builtin_constant_p((bit)) ? constant_test_bit((bit), (word)) : variable_test_bit((bit), (word))))
		return 0;
	return out_of_line_wait_on_bit(word, bit, action, mode);
}
/**
 * wait_on_bit_lock - wait for a bit to be cleared, when wanting to set it
 * @word: the word being waited on, a kernel virtual address
 * @bit: the bit of the word being waited on
 * @action: the function used to sleep, which may take special actions
 * @mode: the task state to sleep in
 *
 * There is a standard hashed waitqueue table for generic use. This
 * is the part of the hashtable's accessor API that waits on a bit
 * when one intends to set it, for instance, trying to lock bitflags.
 * For instance, if one were to have waiters trying to set bitflag
 * and waiting for it to clear before setting it, one would call
 * wait_on_bit() in threads waiting to be able to set the bit.
 * One uses wait_on_bit_lock() where one is waiting for the bit to
 * clear with the intention of setting it, and when done, clearing it.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int wait_on_bit_lock(void *word, int bit,
				int (*action)(void *), unsigned mode)
{
	if (!test_and_set_bit(bit, word))
		return 0;
	return out_of_line_wait_on_bit_lock(word, bit, action, mode);
}
#line 11 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/mmzone.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/bitops.h" 1
#line 12 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/mmzone.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/cache.h" 1
#line 13 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/mmzone.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/threads.h" 1
#line 14 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/mmzone.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/numa.h" 1
#if !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_NODES_SHIFT) || definedEx(CONFIG_NEED_MULTIPLE_NODES)
#endif
#if !definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_NODES_SHIFT)
#endif
#line 15 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/mmzone.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/init.h" 1
#line 16 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/mmzone.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/seqlock.h" 1
#line 17 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/mmzone.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/nodemask.h" 1
/*
 * Nodemasks provide a bitmap suitable for representing the
 * set of Node's in a system, one bit position per Node number.
 *
 * See detailed comments in the file linux/bitmap.h describing the
 * data type on which these nodemasks are based.
 *
 * For details of nodemask_scnprintf() and nodemask_parse_user(),
 * see bitmap_scnprintf() and bitmap_parse_user() in lib/bitmap.c.
 * For details of nodelist_scnprintf() and nodelist_parse(), see
 * bitmap_scnlistprintf() and bitmap_parselist(), also in bitmap.c.
 * For details of node_remap(), see bitmap_bitremap in lib/bitmap.c.
 * For details of nodes_remap(), see bitmap_remap in lib/bitmap.c.
 * For details of nodes_onto(), see bitmap_onto in lib/bitmap.c.
 * For details of nodes_fold(), see bitmap_fold in lib/bitmap.c.
 *
 * The available nodemask operations are:
 *
 * void node_set(node, mask)		turn on bit 'node' in mask
 * void node_clear(node, mask)		turn off bit 'node' in mask
 * void nodes_setall(mask)		set all bits
 * void nodes_clear(mask)		clear all bits
 * int node_isset(node, mask)		true iff bit 'node' set in mask
 * int node_test_and_set(node, mask)	test and set bit 'node' in mask
 *
 * void nodes_and(dst, src1, src2)	dst = src1 & src2  [intersection]
 * void nodes_or(dst, src1, src2)	dst = src1 | src2  [union]
 * void nodes_xor(dst, src1, src2)	dst = src1 ^ src2
 * void nodes_andnot(dst, src1, src2)	dst = src1 & ~src2
 * void nodes_complement(dst, src)	dst = ~src
 *
 * int nodes_equal(mask1, mask2)	Does mask1 == mask2?
 * int nodes_intersects(mask1, mask2)	Do mask1 and mask2 intersect?
 * int nodes_subset(mask1, mask2)	Is mask1 a subset of mask2?
 * int nodes_empty(mask)		Is mask empty (no bits sets)?
 * int nodes_full(mask)			Is mask full (all bits sets)?
 * int nodes_weight(mask)		Hamming weight - number of set bits
 *
 * void nodes_shift_right(dst, src, n)	Shift right
 * void nodes_shift_left(dst, src, n)	Shift left
 *
 * int first_node(mask)			Number lowest set bit, or MAX_NUMNODES
 * int next_node(node, mask)		Next node past 'node', or MAX_NUMNODES
 * int first_unset_node(mask)		First node not set in mask, or 
 *					MAX_NUMNODES.
 *
 * nodemask_t nodemask_of_node(node)	Return nodemask with bit 'node' set
 * NODE_MASK_ALL			Initializer - all bits set
 * NODE_MASK_NONE			Initializer - no bits set
 * unsigned long *nodes_addr(mask)	Array of unsigned long's in mask
 *
 * int nodemask_scnprintf(buf, len, mask) Format nodemask for printing
 * int nodemask_parse_user(ubuf, ulen, mask)	Parse ascii string as nodemask
 * int nodelist_scnprintf(buf, len, mask) Format nodemask as list for printing
 * int nodelist_parse(buf, map)		Parse ascii string as nodelist
 * int node_remap(oldbit, old, new)	newbit = map(old, new)(oldbit)
 * void nodes_remap(dst, src, old, new)	*dst = map(old, new)(src)
 * void nodes_onto(dst, orig, relmap)	*dst = orig relative to relmap
 * void nodes_fold(dst, orig, sz)	dst bits = orig bits mod sz
 *
 * for_each_node_mask(node, mask)	for-loop node over mask
 *
 * int num_online_nodes()		Number of online Nodes
 * int num_possible_nodes()		Number of all possible Nodes
 *
 * int node_online(node)		Is some node online?
 * int node_possible(node)		Is some node possible?
 *
 * int any_online_node(mask)		First online node in mask
 *
 * node_set_online(node)		set bit 'node' in node_online_map
 * node_set_offline(node)		clear bit 'node' in node_online_map
 *
 * for_each_node(node)			for-loop node over node_possible_map
 * for_each_online_node(node)		for-loop node over node_online_map
 *
 * Subtlety:
 * 1) The 'type-checked' form of node_isset() causes gcc (3.3.2, anyway)
 *    to generate slightly worse code.  So use a simple one-line #define
 *    for node_isset(), instead of wrapping an inline inside a macro, the
 *    way we do the other calls.
 *
 * NODEMASK_SCRATCH
 * When doing above logical AND, OR, XOR, Remap operations the callers tend to
 * need temporary nodemask_t's on the stack. But if NODES_SHIFT is large,
 * nodemask_t's consume too much stack space.  NODEMASK_SCRATCH is a helper
 * for such situations. See below and CPUMASK_ALLOC also.
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/kernel.h" 1
#line 95 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/nodemask.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/threads.h" 1
#line 96 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/nodemask.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/bitmap.h" 1
#line 97 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/nodemask.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/numa.h" 1
#line 98 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/nodemask.h" 2
typedef struct { unsigned long bits[((((1 << 
#if !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_NODES_SHIFT) || definedEx(CONFIG_NEED_MULTIPLE_NODES)
#if definedEx(CONFIG_NEED_MULTIPLE_NODES)
3
#endif
#if !definedEx(CONFIG_NEED_MULTIPLE_NODES)
 CONFIG_NODES_SHIFT
#endif
#endif
#if !definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_NODES_SHIFT)
0
#endif
)) + (8 * sizeof(long)) - 1) / (8 * sizeof(long)))]; } nodemask_t;
extern nodemask_t _unused_nodemask_arg_;
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __node_set(int node, volatile nodemask_t *dstp)
{
	set_bit(node, dstp->bits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __node_clear(int node, volatile nodemask_t *dstp)
{
	clear_bit(node, dstp->bits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __nodes_setall(nodemask_t *dstp, int nbits)
{
	bitmap_fill(dstp->bits, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __nodes_clear(nodemask_t *dstp, int nbits)
{
	bitmap_zero(dstp->bits, nbits);
}
/* No static inline type checking - see Subtlety (1) above. */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int __node_test_and_set(int node, nodemask_t *addr)
{
	return test_and_set_bit(node, addr->bits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __nodes_and(nodemask_t *dstp, const nodemask_t *src1p,
					const nodemask_t *src2p, int nbits)
{
	bitmap_and(dstp->bits, src1p->bits, src2p->bits, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __nodes_or(nodemask_t *dstp, const nodemask_t *src1p,
					const nodemask_t *src2p, int nbits)
{
	bitmap_or(dstp->bits, src1p->bits, src2p->bits, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __nodes_xor(nodemask_t *dstp, const nodemask_t *src1p,
					const nodemask_t *src2p, int nbits)
{
	bitmap_xor(dstp->bits, src1p->bits, src2p->bits, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __nodes_andnot(nodemask_t *dstp, const nodemask_t *src1p,
					const nodemask_t *src2p, int nbits)
{
	bitmap_andnot(dstp->bits, src1p->bits, src2p->bits, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __nodes_complement(nodemask_t *dstp,
					const nodemask_t *srcp, int nbits)
{
	bitmap_complement(dstp->bits, srcp->bits, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int __nodes_equal(const nodemask_t *src1p,
					const nodemask_t *src2p, int nbits)
{
	return bitmap_equal(src1p->bits, src2p->bits, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int __nodes_intersects(const nodemask_t *src1p,
					const nodemask_t *src2p, int nbits)
{
	return bitmap_intersects(src1p->bits, src2p->bits, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int __nodes_subset(const nodemask_t *src1p,
					const nodemask_t *src2p, int nbits)
{
	return bitmap_subset(src1p->bits, src2p->bits, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int __nodes_empty(const nodemask_t *srcp, int nbits)
{
	return bitmap_empty(srcp->bits, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int __nodes_full(const nodemask_t *srcp, int nbits)
{
	return bitmap_full(srcp->bits, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int __nodes_weight(const nodemask_t *srcp, int nbits)
{
	return bitmap_weight(srcp->bits, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __nodes_shift_right(nodemask_t *dstp,
					const nodemask_t *srcp, int n, int nbits)
{
	bitmap_shift_right(dstp->bits, srcp->bits, n, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __nodes_shift_left(nodemask_t *dstp,
					const nodemask_t *srcp, int n, int nbits)
{
	bitmap_shift_left(dstp->bits, srcp->bits, n, nbits);
}
/* FIXME: better would be to fix all architectures to never return
          > MAX_NUMNODES, then the silly min_ts could be dropped. */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int __first_node(const nodemask_t *srcp)
{
	return ({ int __min1 = ((1 << 
#if !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_NODES_SHIFT) || definedEx(CONFIG_NEED_MULTIPLE_NODES)
#if definedEx(CONFIG_NEED_MULTIPLE_NODES)
3
#endif
#if !definedEx(CONFIG_NEED_MULTIPLE_NODES)
CONFIG_NODES_SHIFT
#endif
#endif
#if !definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_NODES_SHIFT)
0
#endif
)); int __min2 = (find_first_bit(srcp->bits, (1 << 
#if !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_NODES_SHIFT) || definedEx(CONFIG_NEED_MULTIPLE_NODES)
#if definedEx(CONFIG_NEED_MULTIPLE_NODES)
3
#endif
#if !definedEx(CONFIG_NEED_MULTIPLE_NODES)
CONFIG_NODES_SHIFT
#endif
#endif
#if !definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_NODES_SHIFT)
0
#endif
))); __min1 < __min2 ? __min1: __min2; });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int __next_node(int n, const nodemask_t *srcp)
{
	return ({ int __min1 = ((1 << 
#if !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_NODES_SHIFT) || definedEx(CONFIG_NEED_MULTIPLE_NODES)
#if definedEx(CONFIG_NEED_MULTIPLE_NODES)
3
#endif
#if !definedEx(CONFIG_NEED_MULTIPLE_NODES)
CONFIG_NODES_SHIFT
#endif
#endif
#if !definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_NODES_SHIFT)
0
#endif
)); int __min2 = (find_next_bit(srcp->bits, (1 << 
#if !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_NODES_SHIFT) || definedEx(CONFIG_NEED_MULTIPLE_NODES)
#if definedEx(CONFIG_NEED_MULTIPLE_NODES)
3
#endif
#if !definedEx(CONFIG_NEED_MULTIPLE_NODES)
CONFIG_NODES_SHIFT
#endif
#endif
#if !definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_NODES_SHIFT)
0
#endif
), n+1)); __min1 < __min2 ? __min1: __min2; });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void init_nodemask_of_node(nodemask_t *mask, int node)
{
	__nodes_clear(&(*mask), (1 << 
#if !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_NODES_SHIFT) || definedEx(CONFIG_NEED_MULTIPLE_NODES)
#if definedEx(CONFIG_NEED_MULTIPLE_NODES)
3
#endif
#if !definedEx(CONFIG_NEED_MULTIPLE_NODES)
CONFIG_NODES_SHIFT
#endif
#endif
#if !definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_NODES_SHIFT)
0
#endif
));
	__node_set((node), &(*mask));
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int __first_unset_node(const nodemask_t *maskp)
{
	return ({ int __min1 = ((1 << 
#if !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_NODES_SHIFT) || definedEx(CONFIG_NEED_MULTIPLE_NODES)
#if definedEx(CONFIG_NEED_MULTIPLE_NODES)
3
#endif
#if !definedEx(CONFIG_NEED_MULTIPLE_NODES)
CONFIG_NODES_SHIFT
#endif
#endif
#if !definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_NODES_SHIFT)
0
#endif
)); int __min2 = (
 find_first_zero_bit(maskp->bits, (1 << 
#if !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_NODES_SHIFT) || definedEx(CONFIG_NEED_MULTIPLE_NODES)
#if definedEx(CONFIG_NEED_MULTIPLE_NODES)
3
#endif
#if !definedEx(CONFIG_NEED_MULTIPLE_NODES)
CONFIG_NODES_SHIFT
#endif
#endif
#if !definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_NODES_SHIFT)
0
#endif
))); __min1 < __min2 ? __min1: __min2; });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int __nodemask_scnprintf(char *buf, int len,
					const nodemask_t *srcp, int nbits)
{
	return bitmap_scnprintf(buf, len, srcp->bits, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int __nodemask_parse_user(const char  *buf, int len,
					nodemask_t *dstp, int nbits)
{
	return bitmap_parse_user(buf, len, dstp->bits, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int __nodelist_scnprintf(char *buf, int len,
					const nodemask_t *srcp, int nbits)
{
	return bitmap_scnlistprintf(buf, len, srcp->bits, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int __nodelist_parse(const char *buf, nodemask_t *dstp, int nbits)
{
	return bitmap_parselist(buf, dstp->bits, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int __node_remap(int oldbit,
		const nodemask_t *oldp, const nodemask_t *newp, int nbits)
{
	return bitmap_bitremap(oldbit, oldp->bits, newp->bits, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __nodes_remap(nodemask_t *dstp, const nodemask_t *srcp,
		const nodemask_t *oldp, const nodemask_t *newp, int nbits)
{
	bitmap_remap(dstp->bits, srcp->bits, oldp->bits, newp->bits, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __nodes_onto(nodemask_t *dstp, const nodemask_t *origp,
		const nodemask_t *relmapp, int nbits)
{
	bitmap_onto(dstp->bits, origp->bits, relmapp->bits, nbits);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __nodes_fold(nodemask_t *dstp, const nodemask_t *origp,
		int sz, int nbits)
{
	bitmap_fold(dstp->bits, origp->bits, sz, nbits);
}
#if definedEx(CONFIG_NEED_MULTIPLE_NODES)
#endif
#if !definedEx(CONFIG_NEED_MULTIPLE_NODES)
#endif
/*
 * Bitmasks that are kept for all the nodes.
 */
enum node_states {
	N_POSSIBLE,		/* The node could become online at some point */
	N_ONLINE,		/* The node is online */
	N_NORMAL_MEMORY,	/* The node has regular memory */
#if definedEx(CONFIG_HIGHMEM)
	N_HIGH_MEMORY,		/* The node has regular or high memory */
#endif
#if !definedEx(CONFIG_HIGHMEM)
	N_HIGH_MEMORY = N_NORMAL_MEMORY,
#endif
	N_CPU,		/* The node has one or more cpus */
	NR_NODE_STATES
};
/*
 * The following particular system nodemasks and operations
 * on them manage all possible and online nodes.
 */
extern nodemask_t node_states[NR_NODE_STATES];
#if definedEx(CONFIG_NEED_MULTIPLE_NODES)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int node_state(int node, enum node_states state)
{
	return (__builtin_constant_p(((node))) ? constant_test_bit(((node)), ((node_states[state]).bits)) : variable_test_bit(((node)), ((node_states[state]).bits)));
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void node_set_state(int node, enum node_states state)
{
	__node_set(node, &node_states[state]);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void node_clear_state(int node, enum node_states state)
{
	__node_clear(node, &node_states[state]);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int num_node_state(enum node_states state)
{
	return __nodes_weight(&(node_states[state]), (1 << 3));
}
extern int nr_node_ids;
extern int nr_online_nodes;
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void node_set_online(int nid)
{
	node_set_state(nid, N_ONLINE);
	nr_online_nodes = num_node_state(N_ONLINE);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void node_set_offline(int nid)
{
	node_clear_state(nid, N_ONLINE);
	nr_online_nodes = num_node_state(N_ONLINE);
}
#endif
#if !definedEx(CONFIG_NEED_MULTIPLE_NODES)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int node_state(int node, enum node_states state)
{
	return node == 0;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void node_set_state(int node, enum node_states state)
{
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void node_clear_state(int node, enum node_states state)
{
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int num_node_state(enum node_states state)
{
	return 1;
}
#endif
/*
 * For nodemask scrach area.
 * NODEMASK_ALLOC(type, name) allocates an object with a specified type and
 * name.
 */
/* A example struture for using NODEMASK_ALLOC, used in mempolicy. */
struct nodemask_scratch {
	nodemask_t	mask1;
	nodemask_t	mask2;
};
#line 18 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/mmzone.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/pageblock-flags.h" 1
/*
 * Macros for manipulating and testing flags related to a
 * pageblock_nr_pages number of pages.
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation version 2 of the License
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program; if not, write to the Free Software
 * Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
 *
 * Copyright (C) IBM Corporation, 2006
 *
 * Original author, Mel Gorman
 * Major cleanups and reduction of bit operations, Andy Whitcroft
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 28 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/pageblock-flags.h" 2
/* Bit indices that affect a whole block of pages */
enum pageblock_bits {
	PB_migrate,
	PB_migrate_end = PB_migrate + 3 - 1,
			/* 3 bits required for migrate types */
	NR_PAGEBLOCK_BITS
};
#if definedEx(CONFIG_HUGETLB_PAGE)
 /* Huge pages are a constant size */
#endif
#if !definedEx(CONFIG_HUGETLB_PAGE)
/* If huge pages are not used, group by MAX_ORDER_NR_PAGES */
#endif
/* Forward declaration */
struct page;
/* Declarations for getting and setting flags. See mm/page_alloc.c */
unsigned long get_pageblock_flags_group(struct page *page,
					int start_bitidx, int end_bitidx);
void set_pageblock_flags_group(struct page *page, unsigned long flags,
					int start_bitidx, int end_bitidx);
#line 19 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/mmzone.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/generated/bounds.h" 1
/*
 * DO NOT MODIFY.
 *
 * This file was generated by Kbuild
 *
 */
#line 20 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/mmzone.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic.h" 1
#if definedEx(CONFIG_X86_32)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic_32.h" 1
#line 4 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic.h" 2
#endif
#if !definedEx(CONFIG_X86_32)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic_64.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic.h" 2
#endif
#line 21 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/mmzone.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/page.h" 1
#line 22 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/mmzone.h" 2
/* Free memory management - zoned buddy allocator.  */
/*
 * PAGE_ALLOC_COSTLY_ORDER is the order at which allocations are deemed
 * costly to service.  That is between allocation orders which should
 * coelesce naturally under reasonable reclaim pressure and those which
 * will not.
 */
extern int page_group_by_mobility_disabled;
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int get_pageblock_migratetype(struct page *page)
{
	return get_pageblock_flags_group(page, PB_migrate, PB_migrate_end);
}
struct free_area {
	struct list_head	free_list[5];
	unsigned long		nr_free;
};
struct pglist_data;
/*
 * zone->lock and zone->lru_lock are two of the hottest locks in the kernel.
 * So add a wild amount of padding here to ensure that they fall into separate
 * cachelines.  There are very few zone structures in the machine, so space
 * consumption is not a concern here.
 */
#if definedEx(CONFIG_SMP)
struct zone_padding {
	char x[0];
} __attribute__((__aligned__(1 << (5))));
#endif
#if !definedEx(CONFIG_SMP)
#endif
enum zone_stat_item {
	/* First 128 byte cacheline (assuming 64 bit words) */
	NR_FREE_PAGES,
	NR_LRU_BASE,
	NR_INACTIVE_ANON = NR_LRU_BASE, /* must match order of LRU_[IN]ACTIVE */
	NR_ACTIVE_ANON,		/*  "     "     "   "       "         */
	NR_INACTIVE_FILE,	/*  "     "     "   "       "         */
	NR_ACTIVE_FILE,		/*  "     "     "   "       "         */
	NR_UNEVICTABLE,		/*  "     "     "   "       "         */
	NR_MLOCK,		/* mlock()ed pages found and moved off LRU */
	NR_ANON_PAGES,	/* Mapped anonymous pages */
	NR_FILE_MAPPED,	/* pagecache pages mapped into pagetables.
			   only modified from process context */
	NR_FILE_PAGES,
	NR_FILE_DIRTY,
	NR_WRITEBACK,
	NR_SLAB_RECLAIMABLE,
	NR_SLAB_UNRECLAIMABLE,
	NR_PAGETABLE,		/* used for pagetables */
	NR_KERNEL_STACK,
	/* Second 128 byte cacheline */
	NR_UNSTABLE_NFS,	/* NFS unstable pages */
	NR_BOUNCE,
	NR_VMSCAN_WRITE,
	NR_WRITEBACK_TEMP,	/* Writeback using temporary buffers */
	NR_ISOLATED_ANON,	/* Temporary isolated pages from anon lru */
	NR_ISOLATED_FILE,	/* Temporary isolated pages from file lru */
	NR_SHMEM,		/* shmem pages (included tmpfs/GEM pages) */
#if definedEx(CONFIG_NUMA)
	NUMA_HIT,		/* allocated in intended node */
	NUMA_MISS,		/* allocated in non intended node */
	NUMA_FOREIGN,		/* was intended here, hit elsewhere */
	NUMA_INTERLEAVE_HIT,	/* interleaver preferred this zone */
	NUMA_LOCAL,		/* allocation from local node */
	NUMA_OTHER,		/* allocation from other node */
#endif
	NR_VM_ZONE_STAT_ITEMS };
/*
 * We do arithmetic on the LRU lists in various places in the code,
 * so it is important to keep the active lists LRU_ACTIVE higher in
 * the array than the corresponding inactive lists, and to keep
 * the *_FILE lists LRU_FILE higher than the corresponding _ANON lists.
 *
 * This has to be kept in sync with the statistics in zone_stat_item
 * above and the descriptions in vmstat_text in mm/vmstat.c
 */
enum lru_list {
	LRU_INACTIVE_ANON = 0,
	LRU_ACTIVE_ANON = 0 + 1,
	LRU_INACTIVE_FILE = 0 + 2,
	LRU_ACTIVE_FILE = 0 + 2 + 1,
	LRU_UNEVICTABLE,
	NR_LRU_LISTS
};
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int is_file_lru(enum lru_list l)
{
	return (l == LRU_INACTIVE_FILE || l == LRU_ACTIVE_FILE);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int is_active_lru(enum lru_list l)
{
	return (l == LRU_ACTIVE_ANON || l == LRU_ACTIVE_FILE);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int is_unevictable_lru(enum lru_list l)
{
	return (l == LRU_UNEVICTABLE);
}
enum zone_watermarks {
	WMARK_MIN,
	WMARK_LOW,
	WMARK_HIGH,
	NR_WMARK
};
struct per_cpu_pages {
	int count;		/* number of pages in the list */
	int high;		/* high watermark, emptying needed */
	int batch;		/* chunk size for buddy add/remove */
	/* Lists of pages, one per migrate type stored on the pcp-lists */
	struct list_head lists[3];
};
struct per_cpu_pageset {
	struct per_cpu_pages pcp;
#if definedEx(CONFIG_NUMA)
	s8 expire;
#endif
#if definedEx(CONFIG_SMP)
	s8 stat_threshold;
	s8 vm_stat_diff[NR_VM_ZONE_STAT_ITEMS];
#endif
} 
#if definedEx(CONFIG_SMP)
__attribute__((__aligned__((1 << (5)))))
#endif
#if !definedEx(CONFIG_SMP)
#endif
;
#if definedEx(CONFIG_NUMA)
#endif
#if !definedEx(CONFIG_NUMA)
#endif
enum zone_type {
	/*
	 * ZONE_DMA is used when there are devices that are not able
	 * to do DMA to all of addressable memory (ZONE_NORMAL). Then we
	 * carve out the portion of memory that is needed for these devices.
	 * The range is arch specific.
	 *
	 * Some examples
	 *
	 * Architecture		Limit
	 * ---------------------------
	 * parisc, ia64, sparc	<4G
	 * s390			<2G
	 * arm			Various
	 * alpha		Unlimited or 0-16MB.
	 *
	 * i386, x86_64 and multiple other arches
	 * 			<16M.
	 */
	ZONE_DMA,
#if definedEx(CONFIG_ZONE_DMA32)
	/*
	 * x86_64 needs two ZONE_DMAs because it supports devices that are
	 * only able to do DMA to the lower 16M but also 32 bit devices that
	 * can only do DMA areas below 4G.
	 */
	ZONE_DMA32,
#endif
	/*
	 * Normal addressable memory is in ZONE_NORMAL. DMA operations can be
	 * performed on pages in ZONE_NORMAL if the DMA devices support
	 * transfers to all addressable memory.
	 */
	ZONE_NORMAL,
#if definedEx(CONFIG_HIGHMEM)
	/*
	 * A memory area that is only addressable by the kernel through
	 * mapping portions into its own address space. This is for example
	 * used by i386 to allow the kernel to address the memory beyond
	 * 900MB. The kernel will set up special mappings (page
	 * table entries on i386) for each page that the kernel needs to
	 * access.
	 */
	ZONE_HIGHMEM,
#endif
	ZONE_MOVABLE,
	__MAX_NR_ZONES
};
/*
 * When a memory allocation must conform to specific limitations (such
 * as being suitable for DMA) the caller will pass in hints to the
 * allocator in the gfp_mask, in the zone modifier bits.  These bits
 * are used to select a priority ordered list of memory zones which
 * match the requested limits. See gfp_zone() in include/linux/gfp.h
 */
struct zone_reclaim_stat {
	/*
	 * The pageout code in vmscan.c keeps track of how many of the
	 * mem/swap backed and file backed pages are refeferenced.
	 * The higher the rotated/scanned ratio, the more valuable
	 * that cache is.
	 *
	 * The anon LRU stats live in [0], file LRU stats in [1]
	 */
	unsigned long		recent_rotated[2];
	unsigned long		recent_scanned[2];
	/*
	 * accumulated for batching
	 */
	unsigned long		nr_saved_scan[NR_LRU_LISTS];
};
struct zone {
	/* Fields commonly accessed by the page allocator */
	/* zone watermarks, access with *_wmark_pages(zone) macros */
	unsigned long watermark[NR_WMARK];
	/*
	 * We don't know if the memory that we're going to allocate will be freeable
	 * or/and it will be released eventually, so to avoid totally wasting several
	 * GB of ram we must reserve some of the lower zone memory (otherwise we risk
	 * to run OOM on the lower zones despite there's tons of freeable ram
	 * on the higher zones). This array is recalculated at runtime if the
	 * sysctl_lowmem_reserve_ratio sysctl changes.
	 */
	unsigned long		lowmem_reserve[4];
#if definedEx(CONFIG_NUMA)
	int node;
	/*
	 * zone reclaim becomes active if more unmapped pages exist.
	 */
	unsigned long		min_unmapped_pages;
	unsigned long		min_slab_pages;
	struct per_cpu_pageset	*pageset[
#if definedEx(CONFIG_SMP)
8
#endif
#if !definedEx(CONFIG_SMP)
1
#endif
];
#endif
#if !definedEx(CONFIG_NUMA)
	struct per_cpu_pageset	pageset[
#if definedEx(CONFIG_SMP)
8
#endif
#if !definedEx(CONFIG_SMP)
1
#endif
];
#endif
	/*
	 * free areas of different sizes
	 */
	spinlock_t		lock;
#if definedEx(CONFIG_MEMORY_HOTPLUG)
	/* see spanned/present_pages for more description */
	seqlock_t		span_seqlock;
#endif
	struct free_area	free_area[11];
#if !definedEx(CONFIG_SPARSEMEM)
	/*
	 * Flags for a pageblock_nr_pages block. See pageblock-flags.h.
	 * In SPARSEMEM, this map is stored in struct mem_section
	 */
	unsigned long		*pageblock_flags;
#endif
#if definedEx(CONFIG_SMP)
struct zone_padding _pad1_;
#endif
#if !definedEx(CONFIG_SMP)
#endif
	/* Fields commonly accessed by the page reclaim scanner */
	spinlock_t		lru_lock;	
	struct zone_lru {
		struct list_head list;
	} lru[NR_LRU_LISTS];
	struct zone_reclaim_stat reclaim_stat;
	unsigned long		pages_scanned;	   /* since last reclaim */
	unsigned long		flags;		   /* zone flags, see below */
	/* Zone statistics */
	atomic_long_t		vm_stat[NR_VM_ZONE_STAT_ITEMS];
	/*
	 * prev_priority holds the scanning priority for this zone.  It is
	 * defined as the scanning priority at which we achieved our reclaim
	 * target at the previous try_to_free_pages() or balance_pgdat()
	 * invokation.
	 *
	 * We use prev_priority as a measure of how much stress page reclaim is
	 * under - it drives the swappiness decision: whether to unmap mapped
	 * pages.
	 *
	 * Access to both this field is quite racy even on uniprocessor.  But
	 * it is expected to average out OK.
	 */
	int prev_priority;
	/*
	 * The target ratio of ACTIVE_ANON to INACTIVE_ANON pages on
	 * this zone's LRU.  Maintained by the pageout code.
	 */
	unsigned int inactive_ratio;
#if definedEx(CONFIG_SMP)
struct zone_padding _pad2_;
#endif
#if !definedEx(CONFIG_SMP)
#endif
	/* Rarely used or read-mostly fields */
	/*
	 * wait_table		-- the array holding the hash table
	 * wait_table_hash_nr_entries	-- the size of the hash table array
	 * wait_table_bits	-- wait_table_size == (1 << wait_table_bits)
	 *
	 * The purpose of all these is to keep track of the people
	 * waiting for a page to become available and make them
	 * runnable again when possible. The trouble is that this
	 * consumes a lot of space, especially when so few things
	 * wait on pages at a given time. So instead of using
	 * per-page waitqueues, we use a waitqueue hash table.
	 *
	 * The bucket discipline is to sleep on the same queue when
	 * colliding and wake all in that wait queue when removing.
	 * When something wakes, it must check to be sure its page is
	 * truly available, a la thundering herd. The cost of a
	 * collision is great, but given the expected load of the
	 * table, they should be so rare as to be outweighed by the
	 * benefits from the saved space.
	 *
	 * __wait_on_page_locked() and unlock_page() in mm/filemap.c, are the
	 * primary users of these fields, and in mm/page_alloc.c
	 * free_area_init_core() performs the initialization of them.
	 */
	wait_queue_head_t	* wait_table;
	unsigned long		wait_table_hash_nr_entries;
	unsigned long		wait_table_bits;
	/*
	 * Discontig memory support fields.
	 */
	struct pglist_data	*zone_pgdat;
	/* zone_start_pfn == zone_start_paddr >> PAGE_SHIFT */
	unsigned long		zone_start_pfn;
	/*
	 * zone_start_pfn, spanned_pages and present_pages are all
	 * protected by span_seqlock.  It is a seqlock because it has
	 * to be read outside of zone->lock, and it is done in the main
	 * allocator path.  But, it is written quite infrequently.
	 *
	 * The lock is declared along with zone->lock because it is
	 * frequently read in proximity to zone->lock.  It's good to
	 * give them a chance of being in the same cacheline.
	 */
	unsigned long		spanned_pages;	/* total size, including holes */
	unsigned long		present_pages;	/* amount of memory (excluding holes) */
	/*
	 * rarely used fields:
	 */
	const char		*name;
} 
#if definedEx(CONFIG_SMP)
__attribute__((__aligned__(1 << (5))))
#endif
#if !definedEx(CONFIG_SMP)
#endif
;
typedef enum {
	ZONE_ALL_UNRECLAIMABLE,		/* all pages pinned */
	ZONE_RECLAIM_LOCKED,		/* prevents concurrent reclaim */
	ZONE_OOM_LOCKED,		/* zone is in OOM killer zonelist */
} zone_flags_t;
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void zone_set_flag(struct zone *zone, zone_flags_t flag)
{
	set_bit(flag, &zone->flags);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int zone_test_and_set_flag(struct zone *zone, zone_flags_t flag)
{
	return test_and_set_bit(flag, &zone->flags);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void zone_clear_flag(struct zone *zone, zone_flags_t flag)
{
	clear_bit(flag, &zone->flags);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int zone_is_all_unreclaimable(const struct zone *zone)
{
	return (__builtin_constant_p((ZONE_ALL_UNRECLAIMABLE)) ? constant_test_bit((ZONE_ALL_UNRECLAIMABLE), (&zone->flags)) : variable_test_bit((ZONE_ALL_UNRECLAIMABLE), (&zone->flags)));
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int zone_is_reclaim_locked(const struct zone *zone)
{
	return (__builtin_constant_p((ZONE_RECLAIM_LOCKED)) ? constant_test_bit((ZONE_RECLAIM_LOCKED), (&zone->flags)) : variable_test_bit((ZONE_RECLAIM_LOCKED), (&zone->flags)));
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int zone_is_oom_locked(const struct zone *zone)
{
	return (__builtin_constant_p((ZONE_OOM_LOCKED)) ? constant_test_bit((ZONE_OOM_LOCKED), (&zone->flags)) : variable_test_bit((ZONE_OOM_LOCKED), (&zone->flags)));
}
/*
 * The "priority" of VM scanning is how much of the queues we will scan in one
 * go. A value of 12 for DEF_PRIORITY implies that we will scan 1/4096th of the
 * queues ("queue_length >> 12") during an aging round.
 */
/* Maximum number of zones on a zonelist */
#if definedEx(CONFIG_NUMA)
/*
 * The NUMA zonelists are doubled becausse we need zonelists that restrict the
 * allocations to a single node for GFP_THISNODE.
 *
 * [0]	: Zonelist with fallback
 * [1]	: No fallback (GFP_THISNODE)
 */
/*
 * We cache key information from each zonelist for smaller cache
 * footprint when scanning for free pages in get_page_from_freelist().
 *
 * 1) The BITMAP fullzones tracks which zones in a zonelist have come
 *    up short of free memory since the last time (last_fullzone_zap)
 *    we zero'd fullzones.
 * 2) The array z_to_n[] maps each zone in the zonelist to its node
 *    id, so that we can efficiently evaluate whether that node is
 *    set in the current tasks mems_allowed.
 *
 * Both fullzones and z_to_n[] are one-to-one with the zonelist,
 * indexed by a zones offset in the zonelist zones[] array.
 *
 * The get_page_from_freelist() routine does two scans.  During the
 * first scan, we skip zones whose corresponding bit in 'fullzones'
 * is set or whose corresponding node in current->mems_allowed (which
 * comes from cpusets) is not set.  During the second scan, we bypass
 * this zonelist_cache, to ensure we look methodically at each zone.
 *
 * Once per second, we zero out (zap) fullzones, forcing us to
 * reconsider nodes that might have regained more free memory.
 * The field last_full_zap is the time we last zapped fullzones.
 *
 * This mechanism reduces the amount of time we waste repeatedly
 * reexaming zones for free memory when they just came up low on
 * memory momentarilly ago.
 *
 * The zonelist_cache struct members logically belong in struct
 * zonelist.  However, the mempolicy zonelists constructed for
 * MPOL_BIND are intentionally variable length (and usually much
 * shorter).  A general purpose mechanism for handling structs with
 * multiple variable length members is more mechanism than we want
 * here.  We resort to some special case hackery instead.
 *
 * The MPOL_BIND zonelists don't need this zonelist_cache (in good
 * part because they are shorter), so we put the fixed length stuff
 * at the front of the zonelist struct, ending in a variable length
 * zones[], as is needed by MPOL_BIND.
 *
 * Then we put the optional zonelist cache on the end of the zonelist
 * struct.  This optional stuff is found by a 'zlcache_ptr' pointer in
 * the fixed length portion at the front of the struct.  This pointer
 * both enables us to find the zonelist cache, and in the case of
 * MPOL_BIND zonelists, (which will just set the zlcache_ptr to NULL)
 * to know that the zonelist cache is not there.
 *
 * The end result is that struct zonelists come in two flavors:
 *  1) The full, fixed length version, shown below, and
 *  2) The custom zonelists for MPOL_BIND.
 * The custom MPOL_BIND zonelists have a NULL zlcache_ptr and no zlcache.
 *
 * Even though there may be multiple CPU cores on a node modifying
 * fullzones or last_full_zap in the same zonelist_cache at the same
 * time, we don't lock it.  This is just hint data - if it is wrong now
 * and then, the allocator will still function, perhaps a bit slower.
 */
struct zonelist_cache {
	unsigned short z_to_n[((1 << 
#if !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_NODES_SHIFT) || definedEx(CONFIG_NEED_MULTIPLE_NODES)
#if definedEx(CONFIG_NEED_MULTIPLE_NODES)
3
#endif
#if !definedEx(CONFIG_NEED_MULTIPLE_NODES)
CONFIG_NODES_SHIFT
#endif
#endif
#if !definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_NODES_SHIFT)
0
#endif
) * 4)];		/* zone->nid */
	unsigned long fullzones[(((((1 << 
#if !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_NODES_SHIFT) || definedEx(CONFIG_NEED_MULTIPLE_NODES)
#if definedEx(CONFIG_NEED_MULTIPLE_NODES)
3
#endif
#if !definedEx(CONFIG_NEED_MULTIPLE_NODES)
 CONFIG_NODES_SHIFT
#endif
#endif
#if !definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_NODES_SHIFT)
0
#endif
) * 4)) + (8 * sizeof(long)) - 1) / (8 * sizeof(long)))];	/* zone full? */
	unsigned long last_full_zap;		/* when last zap'd (jiffies) */
};
#endif
#if !definedEx(CONFIG_NUMA)
struct zonelist_cache;
#endif
/*
 * This struct contains information about a zone in a zonelist. It is stored
 * here to avoid dereferences into large structures and lookups of tables
 */
struct zoneref {
	struct zone *zone;	/* Pointer to actual zone */
	int zone_idx;		/* zone_idx(zoneref->zone) */
};
/*
 * One allocation request operates on a zonelist. A zonelist
 * is a list of zones, the first one is the 'goal' of the
 * allocation, the other zones are fallback zones, in decreasing
 * priority.
 *
 * If zlcache_ptr is not NULL, then it is just the address of zlcache,
 * as explained above.  If zlcache_ptr is NULL, there is no zlcache.
 * *
 * To speed the reading of the zonelist, the zonerefs contain the zone index
 * of the entry being read. Helper functions to access information given
 * a struct zoneref are
 *
 * zonelist_zone()	- Return the struct zone * for an entry in _zonerefs
 * zonelist_zone_idx()	- Return the index of the zone for an entry
 * zonelist_node_idx()	- Return the index of the node for an entry
 */
struct zonelist {
	struct zonelist_cache *zlcache_ptr;		     // NULL or &zlcache
	struct zoneref _zonerefs[((1 << 
#if !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_NODES_SHIFT) || definedEx(CONFIG_NEED_MULTIPLE_NODES)
#if definedEx(CONFIG_NEED_MULTIPLE_NODES)
3
#endif
#if !definedEx(CONFIG_NEED_MULTIPLE_NODES)
CONFIG_NODES_SHIFT
#endif
#endif
#if !definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_NODES_SHIFT)
0
#endif
) * 4) + 1];
#if definedEx(CONFIG_NUMA)
	struct zonelist_cache zlcache;			     // optional ...
#endif
};
struct node_active_region {
	unsigned long start_pfn;
	unsigned long end_pfn;
	int nid;
};
#if !definedEx(CONFIG_DISCONTIGMEM)
/* The array of struct pages - for discontigmem use pgdat->lmem_map */
extern struct page *mem_map;
#endif
/*
 * The pg_data_t structure is used in machines with CONFIG_DISCONTIGMEM
 * (mostly NUMA machines?) to denote a higher-level memory zone than the
 * zone denotes.
 *
 * On NUMA machines, each NUMA node would have a pg_data_t to describe
 * it's memory layout.
 *
 * Memory statistics and page replacement data structures are maintained on a
 * per-zone basis.
 */
struct bootmem_data;
typedef struct pglist_data {
	struct zone node_zones[4];
	struct zonelist node_zonelists[
#if definedEx(CONFIG_NUMA)
2
#endif
#if !definedEx(CONFIG_NUMA)
1
#endif
];
	int nr_zones;
#if definedEx(CONFIG_FLAT_NODE_MEM_MAP)
	struct page *node_mem_map;
#if definedEx(CONFIG_CGROUP_MEM_RES_CTLR)
	struct page_cgroup *node_page_cgroup;
#endif
#endif
	struct bootmem_data *bdata;
#if definedEx(CONFIG_MEMORY_HOTPLUG)
	/*
	 * Must be held any time you expect node_start_pfn, node_present_pages
	 * or node_spanned_pages stay constant.  Holding this will also
	 * guarantee that any pfn_valid() stays that way.
	 *
	 * Nests above zone->lock and zone->size_seqlock.
	 */
	spinlock_t node_size_lock;
#endif
	unsigned long node_start_pfn;
	unsigned long node_present_pages; /* total number of physical pages */
	unsigned long node_spanned_pages; /* total size of physical page
					     range, including holes */
	int node_id;
	wait_queue_head_t kswapd_wait;
	struct task_struct *kswapd;
	int kswapd_max_order;
} pg_data_t;
#if definedEx(CONFIG_FLAT_NODE_MEM_MAP)
#endif
#if !definedEx(CONFIG_FLAT_NODE_MEM_MAP)
#endif
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/memory_hotplug.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/mmzone.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/memory_hotplug.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/spinlock.h" 1
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/memory_hotplug.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/notifier.h" 1
/*
 *	Routines to manage notifier chains for passing status changes to any
 *	interested routines. We need this instead of hard coded call lists so
 *	that modules can poke their nose into the innards. The network devices
 *	needed them so here they are for the rest of you.
 *
 *				Alan Cox <Alan.Cox@linux.org>
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/errno.h" 1
#if definedEx(CONFIG_PARAVIRT)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/errno.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/asm-generic/errno.h" 1
#line 3 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/errno.h" 2
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/errno.h" 2
/*
 * These should never be seen by user programs.  To return one of ERESTART*
 * codes, signal_pending() MUST be set.  Note that ptrace can observe these
 * at syscall exit tracing, but they will never be left for the debugged user
 * process to see.
 */
/* Defined for the NFSv3 protocol */
#endif
#line 14 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/notifier.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/mutex.h" 1
/*
 * Mutexes: blocking mutual exclusion locks
 *
 * started by Ingo Molnar:
 *
 *  Copyright (C) 2004, 2005, 2006 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>
 *
 * This file contains the main data structure and API definitions.
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/list.h" 1
#line 15 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/mutex.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/spinlock_types.h" 1
#line 16 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/mutex.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/linkage.h" 1
#line 17 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/mutex.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/lockdep.h" 1
/*
 * Runtime locking correctness validator
 *
 *  Copyright (C) 2006,2007 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>
 *  Copyright (C) 2007 Red Hat, Inc., Peter Zijlstra <pzijlstr@redhat.com>
 *
 * see Documentation/lockdep-design.txt for more details.
 */
#line 18 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/mutex.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic.h" 1
#if definedEx(CONFIG_X86_32)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic_32.h" 1
#line 4 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic.h" 2
#endif
#if !definedEx(CONFIG_X86_32)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic_64.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic.h" 2
#endif
#line 20 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/mutex.h" 2
/*
 * Simple, straightforward mutexes with strict semantics:
 *
 * - only one task can hold the mutex at a time
 * - only the owner can unlock the mutex
 * - multiple unlocks are not permitted
 * - recursive locking is not permitted
 * - a mutex object must be initialized via the API
 * - a mutex object must not be initialized via memset or copying
 * - task may not exit with mutex held
 * - memory areas where held locks reside must not be freed
 * - held mutexes must not be reinitialized
 * - mutexes may not be used in hardware or software interrupt
 *   contexts such as tasklets and timers
 *
 * These semantics are fully enforced when DEBUG_MUTEXES is
 * enabled. Furthermore, besides enforcing the above rules, the mutex
 * debugging code also implements a number of additional features
 * that make lock debugging easier and faster:
 *
 * - uses symbolic names of mutexes, whenever they are printed in debug output
 * - point-of-acquire tracking, symbolic lookup of function names
 * - list of all locks held in the system, printout of them
 * - owner tracking
 * - detects self-recursing locks and prints out all relevant info
 * - detects multi-task circular deadlocks and prints out all affected
 *   locks and tasks (and only those tasks)
 */
struct mutex {
	/* 1: unlocked, 0: locked, negative: locked, possible waiters */
	atomic_t		count;
	spinlock_t		wait_lock;
	struct list_head	wait_list;
#if !definedEx(CONFIG_SMP) && definedEx(CONFIG_DEBUG_MUTEXES) || definedEx(CONFIG_SMP)
	struct thread_info	*owner;
#endif
#if definedEx(CONFIG_DEBUG_MUTEXES)
	const char 		*name;
	void			*magic;
#endif
#if definedEx(CONFIG_DEBUG_LOCK_ALLOC)
	struct lockdep_map	dep_map;
#endif
};
/*
 * This is the control structure for tasks blocked on mutex,
 * which resides on the blocked task's kernel stack:
 */
struct mutex_waiter {
	struct list_head	list;
	struct task_struct	*task;
#if definedEx(CONFIG_DEBUG_MUTEXES)
	void			*magic;
#endif
};
#if definedEx(CONFIG_DEBUG_MUTEXES)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/mutex-debug.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/linkage.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/mutex-debug.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/lockdep.h" 1
/*
 * Runtime locking correctness validator
 *
 *  Copyright (C) 2006,2007 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>
 *  Copyright (C) 2007 Red Hat, Inc., Peter Zijlstra <pzijlstr@redhat.com>
 *
 * see Documentation/lockdep-design.txt for more details.
 */
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/mutex-debug.h" 2
/*
 * Mutexes - debugging helpers:
 */
extern void mutex_destroy(struct mutex *lock);
#line 80 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/mutex.h" 2
#endif
#if !definedEx(CONFIG_DEBUG_MUTEXES)
#endif
#if definedEx(CONFIG_DEBUG_LOCK_ALLOC)
#endif
#if !definedEx(CONFIG_DEBUG_LOCK_ALLOC)
#endif
extern void __mutex_init(struct mutex *lock, const char *name,
			 struct lock_class_key *key);
/**
 * mutex_is_locked - is the mutex locked
 * @lock: the mutex to be queried
 *
 * Returns 1 if the mutex is locked, 0 if unlocked.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int mutex_is_locked(struct mutex *lock)
{
	return atomic_read(&lock->count) != 1;
}
/*
 * See kernel/mutex.c for detailed documentation of these APIs.
 * Also see Documentation/mutex-design.txt.
 */
#if definedEx(CONFIG_DEBUG_LOCK_ALLOC)
extern void mutex_lock_nested(struct mutex *lock, unsigned int subclass);
extern int 
#if definedEx(CONFIG_ENABLE_MUST_CHECK)
__attribute__((warn_unused_result))
#endif
#if !definedEx(CONFIG_ENABLE_MUST_CHECK)
#endif
 mutex_lock_interruptible_nested(struct mutex *lock,
					unsigned int subclass);
extern int 
#if definedEx(CONFIG_ENABLE_MUST_CHECK)
__attribute__((warn_unused_result))
#endif
#if !definedEx(CONFIG_ENABLE_MUST_CHECK)
#endif
 mutex_lock_killable_nested(struct mutex *lock,
					unsigned int subclass);
#endif
#if !definedEx(CONFIG_DEBUG_LOCK_ALLOC)
extern void mutex_lock(struct mutex *lock);
extern int 
#if definedEx(CONFIG_ENABLE_MUST_CHECK)
__attribute__((warn_unused_result))
#endif
#if !definedEx(CONFIG_ENABLE_MUST_CHECK)
#endif
 mutex_lock_interruptible(struct mutex *lock);
extern int 
#if definedEx(CONFIG_ENABLE_MUST_CHECK)
__attribute__((warn_unused_result))
#endif
#if !definedEx(CONFIG_ENABLE_MUST_CHECK)
#endif
 mutex_lock_killable(struct mutex *lock);
#endif
/*
 * NOTE: mutex_trylock() follows the spin_trylock() convention,
 *       not the down_trylock() convention!
 *
 * Returns 1 if the mutex has been acquired successfully, and 0 on contention.
 */
extern int mutex_trylock(struct mutex *lock);
extern void mutex_unlock(struct mutex *lock);
extern int atomic_dec_and_mutex_lock(atomic_t *cnt, struct mutex *lock);
#line 15 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/notifier.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/rwsem.h" 1
/* rwsem.h: R/W semaphores, public interface
 *
 * Written by David Howells (dhowells@redhat.com).
 * Derived from asm-i386/semaphore.h
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/linkage.h" 1
#line 12 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/rwsem.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 14 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/rwsem.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/kernel.h" 1
#line 15 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/rwsem.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/system.h" 1
#line 16 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/rwsem.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic.h" 1
#if definedEx(CONFIG_X86_32)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic_32.h" 1
#line 4 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic.h" 2
#endif
#if !definedEx(CONFIG_X86_32)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic_64.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic.h" 2
#endif
#line 17 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/rwsem.h" 2
struct rw_semaphore;
#if definedEx(CONFIG_RWSEM_GENERIC_SPINLOCK)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/rwsem-spinlock.h" 1
/* rwsem-spinlock.h: fallback C implementation
 *
 * Copyright (c) 2001   David Howells (dhowells@redhat.com).
 * - Derived partially from ideas by Andrea Arcangeli <andrea@suse.de>
 * - Derived also from comments by Linus
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/spinlock.h" 1
#line 17 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/rwsem-spinlock.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/list.h" 1
#line 18 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/rwsem-spinlock.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 22 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/rwsem-spinlock.h" 2
struct rwsem_waiter;
/*
 * the rw-semaphore definition
 * - if activity is 0 then there are no active readers or writers
 * - if activity is +ve then that is the number of active readers
 * - if activity is -1 then there is one active writer
 * - if wait_list is not empty, then there are processes waiting for the semaphore
 */
struct rw_semaphore {
	__s32			activity;
	spinlock_t		wait_lock;
	struct list_head	wait_list;
#if definedEx(CONFIG_DEBUG_LOCK_ALLOC)
	struct lockdep_map dep_map;
#endif
};
#if definedEx(CONFIG_DEBUG_LOCK_ALLOC)
#endif
#if !definedEx(CONFIG_DEBUG_LOCK_ALLOC)
#endif
extern void __init_rwsem(struct rw_semaphore *sem, const char *name,
			 struct lock_class_key *key);
extern void __down_read(struct rw_semaphore *sem);
extern int __down_read_trylock(struct rw_semaphore *sem);
extern void __down_write(struct rw_semaphore *sem);
extern void __down_write_nested(struct rw_semaphore *sem, int subclass);
extern int __down_write_trylock(struct rw_semaphore *sem);
extern void __up_read(struct rw_semaphore *sem);
extern void __up_write(struct rw_semaphore *sem);
extern void __downgrade_write(struct rw_semaphore *sem);
extern int rwsem_is_locked(struct rw_semaphore *sem);
#line 22 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/rwsem.h" 2
#endif
#if !definedEx(CONFIG_RWSEM_GENERIC_SPINLOCK)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/rwsem.h" 1
/* rwsem.h: R/W semaphores implemented using XADD/CMPXCHG for i486+
 *
 * Written by David Howells (dhowells@redhat.com).
 *
 * Derived from asm-x86/semaphore.h
 *
 *
 * The MSW of the count is the negated number of active writers and waiting
 * lockers, and the LSW is the total number of active locks
 *
 * The lock count is initialized to 0 (no active and no waiting lockers).
 *
 * When a writer subtracts WRITE_BIAS, it'll get 0xffff0001 for the case of an
 * uncontended lock. This can be determined because XADD returns the old value.
 * Readers increment by 1 and see a positive value when uncontended, negative
 * if there are writers (and maybe) readers waiting (in which case it goes to
 * sleep).
 *
 * The value of WAITING_BIAS supports up to 32766 waiting processes. This can
 * be extended to 65534 by manually checking the whole MSW rather than relying
 * on the S flag.
 *
 * The value of ACTIVE_BIAS supports up to 65535 active processes.
 *
 * This should be totally fair - if anything is waiting, a process that wants a
 * lock will go to the back of the queue. When the currently active lock is
 * released, if there's a writer at the front of the queue, then that and only
 * that will be woken up; if there's a bunch of consequtive readers at the
 * front, then they'll all be woken up, but no other readers will be.
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/list.h" 1
#line 43 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/rwsem.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/spinlock.h" 1
#line 44 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/rwsem.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/lockdep.h" 1
/*
 * Runtime locking correctness validator
 *
 *  Copyright (C) 2006,2007 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>
 *  Copyright (C) 2007 Red Hat, Inc., Peter Zijlstra <pzijlstr@redhat.com>
 *
 * see Documentation/lockdep-design.txt for more details.
 */
#line 45 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/rwsem.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/asm.h" 1
#line 46 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/rwsem.h" 2
struct rwsem_waiter;
extern 
#if !definedEx(CONFIG_X86_32)
#endif
#if definedEx(CONFIG_X86_32)
__attribute__((regparm(3)))
#endif
 struct rw_semaphore *
 rwsem_down_read_failed(struct rw_semaphore *sem);
extern 
#if !definedEx(CONFIG_X86_32)
#endif
#if definedEx(CONFIG_X86_32)
__attribute__((regparm(3)))
#endif
 struct rw_semaphore *
 rwsem_down_write_failed(struct rw_semaphore *sem);
extern 
#if !definedEx(CONFIG_X86_32)
#endif
#if definedEx(CONFIG_X86_32)
__attribute__((regparm(3)))
#endif
 struct rw_semaphore *
 rwsem_wake(struct rw_semaphore *);
extern 
#if !definedEx(CONFIG_X86_32)
#endif
#if definedEx(CONFIG_X86_32)
__attribute__((regparm(3)))
#endif
 struct rw_semaphore *
 rwsem_downgrade_wake(struct rw_semaphore *sem);
/*
 * the semaphore definition
 *
 * The bias values and the counter type limits the number of
 * potential readers/writers to 32767 for 32 bits and 2147483647
 * for 64 bits.
 */
#if definedEx(CONFIG_X86_64)
#endif
#if !definedEx(CONFIG_X86_64)
#endif
typedef signed long rwsem_count_t;
struct rw_semaphore {
	rwsem_count_t		count;
	spinlock_t		wait_lock;
	struct list_head	wait_list;
#if definedEx(CONFIG_DEBUG_LOCK_ALLOC)
	struct lockdep_map dep_map;
#endif
};
#if definedEx(CONFIG_DEBUG_LOCK_ALLOC)
#endif
#if !definedEx(CONFIG_DEBUG_LOCK_ALLOC)
#endif
extern void __init_rwsem(struct rw_semaphore *sem, const char *name,
			 struct lock_class_key *key);
/*
 * lock for reading
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __down_read(struct rw_semaphore *sem)
{
	asm volatile("# beginning down_read\n\t"
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
#if definedEx(CONFIG_X86_32)
" " "incl" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " "incq" " "
#endif
 "(%1)\n\t"
		     /* adds 0x00000001, returns the old value */
		     "  jns        1f\n"
		     "  call call_rwsem_down_read_failed\n"
		     "1:\n\t"
		     "# ending down_read\n\t"
		     : "+m" (sem->count)
		     : "a" (sem)
		     : "memory", "cc");
}
/*
 * trylock for reading -- returns 1 if successful, 0 if contention
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int __down_read_trylock(struct rw_semaphore *sem)
{
	rwsem_count_t result, tmp;
	asm volatile("# beginning __down_read_trylock\n\t"
		     "  mov          %0,%1\n\t"
		     "1:\n\t"
		     "  mov          %1,%2\n\t"
		     "  add          %3,%2\n\t"
		     "  jle	     2f\n\t"
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "  cmpxchg  %2,%0\n\t"
		     "  jnz	     1b\n\t"
		     "2:\n\t"
		     "# ending __down_read_trylock\n\t"
		     : "+m" (sem->count), "=&a" (result), "=&r" (tmp)
		     : "i" (0x00000001L)
		     : "memory", "cc");
	return result >= 0 ? 1 : 0;
}
/*
 * lock for writing
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __down_write_nested(struct rw_semaphore *sem, int subclass)
{
	rwsem_count_t tmp;
	tmp = ((-
#if definedEx(CONFIG_X86_64) && !definedEx(CONFIG_RWSEM_GENERIC_SPINLOCK)
0xffffffffL
#endif
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_RWSEM_GENERIC_SPINLOCK)
0x0000ffffL
#endif
-1) + 0x00000001L);
	asm volatile("# beginning down_write\n\t"
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "  xadd      %1,(%2)\n\t"
		     /* subtract 0x0000ffff, returns the old value */
		     "  test      %1,%1\n\t"
		     /* was the count 0 before? */
		     "  jz        1f\n"
		     "  call call_rwsem_down_write_failed\n"
		     "1:\n"
		     "# ending down_write"
		     : "+m" (sem->count), "=d" (tmp)
		     : "a" (sem), "1" (tmp)
		     : "memory", "cc");
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __down_write(struct rw_semaphore *sem)
{
	__down_write_nested(sem, 0);
}
/*
 * trylock for writing -- returns 1 if successful, 0 if contention
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int __down_write_trylock(struct rw_semaphore *sem)
{
	rwsem_count_t ret = 
#if !definedEx(CONFIG_X86_32) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_X86_CMPXCHG)
#if definedEx(CONFIG_X86_32)
({ __typeof__(*(((&sem->count)))) __ret; __typeof__(*(((&sem->count)))) __old = (((
 0x00000000L))); __typeof__(*(((&sem->count)))) __new = (((
((-
#if definedEx(CONFIG_X86_64) && !definedEx(CONFIG_RWSEM_GENERIC_SPINLOCK)
0xffffffffL
#endif
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_RWSEM_GENERIC_SPINLOCK)
0x0000ffffL
#endif
-1) + 0x00000001L)))); switch ((sizeof(*&sem->count))) { case 1: asm volatile(
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" 
" " ".balign 4" " "
 "\n" 
" " ".long" " "
 "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "cmpxchgb %b1,%2" : "=a"(__ret) : "q"(__new), "m"(*((struct __xchg_dummy *)(((&sem->count))))), "0"(__old) : "memory"); break; case 2: asm volatile(
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" 
" " ".balign 4" " "
 "\n" 
" " ".long" " "
 "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "cmpxchgw %w1,%2" : "=a"(__ret) : "r"(__new), "m"(*((struct __xchg_dummy *)(((&sem->count))))), "0"(__old) : "memory"); break; case 4: asm volatile(
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" 
" " ".balign 4" " "
 "\n" 
" " ".long" " "
 "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "cmpxchgl %1,%2" : "=a"(__ret) : "r"(__new), "m"(*((struct __xchg_dummy *)(((&sem->count))))), "0"(__old) : "memory"); break; default: __cmpxchg_wrong_size(); } __ret; })
#endif
#if !definedEx(CONFIG_X86_32)
({ __typeof__(*(((&sem->count)))) __ret; __typeof__(*(((&sem->count)))) __old = (((
 0x00000000L))); __typeof__(*(((&sem->count)))) __new = (((
((-
#if definedEx(CONFIG_X86_64) && !definedEx(CONFIG_RWSEM_GENERIC_SPINLOCK)
0xffffffffL
#endif
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_RWSEM_GENERIC_SPINLOCK)
0x0000ffffL
#endif
-1) + 0x00000001L)))); switch ((sizeof(*&sem->count))) { case 1: asm volatile(
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" 
 " " ".balign 8" " "
 "\n" 
 " " ".quad" " "
 "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "cmpxchgb %b1,%2" : "=a"(__ret) : "q"(__new), "m"(*((volatile long *)(((&sem->count))))), "0"(__old) : "memory"); break; case 2: asm volatile(
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" 
 " " ".balign 8" " "
 "\n" 
 " " ".quad" " "
 "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "cmpxchgw %w1,%2" : "=a"(__ret) : "r"(__new), "m"(*((volatile long *)(((&sem->count))))), "0"(__old) : "memory"); break; case 4: asm volatile(
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" 
 " " ".balign 8" " "
 "\n" 
 " " ".quad" " "
 "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "cmpxchgl %k1,%2" : "=a"(__ret) : "r"(__new), "m"(*((volatile long *)(((&sem->count))))), "0"(__old) : "memory"); break; case 8: asm volatile(
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" 
 " " ".balign 8" " "
 "\n" 
 " " ".quad" " "
 "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "cmpxchgq %1,%2" : "=a"(__ret) : "r"(__new), "m"(*((volatile long *)(((&sem->count))))), "0"(__old) : "memory"); break; default: __cmpxchg_wrong_size(); } __ret; })
#endif
#endif
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_X86_CMPXCHG)
({ __typeof__(*(&sem->count)) __ret; if (__builtin_expect(!!(boot_cpu_data.x86 > 3), 1)) __ret = (__typeof__(*(&sem->count)))({ __typeof__(*(((&sem->count)))) __ret; __typeof__(*(((&sem->count)))) __old = (((unsigned long)(
 0x00000000L))); __typeof__(*(((&sem->count)))) __new = (((unsigned long)(
((-
#if definedEx(CONFIG_X86_64) && !definedEx(CONFIG_RWSEM_GENERIC_SPINLOCK)
0xffffffffL
#endif
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_RWSEM_GENERIC_SPINLOCK)
0x0000ffffL
#endif
-1) + 0x00000001L)))); switch ((sizeof(*(&sem->count)))) { case 1: asm volatile(
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" " " ".balign 4" " " "\n" " " ".long" " " "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "cmpxchgb %b1,%2" : "=a"(__ret) : "q"(__new), "m"(*((struct __xchg_dummy *)(((&sem->count))))), "0"(__old) : "memory"); break; case 2: asm volatile(
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" " " ".balign 4" " " "\n" " " ".long" " " "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "cmpxchgw %w1,%2" : "=a"(__ret) : "r"(__new), "m"(*((struct __xchg_dummy *)(((&sem->count))))), "0"(__old) : "memory"); break; case 4: asm volatile(
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" " " ".balign 4" " " "\n" " " ".long" " " "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "cmpxchgl %1,%2" : "=a"(__ret) : "r"(__new), "m"(*((struct __xchg_dummy *)(((&sem->count))))), "0"(__old) : "memory"); break; default: __cmpxchg_wrong_size(); } __ret; }); else __ret = (__typeof__(*(&sem->count)))cmpxchg_386((&sem->count), (unsigned long)(
 0x00000000L), (unsigned long)(
 ((-
#if definedEx(CONFIG_X86_64) && !definedEx(CONFIG_RWSEM_GENERIC_SPINLOCK)
0xffffffffL
#endif
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_RWSEM_GENERIC_SPINLOCK)
0x0000ffffL
#endif
-1) + 0x00000001L)), sizeof(*(&sem->count))); __ret; })
#endif
;
	if (ret == 0x00000000L)
		return 1;
	return 0;
}
/*
 * unlock after reading
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __up_read(struct rw_semaphore *sem)
{
	rwsem_count_t tmp = -0x00000001L;
	asm volatile("# beginning __up_read\n\t"
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "  xadd      %1,(%2)\n\t"
		     /* subtracts 1, returns the old value */
		     "  jns        1f\n\t"
		     "  call call_rwsem_wake\n"
		     "1:\n"
		     "# ending __up_read\n"
		     : "+m" (sem->count), "=d" (tmp)
		     : "a" (sem), "1" (tmp)
		     : "memory", "cc");
}
/*
 * unlock after writing
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __up_write(struct rw_semaphore *sem)
{
	rwsem_count_t tmp;
	asm volatile("# beginning __up_write\n\t"
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "  xadd      %1,(%2)\n\t"
		     /* tries to transition
			0xffff0001 -> 0x00000000 */
		     "  jz       1f\n"
		     "  call call_rwsem_wake\n"
		     "1:\n\t"
		     "# ending __up_write\n"
		     : "+m" (sem->count), "=d" (tmp)
		     : "a" (sem), "1" (-((-
#if definedEx(CONFIG_X86_64) && !definedEx(CONFIG_RWSEM_GENERIC_SPINLOCK)
0xffffffffL
#endif
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_RWSEM_GENERIC_SPINLOCK)
0x0000ffffL
#endif
-1) + 0x00000001L))
		     : "memory", "cc");
}
/*
 * downgrade write lock to read lock
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __downgrade_write(struct rw_semaphore *sem)
{
	asm volatile("# beginning __downgrade_write\n\t"
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
#if definedEx(CONFIG_X86_32)
" " "addl" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " "addq" " "
#endif
 "%2,(%1)\n\t"
		     /*
		      * transitions 0xZZZZ0001 -> 0xYYYY0001 (i386)
		      *     0xZZZZZZZZ00000001 -> 0xYYYYYYYY00000001 (x86_64)
		      */
		     "  jns       1f\n\t"
		     "  call call_rwsem_downgrade_wake\n"
		     "1:\n\t"
		     "# ending __downgrade_write\n"
		     : "+m" (sem->count)
		     : "a" (sem), "er" (-(-
#if definedEx(CONFIG_X86_64) && !definedEx(CONFIG_RWSEM_GENERIC_SPINLOCK)
0xffffffffL
#endif
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_RWSEM_GENERIC_SPINLOCK)
0x0000ffffL
#endif
-1))
		     : "memory", "cc");
}
/*
 * implement atomic add functionality
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void rwsem_atomic_add(rwsem_count_t delta,
				    struct rw_semaphore *sem)
{
	asm volatile(
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
#if definedEx(CONFIG_X86_32)
" " "addl" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " "addq" " "
#endif
 "%1,%0"
		     : "+m" (sem->count)
		     : "er" (delta));
}
/*
 * implement exchange and add functionality
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 rwsem_count_t rwsem_atomic_update(rwsem_count_t delta,
						struct rw_semaphore *sem)
{
	rwsem_count_t tmp = delta;
	asm volatile(
#if definedEx(CONFIG_SMP)
".section .smp_locks,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 "661f\n" ".previous\n" "661:\n\tlock; "
#endif
#if !definedEx(CONFIG_SMP)
""
#endif
 "xadd %0,%1"
		     : "+r" (tmp), "+m" (sem->count)
		     : : "memory");
	return tmp + delta;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int rwsem_is_locked(struct rw_semaphore *sem)
{
	return (sem->count != 0);
}
#line 24 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/rwsem.h" 2
#endif
/*
 * lock for reading
 */
extern void down_read(struct rw_semaphore *sem);
/*
 * trylock for reading -- returns 1 if successful, 0 if contention
 */
extern int down_read_trylock(struct rw_semaphore *sem);
/*
 * lock for writing
 */
extern void down_write(struct rw_semaphore *sem);
/*
 * trylock for writing -- returns 1 if successful, 0 if contention
 */
extern int down_write_trylock(struct rw_semaphore *sem);
/*
 * release a read lock
 */
extern void up_read(struct rw_semaphore *sem);
/*
 * release a write lock
 */
extern void up_write(struct rw_semaphore *sem);
/*
 * downgrade write lock to read lock
 */
extern void downgrade_write(struct rw_semaphore *sem);
#if definedEx(CONFIG_DEBUG_LOCK_ALLOC)
/*
 * nested locking. NOTE: rwsems are not allowed to recurse
 * (which occurs if the same task tries to acquire the same
 * lock instance multiple times), but multiple locks of the
 * same lock class might be taken, if the order of the locks
 * is always the same. This ordering rule can be expressed
 * to lockdep via the _nested() APIs, but enumerating the
 * subclasses that are used. (If the nesting relationship is
 * static then another method for expressing nested locking is
 * the explicit definition of lock class keys and the use of
 * lockdep_set_class() at lock initialization time.
 * See Documentation/lockdep-design.txt for more details.)
 */
extern void down_read_nested(struct rw_semaphore *sem, int subclass);
extern void down_write_nested(struct rw_semaphore *sem, int subclass);
/*
 * Take/release a lock when not the owner will release it.
 *
 * [ This API should be avoided as much as possible - the
 *   proper abstraction for this case is completions. ]
 */
extern void down_read_non_owner(struct rw_semaphore *sem);
extern void up_read_non_owner(struct rw_semaphore *sem);
#endif
#if !definedEx(CONFIG_DEBUG_LOCK_ALLOC)
#endif
#line 16 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/notifier.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/srcu.h" 1
/*
 * Sleepable Read-Copy Update mechanism for mutual exclusion
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation; either version 2 of the License, or
 * (at your option) any later version.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program; if not, write to the Free Software
 * Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
 *
 * Copyright (C) IBM Corporation, 2006
 *
 * Author: Paul McKenney <paulmck@us.ibm.com>
 *
 * For detailed explanation of Read-Copy Update mechanism see -
 * 		Documentation/RCU/ *.txt
 *
 */
struct srcu_struct_array {
	int c[2];
};
struct srcu_struct {
	int completed;
	struct srcu_struct_array *per_cpu_ref;
	struct mutex mutex;
};
#if !definedEx(CONFIG_PREEMPT)
#endif
#if definedEx(CONFIG_PREEMPT)
#endif
int init_srcu_struct(struct srcu_struct *sp);
void cleanup_srcu_struct(struct srcu_struct *sp);
int srcu_read_lock(struct srcu_struct *sp) ;
void srcu_read_unlock(struct srcu_struct *sp, int idx) ;
void synchronize_srcu(struct srcu_struct *sp);
void synchronize_srcu_expedited(struct srcu_struct *sp);
long srcu_batches_completed(struct srcu_struct *sp);
#line 17 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/notifier.h" 2
/*
 * Notifier chains are of four types:
 *
 *	Atomic notifier chains: Chain callbacks run in interrupt/atomic
 *		context. Callouts are not allowed to block.
 *	Blocking notifier chains: Chain callbacks run in process context.
 *		Callouts are allowed to block.
 *	Raw notifier chains: There are no restrictions on callbacks,
 *		registration, or unregistration.  All locking and protection
 *		must be provided by the caller.
 *	SRCU notifier chains: A variant of blocking notifier chains, with
 *		the same restrictions.
 *
 * atomic_notifier_chain_register() may be called from an atomic context,
 * but blocking_notifier_chain_register() and srcu_notifier_chain_register()
 * must be called from a process context.  Ditto for the corresponding
 * _unregister() routines.
 *
 * atomic_notifier_chain_unregister(), blocking_notifier_chain_unregister(),
 * and srcu_notifier_chain_unregister() _must not_ be called from within
 * the call chain.
 *
 * SRCU notifier chains are an alternative form of blocking notifier chains.
 * They use SRCU (Sleepable Read-Copy Update) instead of rw-semaphores for
 * protection of the chain links.  This means there is _very_ low overhead
 * in srcu_notifier_call_chain(): no cache bounces and no memory barriers.
 * As compensation, srcu_notifier_chain_unregister() is rather expensive.
 * SRCU notifier chains should be used when the chain will be called very
 * often but notifier_blocks will seldom be removed.  Also, SRCU notifier
 * chains are slightly more difficult to use because they require special
 * runtime initialization.
 */
struct notifier_block {
	int (*notifier_call)(struct notifier_block *, unsigned long, void *);
	struct notifier_block *next;
	int priority;
};
struct atomic_notifier_head {
	spinlock_t lock;
	struct notifier_block *head;
};
struct blocking_notifier_head {
	struct rw_semaphore rwsem;
	struct notifier_block *head;
};
struct raw_notifier_head {
	struct notifier_block *head;
};
struct srcu_notifier_head {
	struct mutex mutex;
	struct srcu_struct srcu;
	struct notifier_block *head;
};
/* srcu_notifier_heads must be initialized and cleaned up dynamically */
extern void srcu_init_notifier_head(struct srcu_notifier_head *nh);
/* srcu_notifier_heads cannot be initialized statically */
extern int atomic_notifier_chain_register(struct atomic_notifier_head *nh,
		struct notifier_block *nb);
extern int blocking_notifier_chain_register(struct blocking_notifier_head *nh,
		struct notifier_block *nb);
extern int raw_notifier_chain_register(struct raw_notifier_head *nh,
		struct notifier_block *nb);
extern int srcu_notifier_chain_register(struct srcu_notifier_head *nh,
		struct notifier_block *nb);
extern int blocking_notifier_chain_cond_register(
		struct blocking_notifier_head *nh,
		struct notifier_block *nb);
extern int atomic_notifier_chain_unregister(struct atomic_notifier_head *nh,
		struct notifier_block *nb);
extern int blocking_notifier_chain_unregister(struct blocking_notifier_head *nh,
		struct notifier_block *nb);
extern int raw_notifier_chain_unregister(struct raw_notifier_head *nh,
		struct notifier_block *nb);
extern int srcu_notifier_chain_unregister(struct srcu_notifier_head *nh,
		struct notifier_block *nb);
extern int atomic_notifier_call_chain(struct atomic_notifier_head *nh,
		unsigned long val, void *v);
extern int __atomic_notifier_call_chain(struct atomic_notifier_head *nh,
	unsigned long val, void *v, int nr_to_call, int *nr_calls);
extern int blocking_notifier_call_chain(struct blocking_notifier_head *nh,
		unsigned long val, void *v);
extern int __blocking_notifier_call_chain(struct blocking_notifier_head *nh,
	unsigned long val, void *v, int nr_to_call, int *nr_calls);
extern int raw_notifier_call_chain(struct raw_notifier_head *nh,
		unsigned long val, void *v);
extern int __raw_notifier_call_chain(struct raw_notifier_head *nh,
	unsigned long val, void *v, int nr_to_call, int *nr_calls);
extern int srcu_notifier_call_chain(struct srcu_notifier_head *nh,
		unsigned long val, void *v);
extern int __srcu_notifier_call_chain(struct srcu_notifier_head *nh,
	unsigned long val, void *v, int nr_to_call, int *nr_calls);
						/* Bad/Veto action */
/*
 * Clean way to return from the notifier and stop further calls.
 */
/* Encapsulate (negative) errno value (in particular, NOTIFY_BAD <=> EPERM). */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int notifier_from_errno(int err)
{
	return 0x8000 | (0x0001 - err);
}
/* Restore (negative) errno value from notify return value. */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int notifier_to_errno(int ret)
{
	ret &= ~0x8000;
	return ret > 0x0001 ? 0x0001 - ret : 0;
}
/*
 *	Declared notifiers so far. I can imagine quite a few more chains
 *	over time (eg laptop power reset chains, reboot chain (to clean 
 *	device units up), device [un]mount chain, module load/unload chain,
 *	low memory chain, screenblank chain (for plug in modular screenblankers) 
 *	VC switch chains (for loadable kernel svgalib VC switch helpers) etc...
 */
/* netdevice notifier chain */
/* Used for CPU hotplug events occuring while tasks are frozen due to a suspend
 * operation in progress
 */
/* Hibernation and suspend events */
/* Console keyboard events.
 * Note: KBD_KEYCODE is always sent before KBD_UNBOUND_KEYCODE, KBD_UNICODE and
 * KBD_KEYSYM. */
extern struct blocking_notifier_head reboot_notifier_list;
/* Virtual Terminal events. */
#line 8 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/memory_hotplug.h" 2
struct page;
struct zone;
struct pglist_data;
struct mem_section;
#if definedEx(CONFIG_MEMORY_HOTPLUG)
/*
 * Types for free bootmem.
 * The normal smallest mapcount is -1. Here is smaller value than it.
 */
/*
 * pgdat resizing functions
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
void pgdat_resize_lock(struct pglist_data *pgdat, unsigned long *flags)
{
	do { 
#if !definedEx(CONFIG_SMP) && definedEx(CONFIG_DEBUG_SPINLOCK) || definedEx(CONFIG_SMP)
do { ({ unsigned long __dummy; typeof(*flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); *flags = _raw_spin_lock_irqsave(spinlock_check(&pgdat->node_size_lock)); } while (0)
#endif
#if !definedEx(CONFIG_SMP) && !definedEx(CONFIG_DEBUG_SPINLOCK)
do { ({ unsigned long __dummy; typeof(*flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); do { do { ({ unsigned long __dummy; typeof(*flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); do { (*flags) = __raw_local_irq_save(); } while (0); 
#if !definedEx(CONFIG_TRACE_IRQFLAGS)
do { } while (0)
#endif
#if definedEx(CONFIG_TRACE_IRQFLAGS)
trace_hardirqs_off()
#endif
; } while (0); do { 
#if definedEx(CONFIG_PREEMPT)
do { 
#if !definedEx(CONFIG_DEBUG_PREEMPT) && !definedEx(CONFIG_PREEMPT_TRACER)
do { (current_thread_info()->preempt_count) += (1); } while (0)
#endif
#if !definedEx(CONFIG_DEBUG_PREEMPT) && definedEx(CONFIG_PREEMPT_TRACER) || definedEx(CONFIG_DEBUG_PREEMPT)
add_preempt_count(1)
#endif
; __asm__ __volatile__("": : :"memory"); } while (0)
#endif
#if !definedEx(CONFIG_PREEMPT)
do { } while (0)
#endif
; (void)0; (void)(spinlock_check(&pgdat->node_size_lock)); } while (0); } while (0); } while (0)
#endif
; } while (0);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
void pgdat_resize_unlock(struct pglist_data *pgdat, unsigned long *flags)
{
	spin_unlock_irqrestore(&pgdat->node_size_lock, *flags);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
void pgdat_resize_init(struct pglist_data *pgdat)
{
	do { spinlock_check(&pgdat->node_size_lock); 
#if definedEx(CONFIG_DEBUG_SPINLOCK)
do { static struct lock_class_key __key; __raw_spin_lock_init((&(&pgdat->node_size_lock)->rlock), "&(&pgdat->node_size_lock)->rlock", &__key); } while (0)
#endif
#if !definedEx(CONFIG_DEBUG_SPINLOCK)
do { *(&(&pgdat->node_size_lock)->rlock) = (raw_spinlock_t) { .raw_lock = 
#if definedEx(CONFIG_SMP)
{ 0 }
#endif
#if !definedEx(CONFIG_SMP) && !definedEx(CONFIG_DEBUG_SPINLOCK)
{ }
#endif
,  
#if definedEx(CONFIG_DEBUG_LOCK_ALLOC)
.dep_map = { .name = "&(&pgdat->node_size_lock)->rlock" }
#endif
#if !definedEx(CONFIG_DEBUG_LOCK_ALLOC)
#endif
 }; } while (0)
#endif
; } while (0);
}
/*
 * Zone resizing functions
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned zone_span_seqbegin(struct zone *zone)
{
	return read_seqbegin(&zone->span_seqlock);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int zone_span_seqretry(struct zone *zone, unsigned iv)
{
	return read_seqretry(&zone->span_seqlock, iv);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void zone_span_writelock(struct zone *zone)
{
	write_seqlock(&zone->span_seqlock);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void zone_span_writeunlock(struct zone *zone)
{
	write_sequnlock(&zone->span_seqlock);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void zone_seqlock_init(struct zone *zone)
{
	do { (&zone->span_seqlock)->sequence = 0; do { spinlock_check(&(&zone->span_seqlock)->lock); 
#if definedEx(CONFIG_DEBUG_SPINLOCK)
do { static struct lock_class_key __key; __raw_spin_lock_init((&(&(&zone->span_seqlock)->lock)->rlock), "&(&(&zone->span_seqlock)->lock)->rlock", &__key); } while (0)
#endif
#if !definedEx(CONFIG_DEBUG_SPINLOCK)
do { *(&(&(&zone->span_seqlock)->lock)->rlock) = (raw_spinlock_t) { .raw_lock = 
#if definedEx(CONFIG_SMP)
{ 0 }
#endif
#if !definedEx(CONFIG_SMP) && !definedEx(CONFIG_DEBUG_SPINLOCK)
{ }
#endif
,  
#if definedEx(CONFIG_DEBUG_LOCK_ALLOC)
.dep_map = { .name = "&(&(&zone->span_seqlock)->lock)->rlock" }
#endif
#if !definedEx(CONFIG_DEBUG_LOCK_ALLOC)
#endif
 }; } while (0)
#endif
; } while (0); } while (0);
}
extern int zone_grow_free_lists(struct zone *zone, unsigned long new_nr_pages);
extern int zone_grow_waitqueues(struct zone *zone, unsigned long nr_pages);
extern int add_one_highpage(struct page *page, int pfn, int bad_ppro);
/* need some defines for these for archs that don't support it */
extern void online_page(struct page *page);
/* VM interface that may be used by firmware interface */
extern int online_pages(unsigned long, unsigned long);
extern void __offline_isolated_pages(unsigned long, unsigned long);
/* reasonably generic interface to expand the physical pages in a zone  */
extern int __add_pages(int nid, struct zone *zone, unsigned long start_pfn,
	unsigned long nr_pages);
extern int __remove_pages(struct zone *zone, unsigned long start_pfn,
	unsigned long nr_pages);
#if definedEx(CONFIG_NUMA)
extern int memory_add_physaddr_to_nid(u64 start);
#endif
#if !definedEx(CONFIG_NUMA)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int memory_add_physaddr_to_nid(u64 start)
{
	return 0;
}
#endif
#if definedEx(CONFIG_NUMA)
/*
 * If ARCH_HAS_NODEDATA_EXTENSION=n, this func is used to allocate pgdat.
 * XXX: kmalloc_node() can't work well to get new node's memory at this time.
 *	Because, pgdat for the new node is not allocated/initialized yet itself.
 *	To use new node's memory, more consideration will be necessary.
 */
/*
 * This definition is just for error path in node hotadd.
 * For node hotremove, we have to replace this.
 */
extern pg_data_t *node_data[];
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void arch_refresh_nodedata(int nid, pg_data_t *pgdat)
{
	node_data[nid] = pgdat;
}
#endif
#if !definedEx(CONFIG_NUMA)
/* never called */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 pg_data_t *generic_alloc_nodedata(int nid)
{
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/memory_hotplug.h"), "i" (136), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
#if !definedEx(CONFIG_BUG)
do {} while(0)
#endif
;
	return ((void *)0);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void generic_free_nodedata(pg_data_t *pgdat)
{
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void arch_refresh_nodedata(int nid, pg_data_t *pgdat)
{
}
#endif
#if definedEx(CONFIG_SPARSEMEM_VMEMMAP)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void register_page_bootmem_info_node(struct pglist_data *pgdat)
{
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void put_page_bootmem(struct page *page)
{
}
#endif
#if !definedEx(CONFIG_SPARSEMEM_VMEMMAP)
extern void register_page_bootmem_info_node(struct pglist_data *pgdat);
extern void put_page_bootmem(struct page *page);
#endif
#endif
#if !definedEx(CONFIG_MEMORY_HOTPLUG)
/*
 * Stub functions for when hotplug is off
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void pgdat_resize_lock(struct pglist_data *p, unsigned long *f) {}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void pgdat_resize_unlock(struct pglist_data *p, unsigned long *f) {}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void pgdat_resize_init(struct pglist_data *pgdat) {}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned zone_span_seqbegin(struct zone *zone)
{
	return 0;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int zone_span_seqretry(struct zone *zone, unsigned iv)
{
	return 0;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void zone_span_writelock(struct zone *zone) {}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void zone_span_writeunlock(struct zone *zone) {}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void zone_seqlock_init(struct zone *zone) {}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int mhp_notimplemented(const char *func)
{
	printk("<4>" "%s() called, with CONFIG_MEMORY_HOTPLUG disabled\n", func);
	dump_stack();
	return -38;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void register_page_bootmem_info_node(struct pglist_data *pgdat)
{
}
#endif
#if definedEx(CONFIG_MEMORY_HOTREMOVE)
extern int is_mem_section_removable(unsigned long pfn, unsigned long nr_pages);
#endif
#if !definedEx(CONFIG_MEMORY_HOTREMOVE)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int is_mem_section_removable(unsigned long pfn,
					unsigned long nr_pages)
{
	return 0;
}
#endif
extern int add_memory(int nid, u64 start, u64 size);
extern int arch_add_memory(int nid, u64 start, u64 size);
extern int remove_memory(u64 start, u64 size);
extern int sparse_add_one_section(struct zone *zone, unsigned long start_pfn,
								int nr_pages);
extern void sparse_remove_one_section(struct zone *zone, struct mem_section *ms);
extern struct page *sparse_decode_mem_map(unsigned long coded_mem_map,
					  unsigned long pnum);
#line 655 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/mmzone.h" 2
void get_zone_counts(unsigned long *active, unsigned long *inactive,
			unsigned long *free);
void build_all_zonelists(void);
void wakeup_kswapd(struct zone *zone, int order);
int zone_watermark_ok(struct zone *z, int order, unsigned long mark,
		int classzone_idx, int alloc_flags);
enum memmap_context {
	MEMMAP_EARLY,
	MEMMAP_HOTPLUG,
};
extern int init_currently_empty_zone(struct zone *zone, unsigned long start_pfn,
				     unsigned long size,
				     enum memmap_context context);
#if definedEx(CONFIG_HAVE_MEMORY_PRESENT)
void memory_present(int nid, unsigned long start, unsigned long end);
#endif
#if !definedEx(CONFIG_HAVE_MEMORY_PRESENT)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void memory_present(int nid, unsigned long start, unsigned long end) {}
#endif
#if definedEx(CONFIG_NEED_NODE_MEMMAP_SIZE)
unsigned long __attribute__ ((__section__(".init.text"))) __attribute__((__cold__)) __attribute__((no_instrument_function)) node_memmap_size_bytes(int, unsigned long, unsigned long);
#endif
/*
 * zone_idx() returns 0 for the ZONE_DMA zone, 1 for the ZONE_NORMAL zone, etc.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int populated_zone(struct zone *zone)
{
	return (!!zone->present_pages);
}
extern int movable_zone;
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int zone_movable_is_highmem(void)
{
#if definedEx(CONFIG_HIGHMEM)
	return movable_zone == ZONE_HIGHMEM;
#endif
#if !definedEx(CONFIG_HIGHMEM)
	return 0;
#endif
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int is_highmem_idx(enum zone_type idx)
{
#if definedEx(CONFIG_HIGHMEM)
	return (idx == ZONE_HIGHMEM ||
		(idx == ZONE_MOVABLE && zone_movable_is_highmem()));
#endif
#if !definedEx(CONFIG_HIGHMEM)
	return 0;
#endif
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int is_normal_idx(enum zone_type idx)
{
	return (idx == ZONE_NORMAL);
}
/**
 * is_highmem - helper function to quickly check if a struct zone is a 
 *              highmem zone or not.  This is an attempt to keep references
 *              to ZONE_{DMA/NORMAL/HIGHMEM/etc} in general code to a minimum.
 * @zone - pointer to struct zone variable
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int is_highmem(struct zone *zone)
{
#if definedEx(CONFIG_HIGHMEM)
	int zone_off = (char *)zone - (char *)zone->zone_pgdat->node_zones;
	return zone_off == ZONE_HIGHMEM * sizeof(*zone) ||
	       (zone_off == ZONE_MOVABLE * sizeof(*zone) &&
		zone_movable_is_highmem());
#endif
#if !definedEx(CONFIG_HIGHMEM)
	return 0;
#endif
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int is_normal(struct zone *zone)
{
	return zone == zone->zone_pgdat->node_zones + ZONE_NORMAL;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int is_dma32(struct zone *zone)
{
#if definedEx(CONFIG_ZONE_DMA32)
	return zone == zone->zone_pgdat->node_zones + ZONE_DMA32;
#endif
#if !definedEx(CONFIG_ZONE_DMA32)
	return 0;
#endif
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int is_dma(struct zone *zone)
{
	return zone == zone->zone_pgdat->node_zones + ZONE_DMA;
}
/* These two functions are used to setup the per zone pages min values */
struct ctl_table;
int min_free_kbytes_sysctl_handler(struct ctl_table *, int,
					void  *, size_t *, loff_t *);
extern int sysctl_lowmem_reserve_ratio[4-1];
int lowmem_reserve_ratio_sysctl_handler(struct ctl_table *, int,
					void  *, size_t *, loff_t *);
int percpu_pagelist_fraction_sysctl_handler(struct ctl_table *, int,
					void  *, size_t *, loff_t *);
int sysctl_min_unmapped_ratio_sysctl_handler(struct ctl_table *, int,
			void  *, size_t *, loff_t *);
int sysctl_min_slab_ratio_sysctl_handler(struct ctl_table *, int,
			void  *, size_t *, loff_t *);
extern int numa_zonelist_order_handler(struct ctl_table *, int,
			void  *, size_t *, loff_t *);
extern char numa_zonelist_order[];
#if !definedEx(CONFIG_NEED_MULTIPLE_NODES)
extern struct pglist_data contig_page_data;
#endif
#if definedEx(CONFIG_NEED_MULTIPLE_NODES)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/mmzone.h" 1
#if definedEx(CONFIG_X86_32)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/mmzone_32.h" 1
/*
 * Written by Pat Gaughen (gone@us.ibm.com) Mar 2002
 *
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/smp.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/cpumask.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/smp.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/init.h" 1
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/smp.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/percpu.h" 1
#line 8 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/smp.h" 2
/*
 * We need the APIC definitions automatically as part of 'smp.h'
 */
#if definedEx(CONFIG_X86_LOCAL_APIC)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/mpspec.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/init.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/mpspec.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/mpspec_def.h" 1
/*
 * Structure definitions for SMP machines following the
 * Intel Multiprocessing Specification 1.1 and 1.4.
 */
/*
 * This tag identifies where the SMP configuration
 * information is.
 */
/* Intel MP Floating Pointer Structure */
struct mpf_intel {
	char signature[4];		/* "_MP_"			*/
	unsigned int physptr;		/* Configuration table address	*/
	unsigned char length;		/* Our length (paragraphs)	*/
	unsigned char specification;	/* Specification version	*/
	unsigned char checksum;		/* Checksum (makes sum 0)	*/
	unsigned char feature1;		/* Standard or configuration ?	*/
	unsigned char feature2;		/* Bit7 set for IMCR|PIC	*/
	unsigned char feature3;		/* Unused (0)			*/
	unsigned char feature4;		/* Unused (0)			*/
	unsigned char feature5;		/* Unused (0)			*/
};
struct mpc_table {
	char signature[4];
	unsigned short length;		/* Size of table */
	char spec;			/* 0x01 */
	char checksum;
	char oem[8];
	char productid[12];
	unsigned int oemptr;		/* 0 if not present */
	unsigned short oemsize;		/* 0 if not present */
	unsigned short oemcount;
	unsigned int lapic;		/* APIC address */
	unsigned int reserved;
};
/* Followed by entries */
/* Used by IBM NUMA-Q to describe node locality */
struct mpc_cpu {
	unsigned char type;
	unsigned char apicid;		/* Local APIC number */
	unsigned char apicver;		/* Its versions */
	unsigned char cpuflag;
	unsigned int cpufeature;
	unsigned int featureflag;	/* CPUID feature value */
	unsigned int reserved[2];
};
struct mpc_bus {
	unsigned char type;
	unsigned char busid;
	unsigned char bustype[6];
};
/* List of Bus Type string values, Intel MP Spec. */
struct mpc_ioapic {
	unsigned char type;
	unsigned char apicid;
	unsigned char apicver;
	unsigned char flags;
	unsigned int apicaddr;
};
struct mpc_intsrc {
	unsigned char type;
	unsigned char irqtype;
	unsigned short irqflag;
	unsigned char srcbus;
	unsigned char srcbusirq;
	unsigned char dstapic;
	unsigned char dstirq;
};
enum mp_irq_source_types {
	mp_INT = 0,
	mp_NMI = 1,
	mp_SMI = 2,
	mp_ExtINT = 3
};
struct mpc_lintsrc {
	unsigned char type;
	unsigned char irqtype;
	unsigned short irqflag;
	unsigned char srcbusid;
	unsigned char srcbusirq;
	unsigned char destapic;
	unsigned char destapiclint;
};
struct mpc_oemtable {
	char signature[4];
	unsigned short length;		/* Size of table */
	char  rev;			/* 0x01 */
	char  checksum;
	char  mpc[8];
};
/*
 *	Default configurations
 *
 *	1	2 CPU ISA 82489DX
 *	2	2 CPU EISA 82489DX neither IRQ 0 timer nor IRQ 13 DMA chaining
 *	3	2 CPU EISA 82489DX
 *	4	2 CPU MCA 82489DX
 *	5	2 CPU ISA+PCI
 *	6	2 CPU EISA+PCI
 *	7	2 CPU MCA+PCI
 */
enum mp_bustype {
	MP_BUS_ISA = 1,
	MP_BUS_EISA,
	MP_BUS_PCI,
	MP_BUS_MCA,
};
#line 8 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/mpspec.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/x86_init.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/pgtable_types.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/x86_init.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/bootparam.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/bootparam.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/screen_info.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/screen_info.h" 2
/*
 * These are set up by the setup-routine at boot-time:
 */
struct screen_info {
	__u8  orig_x;		/* 0x00 */
	__u8  orig_y;		/* 0x01 */
	__u16 ext_mem_k;	/* 0x02 */
	__u16 orig_video_page;	/* 0x04 */
	__u8  orig_video_mode;	/* 0x06 */
	__u8  orig_video_cols;	/* 0x07 */
	__u8  flags;		/* 0x08 */
	__u8  unused2;		/* 0x09 */
	__u16 orig_video_ega_bx;/* 0x0a */
	__u16 unused3;		/* 0x0c */
	__u8  orig_video_lines;	/* 0x0e */
	__u8  orig_video_isVGA;	/* 0x0f */
	__u16 orig_video_points;/* 0x10 */
	/* VESA graphic mode -- linear frame buffer */
	__u16 lfb_width;	/* 0x12 */
	__u16 lfb_height;	/* 0x14 */
	__u16 lfb_depth;	/* 0x16 */
	__u32 lfb_base;		/* 0x18 */
	__u32 lfb_size;		/* 0x1c */
	__u16 cl_magic, cl_offset; /* 0x20 */
	__u16 lfb_linelength;	/* 0x24 */
	__u8  red_size;		/* 0x26 */
	__u8  red_pos;		/* 0x27 */
	__u8  green_size;	/* 0x28 */
	__u8  green_pos;	/* 0x29 */
	__u8  blue_size;	/* 0x2a */
	__u8  blue_pos;		/* 0x2b */
	__u8  rsvd_size;	/* 0x2c */
	__u8  rsvd_pos;		/* 0x2d */
	__u16 vesapm_seg;	/* 0x2e */
	__u16 vesapm_off;	/* 0x30 */
	__u16 pages;		/* 0x32 */
	__u16 vesa_attributes;	/* 0x34 */
	__u32 capabilities;     /* 0x36 */
	__u8  _reserved[6];	/* 0x3a */
} __attribute__((packed));
extern struct screen_info screen_info;
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/bootparam.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/apm_bios.h" 1
/*
 * Include file for the interface to an APM BIOS
 * Copyright 1994-2001 Stephen Rothwell (sfr@canb.auug.org.au)
 *
 * This program is free software; you can redistribute it and/or modify it
 * under the terms of the GNU General Public License as published by the
 * Free Software Foundation; either version 2, or (at your option) any
 * later version.
 *
 * This program is distributed in the hope that it will be useful, but
 * WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
 * General Public License for more details.
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 21 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/apm_bios.h" 2
typedef unsigned short	apm_event_t;
typedef unsigned short	apm_eventinfo_t;
struct apm_bios_info {
	__u16	version;
	__u16	cseg;
	__u32	offset;
	__u16	cseg_16;
	__u16	dseg;
	__u16	flags;
	__u16	cseg_len;
	__u16	cseg_16_len;
	__u16	dseg_len;
};
/* Results of APM Installation Check */
/*
 * Data for APM that is persistent across module unload/load
 */
struct apm_info {
	struct apm_bios_info	bios;
	unsigned short		connection_version;
	int			get_power_status_broken;
	int			get_power_status_swabinminutes;
	int			allow_ints;
	int			forbid_idle;
	int			realmode_power_off;
	int			disabled;
};
/*
 * The APM function codes
 */
/*
 * Function code for APM_FUNC_RESUME_TIMER
 */
/*
 * Function code for APM_FUNC_RESUME_ON_RING
 */
/*
 * Function code for APM_FUNC_TIMER_STATUS
 */
/*
 * in arch/i386/kernel/setup.c
 */
extern struct apm_info	apm_info;
/*
 * Power states
 */
/*
 * Events (results of Get PM Event)
 */
/*
 * Error codes
 */
/*
 * APM Device IDs
 */
/*
 * This is the "All Devices" ID communicated to the BIOS
 */
/*
 * Battery status
 */
/*
 * APM defined capability bit flags
 */
/*
 * ioctl operations
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/ioctl.h" 1
#line 217 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/apm_bios.h" 2
#line 8 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/bootparam.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/edd.h" 1
/*
 * linux/include/linux/edd.h
 *  Copyright (C) 2002, 2003, 2004 Dell Inc.
 *  by Matt Domsch <Matt_Domsch@dell.com>
 *
 * structures and definitions for the int 13h, ax={41,48}h
 * BIOS Enhanced Disk Drive Services
 * This is based on the T13 group document D1572 Revision 0 (August 14 2002)
 * available at http://www.t13.org/docs2002/d1572r0.pdf.  It is
 * very similar to D1484 Revision 3 http://www.t13.org/docs2002/d1484r3.pdf
 *
 * In a nutshell, arch/{i386,x86_64}/boot/setup.S populates a scratch
 * table in the boot_params that contains a list of BIOS-enumerated
 * boot devices.
 * In arch/{i386,x86_64}/kernel/setup.c, this information is
 * transferred into the edd structure, and in drivers/firmware/edd.c, that
 * information is used to identify BIOS boot disk.  The code in setup.S
 * is very sensitive to the size of these structures.
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License v2.0 as published by
 * the Free Software Foundation
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 35 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/edd.h" 2
struct edd_device_params {
	__u16 length;
	__u16 info_flags;
	__u32 num_default_cylinders;
	__u32 num_default_heads;
	__u32 sectors_per_track;
	__u64 number_of_sectors;
	__u16 bytes_per_sector;
	__u32 dpte_ptr;		/* 0xFFFFFFFF for our purposes */
	__u16 key;		/* = 0xBEDD */
	__u8 device_path_info_length;	/* = 44 */
	__u8 reserved2;
	__u16 reserved3;
	__u8 host_bus_type[4];
	__u8 interface_type[8];
	union {
		struct {
			__u16 base_address;
			__u16 reserved1;
			__u32 reserved2;
		} __attribute__ ((packed)) isa;
		struct {
			__u8 bus;
			__u8 slot;
			__u8 function;
			__u8 channel;
			__u32 reserved;
		} __attribute__ ((packed)) pci;
		/* pcix is same as pci */
		struct {
			__u64 reserved;
		} __attribute__ ((packed)) ibnd;
		struct {
			__u64 reserved;
		} __attribute__ ((packed)) xprs;
		struct {
			__u64 reserved;
		} __attribute__ ((packed)) htpt;
		struct {
			__u64 reserved;
		} __attribute__ ((packed)) unknown;
	} interface_path;
	union {
		struct {
			__u8 device;
			__u8 reserved1;
			__u16 reserved2;
			__u32 reserved3;
			__u64 reserved4;
		} __attribute__ ((packed)) ata;
		struct {
			__u8 device;
			__u8 lun;
			__u8 reserved1;
			__u8 reserved2;
			__u32 reserved3;
			__u64 reserved4;
		} __attribute__ ((packed)) atapi;
		struct {
			__u16 id;
			__u64 lun;
			__u16 reserved1;
			__u32 reserved2;
		} __attribute__ ((packed)) scsi;
		struct {
			__u64 serial_number;
			__u64 reserved;
		} __attribute__ ((packed)) usb;
		struct {
			__u64 eui;
			__u64 reserved;
		} __attribute__ ((packed)) i1394;
		struct {
			__u64 wwid;
			__u64 lun;
		} __attribute__ ((packed)) fibre;
		struct {
			__u64 identity_tag;
			__u64 reserved;
		} __attribute__ ((packed)) i2o;
		struct {
			__u32 array_number;
			__u32 reserved1;
			__u64 reserved2;
		} __attribute__ ((packed)) raid;
		struct {
			__u8 device;
			__u8 reserved1;
			__u16 reserved2;
			__u32 reserved3;
			__u64 reserved4;
		} __attribute__ ((packed)) sata;
		struct {
			__u64 reserved1;
			__u64 reserved2;
		} __attribute__ ((packed)) unknown;
	} device_path;
	__u8 reserved4;
	__u8 checksum;
} __attribute__ ((packed));
struct edd_info {
	__u8 device;
	__u8 version;
	__u16 interface_support;
	__u16 legacy_max_cylinder;
	__u8 legacy_max_head;
	__u8 legacy_sectors_per_track;
	struct edd_device_params params;
} __attribute__ ((packed));
struct edd {
	unsigned int mbr_signature[16];
	struct edd_info edd_info[6];
	unsigned char mbr_signature_nr;
	unsigned char edd_info_nr;
};
extern struct edd edd;
#line 9 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/bootparam.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/e820.h" 1
/*
 * Legacy E820 BIOS limits us to 128 (E820MAX) nodes due to the
 * constrained space in the zeropage.  If we have more nodes than
 * that, and if we've booted off EFI firmware, then the EFI tables
 * passed us from the EFI firmware can list more nodes.  Size our
 * internal memory map tables to have room for these additional
 * nodes, based on up to three entries per node for which the
 * kernel was built: MAX_NUMNODES == (1 << CONFIG_NODES_SHIFT),
 * plus E820MAX, allowing space for the possible duplicate E820
 * entries that might need room in the same arrays, prior to the
 * call to sanitize_e820_map() to remove duplicates.  The allowance
 * of three memory map entries per node is "enough" entries for
 * the initial hardware platform motivating this mechanism to make
 * use of additional EFI map entries.  Future platforms may want
 * to allow more than three entries per node or otherwise refine
 * this size.
 */
/*
 * Odd: 'make headers_check' complains about numa.h if I try
 * to collapse the next two #ifdef lines to a single line:
 *	#if defined(__KERNEL__) && defined(CONFIG_EFI)
 */
#if definedEx(CONFIG_EFI)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/numa.h" 1
#line 33 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/e820.h" 2
#endif
#if !definedEx(CONFIG_EFI)
#endif
/* reserved RAM used by kernel itself */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 54 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/e820.h" 2
struct e820entry {
	__u64 addr;	/* start of memory segment */
	__u64 size;	/* size of memory segment */
	__u32 type;	/* type of memory segment */
} __attribute__((packed));
struct e820map {
	__u32 nr_map;
	struct e820entry map[
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_X86_LOCAL_APIC) && definedEx(CONFIG_EFI)
(128 + 3 * (1 << 3))
#endif
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_X86_LOCAL_APIC) && !definedEx(CONFIG_EFI)
128
#endif
];
};
/* see comment in arch/x86/kernel/e820.c */
extern struct e820map e820;
extern struct e820map e820_saved;
extern unsigned long pci_mem_start;
extern int e820_any_mapped(u64 start, u64 end, unsigned type);
extern int e820_all_mapped(u64 start, u64 end, unsigned type);
extern void e820_add_region(u64 start, u64 size, int type);
extern void e820_print_map(char *who);
extern int
sanitize_e820_map(struct e820entry *biosmap, int max_nr_map, u32 *pnr_map);
extern u64 e820_update_range(u64 start, u64 size, unsigned old_type,
			       unsigned new_type);
extern u64 e820_remove_range(u64 start, u64 size, unsigned old_type,
			     int checktype);
extern void update_e820(void);
extern void e820_setup_gap(void);
extern int e820_search_gap(unsigned long *gapstart, unsigned long *gapsize,
			unsigned long start_addr, unsigned long long end_addr);
struct setup_data;
extern void parse_e820_ext(struct setup_data *data, unsigned long pa_data);
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_HIBERNATION) && definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_HIBERNATION)
extern void e820_mark_nosave_regions(unsigned long limit_pfn);
#endif
#if !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_HIBERNATION) && !definedEx(CONFIG_X86_64)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void e820_mark_nosave_regions(unsigned long limit_pfn)
{
}
#endif
#if definedEx(CONFIG_MEMTEST)
extern void early_memtest(unsigned long start, unsigned long end);
#endif
#if !definedEx(CONFIG_MEMTEST)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void early_memtest(unsigned long start, unsigned long end)
{
}
#endif
extern unsigned long end_user_pfn;
extern u64 find_e820_area(u64 start, u64 end, u64 size, u64 align);
extern u64 find_e820_area_size(u64 start, u64 *sizep, u64 align);
extern void reserve_early(u64 start, u64 end, char *name);
extern void reserve_early_overlap_ok(u64 start, u64 end, char *name);
extern void free_early(u64 start, u64 end);
extern void early_res_to_bootmem(u64 start, u64 end);
extern u64 early_reserve_e820(u64 startt, u64 sizet, u64 align);
extern unsigned long e820_end_of_ram_pfn(void);
extern unsigned long e820_end_of_low_ram_pfn(void);
extern int e820_find_active_region(const struct e820entry *ei,
				  unsigned long start_pfn,
				  unsigned long last_pfn,
				  unsigned long *ei_startpfn,
				  unsigned long *ei_endpfn);
extern void e820_register_active_regions(int nid, unsigned long start_pfn,
					 unsigned long end_pfn);
extern u64 e820_hole_size(u64 start, u64 end);
extern void finish_e820_parsing(void);
extern void e820_reserve_resources(void);
extern void e820_reserve_resources_late(void);
extern void setup_memory_map(void);
extern char *default_machine_specific_memory_setup(void);
/*
 * Returns true iff the specified range [s,e) is completely contained inside
 * the ISA region.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 bool is_ISA_range(u64 s, u64 e)
{
	return s >= 0xa0000 && e <= 0x100000;
}
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/ioport.h" 1
/*
 * ioport.h	Definitions of routines for detecting, reserving and
 *		allocating system resources.
 *
 * Authors:	Linus Torvalds
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/compiler.h" 1
#line 14 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/ioport.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 15 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/ioport.h" 2
/*
 * Resources are tree-like, allowing
 * nesting etc..
 */
struct resource {
	resource_size_t start;
	resource_size_t end;
	const char *name;
	unsigned long flags;
	struct resource *parent, *sibling, *child;
};
struct resource_list {
	struct resource_list *next;
	struct resource *res;
	struct pci_dev *dev;
};
/*
 * IO resources have these defined flags.
 */
/* PnP IRQ specific bits (IORESOURCE_BITS) */
/* PnP DMA specific bits (IORESOURCE_BITS) */
/* PnP memory I/O specific bits (IORESOURCE_BITS) */
/* PnP I/O specific bits (IORESOURCE_BITS) */
/* PCI ROM control bits (IORESOURCE_BITS) */
/* PCI control bits.  Shares IORESOURCE_BITS with above PCI ROM.  */
/* PC/ISA/whatever - the normal PC address spaces: IO and memory */
extern struct resource ioport_resource;
extern struct resource iomem_resource;
extern int request_resource(struct resource *root, struct resource *new);
extern int release_resource(struct resource *new);
extern void reserve_region_with_split(struct resource *root,
			     resource_size_t start, resource_size_t end,
			     const char *name);
extern int insert_resource(struct resource *parent, struct resource *new);
extern void insert_resource_expand_to_fit(struct resource *root, struct resource *new);
extern int allocate_resource(struct resource *root, struct resource *new,
			     resource_size_t size, resource_size_t min,
			     resource_size_t max, resource_size_t align,
			     void (*alignf)(void *, struct resource *,
					    resource_size_t, resource_size_t),
			     void *alignf_data);
int adjust_resource(struct resource *res, resource_size_t start,
		    resource_size_t size);
resource_size_t resource_alignment(struct resource *res);
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 resource_size_t resource_size(const struct resource *res)
{
	return res->end - res->start + 1;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long resource_type(const struct resource *res)
{
	return res->flags & 0x00000f00;
}
/* Convenience shorthand with allocation */
extern struct resource * __request_region(struct resource *,
					resource_size_t start,
					resource_size_t n,
					const char *name, int flags);
/* Compatibility cruft */
extern int __check_region(struct resource *, resource_size_t, resource_size_t);
extern void __release_region(struct resource *, resource_size_t,
				resource_size_t);
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int 
#if definedEx(CONFIG_ENABLE_WARN_DEPRECATED)
__attribute__((deprecated))
#endif
#if !definedEx(CONFIG_ENABLE_WARN_DEPRECATED)
#endif
 check_region(resource_size_t s,
						resource_size_t n)
{
	return __check_region(&ioport_resource, s, n);
}
/* Wrappers for managed devices */
struct device;
extern struct resource * __devm_request_region(struct device *dev,
				struct resource *parent, resource_size_t start,
				resource_size_t n, const char *name);
extern void __devm_release_region(struct device *dev, struct resource *parent,
				  resource_size_t start, resource_size_t n);
extern int iomem_map_sanity_check(resource_size_t addr, unsigned long size);
extern int iomem_is_exclusive(u64 addr);
extern int
walk_system_ram_range(unsigned long start_pfn, unsigned long nr_pages,
		void *arg, int (*func)(unsigned long, unsigned long, void *));
#line 151 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/e820.h" 2
#line 10 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/bootparam.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/ist.h" 1
/*
 * Include file for the interface to IST BIOS
 * Copyright 2002 Andy Grover <andrew.grover@intel.com>
 *
 * This program is free software; you can redistribute it and/or modify it
 * under the terms of the GNU General Public License as published by the
 * Free Software Foundation; either version 2, or (at your option) any
 * later version.
 *
 * This program is distributed in the hope that it will be useful, but
 * WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
 * General Public License for more details.
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 22 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/ist.h" 2
struct ist_info {
	__u32 signature;
	__u32 command;
	__u32 event;
	__u32 perf_level;
};
extern struct ist_info ist_info;
#line 11 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/bootparam.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/video/edid.h" 1
struct edid_info {
	unsigned char dummy[128];
};
extern struct edid_info edid_info;
#line 12 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/bootparam.h" 2
/* setup data types */
/* extensible setup data list node */
struct setup_data {
	__u64 next;
	__u32 type;
	__u32 len;
	__u8 data[0];
};
struct setup_header {
	__u8	setup_sects;
	__u16	root_flags;
	__u32	syssize;
	__u16	ram_size;
	__u16	vid_mode;
	__u16	root_dev;
	__u16	boot_flag;
	__u16	jump;
	__u32	header;
	__u16	version;
	__u32	realmode_swtch;
	__u16	start_sys;
	__u16	kernel_version;
	__u8	type_of_loader;
	__u8	loadflags;
	__u16	setup_move_size;
	__u32	code32_start;
	__u32	ramdisk_image;
	__u32	ramdisk_size;
	__u32	bootsect_kludge;
	__u16	heap_end_ptr;
	__u8	ext_loader_ver;
	__u8	ext_loader_type;
	__u32	cmd_line_ptr;
	__u32	initrd_addr_max;
	__u32	kernel_alignment;
	__u8	relocatable_kernel;
	__u8	_pad2[3];
	__u32	cmdline_size;
	__u32	hardware_subarch;
	__u64	hardware_subarch_data;
	__u32	payload_offset;
	__u32	payload_length;
	__u64	setup_data;
} __attribute__((packed));
struct sys_desc_table {
	__u16 length;
	__u8  table[14];
};
struct efi_info {
	__u32 efi_loader_signature;
	__u32 efi_systab;
	__u32 efi_memdesc_size;
	__u32 efi_memdesc_version;
	__u32 efi_memmap;
	__u32 efi_memmap_size;
	__u32 efi_systab_hi;
	__u32 efi_memmap_hi;
};
/* The so-called "zeropage" */
struct boot_params {
	struct screen_info screen_info;			/* 0x000 */
	struct apm_bios_info apm_bios_info;		/* 0x040 */
	__u8  _pad2[4];					/* 0x054 */
	__u64  tboot_addr;				/* 0x058 */
	struct ist_info ist_info;			/* 0x060 */
	__u8  _pad3[16];				/* 0x070 */
	__u8  hd0_info[16];	/* obsolete! */		/* 0x080 */
	__u8  hd1_info[16];	/* obsolete! */		/* 0x090 */
	struct sys_desc_table sys_desc_table;		/* 0x0a0 */
	__u8  _pad4[144];				/* 0x0b0 */
	struct edid_info edid_info;			/* 0x140 */
	struct efi_info efi_info;			/* 0x1c0 */
	__u32 alt_mem_k;				/* 0x1e0 */
	__u32 scratch;		/* Scratch field! */	/* 0x1e4 */
	__u8  e820_entries;				/* 0x1e8 */
	__u8  eddbuf_entries;				/* 0x1e9 */
	__u8  edd_mbr_sig_buf_entries;			/* 0x1ea */
	__u8  _pad6[6];					/* 0x1eb */
	struct setup_header hdr;    /* setup header */	/* 0x1f1 */
	__u8  _pad7[0x290-0x1f1-sizeof(struct setup_header)];
	__u32 edd_mbr_sig_buffer[16];	/* 0x290 */
	struct e820entry e820_map[128];		/* 0x2d0 */
	__u8  _pad8[48];				/* 0xcd0 */
	struct edd_info eddbuf[6];		/* 0xd00 */
	__u8  _pad9[276];				/* 0xeec */
} __attribute__((packed));
enum {
	X86_SUBARCH_PC = 0,
	X86_SUBARCH_LGUEST,
	X86_SUBARCH_XEN,
	X86_SUBARCH_MRST,
	X86_NR_SUBARCHS,
};
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/x86_init.h" 2
struct mpc_bus;
struct mpc_cpu;
struct mpc_table;
/**
 * struct x86_init_mpparse - platform specific mpparse ops
 * @mpc_record:			platform specific mpc record accounting
 * @setup_ioapic_ids:		platform specific ioapic id override
 * @mpc_apic_id:		platform specific mpc apic id assignment
 * @smp_read_mpc_oem:		platform specific oem mpc table setup
 * @mpc_oem_pci_bus:		platform specific pci bus setup (default NULL)
 * @mpc_oem_bus_info:		platform specific mpc bus info
 * @find_smp_config:		find the smp configuration
 * @get_smp_config:		get the smp configuration
 */
struct x86_init_mpparse {
	void (*mpc_record)(unsigned int mode);
	void (*setup_ioapic_ids)(void);
	int (*mpc_apic_id)(struct mpc_cpu *m);
	void (*smp_read_mpc_oem)(struct mpc_table *mpc);
	void (*mpc_oem_pci_bus)(struct mpc_bus *m);
	void (*mpc_oem_bus_info)(struct mpc_bus *m, char *name);
	void (*find_smp_config)(void);
	void (*get_smp_config)(unsigned int early);
};
/**
 * struct x86_init_resources - platform specific resource related ops
 * @probe_roms:			probe BIOS roms
 * @reserve_resources:		reserve the standard resources for the
 *				platform
 * @memory_setup:		platform specific memory setup
 *
 */
struct x86_init_resources {
	void (*probe_roms)(void);
	void (*reserve_resources)(void);
	char *(*memory_setup)(void);
};
/**
 * struct x86_init_irqs - platform specific interrupt setup
 * @pre_vector_init:		init code to run before interrupt vectors
 *				are set up.
 * @intr_init:			interrupt init code
 * @trap_init:			platform specific trap setup
 */
struct x86_init_irqs {
	void (*pre_vector_init)(void);
	void (*intr_init)(void);
	void (*trap_init)(void);
};
/**
 * struct x86_init_oem - oem platform specific customizing functions
 * @arch_setup:			platform specific architecure setup
 * @banner:			print a platform specific banner
 */
struct x86_init_oem {
	void (*arch_setup)(void);
	void (*banner)(void);
};
/**
 * struct x86_init_paging - platform specific paging functions
 * @pagetable_setup_start:	platform specific pre paging_init() call
 * @pagetable_setup_done:	platform specific post paging_init() call
 */
struct x86_init_paging {
	void (*pagetable_setup_start)(pgd_t *base);
	void (*pagetable_setup_done)(pgd_t *base);
};
/**
 * struct x86_init_timers - platform specific timer setup
 * @setup_perpcu_clockev:	set up the per cpu clock event device for the
 *				boot cpu
 * @tsc_pre_init:		platform function called before TSC init
 * @timer_init:			initialize the platform timer (default PIT/HPET)
 */
struct x86_init_timers {
	void (*setup_percpu_clockev)(void);
	void (*tsc_pre_init)(void);
	void (*timer_init)(void);
};
/**
 * struct x86_init_iommu - platform specific iommu setup
 * @iommu_init:			platform specific iommu setup
 */
struct x86_init_iommu {
	int (*iommu_init)(void);
};
/**
 * struct x86_init_ops - functions for platform specific setup
 *
 */
struct x86_init_ops {
	struct x86_init_resources	resources;
	struct x86_init_mpparse		mpparse;
	struct x86_init_irqs		irqs;
	struct x86_init_oem		oem;
	struct x86_init_paging		paging;
	struct x86_init_timers		timers;
	struct x86_init_iommu		iommu;
};
/**
 * struct x86_cpuinit_ops - platform specific cpu hotplug setups
 * @setup_percpu_clockev:	set up the per cpu clock event device
 */
struct x86_cpuinit_ops {
	void (*setup_percpu_clockev)(void);
};
/**
 * struct x86_platform_ops - platform specific runtime functions
 * @calibrate_tsc:		calibrate TSC
 * @get_wallclock:		get time from HW clock like RTC etc.
 * @set_wallclock:		set time back to HW clock
 * @is_untracked_pat_range	exclude from PAT logic
 */
struct x86_platform_ops {
	unsigned long (*calibrate_tsc)(void);
	unsigned long (*get_wallclock)(void);
	int (*set_wallclock)(unsigned long nowtime);
	void (*iommu_shutdown)(void);
	bool (*is_untracked_pat_range)(u64 start, u64 end);
};
extern struct x86_init_ops x86_init;
extern struct x86_cpuinit_ops x86_cpuinit;
extern struct x86_platform_ops x86_platform;
extern void x86_init_noop(void);
extern void x86_init_uint_noop(unsigned int unused);
#line 9 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/mpspec.h" 2
extern int apic_version[256];
extern int pic_mode;
/*
 * Summit or generic (i.e. installer) kernels need lots of bus entries.
 * Maximum 256 PCI busses, plus 1 ISA bus in each of 4 cabinets.
 */
extern unsigned int def_to_bigsmp;
extern u8 apicid_2_node[];
#if definedEx(CONFIG_X86_NUMAQ)
extern int mp_bus_id_to_node[260];
extern int mp_bus_id_to_local[260];
extern int quad_local_to_mp_bus_id [8/4][4];
#endif
#if !definedEx(CONFIG_MCA) && definedEx(CONFIG_EISA) || definedEx(CONFIG_MCA)
extern int mp_bus_id_to_type[260];
#endif
extern unsigned long mp_bus_not_pci[(((260) + (8 * sizeof(long)) - 1) / (8 * sizeof(long)))];
extern unsigned int boot_cpu_physical_apicid;
extern unsigned int max_physical_apicid;
extern int mpc_default_type;
extern unsigned long mp_lapic_addr;
extern int smp_found_config;
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void get_smp_config(void)
{
	x86_init.mpparse.get_smp_config(0);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void early_get_smp_config(void)
{
	x86_init.mpparse.get_smp_config(1);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void find_smp_config(void)
{
	x86_init.mpparse.find_smp_config();
}
#if definedEx(CONFIG_X86_MPPARSE)
extern void early_reserve_e820_mpc_new(void);
extern int enable_update_mptable;
extern int default_mpc_apic_id(struct mpc_cpu *m);
extern void default_smp_read_mpc_oem(struct mpc_table *mpc);
#if definedEx(CONFIG_X86_IO_APIC)
extern void default_mpc_oem_bus_info(struct mpc_bus *m, char *str);
#endif
#if !definedEx(CONFIG_X86_IO_APIC)
#endif
extern void default_find_smp_config(void);
extern void default_get_smp_config(unsigned int early);
#endif
#if !definedEx(CONFIG_X86_MPPARSE)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void early_reserve_e820_mpc_new(void) { }
#endif
void __attribute__ ((__section__(".cpuinit.text"))) __attribute__((__cold__)) generic_processor_info(int apicid, int version);
#if definedEx(CONFIG_ACPI)
extern void mp_register_ioapic(int id, u32 address, u32 gsi_base);
extern void mp_override_legacy_irq(u8 bus_irq, u8 polarity, u8 trigger,
				   u32 gsi);
extern void mp_config_acpi_legacy_irqs(void);
struct device;
extern int mp_register_gsi(struct device *dev, u32 gsi, int edge_level,
				 int active_high_low);
extern int acpi_probe_gsi(void);
#if definedEx(CONFIG_X86_IO_APIC)
extern int mp_find_ioapic(int gsi);
extern int mp_find_ioapic_pin(int ioapic, int gsi);
#endif
#endif
#if !definedEx(CONFIG_ACPI)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int acpi_probe_gsi(void)
{
	return 0;
}
#endif
struct physid_mask {
	unsigned long mask[(((256) + (8 * sizeof(long)) - 1) / (8 * sizeof(long)))];
};
typedef struct physid_mask physid_mask_t;
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long physids_coerce(physid_mask_t *map)
{
	return map->mask[0];
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void physids_promote(unsigned long physids, physid_mask_t *map)
{
	bitmap_zero((*map).mask, 256);
	map->mask[0] = physids;
}
/* Note: will create very large stack frames if physid_mask_t is big */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void physid_set_mask_of_physid(int physid, physid_mask_t *map)
{
	bitmap_zero((*map).mask, 256);
	set_bit(physid, (*map).mask);
}
extern physid_mask_t phys_cpu_present_map;
extern int generic_mps_oem_check(struct mpc_table *, char *, char *);
extern int default_acpi_madt_oem_check(char *, char *);
#line 14 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/smp.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/apic.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/cpumask.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/apic.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/delay.h" 1
/*
 * Copyright (C) 1993 Linus Torvalds
 *
 * Delay routines, using a pre-computed "loops_per_jiffy" value.
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/kernel.h" 1
#line 12 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/delay.h" 2
extern unsigned long loops_per_jiffy;
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/delay.h" 1
/*
 * Copyright (C) 1993 Linus Torvalds
 *
 * Delay routines calling functions in arch/x86/lib/delay.c
 */
/* Undefined functions to get compile-time errors */
extern void __bad_udelay(void);
extern void __bad_ndelay(void);
extern void __udelay(unsigned long usecs);
extern void __ndelay(unsigned long nsecs);
extern void __const_udelay(unsigned long xloops);
extern void __delay(unsigned long loops);
/* 0x10c7 is 2**32 / 1000000 (rounded up) */
/* 0x5 is 2**32 / 1000000000 (rounded up) */
void use_tsc_delay(void);
#line 16 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/delay.h" 2
/*
 * Using udelay() for intervals greater than a few milliseconds can
 * risk overflow for high loops_per_jiffy (high bogomips) machines. The
 * mdelay() provides a wrapper to prevent this.  For delays greater
 * than MAX_UDELAY_MS milliseconds, the wrapper is used.  Architecture
 * specific values can be defined in asm-???/delay.h as an override.
 * The 2nd mdelay() definition ensures GCC will optimize away the 
 * while loop for the common cases where n <= MAX_UDELAY_MS  --  Paul G.
 */
extern unsigned long lpj_fine;
void calibrate_delay(void);
void msleep(unsigned int msecs);
unsigned long msleep_interruptible(unsigned int msecs);
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void ssleep(unsigned int seconds)
{
	msleep(seconds * 1000);
}
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/apic.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/pm.h" 1
/*
 *  pm.h - Power management interface
 *
 *  Copyright (C) 2000 Andrew Henroid
 *
 *  This program is free software; you can redistribute it and/or modify
 *  it under the terms of the GNU General Public License as published by
 *  the Free Software Foundation; either version 2 of the License, or
 *  (at your option) any later version.
 *
 *  This program is distributed in the hope that it will be useful,
 *  but WITHOUT ANY WARRANTY; without even the implied warranty of
 *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *  GNU General Public License for more details.
 *
 *  You should have received a copy of the GNU General Public License
 *  along with this program; if not, write to the Free Software
 *  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/list.h" 1
#line 26 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/pm.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/workqueue.h" 1
/*
 * workqueue.h --- work queue handling for Linux.
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/timer.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/list.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/timer.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/ktime.h" 1
/*
 *  include/linux/ktime.h
 *
 *  ktime_t - nanosecond-resolution time format.
 *
 *   Copyright(C) 2005, Thomas Gleixner <tglx@linutronix.de>
 *   Copyright(C) 2005, Red Hat, Inc., Ingo Molnar
 *
 *  data type definitions, declarations, prototypes and macros.
 *
 *  Started by: Thomas Gleixner and Ingo Molnar
 *
 *  Credits:
 *
 *  	Roman Zippel provided the ideas and primary code snippets of
 *  	the ktime_t union and further simplifications of the original
 *  	code.
 *
 *  For licencing details see kernel-base/COPYING
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/time.h" 1
#line 26 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/ktime.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/jiffies.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/math64.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/jiffies.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/kernel.h" 1
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/jiffies.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 8 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/jiffies.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/time.h" 1
#line 9 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/jiffies.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/timex.h" 1
/*****************************************************************************
 *                                                                           *
 * Copyright (c) David L. Mills 1993                                         *
 *                                                                           *
 * Permission to use, copy, modify, and distribute this software and its     *
 * documentation for any purpose and without fee is hereby granted, provided *
 * that the above copyright notice appears in all copies and that both the   *
 * copyright notice and this permission notice appear in supporting          *
 * documentation, and that the name University of Delaware not be used in    *
 * advertising or publicity pertaining to distribution of the software       *
 * without specific, written prior permission.  The University of Delaware   *
 * makes no representations about the suitability this software for any      *
 * purpose.  It is provided "as is" without express or implied warranty.     *
 *                                                                           *
 *****************************************************************************/
/*
 * Modification history timex.h
 *
 * 29 Dec 97	Russell King
 *	Moved CLOCK_TICK_RATE, CLOCK_TICK_FACTOR and FINETUNE to asm/timex.h
 *	for ARM machines
 *
 *  9 Jan 97    Adrian Sun
 *      Shifted LATCH define to allow access to alpha machines.
 *
 * 26 Sep 94	David L. Mills
 *	Added defines for hybrid phase/frequency-lock loop.
 *
 * 19 Mar 94	David L. Mills
 *	Moved defines from kernel routines to header file and added new
 *	defines for PPS phase-lock loop.
 *
 * 20 Feb 94	David L. Mills
 *	Revised status codes and structures for external clock and PPS
 *	signal discipline.
 *
 * 28 Nov 93	David L. Mills
 *	Adjusted parameters to improve stability and increase poll
 *	interval.
 *
 * 17 Sep 93    David L. Mills
 *      Created file $NTP/include/sys/timex.h
 * 07 Oct 93    Torsten Duwe
 *      Derived linux/timex.h
 * 1995-08-13    Torsten Duwe
 *      kernel PLL updated to 1994-12-13 specs (rfc-1589)
 * 1997-08-30    Ulrich Windl
 *      Added new constant NTP_PHASE_LIMIT
 * 2004-08-12    Christoph Lameter
 *      Reworked time interpolation logic
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/time.h" 1
#line 58 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/timex.h" 2
/*
 * syscall interface - used (mainly by NTP daemon)
 * to discipline kernel clock oscillator
 */
struct timex {
	unsigned int modes;	/* mode selector */
	long offset;		/* time offset (usec) */
	long freq;		/* frequency offset (scaled ppm) */
	long maxerror;		/* maximum error (usec) */
	long esterror;		/* estimated error (usec) */
	int status;		/* clock command/status */
	long constant;		/* pll time constant */
	long precision;		/* clock precision (usec) (read only) */
	long tolerance;		/* clock frequency tolerance (ppm)
				 * (read only)
				 */
	struct timeval time;	/* (read only) */
	long tick;		/* (modified) usecs between clock ticks */
	long ppsfreq;           /* pps frequency (scaled ppm) (ro) */
	long jitter;            /* pps jitter (us) (ro) */
	int shift;              /* interval duration (s) (shift) (ro) */
	long stabil;            /* pps stability (scaled ppm) (ro) */
	long jitcnt;            /* jitter limit exceeded (ro) */
	long calcnt;            /* calibration intervals (ro) */
	long errcnt;            /* calibration errors (ro) */
	long stbcnt;            /* stability limit exceeded (ro) */
	int tai;		/* TAI offset (ro) */
	int  :32; int  :32; int  :32; int  :32;
	int  :32; int  :32; int  :32; int  :32;
	int  :32; int  :32; int  :32;
};
/*
 * Mode codes (timex.mode)
 */
/* NTP userland likes the MOD_ prefix better */
/*
 * Status codes (timex.status)
 */
/* read-only bits */
/*
 * Clock states (time_state)
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/compiler.h" 1
#line 171 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/timex.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 172 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/timex.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/param.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/param.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/asm-generic/param.h" 1
#line 3 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/param.h" 2
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/param.h" 2
#line 173 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/timex.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/timex.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/processor.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/timex.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/tsc.h" 1
/*
 * x86 TSC related functions
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/processor.h" 1
#line 9 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/tsc.h" 2
/*
 * Standard way to access the cycle counter.
 */
typedef unsigned long long cycles_t;
extern unsigned int cpu_khz;
extern unsigned int tsc_khz;
extern void disable_TSC(void);
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 cycles_t get_cycles(void)
{
	unsigned long long ret = 0;
#if !definedEx(CONFIG_X86_TSC)
	if (!(__builtin_constant_p((0*32+ 4)) && ( ((((0*32+ 4))>>5)==0 && (1UL<<(((0*32+ 4))&31) & (
#if !definedEx(CONFIG_MATH_EMULATION)
(1<<((0*32+ 0) & 31))
#endif
#if definedEx(CONFIG_MATH_EMULATION)
0
#endif
|
#if !definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_64) && definedEx(CONFIG_PARAVIRT)
0
#endif
#if definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT)
(1<<((0*32+ 3)) & 31)
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+ 5) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if !definedEx(CONFIG_X86_PAE) && definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_PAE)
(1<<((0*32+ 6) & 31))
#endif
#if !definedEx(CONFIG_X86_PAE) && !definedEx(CONFIG_X86_64)
0
#endif
| 
#if definedEx(CONFIG_X86_CMPXCHG64)
(1<<((0*32+ 8) & 31))
#endif
#if !definedEx(CONFIG_X86_CMPXCHG64)
0
#endif
|
#if !definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_64) && definedEx(CONFIG_PARAVIRT)
0
#endif
#if definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT)
(1<<((0*32+13)) & 31)
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+24) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if !definedEx(CONFIG_X86_64) && definedEx(CONFIG_X86_CMOV) || definedEx(CONFIG_X86_64)
(1<<((0*32+15) & 31))
#endif
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_X86_CMOV)
0
#endif
| 
#if definedEx(CONFIG_X86_64)
(1<<((0*32+25) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+26) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
))) || ((((0*32+ 4))>>5)==1 && (1UL<<(((0*32+ 4))&31) & (
#if definedEx(CONFIG_X86_64)
(1<<((1*32+29) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if definedEx(CONFIG_X86_USE_3DNOW)
(1<<((1*32+31) & 31))
#endif
#if !definedEx(CONFIG_X86_USE_3DNOW)
0
#endif
))) || ((((0*32+ 4))>>5)==2 && (1UL<<(((0*32+ 4))&31) & 0)) || ((((0*32+ 4))>>5)==3 && (1UL<<(((0*32+ 4))&31) & (
#if !definedEx(CONFIG_X86_64) && definedEx(CONFIG_X86_P6_NOP) || definedEx(CONFIG_X86_64)
(1<<((3*32+20) & 31))
#endif
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_X86_P6_NOP)
0
#endif
))) || ((((0*32+ 4))>>5)==4 && (1UL<<(((0*32+ 4))&31) & 0)) || ((((0*32+ 4))>>5)==5 && (1UL<<(((0*32+ 4))&31) & 0)) || ((((0*32+ 4))>>5)==6 && (1UL<<(((0*32+ 4))&31) & 0)) || ((((0*32+ 4))>>5)==7 && (1UL<<(((0*32+ 4))&31) & 0)) ) ? 1 : (__builtin_constant_p(((0*32+ 4))) ? constant_test_bit(((0*32+ 4)), ((unsigned long *)((&boot_cpu_data)->x86_capability))) : variable_test_bit(((0*32+ 4)), ((unsigned long *)((&boot_cpu_data)->x86_capability))))))
		return 0;
#endif
#if definedEx(CONFIG_PARAVIRT)
(ret = paravirt_read_tsc())
#endif
#if !definedEx(CONFIG_PARAVIRT)
((ret) = __native_read_tsc())
#endif
;
	return ret;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __attribute__((always_inline)) cycles_t vget_cycles(void)
{
	/*
	 * We only do VDSOs on TSC capable CPUs, so this shouldnt
	 * access boot_cpu_data (which is not VDSO-safe):
	 */
#if !definedEx(CONFIG_X86_TSC)
	if (!(__builtin_constant_p((0*32+ 4)) && ( ((((0*32+ 4))>>5)==0 && (1UL<<(((0*32+ 4))&31) & (
#if !definedEx(CONFIG_MATH_EMULATION)
(1<<((0*32+ 0) & 31))
#endif
#if definedEx(CONFIG_MATH_EMULATION)
0
#endif
|
#if !definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_64) && definedEx(CONFIG_PARAVIRT)
0
#endif
#if definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT)
(1<<((0*32+ 3)) & 31)
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+ 5) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if !definedEx(CONFIG_X86_PAE) && definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_PAE)
(1<<((0*32+ 6) & 31))
#endif
#if !definedEx(CONFIG_X86_PAE) && !definedEx(CONFIG_X86_64)
0
#endif
| 
#if definedEx(CONFIG_X86_CMPXCHG64)
(1<<((0*32+ 8) & 31))
#endif
#if !definedEx(CONFIG_X86_CMPXCHG64)
0
#endif
|
#if !definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_64) && definedEx(CONFIG_PARAVIRT)
0
#endif
#if definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT)
(1<<((0*32+13)) & 31)
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+24) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if !definedEx(CONFIG_X86_64) && definedEx(CONFIG_X86_CMOV) || definedEx(CONFIG_X86_64)
(1<<((0*32+15) & 31))
#endif
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_X86_CMOV)
0
#endif
| 
#if definedEx(CONFIG_X86_64)
(1<<((0*32+25) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+26) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
))) || ((((0*32+ 4))>>5)==1 && (1UL<<(((0*32+ 4))&31) & (
#if definedEx(CONFIG_X86_64)
(1<<((1*32+29) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if definedEx(CONFIG_X86_USE_3DNOW)
(1<<((1*32+31) & 31))
#endif
#if !definedEx(CONFIG_X86_USE_3DNOW)
0
#endif
))) || ((((0*32+ 4))>>5)==2 && (1UL<<(((0*32+ 4))&31) & 0)) || ((((0*32+ 4))>>5)==3 && (1UL<<(((0*32+ 4))&31) & (
#if !definedEx(CONFIG_X86_64) && definedEx(CONFIG_X86_P6_NOP) || definedEx(CONFIG_X86_64)
(1<<((3*32+20) & 31))
#endif
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_X86_P6_NOP)
0
#endif
))) || ((((0*32+ 4))>>5)==4 && (1UL<<(((0*32+ 4))&31) & 0)) || ((((0*32+ 4))>>5)==5 && (1UL<<(((0*32+ 4))&31) & 0)) || ((((0*32+ 4))>>5)==6 && (1UL<<(((0*32+ 4))&31) & 0)) || ((((0*32+ 4))>>5)==7 && (1UL<<(((0*32+ 4))&31) & 0)) ) ? 1 : (__builtin_constant_p(((0*32+ 4))) ? constant_test_bit(((0*32+ 4)), ((unsigned long *)((&boot_cpu_data)->x86_capability))) : variable_test_bit(((0*32+ 4)), ((unsigned long *)((&boot_cpu_data)->x86_capability))))))
		return 0;
#endif
	return (cycles_t)__native_read_tsc();
}
extern void tsc_init(void);
extern void mark_tsc_unstable(char *reason);
extern int unsynchronized_tsc(void);
extern int check_tsc_unstable(void);
extern unsigned long native_calibrate_tsc(void);
/*
 * Boot-time check whether the TSCs are synchronized across
 * all CPUs/cores:
 */
extern void check_tsc_sync_source(int cpu);
extern void check_tsc_sync_target(void);
extern int notsc_setup(char *);
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/timex.h" 2
/* Assume we use the PIT time source for the clock tick */
#line 175 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/timex.h" 2
/*
 * SHIFT_PLL is used as a dampening factor to define how much we
 * adjust the frequency correction for a given offset in PLL mode.
 * It also used in dampening the offset correction, to define how
 * much of the current value in time_offset we correct for each
 * second. Changing this value changes the stiffness of the ntp
 * adjustment code. A lower value makes it more flexible, reducing
 * NTP convergence time. A higher value makes it stiffer, increasing
 * convergence time, but making the clock more stable.
 *
 * In David Mills' nanokernel reference implementation SHIFT_PLL is 4.
 * However this seems to increase convergence time much too long.
 *
 * https://lists.ntp.org/pipermail/hackers/2008-January/003487.html
 *
 * In the above mailing list discussion, it seems the value of 4
 * was appropriate for other Unix systems with HZ=100, and that
 * SHIFT_PLL should be decreased as HZ increases. However, Linux's
 * clock steering implementation is HZ independent.
 *
 * Through experimentation, a SHIFT_PLL value of 2 was found to allow
 * for fast convergence (very similar to the NTPv3 code used prior to
 * v2.6.19), with good clock stability.
 *
 *
 * SHIFT_FLL is used as a dampening factor to define how much we
 * adjust the frequency correction for a given offset in FLL mode.
 * In David Mills' nanokernel reference implementation SHIFT_FLL is 2.
 *
 * MAXTC establishes the maximum time constant of the PLL.
 */
/*
 * SHIFT_USEC defines the scaling (shift) of the time_freq and
 * time_tolerance variables, which represent the current frequency
 * offset and maximum frequency tolerance.
 */
/*
 * kernel variables
 * Note: maximum error = NTP synch distance = dispersion + delay / 2;
 * estimated error = NTP dispersion.
 */
extern unsigned long tick_usec;		/* USER_HZ period (usec) */
extern unsigned long tick_nsec;		/* ACTHZ          period (nsec) */
extern int tickadj;			/* amount of adjustment per tick */
/*
 * phase-lock loop variables
 */
extern int time_status;		/* clock synchronization status bits */
extern long time_maxerror;	/* maximum error */
extern long time_esterror;	/* estimated error */
extern long time_adjust;	/* The amount of adjtime left */
extern void ntp_init(void);
extern void ntp_clear(void);
/**
 * ntp_synced - Returns 1 if the NTP status is not UNSYNC
 *
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int ntp_synced(void)
{
	return !(time_status & 0x0040);
}
/* Required to safely shift negative values */
/* Returns how long ticks are at present, in ns / 2^NTP_SCALE_SHIFT. */
extern u64 tick_length;
extern void second_overflow(void);
extern void update_ntp_one_tick(void);
extern int do_adjtimex(struct timex *);
/* Don't use! Compatibility define for existing users. */
int read_current_timer(unsigned long *timer_val);
/* The clock frequency of the i8253/i8254 PIT */
#line 10 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/jiffies.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/param.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/asm-generic/param.h" 1
#line 3 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/param.h" 2
#line 11 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/jiffies.h" 2
/*
 * The following defines establish the engineering parameters of the PLL
 * model. The HZ variable establishes the timer interrupt frequency, 100 Hz
 * for the SunOS kernel, 256 Hz for the Ultrix kernel and 1024 Hz for the
 * OSF/1 kernel. The SHIFT_HZ define expresses the same value as the
 * nearest power of two in order to avoid hardware multiply operations.
 */
/* LATCH is used in the interval timer and ftape setup. */
/* Suppose we want to devide two numbers NOM and DEN: NOM/DEN, then we can
 * improve accuracy by shifting LSH bits, hence calculating:
 *     (NOM << LSH) / DEN
 * This however means trouble for large NOM, because (NOM << LSH) may no
 * longer fit in 32 bits. The following way of calculating this gives us
 * some slack, under the following conditions:
 *   - (NOM / DEN) fits in (32 - LSH) bits.
 *   - (NOM % DEN) fits in (32 - LSH) bits.
 */
/* HZ is the requested value. ACTHZ is actual HZ ("<< 8" is for accuracy) */
/* TICK_NSEC is the time between ticks in nsec assuming real ACTHZ */
/* TICK_USEC is the time between ticks in usec assuming fake USER_HZ */
/* TICK_USEC_TO_NSEC is the time between ticks in nsec assuming real ACTHZ and	*/
/* a value TUSEC for TICK_USEC (can be set bij adjtimex)		*/
/* some arch's have a small-data section that can be accessed register-relative
 * but that can only take up to, say, 4-byte variables. jiffies being part of
 * an 8-byte variable may not be correctly accessed unless we force the issue
 */
/*
 * The 64-bit value is not atomic - you MUST NOT read it
 * without sampling the sequence number in xtime_lock.
 * get_jiffies_64() will do this for you as appropriate.
 */
extern u64 __attribute__((section(".data"))) jiffies_64;
extern unsigned long volatile __attribute__((section(".data"))) jiffies;
u64 get_jiffies_64(void);
/*
 *	These inlines deal with timer wrapping correctly. You are 
 *	strongly encouraged to use them
 *	1. Because people otherwise forget
 *	2. Because if the timer wrap changes in future you won't have to
 *	   alter your driver code.
 *
 * time_after(a,b) returns true if the time a is after time b.
 *
 * Do this with "<0" and ">=0" to only test the sign of the result. A
 * good compiler would generate better code (and a really good compiler
 * wouldn't care). Gcc is currently neither.
 */
/*
 * Calculate whether a is in the range of [b, c].
 */
/*
 * Calculate whether a is in the range of [b, c).
 */
/* Same as above, but does so with platform independent 64bit types.
 * These must be used when utilizing jiffies_64 (i.e. return value of
 * get_jiffies_64() */
/*
 * These four macros compare jiffies and 'a' for convenience.
 */
/* time_is_before_jiffies(a) return true if a is before jiffies */
/* time_is_after_jiffies(a) return true if a is after jiffies */
/* time_is_before_eq_jiffies(a) return true if a is before or equal to jiffies*/
/* time_is_after_eq_jiffies(a) return true if a is after or equal to jiffies*/
/*
 * Have the 32 bit jiffies value wrap 5 minutes after boot
 * so jiffies wrap bugs show up earlier.
 */
/*
 * Change timeval to jiffies, trying to avoid the
 * most obvious overflows..
 *
 * And some not so obvious.
 *
 * Note that we don't want to return LONG_MAX, because
 * for various timeout reasons we often end up having
 * to wait "jiffies+1" in order to guarantee that we wait
 * at _least_ "jiffies" - so "jiffies+1" had better still
 * be positive.
 */
extern unsigned long preset_lpj;
/*
 * We want to do realistic conversions of time so we need to use the same
 * values the update wall clock code uses as the jiffies size.  This value
 * is: TICK_NSEC (which is defined in timex.h).  This
 * is a constant and is in nanoseconds.  We will use scaled math
 * with a set of scales defined here as SEC_JIFFIE_SC,  USEC_JIFFIE_SC and
 * NSEC_JIFFIE_SC.  Note that these defines contain nothing but
 * constants and so are computed at compile time.  SHIFT_HZ (computed in
 * timex.h) adjusts the scaling for different HZ values.
 * Scaled math???  What is that?
 *
 * Scaled math is a way to do integer math on values that would,
 * otherwise, either overflow, underflow, or cause undesired div
 * instructions to appear in the execution path.  In short, we "scale"
 * up the operands so they take more bits (more precision, less
 * underflow), do the desired operation and then "scale" the result back
 * by the same amount.  If we do the scaling by shifting we avoid the
 * costly mpy and the dastardly div instructions.
 * Suppose, for example, we want to convert from seconds to jiffies
 * where jiffies is defined in nanoseconds as NSEC_PER_JIFFIE.  The
 * simple math is: jiff = (sec * NSEC_PER_SEC) / NSEC_PER_JIFFIE; We
 * observe that (NSEC_PER_SEC / NSEC_PER_JIFFIE) is a constant which we
 * might calculate at compile time, however, the result will only have
 * about 3-4 bits of precision (less for smaller values of HZ).
 *
 * So, we scale as follows:
 * jiff = (sec) * (NSEC_PER_SEC / NSEC_PER_JIFFIE);
 * jiff = ((sec) * ((NSEC_PER_SEC * SCALE)/ NSEC_PER_JIFFIE)) / SCALE;
 * Then we make SCALE a power of two so:
 * jiff = ((sec) * ((NSEC_PER_SEC << SCALE)/ NSEC_PER_JIFFIE)) >> SCALE;
 * Now we define:
 * #define SEC_CONV = ((NSEC_PER_SEC << SCALE)/ NSEC_PER_JIFFIE))
 * jiff = (sec * SEC_CONV) >> SCALE;
 *
 * Often the math we use will expand beyond 32-bits so we tell C how to
 * do this and pass the 64-bit result of the mpy through the ">> SCALE"
 * which should take the result back to 32-bits.  We want this expansion
 * to capture as much precision as possible.  At the same time we don't
 * want to overflow so we pick the SCALE to avoid this.  In this file,
 * that means using a different scale for each range of HZ values (as
 * defined in timex.h).
 *
 * For those who want to know, gcc will give a 64-bit result from a "*"
 * operator if the result is a long long AND at least one of the
 * operands is cast to long long (usually just prior to the "*" so as
 * not to confuse it into thinking it really has a 64-bit operand,
 * which, buy the way, it can do, but it takes more code and at least 2
 * mpys).
 * We also need to be aware that one second in nanoseconds is only a
 * couple of bits away from overflowing a 32-bit word, so we MUST use
 * 64-bits to get the full range time in nanoseconds.
 */
/*
 * Here are the scales we will use.  One for seconds, nanoseconds and
 * microseconds.
 *
 * Within the limits of cpp we do a rough cut at the SEC_JIFFIE_SC and
 * check if the sign bit is set.  If not, we bump the shift count by 1.
 * (Gets an extra bit of precision where we can use it.)
 * We know it is set for HZ = 1024 and HZ = 100 not for 1000.
 * Haven't tested others.
 * Limits of cpp (for #if expressions) only long (no long long), but
 * then we only need the most signicant bit.
 */
/*
 * USEC_ROUND is used in the timeval to jiffie conversion.  See there
 * for more details.  It is the scaled resolution rounding value.  Note
 * that it is a 64-bit value.  Since, when it is applied, we are already
 * in jiffies (albit scaled), it is nothing but the bits we will shift
 * off.
 */
/*
 * The maximum jiffie value is (MAX_INT >> 1).  Here we translate that
 * into seconds.  The 64-bit case will overflow if we are not careful,
 * so use the messy SH_DIV macro to do it.  Still all constants.
 */
/*
 * Convert various time units to each other:
 */
extern unsigned int jiffies_to_msecs(const unsigned long j);
extern unsigned int jiffies_to_usecs(const unsigned long j);
extern unsigned long msecs_to_jiffies(const unsigned int m);
extern unsigned long usecs_to_jiffies(const unsigned int u);
extern unsigned long timespec_to_jiffies(const struct timespec *value);
extern void jiffies_to_timespec(const unsigned long jiffies,
				struct timespec *value);
extern unsigned long timeval_to_jiffies(const struct timeval *value);
extern void jiffies_to_timeval(const unsigned long jiffies,
			       struct timeval *value);
extern clock_t jiffies_to_clock_t(long x);
extern unsigned long clock_t_to_jiffies(unsigned long x);
extern u64 jiffies_64_to_clock_t(u64 x);
extern u64 nsec_to_clock_t(u64 x);
extern unsigned long nsecs_to_jiffies(u64 n);
#line 27 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/ktime.h" 2
/*
 * ktime_t:
 *
 * On 64-bit CPUs a single 64-bit variable is used to store the hrtimers
 * internal representation of time values in scalar nanoseconds. The
 * design plays out best on 64-bit CPUs, where most conversions are
 * NOPs and most arithmetic ktime_t operations are plain arithmetic
 * operations.
 *
 * On 32-bit CPUs an optimized representation of the timespec structure
 * is used to avoid expensive conversions from and to timespecs. The
 * endian-aware order of the tv struct members is choosen to allow
 * mathematical operations on the tv64 member of the union too, which
 * for certain operations produces better code.
 *
 * For architectures with efficient support for 64/32-bit conversions the
 * plain scalar nanosecond based representation can be selected by the
 * config switch CONFIG_KTIME_SCALAR.
 */
union ktime {
	s64	tv64;
#if !definedEx(CONFIG_KTIME_SCALAR)
	struct {
 	s32	nsec, sec;
	} tv;
#endif
};
typedef union ktime ktime_t;		/* Kill this */
/*
 * ktime_t definitions when using the 64-bit scalar representation:
 */
#if definedEx(CONFIG_KTIME_SCALAR)
/**
 * ktime_set - Set a ktime_t variable from a seconds/nanoseconds value
 * @secs:	seconds to set
 * @nsecs:	nanoseconds to set
 *
 * Return the ktime_t representation of the value
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 ktime_t ktime_set(const long secs, const unsigned long nsecs)
{
	return (ktime_t) { .tv64 = (s64)secs * 1000000000L + (s64)nsecs };
}
/* Subtract two ktime_t variables. rem = lhs -rhs: */
/* Add two ktime_t variables. res = lhs + rhs: */
/*
 * Add a ktime_t variable and a scalar nanosecond value.
 * res = kt + nsval:
 */
/*
 * Subtract a scalar nanosecod from a ktime_t variable
 * res = kt - nsval:
 */
/* convert a timespec to ktime_t format: */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 ktime_t timespec_to_ktime(struct timespec ts)
{
	return ktime_set(ts.tv_sec, ts.tv_nsec);
}
/* convert a timeval to ktime_t format: */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 ktime_t timeval_to_ktime(struct timeval tv)
{
	return ktime_set(tv.tv_sec, tv.tv_usec * 1000L);
}
/* Map the ktime_t to timespec conversion to ns_to_timespec function */
/* Map the ktime_t to timeval conversion to ns_to_timeval function */
/* Convert ktime_t to nanoseconds - NOP in the scalar storage format: */
#endif
#if !definedEx(CONFIG_KTIME_SCALAR)
/*
 * Helper macros/inlines to get the ktime_t math right in the timespec
 * representation. The macros are sometimes ugly - their actual use is
 * pretty okay-ish, given the circumstances. We do all this for
 * performance reasons. The pure scalar nsec_t based code was nice and
 * simple, but created too many 64-bit / 32-bit conversions and divisions.
 *
 * Be especially aware that negative values are represented in a way
 * that the tv.sec field is negative and the tv.nsec field is greater
 * or equal to zero but less than nanoseconds per second. This is the
 * same representation which is used by timespecs.
 *
 *   tv.sec < 0 and 0 >= tv.nsec < NSEC_PER_SEC
 */
/* Set a ktime_t variable to a value in sec/nsec representation: */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 ktime_t ktime_set(const long secs, const unsigned long nsecs)
{
	return (ktime_t) { .tv = { .sec = secs, .nsec = nsecs } };
}
/**
 * ktime_sub - subtract two ktime_t variables
 * @lhs:	minuend
 * @rhs:	subtrahend
 *
 * Returns the remainder of the substraction
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 ktime_t ktime_sub(const ktime_t lhs, const ktime_t rhs)
{
	ktime_t res;
	res.tv64 = lhs.tv64 - rhs.tv64;
	if (res.tv.nsec < 0)
		res.tv.nsec += 1000000000L;
	return res;
}
/**
 * ktime_add - add two ktime_t variables
 * @add1:	addend1
 * @add2:	addend2
 *
 * Returns the sum of @add1 and @add2.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 ktime_t ktime_add(const ktime_t add1, const ktime_t add2)
{
	ktime_t res;
	res.tv64 = add1.tv64 + add2.tv64;
	/*
	 * performance trick: the (u32) -NSEC gives 0x00000000Fxxxxxxx
	 * so we subtract NSEC_PER_SEC and add 1 to the upper 32 bit.
	 *
	 * it's equivalent to:
	 *   tv.nsec -= NSEC_PER_SEC
	 *   tv.sec ++;
	 */
	if (res.tv.nsec >= 1000000000L)
		res.tv64 += (u32)-1000000000L;
	return res;
}
/**
 * ktime_add_ns - Add a scalar nanoseconds value to a ktime_t variable
 * @kt:		addend
 * @nsec:	the scalar nsec value to add
 *
 * Returns the sum of @kt and @nsec in ktime_t format
 */
extern ktime_t ktime_add_ns(const ktime_t kt, u64 nsec);
/**
 * ktime_sub_ns - Subtract a scalar nanoseconds value from a ktime_t variable
 * @kt:		minuend
 * @nsec:	the scalar nsec value to subtract
 *
 * Returns the subtraction of @nsec from @kt in ktime_t format
 */
extern ktime_t ktime_sub_ns(const ktime_t kt, u64 nsec);
/**
 * timespec_to_ktime - convert a timespec to ktime_t format
 * @ts:		the timespec variable to convert
 *
 * Returns a ktime_t variable with the converted timespec value
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 ktime_t timespec_to_ktime(const struct timespec ts)
{
	return (ktime_t) { .tv = { .sec = (s32)ts.tv_sec,
			   	   .nsec = (s32)ts.tv_nsec } };
}
/**
 * timeval_to_ktime - convert a timeval to ktime_t format
 * @tv:		the timeval variable to convert
 *
 * Returns a ktime_t variable with the converted timeval value
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 ktime_t timeval_to_ktime(const struct timeval tv)
{
	return (ktime_t) { .tv = { .sec = (s32)tv.tv_sec,
				   .nsec = (s32)tv.tv_usec * 1000 } };
}
/**
 * ktime_to_timespec - convert a ktime_t variable to timespec format
 * @kt:		the ktime_t variable to convert
 *
 * Returns the timespec representation of the ktime value
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 struct timespec ktime_to_timespec(const ktime_t kt)
{
	return (struct timespec) { .tv_sec = (time_t) kt.tv.sec,
				   .tv_nsec = (long) kt.tv.nsec };
}
/**
 * ktime_to_timeval - convert a ktime_t variable to timeval format
 * @kt:		the ktime_t variable to convert
 *
 * Returns the timeval representation of the ktime value
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 struct timeval ktime_to_timeval(const ktime_t kt)
{
	return (struct timeval) {
		.tv_sec = (time_t) kt.tv.sec,
		.tv_usec = (suseconds_t) (kt.tv.nsec / 1000L) };
}
/**
 * ktime_to_ns - convert a ktime_t variable to scalar nanoseconds
 * @kt:		the ktime_t variable to convert
 *
 * Returns the scalar nanoseconds representation of @kt
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 s64 ktime_to_ns(const ktime_t kt)
{
	return (s64) kt.tv.sec * 1000000000L + kt.tv.nsec;
}
#endif
/**
 * ktime_equal - Compares two ktime_t variables to see if they are equal
 * @cmp1:	comparable1
 * @cmp2:	comparable2
 *
 * Compare two ktime_t variables, returns 1 if equal
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int ktime_equal(const ktime_t cmp1, const ktime_t cmp2)
{
	return cmp1.tv64 == cmp2.tv64;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 s64 ktime_to_us(const ktime_t kt)
{
	struct timeval tv = 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_X86_LOCAL_APIC) && definedEx(CONFIG_KTIME_SCALAR)
ns_to_timeval((kt).tv64)
#endif
#if !definedEx(CONFIG_X86_32) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_X86_LOCAL_APIC) && !definedEx(CONFIG_KTIME_SCALAR)
ktime_to_timeval(kt)
#endif
;
	return (s64) tv.tv_sec * 1000000L + tv.tv_usec;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 s64 ktime_us_delta(const ktime_t later, const ktime_t earlier)
{
       return ktime_to_us(
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_X86_LOCAL_APIC) && definedEx(CONFIG_KTIME_SCALAR)
({ (ktime_t){ .tv64 = (later).tv64 - (earlier).tv64 }; })
#endif
#if !definedEx(CONFIG_X86_32) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_X86_LOCAL_APIC) && !definedEx(CONFIG_KTIME_SCALAR)
ktime_sub(later, earlier)
#endif
);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 ktime_t ktime_add_us(const ktime_t kt, const u64 usec)
{
	return 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_X86_LOCAL_APIC) && definedEx(CONFIG_KTIME_SCALAR)
({ (ktime_t){ .tv64 = (kt).tv64 + (usec * 1000) }; })
#endif
#if !definedEx(CONFIG_X86_32) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_X86_LOCAL_APIC) && !definedEx(CONFIG_KTIME_SCALAR)
ktime_add_ns(kt, usec * 1000)
#endif
;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 ktime_t ktime_sub_us(const ktime_t kt, const u64 usec)
{
	return 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_X86_LOCAL_APIC) && definedEx(CONFIG_KTIME_SCALAR)
({ (ktime_t){ .tv64 = (kt).tv64 - (usec * 1000) }; })
#endif
#if !definedEx(CONFIG_X86_32) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_X86_LOCAL_APIC) && !definedEx(CONFIG_KTIME_SCALAR)
ktime_sub_ns(kt, usec * 1000)
#endif
;
}
extern ktime_t ktime_add_safe(const ktime_t lhs, const ktime_t rhs);
/*
 * The resolution of the clocks. The resolution value is returned in
 * the clock_getres() system call to give application programmers an
 * idea of the (in)accuracy of timers. Timer values are rounded up to
 * this resolution values.
 */
/* Get the monotonic time in timespec format: */
extern void ktime_get_ts(struct timespec *ts);
/* Get the real (wall-) time in timespec format: */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 ktime_t ns_to_ktime(u64 ns)
{
	static const ktime_t ktime_zero = { .tv64 = 0 };
	return 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_X86_LOCAL_APIC) && definedEx(CONFIG_KTIME_SCALAR)
({ (ktime_t){ .tv64 = (ktime_zero).tv64 + (ns) }; })
#endif
#if !definedEx(CONFIG_X86_32) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_X86_LOCAL_APIC) && !definedEx(CONFIG_KTIME_SCALAR)
ktime_add_ns(ktime_zero, ns)
#endif
;
}
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/timer.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/stddef.h" 1
#line 8 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/timer.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/debugobjects.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/list.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/debugobjects.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/spinlock.h" 1
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/debugobjects.h" 2
enum debug_obj_state {
	ODEBUG_STATE_NONE,
	ODEBUG_STATE_INIT,
	ODEBUG_STATE_INACTIVE,
	ODEBUG_STATE_ACTIVE,
	ODEBUG_STATE_DESTROYED,
	ODEBUG_STATE_NOTAVAILABLE,
	ODEBUG_STATE_MAX,
};
struct debug_obj_descr;
/**
 * struct debug_obj - representaion of an tracked object
 * @node:	hlist node to link the object into the tracker list
 * @state:	tracked object state
 * @object:	pointer to the real object
 * @descr:	pointer to an object type specific debug description structure
 */
struct debug_obj {
	struct hlist_node	node;
	enum debug_obj_state	state;
	void			*object;
	struct debug_obj_descr	*descr;
};
/**
 * struct debug_obj_descr - object type specific debug description structure
 * @name:		name of the object typee
 * @fixup_init:		fixup function, which is called when the init check
 *			fails
 * @fixup_activate:	fixup function, which is called when the activate check
 *			fails
 * @fixup_destroy:	fixup function, which is called when the destroy check
 *			fails
 * @fixup_free:		fixup function, which is called when the free check
 *			fails
 */
struct debug_obj_descr {
	const char		*name;
	int (*fixup_init)	(void *addr, enum debug_obj_state state);
	int (*fixup_activate)	(void *addr, enum debug_obj_state state);
	int (*fixup_destroy)	(void *addr, enum debug_obj_state state);
	int (*fixup_free)	(void *addr, enum debug_obj_state state);
};
#if definedEx(CONFIG_DEBUG_OBJECTS)
extern void debug_object_init      (void *addr, struct debug_obj_descr *descr);
extern void
debug_object_init_on_stack(void *addr, struct debug_obj_descr *descr);
extern void debug_object_activate  (void *addr, struct debug_obj_descr *descr);
extern void debug_object_deactivate(void *addr, struct debug_obj_descr *descr);
extern void debug_object_destroy   (void *addr, struct debug_obj_descr *descr);
extern void debug_object_free      (void *addr, struct debug_obj_descr *descr);
extern void debug_objects_early_init(void);
extern void debug_objects_mem_init(void);
#endif
#if !definedEx(CONFIG_DEBUG_OBJECTS)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void
debug_object_init      (void *addr, struct debug_obj_descr *descr) { }
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void
debug_object_init_on_stack(void *addr, struct debug_obj_descr *descr) { }
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void
debug_object_activate  (void *addr, struct debug_obj_descr *descr) { }
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void
debug_object_deactivate(void *addr, struct debug_obj_descr *descr) { }
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void
debug_object_destroy   (void *addr, struct debug_obj_descr *descr) { }
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void
debug_object_free      (void *addr, struct debug_obj_descr *descr) { }
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void debug_objects_early_init(void) { }
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void debug_objects_mem_init(void) { }
#endif
#if definedEx(CONFIG_DEBUG_OBJECTS_FREE)
extern void debug_check_no_obj_freed(const void *address, unsigned long size);
#endif
#if !definedEx(CONFIG_DEBUG_OBJECTS_FREE)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void
debug_check_no_obj_freed(const void *address, unsigned long size) { }
#endif
#line 9 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/timer.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/stringify.h" 1
#line 10 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/timer.h" 2
struct tvec_base;
struct timer_list {
	struct list_head entry;
	unsigned long expires;
	void (*function)(unsigned long);
	unsigned long data;
	struct tvec_base *base;
#if definedEx(CONFIG_TIMER_STATS)
	void *start_site;
	char start_comm[16];
	int start_pid;
#endif
#if definedEx(CONFIG_LOCKDEP)
	struct lockdep_map lockdep_map;
#endif
};
extern struct tvec_base boot_tvec_bases;
#if definedEx(CONFIG_LOCKDEP)
/*
 * NB: because we have to copy the lockdep_map, setting the lockdep_map key
 * (second argument) here is required, otherwise it could be initialised to
 * the copy of the lockdep_map later! We use the pointer to and the string
 * "<file>:<line>" as the key resp. the name of the lockdep_map.
 */
#endif
#if !definedEx(CONFIG_LOCKDEP)
#endif
void init_timer_key(struct timer_list *timer,
		    const char *name,
		    struct lock_class_key *key);
void init_timer_deferrable_key(struct timer_list *timer,
			       const char *name,
			       struct lock_class_key *key);
#if definedEx(CONFIG_LOCKDEP)
#endif
#if !definedEx(CONFIG_LOCKDEP)
#endif
#if definedEx(CONFIG_DEBUG_OBJECTS_TIMERS)
extern void init_timer_on_stack_key(struct timer_list *timer,
				    const char *name,
				    struct lock_class_key *key);
extern void destroy_timer_on_stack(struct timer_list *timer);
#endif
#if !definedEx(CONFIG_DEBUG_OBJECTS_TIMERS)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void destroy_timer_on_stack(struct timer_list *timer) { }
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void init_timer_on_stack_key(struct timer_list *timer,
					   const char *name,
					   struct lock_class_key *key)
{
	init_timer_key(timer, name, key);
}
#endif
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void setup_timer_key(struct timer_list * timer,
				const char *name,
				struct lock_class_key *key,
				void (*function)(unsigned long),
				unsigned long data)
{
	timer->function = function;
	timer->data = data;
	init_timer_key(timer, name, key);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void setup_timer_on_stack_key(struct timer_list *timer,
					const char *name,
					struct lock_class_key *key,
					void (*function)(unsigned long),
					unsigned long data)
{
	timer->function = function;
	timer->data = data;
	init_timer_on_stack_key(timer, name, key);
}
/**
 * timer_pending - is a timer pending?
 * @timer: the timer in question
 *
 * timer_pending will tell whether a given timer is currently pending,
 * or not. Callers must ensure serialization wrt. other operations done
 * to this timer, eg. interrupt contexts, or other CPUs on SMP.
 *
 * return value: 1 if the timer is pending, 0 if not.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int timer_pending(const struct timer_list * timer)
{
	return timer->entry.next != ((void *)0);
}
extern void add_timer_on(struct timer_list *timer, int cpu);
extern int del_timer(struct timer_list * timer);
extern int mod_timer(struct timer_list *timer, unsigned long expires);
extern int mod_timer_pending(struct timer_list *timer, unsigned long expires);
extern int mod_timer_pinned(struct timer_list *timer, unsigned long expires);
/*
 * The jiffies value which is added to now, when there is no timer
 * in the timer wheel:
 */
/*
 * Return when the next timer-wheel timeout occurs (in absolute jiffies),
 * locks the timer base and does the comparison against the given
 * jiffie.
 */
extern unsigned long get_next_timer_interrupt(unsigned long now);
/*
 * Timer-statistics info:
 */
#if definedEx(CONFIG_TIMER_STATS)
extern int timer_stats_active;
extern void init_timer_stats(void);
extern void timer_stats_update_stats(void *timer, pid_t pid, void *startf,
				     void *timerf, char *comm,
				     unsigned int timer_flag);
extern void __timer_stats_timer_set_start_info(struct timer_list *timer,
					       void *addr);
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void timer_stats_timer_set_start_info(struct timer_list *timer)
{
	if (__builtin_expect(!!(!timer_stats_active), 1))
		return;
	__timer_stats_timer_set_start_info(timer, __builtin_return_address(0));
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void timer_stats_timer_clear_start_info(struct timer_list *timer)
{
	timer->start_site = ((void *)0);
}
#endif
#if !definedEx(CONFIG_TIMER_STATS)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void init_timer_stats(void)
{
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void timer_stats_timer_set_start_info(struct timer_list *timer)
{
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void timer_stats_timer_clear_start_info(struct timer_list *timer)
{
}
#endif
extern void add_timer(struct timer_list *timer);
  extern int try_to_del_timer_sync(struct timer_list *timer);
  extern int del_timer_sync(struct timer_list *timer);
extern void init_timers(void);
extern void run_local_timers(void);
struct hrtimer;
extern enum hrtimer_restart it_real_fn(struct hrtimer *);
unsigned long __round_jiffies(unsigned long j, int cpu);
unsigned long __round_jiffies_relative(unsigned long j, int cpu);
unsigned long round_jiffies(unsigned long j);
unsigned long round_jiffies_relative(unsigned long j);
unsigned long __round_jiffies_up(unsigned long j, int cpu);
unsigned long __round_jiffies_up_relative(unsigned long j, int cpu);
unsigned long round_jiffies_up(unsigned long j);
unsigned long round_jiffies_up_relative(unsigned long j);
#line 10 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/workqueue.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/linkage.h" 1
#line 11 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/workqueue.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/bitops.h" 1
#line 12 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/workqueue.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/lockdep.h" 1
/*
 * Runtime locking correctness validator
 *
 *  Copyright (C) 2006,2007 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>
 *  Copyright (C) 2007 Red Hat, Inc., Peter Zijlstra <pzijlstr@redhat.com>
 *
 * see Documentation/lockdep-design.txt for more details.
 */
#line 13 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/workqueue.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic_32.h" 1
#line 4 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic.h" 2
#line 14 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/workqueue.h" 2
struct workqueue_struct;
struct work_struct;
typedef void (*work_func_t)(struct work_struct *work);
/*
 * The first word is the work queue pointer and the flags rolled into
 * one
 */
struct work_struct {
	atomic_long_t data;
	struct list_head entry;
	work_func_t func;
#if definedEx(CONFIG_LOCKDEP)
	struct lockdep_map lockdep_map;
#endif
};
struct delayed_work {
	struct work_struct work;
	struct timer_list timer;
};
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 struct delayed_work *to_delayed_work(struct work_struct *work)
{
	return ({ const typeof( ((struct delayed_work *)0)->work ) *__mptr = (work); (struct delayed_work *)( (char *)__mptr - __builtin_offsetof(struct delayed_work,work) );});
}
struct execute_work {
	struct work_struct work;
};
#if definedEx(CONFIG_LOCKDEP)
/*
 * NB: because we have to copy the lockdep_map, setting _key
 * here is required, otherwise it could get initialised to the
 * copy of the lockdep_map!
 */
#endif
#if !definedEx(CONFIG_LOCKDEP)
#endif
/*
 * initialize a work item's function pointer
 */
#if definedEx(CONFIG_DEBUG_OBJECTS_WORK)
extern void __init_work(struct work_struct *work, int onstack);
extern void destroy_work_on_stack(struct work_struct *work);
#endif
#if !definedEx(CONFIG_DEBUG_OBJECTS_WORK)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __init_work(struct work_struct *work, int onstack) { }
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void destroy_work_on_stack(struct work_struct *work) { }
#endif
/*
 * initialize all of a work item in one go
 *
 * NOTE! No point in using "atomic_long_set()": using a direct
 * assignment of the work data initializer allows the compiler
 * to generate better code.
 */
#if definedEx(CONFIG_LOCKDEP)
#endif
#if !definedEx(CONFIG_LOCKDEP)
#endif
/**
 * work_pending - Find out whether a work item is currently pending
 * @work: The work item in question
 */
/**
 * delayed_work_pending - Find out whether a delayable work item is currently
 * pending
 * @work: The work item in question
 */
/**
 * work_clear_pending - for internal use only, mark a work item as not pending
 * @work: The work item in question
 */
extern struct workqueue_struct *
__create_workqueue_key(const char *name, int singlethread,
		       int freezeable, int rt, struct lock_class_key *key,
		       const char *lock_name);
#if definedEx(CONFIG_LOCKDEP)
#endif
#if !definedEx(CONFIG_LOCKDEP)
#endif
extern void destroy_workqueue(struct workqueue_struct *wq);
extern int queue_work(struct workqueue_struct *wq, struct work_struct *work);
extern int queue_work_on(int cpu, struct workqueue_struct *wq,
			struct work_struct *work);
extern int queue_delayed_work(struct workqueue_struct *wq,
			struct delayed_work *work, unsigned long delay);
extern int queue_delayed_work_on(int cpu, struct workqueue_struct *wq,
			struct delayed_work *work, unsigned long delay);
extern void flush_workqueue(struct workqueue_struct *wq);
extern void flush_scheduled_work(void);
extern void flush_delayed_work(struct delayed_work *work);
extern int schedule_work(struct work_struct *work);
extern int schedule_work_on(int cpu, struct work_struct *work);
extern int schedule_delayed_work(struct delayed_work *work, unsigned long delay);
extern int schedule_delayed_work_on(int cpu, struct delayed_work *work,
					unsigned long delay);
extern int schedule_on_each_cpu(work_func_t func);
extern int current_is_keventd(void);
extern int keventd_up(void);
extern void init_workqueues(void);
int execute_in_process_context(work_func_t fn, struct execute_work *);
extern int flush_work(struct work_struct *work);
extern int cancel_work_sync(struct work_struct *work);
/*
 * Kill off a pending schedule_delayed_work().  Note that the work callback
 * function may still be running on return from cancel_delayed_work(), unless
 * it returns 1 and the work doesn't re-arm itself. Run flush_workqueue() or
 * cancel_work_sync() to wait on it.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int cancel_delayed_work(struct delayed_work *work)
{
	int ret;
	ret = del_timer_sync(&work->timer);
	if (ret)
		clear_bit(0, ((unsigned long *)(&(&work->work)->data)));
	return ret;
}
/*
 * Like above, but uses del_timer() instead of del_timer_sync(). This means,
 * if it returns 0 the timer function may be running and the queueing is in
 * progress.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int __cancel_delayed_work(struct delayed_work *work)
{
	int ret;
	ret = del_timer(&work->timer);
	if (ret)
		clear_bit(0, ((unsigned long *)(&(&work->work)->data)));
	return ret;
}
extern int cancel_delayed_work_sync(struct delayed_work *work);
/* Obsolete. use cancel_delayed_work_sync() */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
void cancel_rearming_delayed_workqueue(struct workqueue_struct *wq,
					struct delayed_work *work)
{
	cancel_delayed_work_sync(work);
}
/* Obsolete. use cancel_delayed_work_sync() */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
void cancel_rearming_delayed_work(struct delayed_work *work)
{
	cancel_delayed_work_sync(work);
}
 long work_on_cpu(unsigned int cpu, long (*fn)(void *), void *arg);
#line 27 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/pm.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/spinlock.h" 1
#line 28 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/pm.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/wait.h" 1
#line 29 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/pm.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/timer.h" 1
#line 30 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/pm.h" 2
/*
 * Callbacks for platform drivers to implement.
 */
extern void (*pm_idle)(void);
extern void (*pm_power_off)(void);
extern void (*pm_power_off_prepare)(void);
/*
 * Device power management
 */
struct device;
typedef struct pm_message {
	int event;
} pm_message_t;
/**
 * struct dev_pm_ops - device PM callbacks
 *
 * Several driver power state transitions are externally visible, affecting
 * the state of pending I/O queues and (for drivers that touch hardware)
 * interrupts, wakeups, DMA, and other hardware state.  There may also be
 * internal transitions to various low power modes, which are transparent
 * to the rest of the driver stack (such as a driver that's ON gating off
 * clocks which are not in active use).
 *
 * The externally visible transitions are handled with the help of the following
 * callbacks included in this structure:
 *
 * @prepare: Prepare the device for the upcoming transition, but do NOT change
 *	its hardware state.  Prevent new children of the device from being
 *	registered after @prepare() returns (the driver's subsystem and
 *	generally the rest of the kernel is supposed to prevent new calls to the
 *	probe method from being made too once @prepare() has succeeded).  If
 *	@prepare() detects a situation it cannot handle (e.g. registration of a
 *	child already in progress), it may return -EAGAIN, so that the PM core
 *	can execute it once again (e.g. after the new child has been registered)
 *	to recover from the race condition.  This method is executed for all
 *	kinds of suspend transitions and is followed by one of the suspend
 *	callbacks: @suspend(), @freeze(), or @poweroff().
 *	The PM core executes @prepare() for all devices before starting to
 *	execute suspend callbacks for any of them, so drivers may assume all of
 *	the other devices to be present and functional while @prepare() is being
 *	executed.  In particular, it is safe to make GFP_KERNEL memory
 *	allocations from within @prepare().  However, drivers may NOT assume
 *	anything about the availability of the user space at that time and it
 *	is not correct to request firmware from within @prepare() (it's too
 *	late to do that).  [To work around this limitation, drivers may
 *	register suspend and hibernation notifiers that are executed before the
 *	freezing of tasks.]
 *
 * @complete: Undo the changes made by @prepare().  This method is executed for
 *	all kinds of resume transitions, following one of the resume callbacks:
 *	@resume(), @thaw(), @restore().  Also called if the state transition
 *	fails before the driver's suspend callback (@suspend(), @freeze(),
 *	@poweroff()) can be executed (e.g. if the suspend callback fails for one
 *	of the other devices that the PM core has unsuccessfully attempted to
 *	suspend earlier).
 *	The PM core executes @complete() after it has executed the appropriate
 *	resume callback for all devices.
 *
 * @suspend: Executed before putting the system into a sleep state in which the
 *	contents of main memory are preserved.  Quiesce the device, put it into
 *	a low power state appropriate for the upcoming system state (such as
 *	PCI_D3hot), and enable wakeup events as appropriate.
 *
 * @resume: Executed after waking the system up from a sleep state in which the
 *	contents of main memory were preserved.  Put the device into the
 *	appropriate state, according to the information saved in memory by the
 *	preceding @suspend().  The driver starts working again, responding to
 *	hardware events and software requests.  The hardware may have gone
 *	through a power-off reset, or it may have maintained state from the
 *	previous suspend() which the driver may rely on while resuming.  On most
 *	platforms, there are no restrictions on availability of resources like
 *	clocks during @resume().
 *
 * @freeze: Hibernation-specific, executed before creating a hibernation image.
 *	Quiesce operations so that a consistent image can be created, but do NOT
 *	otherwise put the device into a low power device state and do NOT emit
 *	system wakeup events.  Save in main memory the device settings to be
 *	used by @restore() during the subsequent resume from hibernation or by
 *	the subsequent @thaw(), if the creation of the image or the restoration
 *	of main memory contents from it fails.
 *
 * @thaw: Hibernation-specific, executed after creating a hibernation image OR
 *	if the creation of the image fails.  Also executed after a failing
 *	attempt to restore the contents of main memory from such an image.
 *	Undo the changes made by the preceding @freeze(), so the device can be
 *	operated in the same way as immediately before the call to @freeze().
 *
 * @poweroff: Hibernation-specific, executed after saving a hibernation image.
 *	Quiesce the device, put it into a low power state appropriate for the
 *	upcoming system state (such as PCI_D3hot), and enable wakeup events as
 *	appropriate.
 *
 * @restore: Hibernation-specific, executed after restoring the contents of main
 *	memory from a hibernation image.  Driver starts working again,
 *	responding to hardware events and software requests.  Drivers may NOT
 *	make ANY assumptions about the hardware state right prior to @restore().
 *	On most platforms, there are no restrictions on availability of
 *	resources like clocks during @restore().
 *
 * @suspend_noirq: Complete the operations of ->suspend() by carrying out any
 *	actions required for suspending the device that need interrupts to be
 *	disabled
 *
 * @resume_noirq: Prepare for the execution of ->resume() by carrying out any
 *	actions required for resuming the device that need interrupts to be
 *	disabled
 *
 * @freeze_noirq: Complete the operations of ->freeze() by carrying out any
 *	actions required for freezing the device that need interrupts to be
 *	disabled
 *
 * @thaw_noirq: Prepare for the execution of ->thaw() by carrying out any
 *	actions required for thawing the device that need interrupts to be
 *	disabled
 *
 * @poweroff_noirq: Complete the operations of ->poweroff() by carrying out any
 *	actions required for handling the device that need interrupts to be
 *	disabled
 *
 * @restore_noirq: Prepare for the execution of ->restore() by carrying out any
 *	actions required for restoring the operations of the device that need
 *	interrupts to be disabled
 *
 * All of the above callbacks, except for @complete(), return error codes.
 * However, the error codes returned by the resume operations, @resume(),
 * @thaw(), @restore(), @resume_noirq(), @thaw_noirq(), and @restore_noirq() do
 * not cause the PM core to abort the resume transition during which they are
 * returned.  The error codes returned in that cases are only printed by the PM
 * core to the system logs for debugging purposes.  Still, it is recommended
 * that drivers only return error codes from their resume methods in case of an
 * unrecoverable failure (i.e. when the device being handled refuses to resume
 * and becomes unusable) to allow us to modify the PM core in the future, so
 * that it can avoid attempting to handle devices that failed to resume and
 * their children.
 *
 * It is allowed to unregister devices while the above callbacks are being
 * executed.  However, it is not allowed to unregister a device from within any
 * of its own callbacks.
 *
 * There also are the following callbacks related to run-time power management
 * of devices:
 *
 * @runtime_suspend: Prepare the device for a condition in which it won't be
 *	able to communicate with the CPU(s) and RAM due to power management.
 *	This need not mean that the device should be put into a low power state.
 *	For example, if the device is behind a link which is about to be turned
 *	off, the device may remain at full power.  If the device does go to low
 *	power and is capable of generating run-time wake-up events, remote
 *	wake-up (i.e., a hardware mechanism allowing the device to request a
 *	change of its power state via a wake-up event, such as PCI PME) should
 *	be enabled for it.
 *
 * @runtime_resume: Put the device into the fully active state in response to a
 *	wake-up event generated by hardware or at the request of software.  If
 *	necessary, put the device into the full power state and restore its
 *	registers, so that it is fully operational.
 *
 * @runtime_idle: Device appears to be inactive and it might be put into a low
 *	power state if all of the necessary conditions are satisfied.  Check
 *	these conditions and handle the device as appropriate, possibly queueing
 *	a suspend request for it.  The return value is ignored by the PM core.
 */
struct dev_pm_ops {
	int (*prepare)(struct device *dev);
	void (*complete)(struct device *dev);
	int (*suspend)(struct device *dev);
	int (*resume)(struct device *dev);
	int (*freeze)(struct device *dev);
	int (*thaw)(struct device *dev);
	int (*poweroff)(struct device *dev);
	int (*restore)(struct device *dev);
	int (*suspend_noirq)(struct device *dev);
	int (*resume_noirq)(struct device *dev);
	int (*freeze_noirq)(struct device *dev);
	int (*thaw_noirq)(struct device *dev);
	int (*poweroff_noirq)(struct device *dev);
	int (*restore_noirq)(struct device *dev);
	int (*runtime_suspend)(struct device *dev);
	int (*runtime_resume)(struct device *dev);
	int (*runtime_idle)(struct device *dev);
};
/*
 * Use this if you want to use the same suspend and resume callbacks for suspend
 * to RAM and hibernation.
 */
/**
 * PM_EVENT_ messages
 *
 * The following PM_EVENT_ messages are defined for the internal use of the PM
 * core, in order to provide a mechanism allowing the high level suspend and
 * hibernation code to convey the necessary information to the device PM core
 * code:
 *
 * ON		No transition.
 *
 * FREEZE 	System is going to hibernate, call ->prepare() and ->freeze()
 *		for all devices.
 *
 * SUSPEND	System is going to suspend, call ->prepare() and ->suspend()
 *		for all devices.
 *
 * HIBERNATE	Hibernation image has been saved, call ->prepare() and
 *		->poweroff() for all devices.
 *
 * QUIESCE	Contents of main memory are going to be restored from a (loaded)
 *		hibernation image, call ->prepare() and ->freeze() for all
 *		devices.
 *
 * RESUME	System is resuming, call ->resume() and ->complete() for all
 *		devices.
 *
 * THAW		Hibernation image has been created, call ->thaw() and
 *		->complete() for all devices.
 *
 * RESTORE	Contents of main memory have been restored from a hibernation
 *		image, call ->restore() and ->complete() for all devices.
 *
 * RECOVER	Creation of a hibernation image or restoration of the main
 *		memory contents from a hibernation image has failed, call
 *		->thaw() and ->complete() for all devices.
 *
 * The following PM_EVENT_ messages are defined for internal use by
 * kernel subsystems.  They are never issued by the PM core.
 *
 * USER_SUSPEND		Manual selective suspend was issued by userspace.
 *
 * USER_RESUME		Manual selective resume was issued by userspace.
 *
 * REMOTE_WAKEUP	Remote-wakeup request was received from the device.
 *
 * AUTO_SUSPEND		Automatic (device idle) runtime suspend was
 *			initiated by the subsystem.
 *
 * AUTO_RESUME		Automatic (device needed) runtime resume was
 *			requested by a driver.
 */
/**
 * Device power management states
 *
 * These state labels are used internally by the PM core to indicate the current
 * status of a device with respect to the PM core operations.
 *
 * DPM_ON		Device is regarded as operational.  Set this way
 *			initially and when ->complete() is about to be called.
 *			Also set when ->prepare() fails.
 *
 * DPM_PREPARING	Device is going to be prepared for a PM transition.  Set
 *			when ->prepare() is about to be called.
 *
 * DPM_RESUMING		Device is going to be resumed.  Set when ->resume(),
 *			->thaw(), or ->restore() is about to be called.
 *
 * DPM_SUSPENDING	Device has been prepared for a power transition.  Set
 *			when ->prepare() has just succeeded.
 *
 * DPM_OFF		Device is regarded as inactive.  Set immediately after
 *			->suspend(), ->freeze(), or ->poweroff() has succeeded.
 *			Also set when ->resume()_noirq, ->thaw_noirq(), or
 *			->restore_noirq() is about to be called.
 *
 * DPM_OFF_IRQ		Device is in a "deep sleep".  Set immediately after
 *			->suspend_noirq(), ->freeze_noirq(), or
 *			->poweroff_noirq() has just succeeded.
 */
enum dpm_state {
	DPM_INVALID,
	DPM_ON,
	DPM_PREPARING,
	DPM_RESUMING,
	DPM_SUSPENDING,
	DPM_OFF,
	DPM_OFF_IRQ,
};
/**
 * Device run-time power management status.
 *
 * These status labels are used internally by the PM core to indicate the
 * current status of a device with respect to the PM core operations.  They do
 * not reflect the actual power state of the device or its status as seen by the
 * driver.
 *
 * RPM_ACTIVE		Device is fully operational.  Indicates that the device
 *			bus type's ->runtime_resume() callback has completed
 *			successfully.
 *
 * RPM_SUSPENDED	Device bus type's ->runtime_suspend() callback has
 *			completed successfully.  The device is regarded as
 *			suspended.
 *
 * RPM_RESUMING		Device bus type's ->runtime_resume() callback is being
 *			executed.
 *
 * RPM_SUSPENDING	Device bus type's ->runtime_suspend() callback is being
 *			executed.
 */
enum rpm_status {
	RPM_ACTIVE = 0,
	RPM_RESUMING,
	RPM_SUSPENDED,
	RPM_SUSPENDING,
};
/**
 * Device run-time power management request types.
 *
 * RPM_REQ_NONE		Do nothing.
 *
 * RPM_REQ_IDLE		Run the device bus type's ->runtime_idle() callback
 *
 * RPM_REQ_SUSPEND	Run the device bus type's ->runtime_suspend() callback
 *
 * RPM_REQ_RESUME	Run the device bus type's ->runtime_resume() callback
 */
enum rpm_request {
	RPM_REQ_NONE = 0,
	RPM_REQ_IDLE,
	RPM_REQ_SUSPEND,
	RPM_REQ_RESUME,
};
struct dev_pm_info {
	pm_message_t		power_state;
	unsigned int		can_wakeup:1;
	unsigned int		should_wakeup:1;
	enum dpm_state		status;		/* Owned by the PM core */
#if definedEx(CONFIG_PM_SLEEP)
	struct list_head	entry;
#endif
#if definedEx(CONFIG_PM_RUNTIME)
	struct timer_list	suspend_timer;
	unsigned long		timer_expires;
	struct work_struct	work;
	wait_queue_head_t	wait_queue;
	spinlock_t		lock;
	atomic_t		usage_count;
	atomic_t		child_count;
	unsigned int		disable_depth:3;
	unsigned int		ignore_children:1;
	unsigned int		idle_notification:1;
	unsigned int		request_pending:1;
	unsigned int		deferred_resume:1;
	unsigned int		run_wake:1;
	enum rpm_request	request;
	enum rpm_status		runtime_status;
	int			runtime_error;
#endif
};
/*
 * The PM_EVENT_ messages are also used by drivers implementing the legacy
 * suspend framework, based on the ->suspend() and ->resume() callbacks common
 * for suspend and hibernation transitions, according to the rules below.
 */
/* Necessary, because several drivers use PM_EVENT_PRETHAW */
/*
 * One transition is triggered by resume(), after a suspend() call; the
 * message is implicit:
 *
 * ON		Driver starts working again, responding to hardware events
 * 		and software requests.  The hardware may have gone through
 * 		a power-off reset, or it may have maintained state from the
 * 		previous suspend() which the driver will rely on while
 * 		resuming.  On most platforms, there are no restrictions on
 * 		availability of resources like clocks during resume().
 *
 * Other transitions are triggered by messages sent using suspend().  All
 * these transitions quiesce the driver, so that I/O queues are inactive.
 * That commonly entails turning off IRQs and DMA; there may be rules
 * about how to quiesce that are specific to the bus or the device's type.
 * (For example, network drivers mark the link state.)  Other details may
 * differ according to the message:
 *
 * SUSPEND	Quiesce, enter a low power device state appropriate for
 * 		the upcoming system state (such as PCI_D3hot), and enable
 * 		wakeup events as appropriate.
 *
 * HIBERNATE	Enter a low power device state appropriate for the hibernation
 * 		state (eg. ACPI S4) and enable wakeup events as appropriate.
 *
 * FREEZE	Quiesce operations so that a consistent image can be saved;
 * 		but do NOT otherwise enter a low power device state, and do
 * 		NOT emit system wakeup events.
 *
 * PRETHAW	Quiesce as if for FREEZE; additionally, prepare for restoring
 * 		the system from a snapshot taken after an earlier FREEZE.
 * 		Some drivers will need to reset their hardware state instead
 * 		of preserving it, to ensure that it's never mistaken for the
 * 		state which that earlier snapshot had set up.
 *
 * A minimally power-aware driver treats all messages as SUSPEND, fully
 * reinitializes its device during resume() -- whether or not it was reset
 * during the suspend/resume cycle -- and can't issue wakeup events.
 *
 * More power-aware drivers may also use low power states at runtime as
 * well as during system sleep states like PM_SUSPEND_STANDBY.  They may
 * be able to use wakeup events to exit from runtime low-power states,
 * or from system low-power states such as standby or suspend-to-RAM.
 */
#if definedEx(CONFIG_PM_SLEEP)
extern void device_pm_lock(void);
extern int sysdev_resume(void);
extern void dpm_resume_noirq(pm_message_t state);
extern void dpm_resume_end(pm_message_t state);
extern void device_pm_unlock(void);
extern int sysdev_suspend(pm_message_t state);
extern int dpm_suspend_noirq(pm_message_t state);
extern int dpm_suspend_start(pm_message_t state);
extern void __suspend_report_result(const char *function, void *fn, int ret);
#endif
#if !definedEx(CONFIG_PM_SLEEP)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int dpm_suspend_start(pm_message_t state)
{
	return 0;
}
#endif
/* How to reorder dpm_list after device_move() */
enum dpm_order {
	DPM_ORDER_NONE,
	DPM_ORDER_DEV_AFTER_PARENT,
	DPM_ORDER_PARENT_BEFORE_DEV,
	DPM_ORDER_DEV_LAST,
};
/*
 * Global Power Management flags
 * Used to keep APM and ACPI from both being active
 */
extern unsigned int	pm_flags;
#line 8 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/apic.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/alternative.h" 1
#line 10 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/apic.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/cpufeature.h" 1
/*
 * Defines x86 CPU feature bits
 */
#line 11 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/apic.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/processor.h" 1
#line 12 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/apic.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/apicdef.h" 1
/*
 * Constants for various Intel APICs. (local APIC, IOAPIC, etc.)
 *
 * Alan Cox <Alan.Cox@linux.org>, 1995.
 * Ingo Molnar <mingo@redhat.com>, 1999, 2000
 */
/*
 * This is the IO-APIC register space as specified
 * by Intel docs:
 */
/*
 * All x86-64 systems are xAPIC compatible.
 * In the following, "apicid" is a physical APIC ID.
 */
/*
 * the local APIC register structure, memory mapped. Not terribly well
 * tested, but we might eventually use this one in the future - the
 * problem why we cannot use it right now is the P5 APIC, it has an
 * errata which cannot take 8-bit reads and writes, only 32-bit ones ...
 */
struct local_apic {
/*000*/	struct { unsigned int __reserved[4]; } __reserved_01;
/*010*/	struct { unsigned int __reserved[4]; } __reserved_02;
/*020*/	struct { /* APIC ID Register */
		unsigned int   __reserved_1	: 24,
			phys_apic_id	:  4,
			__reserved_2	:  4;
		unsigned int __reserved[3];
	} id;
/*030*/	const
	struct { /* APIC Version Register */
		unsigned int   version		:  8,
			__reserved_1	:  8,
			max_lvt		:  8,
			__reserved_2	:  8;
		unsigned int __reserved[3];
	} version;
/*040*/	struct { unsigned int __reserved[4]; } __reserved_03;
/*050*/	struct { unsigned int __reserved[4]; } __reserved_04;
/*060*/	struct { unsigned int __reserved[4]; } __reserved_05;
/*070*/	struct { unsigned int __reserved[4]; } __reserved_06;
/*080*/	struct { /* Task Priority Register */
		unsigned int   priority	:  8,
			__reserved_1	: 24;
		unsigned int __reserved_2[3];
	} tpr;
/*090*/	const
	struct { /* Arbitration Priority Register */
		unsigned int   priority	:  8,
			__reserved_1	: 24;
		unsigned int __reserved_2[3];
	} apr;
/*0A0*/	const
	struct { /* Processor Priority Register */
		unsigned int   priority	:  8,
			__reserved_1	: 24;
		unsigned int __reserved_2[3];
	} ppr;
/*0B0*/	struct { /* End Of Interrupt Register */
		unsigned int   eoi;
		unsigned int __reserved[3];
	} eoi;
/*0C0*/	struct { unsigned int __reserved[4]; } __reserved_07;
/*0D0*/	struct { /* Logical Destination Register */
		unsigned int   __reserved_1	: 24,
			logical_dest	:  8;
		unsigned int __reserved_2[3];
	} ldr;
/*0E0*/	struct { /* Destination Format Register */
		unsigned int   __reserved_1	: 28,
			model		:  4;
		unsigned int __reserved_2[3];
	} dfr;
/*0F0*/	struct { /* Spurious Interrupt Vector Register */
		unsigned int	spurious_vector	:  8,
			apic_enabled	:  1,
			focus_cpu	:  1,
			__reserved_2	: 22;
		unsigned int __reserved_3[3];
	} svr;
/*100*/	struct { /* In Service Register */
/*170*/		unsigned int bitfield;
		unsigned int __reserved[3];
	} isr [8];
/*180*/	struct { /* Trigger Mode Register */
/*1F0*/		unsigned int bitfield;
		unsigned int __reserved[3];
	} tmr [8];
/*200*/	struct { /* Interrupt Request Register */
/*270*/		unsigned int bitfield;
		unsigned int __reserved[3];
	} irr [8];
/*280*/	union { /* Error Status Register */
		struct {
			unsigned int   send_cs_error			:  1,
				receive_cs_error		:  1,
				send_accept_error		:  1,
				receive_accept_error		:  1,
				__reserved_1			:  1,
				send_illegal_vector		:  1,
				receive_illegal_vector		:  1,
				illegal_register_address	:  1,
				__reserved_2			: 24;
			unsigned int __reserved_3[3];
		} error_bits;
		struct {
			unsigned int errors;
			unsigned int __reserved_3[3];
		} all_errors;
	} esr;
/*290*/	struct { unsigned int __reserved[4]; } __reserved_08;
/*2A0*/	struct { unsigned int __reserved[4]; } __reserved_09;
/*2B0*/	struct { unsigned int __reserved[4]; } __reserved_10;
/*2C0*/	struct { unsigned int __reserved[4]; } __reserved_11;
/*2D0*/	struct { unsigned int __reserved[4]; } __reserved_12;
/*2E0*/	struct { unsigned int __reserved[4]; } __reserved_13;
/*2F0*/	struct { unsigned int __reserved[4]; } __reserved_14;
/*300*/	struct { /* Interrupt Command Register 1 */
		unsigned int   vector			:  8,
			delivery_mode		:  3,
			destination_mode	:  1,
			delivery_status		:  1,
			__reserved_1		:  1,
			level			:  1,
			trigger			:  1,
			__reserved_2		:  2,
			shorthand		:  2,
			__reserved_3		:  12;
		unsigned int __reserved_4[3];
	} icr1;
/*310*/	struct { /* Interrupt Command Register 2 */
		union {
			unsigned int   __reserved_1	: 24,
				phys_dest	:  4,
				__reserved_2	:  4;
			unsigned int   __reserved_3	: 24,
				logical_dest	:  8;
		} dest;
		unsigned int __reserved_4[3];
	} icr2;
/*320*/	struct { /* LVT - Timer */
		unsigned int   vector		:  8,
			__reserved_1	:  4,
			delivery_status	:  1,
			__reserved_2	:  3,
			mask		:  1,
			timer_mode	:  1,
			__reserved_3	: 14;
		unsigned int __reserved_4[3];
	} lvt_timer;
/*330*/	struct { /* LVT - Thermal Sensor */
		unsigned int  vector		:  8,
			delivery_mode	:  3,
			__reserved_1	:  1,
			delivery_status	:  1,
			__reserved_2	:  3,
			mask		:  1,
			__reserved_3	: 15;
		unsigned int __reserved_4[3];
	} lvt_thermal;
/*340*/	struct { /* LVT - Performance Counter */
		unsigned int   vector		:  8,
			delivery_mode	:  3,
			__reserved_1	:  1,
			delivery_status	:  1,
			__reserved_2	:  3,
			mask		:  1,
			__reserved_3	: 15;
		unsigned int __reserved_4[3];
	} lvt_pc;
/*350*/	struct { /* LVT - LINT0 */
		unsigned int   vector		:  8,
			delivery_mode	:  3,
			__reserved_1	:  1,
			delivery_status	:  1,
			polarity	:  1,
			remote_irr	:  1,
			trigger		:  1,
			mask		:  1,
			__reserved_2	: 15;
		unsigned int __reserved_3[3];
	} lvt_lint0;
/*360*/	struct { /* LVT - LINT1 */
		unsigned int   vector		:  8,
			delivery_mode	:  3,
			__reserved_1	:  1,
			delivery_status	:  1,
			polarity	:  1,
			remote_irr	:  1,
			trigger		:  1,
			mask		:  1,
			__reserved_2	: 15;
		unsigned int __reserved_3[3];
	} lvt_lint1;
/*370*/	struct { /* LVT - Error */
		unsigned int   vector		:  8,
			__reserved_1	:  4,
			delivery_status	:  1,
			__reserved_2	:  3,
			mask		:  1,
			__reserved_3	: 15;
		unsigned int __reserved_4[3];
	} lvt_error;
/*380*/	struct { /* Timer Initial Count Register */
		unsigned int   initial_count;
		unsigned int __reserved_2[3];
	} timer_icr;
/*390*/	const
	struct { /* Timer Current Count Register */
		unsigned int   curr_count;
		unsigned int __reserved_2[3];
	} timer_ccr;
/*3A0*/	struct { unsigned int __reserved[4]; } __reserved_16;
/*3B0*/	struct { unsigned int __reserved[4]; } __reserved_17;
/*3C0*/	struct { unsigned int __reserved[4]; } __reserved_18;
/*3D0*/	struct { unsigned int __reserved[4]; } __reserved_19;
/*3E0*/	struct { /* Timer Divide Configuration Register */
		unsigned int   divisor		:  4,
			__reserved_1	: 28;
		unsigned int __reserved_2[3];
	} timer_dcr;
/*3F0*/	struct { unsigned int __reserved[4]; } __reserved_20;
} __attribute__ ((packed));
#line 13 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/apic.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic_32.h" 1
#line 4 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic.h" 2
#line 14 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/apic.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/fixmap.h" 1
/*
 * fixmap.h: compile-time virtual memory allocation
 *
 * This file is subject to the terms and conditions of the GNU General Public
 * License.  See the file "COPYING" in the main directory of this archive
 * for more details.
 *
 * Copyright (C) 1998 Ingo Molnar
 *
 * Support of BIGMEM added by Gerhard Wichert, Siemens AG, July 1999
 * x86_32 and x86_64 integration by Gustavo F. Padovan, February 2009
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/kernel.h" 1
#line 20 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/fixmap.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/acpi.h" 1
/*
 *  Copyright (C) 2001 Paul Diefenbaugh <paul.s.diefenbaugh@intel.com>
 *  Copyright (C) 2001 Patrick Mochel <mochel@osdl.org>
 *
 * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 *
 *  This program is free software; you can redistribute it and/or modify
 *  it under the terms of the GNU General Public License as published by
 *  the Free Software Foundation; either version 2 of the License, or
 *  (at your option) any later version.
 *
 *  This program is distributed in the hope that it will be useful,
 *  but WITHOUT ANY WARRANTY; without even the implied warranty of
 *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *  GNU General Public License for more details.
 *
 *  You should have received a copy of the GNU General Public License
 *  along with this program; if not, write to the Free Software
 *  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
 *
 * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/acpi/pdc_intel.h" 1
/* _PDC bit definition for Intel processors */
#line 28 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/acpi.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/numa.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/numa_32.h" 1
extern int pxm_to_nid(int pxm);
extern void numa_remove_cpu(int cpu);
#if definedEx(CONFIG_HIGHMEM)
extern void set_highmem_pages_init(void);
#endif
#if !definedEx(CONFIG_HIGHMEM)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void set_highmem_pages_init(void)
{
}
#endif
#line 4 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/numa.h" 2
#line 30 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/acpi.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/processor.h" 1
#line 31 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/acpi.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/mmu.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/spinlock.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/mmu.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/mutex.h" 1
/*
 * Mutexes: blocking mutual exclusion locks
 *
 * started by Ingo Molnar:
 *
 *  Copyright (C) 2004, 2005, 2006 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>
 *
 * This file contains the main data structure and API definitions.
 */
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/mmu.h" 2
/*
 * The x86 doesn't have a mmu context, but
 * we put the segment information here.
 */
typedef struct {
	void *ldt;
	int size;
	struct mutex lock;
	void *vdso;
} mm_context_t;
void leave_mm(int cpu);
#line 32 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/acpi.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/mpspec.h" 1
#line 33 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/acpi.h" 2
/*
 * Calling conventions:
 *
 * ACPI_SYSTEM_XFACE        - Interfaces to host OS (handlers, threads)
 * ACPI_EXTERNAL_XFACE      - External ACPI interfaces
 * ACPI_INTERNAL_XFACE      - Internal ACPI interfaces
 * ACPI_INTERNAL_VAR_XFACE  - Internal variable-parameter list interfaces
 */
/* Asm macros */
int __acpi_acquire_global_lock(unsigned int *lock);
int __acpi_release_global_lock(unsigned int *lock);
/*
 * Math helper asm macros
 */
#if definedEx(CONFIG_ACPI)
extern int acpi_lapic;
extern int acpi_ioapic;
extern int acpi_noirq;
extern int acpi_strict;
extern int acpi_disabled;
extern int acpi_ht;
extern int acpi_pci_disabled;
extern int acpi_skip_timer_override;
extern int acpi_use_timer_override;
extern u8 acpi_sci_flags;
extern int acpi_sci_override_gsi;
void acpi_pic_sci_set_trigger(unsigned int, u16);
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void disable_acpi(void)
{
	acpi_disabled = 1;
	acpi_ht = 0;
	acpi_pci_disabled = 1;
	acpi_noirq = 1;
}
extern int acpi_gsi_to_irq(u32 gsi, unsigned int *irq);
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void acpi_noirq_set(void) { acpi_noirq = 1; }
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void acpi_disable_pci(void)
{
	acpi_pci_disabled = 1;
	acpi_noirq_set();
}
/* routines for saving/restoring kernel state */
extern int acpi_save_state_mem(void);
extern void acpi_restore_state_mem(void);
extern unsigned long acpi_wakeup_address;
/* early initialization routine */
extern void acpi_reserve_wakeup_memory(void);
/*
 * Check if the CPU can handle C2 and deeper
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned int acpi_processor_cstate_check(unsigned int max_cstate)
{
	/*
	 * Early models (<=5) of AMD Opterons are not supposed to go into
	 * C2 state.
	 *
	 * Steppings 0x0A and later are good
	 */
	if (boot_cpu_data.x86 == 0x0F &&
	    boot_cpu_data.x86_vendor == 2 &&
	    boot_cpu_data.x86_model <= 0x05 &&
	    boot_cpu_data.x86_mask < 0x0A)
		return 1;
	else if ((__builtin_constant_p((3*32+21)) && ( ((((3*32+21))>>5)==0 && (1UL<<(((3*32+21))&31) & (
#if !definedEx(CONFIG_MATH_EMULATION)
(1<<((0*32+ 0) & 31))
#endif
#if definedEx(CONFIG_MATH_EMULATION)
0
#endif
|
#if !definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_64) && definedEx(CONFIG_PARAVIRT)
0
#endif
#if definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT)
(1<<((0*32+ 3)) & 31)
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+ 5) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if !definedEx(CONFIG_X86_PAE) && definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_PAE)
(1<<((0*32+ 6) & 31))
#endif
#if !definedEx(CONFIG_X86_PAE) && !definedEx(CONFIG_X86_64)
0
#endif
| 
#if definedEx(CONFIG_X86_CMPXCHG64)
(1<<((0*32+ 8) & 31))
#endif
#if !definedEx(CONFIG_X86_CMPXCHG64)
0
#endif
|
#if !definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_64) && definedEx(CONFIG_PARAVIRT)
0
#endif
#if definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT)
(1<<((0*32+13)) & 31)
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+24) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if !definedEx(CONFIG_X86_64) && definedEx(CONFIG_X86_CMOV) || definedEx(CONFIG_X86_64)
(1<<((0*32+15) & 31))
#endif
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_X86_CMOV)
0
#endif
| 
#if definedEx(CONFIG_X86_64)
(1<<((0*32+25) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+26) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
))) || ((((3*32+21))>>5)==1 && (1UL<<(((3*32+21))&31) & (
#if definedEx(CONFIG_X86_64)
(1<<((1*32+29) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if definedEx(CONFIG_X86_USE_3DNOW)
(1<<((1*32+31) & 31))
#endif
#if !definedEx(CONFIG_X86_USE_3DNOW)
0
#endif
))) || ((((3*32+21))>>5)==2 && (1UL<<(((3*32+21))&31) & 0)) || ((((3*32+21))>>5)==3 && (1UL<<(((3*32+21))&31) & (
#if !definedEx(CONFIG_X86_64) && definedEx(CONFIG_X86_P6_NOP) || definedEx(CONFIG_X86_64)
(1<<((3*32+20) & 31))
#endif
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_X86_P6_NOP)
0
#endif
))) || ((((3*32+21))>>5)==4 && (1UL<<(((3*32+21))&31) & 0)) || ((((3*32+21))>>5)==5 && (1UL<<(((3*32+21))&31) & 0)) || ((((3*32+21))>>5)==6 && (1UL<<(((3*32+21))&31) & 0)) || ((((3*32+21))>>5)==7 && (1UL<<(((3*32+21))&31) & 0)) ) ? 1 : (__builtin_constant_p(((3*32+21))) ? constant_test_bit(((3*32+21)), ((unsigned long *)((&boot_cpu_data)->x86_capability))) : variable_test_bit(((3*32+21)), ((unsigned long *)((&boot_cpu_data)->x86_capability))))))
		return 1;
	else
		return max_cstate;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 bool arch_has_acpi_pdc(void)
{
	struct cpuinfo_x86 *c = &(*({ unsigned long __ptr; __asm__ ("" : "=r"(__ptr) : "0"((&per_cpu__cpu_info))); (typeof((&per_cpu__cpu_info))) (__ptr + (((__per_cpu_offset[0])))); }));
	return (c->x86_vendor == 0 ||
		c->x86_vendor == 5);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void arch_acpi_set_pdc_bits(u32 *buf)
{
	struct cpuinfo_x86 *c = &(*({ unsigned long __ptr; __asm__ ("" : "=r"(__ptr) : "0"((&per_cpu__cpu_info))); (typeof((&per_cpu__cpu_info))) (__ptr + (((__per_cpu_offset[0])))); }));
	buf[2] |= ((0x0010) | (0x0008) | (0x0002) | (0x0100) | (0x0200));
	if ((__builtin_constant_p((4*32+ 7)) && ( ((((4*32+ 7))>>5)==0 && (1UL<<(((4*32+ 7))&31) & (
#if !definedEx(CONFIG_MATH_EMULATION)
(1<<((0*32+ 0) & 31))
#endif
#if definedEx(CONFIG_MATH_EMULATION)
0
#endif
|
#if !definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_64) && definedEx(CONFIG_PARAVIRT)
0
#endif
#if definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT)
(1<<((0*32+ 3)) & 31)
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+ 5) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if !definedEx(CONFIG_X86_PAE) && definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_PAE)
(1<<((0*32+ 6) & 31))
#endif
#if !definedEx(CONFIG_X86_PAE) && !definedEx(CONFIG_X86_64)
0
#endif
| 
#if definedEx(CONFIG_X86_CMPXCHG64)
(1<<((0*32+ 8) & 31))
#endif
#if !definedEx(CONFIG_X86_CMPXCHG64)
0
#endif
|
#if !definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_64) && definedEx(CONFIG_PARAVIRT)
0
#endif
#if definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT)
(1<<((0*32+13)) & 31)
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+24) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if !definedEx(CONFIG_X86_64) && definedEx(CONFIG_X86_CMOV) || definedEx(CONFIG_X86_64)
(1<<((0*32+15) & 31))
#endif
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_X86_CMOV)
0
#endif
| 
#if definedEx(CONFIG_X86_64)
(1<<((0*32+25) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+26) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
))) || ((((4*32+ 7))>>5)==1 && (1UL<<(((4*32+ 7))&31) & (
#if definedEx(CONFIG_X86_64)
(1<<((1*32+29) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if definedEx(CONFIG_X86_USE_3DNOW)
(1<<((1*32+31) & 31))
#endif
#if !definedEx(CONFIG_X86_USE_3DNOW)
0
#endif
))) || ((((4*32+ 7))>>5)==2 && (1UL<<(((4*32+ 7))&31) & 0)) || ((((4*32+ 7))>>5)==3 && (1UL<<(((4*32+ 7))&31) & (
#if !definedEx(CONFIG_X86_64) && definedEx(CONFIG_X86_P6_NOP) || definedEx(CONFIG_X86_64)
(1<<((3*32+20) & 31))
#endif
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_X86_P6_NOP)
0
#endif
))) || ((((4*32+ 7))>>5)==4 && (1UL<<(((4*32+ 7))&31) & 0)) || ((((4*32+ 7))>>5)==5 && (1UL<<(((4*32+ 7))&31) & 0)) || ((((4*32+ 7))>>5)==6 && (1UL<<(((4*32+ 7))&31) & 0)) || ((((4*32+ 7))>>5)==7 && (1UL<<(((4*32+ 7))&31) & 0)) ) ? 1 : (__builtin_constant_p(((4*32+ 7))) ? constant_test_bit(((4*32+ 7)), ((unsigned long *)((c)->x86_capability))) : variable_test_bit(((4*32+ 7)), ((unsigned long *)((c)->x86_capability))))))
		buf[2] |= ((0x0008) | (0x0002) | (0x0020) | (0x0800) | (0x0001));
	if ((__builtin_constant_p((0*32+22)) && ( ((((0*32+22))>>5)==0 && (1UL<<(((0*32+22))&31) & (
#if !definedEx(CONFIG_MATH_EMULATION)
(1<<((0*32+ 0) & 31))
#endif
#if definedEx(CONFIG_MATH_EMULATION)
0
#endif
|
#if !definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_64) && definedEx(CONFIG_PARAVIRT)
0
#endif
#if definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT)
(1<<((0*32+ 3)) & 31)
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+ 5) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if !definedEx(CONFIG_X86_PAE) && definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_PAE)
(1<<((0*32+ 6) & 31))
#endif
#if !definedEx(CONFIG_X86_PAE) && !definedEx(CONFIG_X86_64)
0
#endif
| 
#if definedEx(CONFIG_X86_CMPXCHG64)
(1<<((0*32+ 8) & 31))
#endif
#if !definedEx(CONFIG_X86_CMPXCHG64)
0
#endif
|
#if !definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_64) && definedEx(CONFIG_PARAVIRT)
0
#endif
#if definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT)
(1<<((0*32+13)) & 31)
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+24) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if !definedEx(CONFIG_X86_64) && definedEx(CONFIG_X86_CMOV) || definedEx(CONFIG_X86_64)
(1<<((0*32+15) & 31))
#endif
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_X86_CMOV)
0
#endif
| 
#if definedEx(CONFIG_X86_64)
(1<<((0*32+25) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+26) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
))) || ((((0*32+22))>>5)==1 && (1UL<<(((0*32+22))&31) & (
#if definedEx(CONFIG_X86_64)
(1<<((1*32+29) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if definedEx(CONFIG_X86_USE_3DNOW)
(1<<((1*32+31) & 31))
#endif
#if !definedEx(CONFIG_X86_USE_3DNOW)
0
#endif
))) || ((((0*32+22))>>5)==2 && (1UL<<(((0*32+22))&31) & 0)) || ((((0*32+22))>>5)==3 && (1UL<<(((0*32+22))&31) & (
#if !definedEx(CONFIG_X86_64) && definedEx(CONFIG_X86_P6_NOP) || definedEx(CONFIG_X86_64)
(1<<((3*32+20) & 31))
#endif
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_X86_P6_NOP)
0
#endif
))) || ((((0*32+22))>>5)==4 && (1UL<<(((0*32+22))&31) & 0)) || ((((0*32+22))>>5)==5 && (1UL<<(((0*32+22))&31) & 0)) || ((((0*32+22))>>5)==6 && (1UL<<(((0*32+22))&31) & 0)) || ((((0*32+22))>>5)==7 && (1UL<<(((0*32+22))&31) & 0)) ) ? 1 : (__builtin_constant_p(((0*32+22))) ? constant_test_bit(((0*32+22)), ((unsigned long *)((c)->x86_capability))) : variable_test_bit(((0*32+22)), ((unsigned long *)((c)->x86_capability))))))
		buf[2] |= (0x0004);
	/*
	 * If mwait/monitor is unsupported, C2/C3_FFH will be disabled
	 */
	if (!(__builtin_constant_p((4*32+ 3)) && ( ((((4*32+ 3))>>5)==0 && (1UL<<(((4*32+ 3))&31) & (
#if !definedEx(CONFIG_MATH_EMULATION)
(1<<((0*32+ 0) & 31))
#endif
#if definedEx(CONFIG_MATH_EMULATION)
0
#endif
|
#if !definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_64) && definedEx(CONFIG_PARAVIRT)
0
#endif
#if definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT)
(1<<((0*32+ 3)) & 31)
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+ 5) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if !definedEx(CONFIG_X86_PAE) && definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_PAE)
(1<<((0*32+ 6) & 31))
#endif
#if !definedEx(CONFIG_X86_PAE) && !definedEx(CONFIG_X86_64)
0
#endif
| 
#if definedEx(CONFIG_X86_CMPXCHG64)
(1<<((0*32+ 8) & 31))
#endif
#if !definedEx(CONFIG_X86_CMPXCHG64)
0
#endif
|
#if !definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_64) && definedEx(CONFIG_PARAVIRT)
0
#endif
#if definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT)
(1<<((0*32+13)) & 31)
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+24) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if !definedEx(CONFIG_X86_64) && definedEx(CONFIG_X86_CMOV) || definedEx(CONFIG_X86_64)
(1<<((0*32+15) & 31))
#endif
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_X86_CMOV)
0
#endif
| 
#if definedEx(CONFIG_X86_64)
(1<<((0*32+25) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+26) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
))) || ((((4*32+ 3))>>5)==1 && (1UL<<(((4*32+ 3))&31) & (
#if definedEx(CONFIG_X86_64)
(1<<((1*32+29) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if definedEx(CONFIG_X86_USE_3DNOW)
(1<<((1*32+31) & 31))
#endif
#if !definedEx(CONFIG_X86_USE_3DNOW)
0
#endif
))) || ((((4*32+ 3))>>5)==2 && (1UL<<(((4*32+ 3))&31) & 0)) || ((((4*32+ 3))>>5)==3 && (1UL<<(((4*32+ 3))&31) & (
#if !definedEx(CONFIG_X86_64) && definedEx(CONFIG_X86_P6_NOP) || definedEx(CONFIG_X86_64)
(1<<((3*32+20) & 31))
#endif
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_X86_P6_NOP)
0
#endif
))) || ((((4*32+ 3))>>5)==4 && (1UL<<(((4*32+ 3))&31) & 0)) || ((((4*32+ 3))>>5)==5 && (1UL<<(((4*32+ 3))&31) & 0)) || ((((4*32+ 3))>>5)==6 && (1UL<<(((4*32+ 3))&31) & 0)) || ((((4*32+ 3))>>5)==7 && (1UL<<(((4*32+ 3))&31) & 0)) ) ? 1 : (__builtin_constant_p(((4*32+ 3))) ? constant_test_bit(((4*32+ 3)), ((unsigned long *)((c)->x86_capability))) : variable_test_bit(((4*32+ 3)), ((unsigned long *)((c)->x86_capability))))))
		buf[2] &= ~((0x0200));
}
#endif
#if !definedEx(CONFIG_ACPI)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void acpi_noirq_set(void) { }
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void acpi_disable_pci(void) { }
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void disable_acpi(void) { }
#endif
struct bootnode;
#if definedEx(CONFIG_ACPI_NUMA)
extern int acpi_numa;
extern int acpi_get_nodes(struct bootnode *physnodes);
extern int acpi_scan_nodes(unsigned long start, unsigned long end);
extern void acpi_fake_nodes(const struct bootnode *fake_nodes,
				   int num_nodes);
#endif
#if !definedEx(CONFIG_ACPI_NUMA)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void acpi_fake_nodes(const struct bootnode *fake_nodes,
				   int num_nodes)
{
}
#endif
#line 21 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/fixmap.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/apicdef.h" 1
#line 22 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/fixmap.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/page.h" 1
#line 23 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/fixmap.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/threads.h" 1
#line 25 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/fixmap.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/kmap_types.h" 1
#if !definedEx(CONFIG_PARAVIRT)
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_DEBUG_HIGHMEM)
#endif
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/asm-generic/kmap_types.h" 1
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
#endif
#if !definedEx(CONFIG_X86_32) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_DEBUG_HIGHMEM) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_DEBUG_HIGHMEM) && !definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_PARAVIRT)
#endif
enum km_type {
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
__KM_FENCE_0 ,
#endif
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
#endif
	KM_BOUNCE_READ,
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
__KM_FENCE_1 ,
#endif
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
#endif
	KM_SKB_SUNRPC_DATA,
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
__KM_FENCE_2 ,
#endif
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
#endif
	KM_SKB_DATA_SOFTIRQ,
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
__KM_FENCE_3 ,
#endif
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
#endif
	KM_USER0,
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
__KM_FENCE_4 ,
#endif
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
#endif
	KM_USER1,
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
__KM_FENCE_5 ,
#endif
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
#endif
	KM_BIO_SRC_IRQ,
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
__KM_FENCE_6 ,
#endif
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
#endif
	KM_BIO_DST_IRQ,
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
__KM_FENCE_7 ,
#endif
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
#endif
	KM_PTE0,
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
__KM_FENCE_8 ,
#endif
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
#endif
	KM_PTE1,
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
__KM_FENCE_9 ,
#endif
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
#endif
	KM_IRQ0,
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
__KM_FENCE_10 ,
#endif
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
#endif
	KM_IRQ1,
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
__KM_FENCE_11 ,
#endif
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
#endif
	KM_SOFTIRQ0,
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
__KM_FENCE_12 ,
#endif
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
#endif
	KM_SOFTIRQ1,
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
__KM_FENCE_13 ,
#endif
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
#endif
	KM_SYNC_ICACHE,
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
__KM_FENCE_14 ,
#endif
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
#endif
	KM_SYNC_DCACHE,
/* UML specific, for copy_*_user - used in do_op_one_page */
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
__KM_FENCE_15 ,
#endif
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
#endif
	KM_UML_USERCOPY,
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
__KM_FENCE_16 ,
#endif
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
#endif
	KM_IRQ_PTE,
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
__KM_FENCE_17 ,
#endif
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
#endif
	KM_NMI,
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
__KM_FENCE_18 ,
#endif
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
#endif
	KM_NMI_PTE,
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
__KM_FENCE_19 ,
#endif
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
#endif
	KM_TYPE_NR
};
#line 10 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/kmap_types.h" 2
#endif
#line 26 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/fixmap.h" 2
/*
 * We can't declare FIXADDR_TOP as variable for x86_64 because vsyscall
 * uses fixmaps that relies on FIXADDR_TOP for proper address calculation.
 * Because of this, FIXADDR_TOP x86 integration was left as later work.
 */
/* used by vmalloc.c, vsyscall.lds.S.
 *
 * Leave one empty page between vmalloc'ed areas and
 * the start of the fixmap.
 */
extern unsigned long __FIXADDR_TOP;
/*
 * Here we define all the compile-time 'special' virtual
 * addresses. The point is to have a constant address at
 * compile time, but to set the physical address only
 * in the boot process.
 * for x86_32: We allocate these special addresses
 * from the end of virtual memory (0xfffff000) backwards.
 * Also this lets us do fail-safe vmalloc(), we
 * can guarantee that these special addresses and
 * vmalloc()-ed addresses never overlap.
 *
 * These 'compile-time allocated' memory buffers are
 * fixed-size 4k pages (or larger if used with an increment
 * higher than 1). Use set_fixmap(idx,phys) to associate
 * physical memory with fixmap indices.
 *
 * TLB entries of such buffers will not be flushed across
 * task switches.
 */
enum fixed_addresses {
	FIX_HOLE,
	FIX_VDSO,
	FIX_DBGP_BASE,
	FIX_EARLYCON_MEM_BASE,
#if definedEx(CONFIG_PROVIDE_OHCI1394_DMA_INIT)
	FIX_OHCI1394_BASE,
#endif
	FIX_APIC_BASE,	/* local (CPU) APIC) -- required for SMP or not */
#if definedEx(CONFIG_X86_IO_APIC)
	FIX_IO_APIC_BASE_0,
	FIX_IO_APIC_BASE_END = FIX_IO_APIC_BASE_0 + 64 - 1,
#endif
#if definedEx(CONFIG_X86_VISWS_APIC)
	FIX_CO_CPU,	/* Cobalt timer */
	FIX_CO_APIC,	/* Cobalt APIC Redirection Table */
	FIX_LI_PCIA,	/* Lithium PCI Bridge A */
	FIX_LI_PCIB,	/* Lithium PCI Bridge B */
#endif
#if definedEx(CONFIG_X86_F00F_BUG)
	FIX_F00F_IDT,	/* Virtual mapping for IDT */
#endif
#if definedEx(CONFIG_X86_CYCLONE_TIMER)
	FIX_CYCLONE_TIMER, /*cyclone timer register*/
#endif
	FIX_KMAP_BEGIN,	/* reserved pte's for temporary kernel mappings */
	FIX_KMAP_END = FIX_KMAP_BEGIN+(KM_TYPE_NR*8)-1,
#if definedEx(CONFIG_PCI_MMCONFIG)
	FIX_PCIE_MCFG,
#endif
#if definedEx(CONFIG_PARAVIRT)
	FIX_PARAVIRT_BOOTMAP,
#endif
	FIX_TEXT_POKE1,	/* reserve 2 pages for text_poke() */
	FIX_TEXT_POKE0, /* first page is last, because allocation is backward */
	__end_of_permanent_fixed_addresses,
	/*
	 * 256 temporary boot-time mappings, used by early_ioremap(),
	 * before ioremap() is functional.
	 *
	 * We round it up to the next 256 pages boundary so that we
	 * can have a single pgd entry and a single pte table:
	 */
	FIX_BTMAP_END = __end_of_permanent_fixed_addresses + 256 -
			(__end_of_permanent_fixed_addresses & 255),
	FIX_BTMAP_BEGIN = FIX_BTMAP_END + 64*4 - 1,
	FIX_WP_TEST,
#if definedEx(CONFIG_INTEL_TXT)
	FIX_TBOOT_BASE,
#endif
	__end_of_fixed_addresses
};
extern void reserve_top_address(unsigned long reserve);
extern int fixmaps_set;
extern pte_t *kmap_pte;
extern pgprot_t kmap_prot;
extern pte_t *pkmap_page_table;
void __native_set_fixmap(enum fixed_addresses idx, pte_t pte);
void native_set_fixmap(enum fixed_addresses idx,
		       phys_addr_t phys, pgprot_t flags);
#if !definedEx(CONFIG_PARAVIRT)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __set_fixmap(enum fixed_addresses idx,
				phys_addr_t phys, pgprot_t flags)
{
	native_set_fixmap(idx, phys, flags);
}
#endif
/*
 * Some hardware wants to get fixmapped without caching.
 */
extern void __this_fixmap_does_not_exist(void);
/*
 * 'index to address' translation. If anyone tries to use the idx
 * directly without translation, we catch the bug with a NULL-deference
 * kernel oops. Illegal ranges of incoming indices are caught too.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __attribute__((always_inline)) unsigned long fix_to_virt(const unsigned int idx)
{
	/*
	 * this branch gets completely eliminated after inlining,
	 * except when someone tries to use fixaddr indices in an
	 * illegal way. (such as mixing up address types or using
	 * out-of-range indices).
	 *
	 * If it doesn't get removed, the linker will complain
	 * loudly with a reasonably clear error message..
	 */
	if (idx >= __end_of_fixed_addresses)
		__this_fixmap_does_not_exist();
	return (((unsigned long)__FIXADDR_TOP) - ((idx) << 12));
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long virt_to_fix(const unsigned long vaddr)
{
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(vaddr >=((unsigned long)__FIXADDR_TOP) || vaddr <(((unsigned long)__FIXADDR_TOP) -(__end_of_permanent_fixed_addresses << 12))), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b, %c0\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/fixmap.h"), "i" (208), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (vaddr >= ((unsigned long)__FIXADDR_TOP) || vaddr < (((unsigned long)__FIXADDR_TOP) - (__end_of_permanent_fixed_addresses << 12))) ; } while(0)
#endif
;
	return ((((unsigned long)__FIXADDR_TOP) - ((vaddr)&(~(((1UL) << 12)-1)))) >> 12);
}
#line 15 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/apic.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/mpspec.h" 1
#line 16 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/apic.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/system.h" 1
#line 17 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/apic.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/msr.h" 1
#line 18 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/apic.h" 2
/*
 * Debugging macros
 */
/*
 * Define the default level of output to be very little
 * This can be turned up by using apic=verbose for more
 * information and apic=debug for _lots_ of information.
 * apic_verbosity is defined in apic.c
 */
extern void generic_apic_probe(void);
extern unsigned int apic_verbosity;
extern int local_apic_timer_c2_ok;
extern int disable_apic;
extern void __inquire_remote_apic(int apicid);
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void default_inquire_remote_apic(int apicid)
{
	if (apic_verbosity >= 2)
		__inquire_remote_apic(apicid);
}
/*
 * With 82489DX we can't rely on apic feature bit
 * retrieved via cpuid but still have to deal with
 * such an apic chip so we assume that SMP configuration
 * is found from MP table (64bit case uses ACPI mostly
 * which set smp presence flag as well so we are safe
 * to use this helper too).
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 bool apic_from_smp_config(void)
{
	return smp_found_config && !disable_apic;
}
/*
 * Basic functions accessing APICs.
 */
#if definedEx(CONFIG_PARAVIRT)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h" 1
#line 87 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/apic.h" 2
#endif
#if definedEx(CONFIG_X86_64)
extern int is_vsmp_box(void);
#endif
#if !definedEx(CONFIG_X86_64)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int is_vsmp_box(void)
{
	return 0;
}
#endif
extern void xapic_wait_icr_idle(void);
extern u32 safe_xapic_wait_icr_idle(void);
extern void xapic_icr_write(u32, u32);
extern int setup_profiling_timer(unsigned int);
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void native_apic_mem_write(u32 reg, u32 v)
{
	volatile u32 *addr = (volatile u32 *)((fix_to_virt(FIX_APIC_BASE)) + reg);
	asm volatile ("661:\n\t" "movl %0, %1" "\n662:\n" ".section .altinstructions,\"a\"\n" " " ".balign 4" " " "\n" " " ".long" " " "661b\n" " " ".long" " " "663f\n" "	 .byte " "(3*32+19)" "\n" "	 .byte 662b-661b\n" "	 .byte 664f-663f\n" "	 .byte 0xff + (664f-663f) - (662b-661b)\n" ".previous\n" ".section .altinstr_replacement, \"ax\"\n" "663:\n\t" "xchgl %0, %1" "\n664:\n" ".previous" : 
 "=r"(v), "=m"(*addr) : "i" (0),"0"(v), "m"(*addr));
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 u32 native_apic_mem_read(u32 reg)
{
	return *((volatile u32 *)((fix_to_virt(FIX_APIC_BASE)) + reg));
}
extern void native_apic_wait_icr_idle(void);
extern u32 native_safe_apic_wait_icr_idle(void);
extern void native_apic_icr_write(u32 low, u32 id);
extern u64 native_apic_icr_read(void);
extern int x2apic_mode;
#if definedEx(CONFIG_X86_X2APIC)
/*
 * Make previous memory operations globally visible before
 * sending the IPI through x2apic wrmsr. We need a serializing instruction or
 * mfence for this.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void x2apic_wrmsr_fence(void)
{
	asm volatile("mfence" : : : "memory");
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void native_apic_msr_write(u32 reg, u32 v)
{
	if (reg == 0xE0 || reg == 0x20 || reg == 0xD0 ||
	    reg == 0x30)
		return;
#if definedEx(CONFIG_PARAVIRT)
do { paravirt_write_msr(0x800 +(reg >> 4), v, 0); } while (0)
#endif
#if !definedEx(CONFIG_PARAVIRT)
wrmsr(0x800 + (reg >> 4), v, 0)
#endif
;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 u32 native_apic_msr_read(u32 reg)
{
	u32 low, high;
	if (reg == 0xE0)
		return -1;
#if definedEx(CONFIG_PARAVIRT)
do { int _err; u64 _l = paravirt_read_msr(0x800 +(reg >> 4), &_err); low = (u32)_l; high = _l >> 32; } while (0)
#endif
#if !definedEx(CONFIG_PARAVIRT)
do { u64 __val = native_read_msr((0x800 +(reg >> 4))); (low) = (u32)__val; (high) = (u32)(__val >> 32); } while (0)
#endif
;
	return low;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void native_x2apic_wait_icr_idle(void)
{
	/* no need to wait for icr idle in x2apic */
	return;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 u32 native_safe_x2apic_wait_icr_idle(void)
{
	/* no need to wait for icr idle in x2apic */
	return 0;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void native_x2apic_icr_write(u32 low, u32 id)
{
#if definedEx(CONFIG_PARAVIRT)
do { paravirt_write_msr(0x800 +(0x300 >> 4), (u32)((u64)(((__u64) id) << 32 | low)), ((u64)(((__u64) id) << 32 | low))>>32); } while (0)
#endif
#if !definedEx(CONFIG_PARAVIRT)
native_write_msr((0x800 +(0x300 >> 4)), (u32)((u64)(((__u64) id) << 32 | low)), (u32)((u64)(((__u64) id) << 32 | low) >> 32))
#endif
;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 u64 native_x2apic_icr_read(void)
{
	unsigned long val;
#if definedEx(CONFIG_PARAVIRT)
do { int _err; val = paravirt_read_msr(0x800 +(0x300 >> 4), &_err); } while (0)
#endif
#if !definedEx(CONFIG_PARAVIRT)
((val) = native_read_msr((0x800 +(0x300 >> 4))))
#endif
;
	return val;
}
extern int x2apic_phys;
extern void check_x2apic(void);
extern void enable_x2apic(void);
extern void x2apic_icr_write(u32 low, u32 id);
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int x2apic_enabled(void)
{
	int msr, msr2;
	if (!(__builtin_constant_p((4*32+21)) && ( ((((4*32+21))>>5)==0 && (1UL<<(((4*32+21))&31) & (
#if !definedEx(CONFIG_MATH_EMULATION)
(1<<((0*32+ 0) & 31))
#endif
#if definedEx(CONFIG_MATH_EMULATION)
0
#endif
|
#if !definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_64) && definedEx(CONFIG_PARAVIRT)
0
#endif
#if definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT)
(1<<((0*32+ 3)) & 31)
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+ 5) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if !definedEx(CONFIG_X86_PAE) && definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_PAE)
(1<<((0*32+ 6) & 31))
#endif
#if !definedEx(CONFIG_X86_PAE) && !definedEx(CONFIG_X86_64)
0
#endif
| 
#if definedEx(CONFIG_X86_CMPXCHG64)
(1<<((0*32+ 8) & 31))
#endif
#if !definedEx(CONFIG_X86_CMPXCHG64)
0
#endif
|
#if !definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_64) && definedEx(CONFIG_PARAVIRT)
0
#endif
#if definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT)
(1<<((0*32+13)) & 31)
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+24) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if !definedEx(CONFIG_X86_64) && definedEx(CONFIG_X86_CMOV) || definedEx(CONFIG_X86_64)
(1<<((0*32+15) & 31))
#endif
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_X86_CMOV)
0
#endif
| 
#if definedEx(CONFIG_X86_64)
(1<<((0*32+25) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+26) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
))) || ((((4*32+21))>>5)==1 && (1UL<<(((4*32+21))&31) & (
#if definedEx(CONFIG_X86_64)
(1<<((1*32+29) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if definedEx(CONFIG_X86_USE_3DNOW)
(1<<((1*32+31) & 31))
#endif
#if !definedEx(CONFIG_X86_USE_3DNOW)
0
#endif
))) || ((((4*32+21))>>5)==2 && (1UL<<(((4*32+21))&31) & 0)) || ((((4*32+21))>>5)==3 && (1UL<<(((4*32+21))&31) & (
#if !definedEx(CONFIG_X86_64) && definedEx(CONFIG_X86_P6_NOP) || definedEx(CONFIG_X86_64)
(1<<((3*32+20) & 31))
#endif
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_X86_P6_NOP)
0
#endif
))) || ((((4*32+21))>>5)==4 && (1UL<<(((4*32+21))&31) & 0)) || ((((4*32+21))>>5)==5 && (1UL<<(((4*32+21))&31) & 0)) || ((((4*32+21))>>5)==6 && (1UL<<(((4*32+21))&31) & 0)) || ((((4*32+21))>>5)==7 && (1UL<<(((4*32+21))&31) & 0)) ) ? 1 : (__builtin_constant_p(((4*32+21))) ? constant_test_bit(((4*32+21)), ((unsigned long *)((&boot_cpu_data)->x86_capability))) : variable_test_bit(((4*32+21)), ((unsigned long *)((&boot_cpu_data)->x86_capability))))))
		return 0;
#if definedEx(CONFIG_PARAVIRT)
do { int _err; u64 _l = paravirt_read_msr(0x0000001b, &_err); msr = (u32)_l; msr2 = _l >> 32; } while (0)
#endif
#if !definedEx(CONFIG_PARAVIRT)
do { u64 __val = native_read_msr((0x0000001b)); (msr) = (u32)__val; (msr2) = (u32)(__val >> 32); } while (0)
#endif
;
	if (msr & (1UL << 10))
		return 1;
	return 0;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void x2apic_force_phys(void)
{
	x2apic_phys = 1;
}
#endif
#if !definedEx(CONFIG_X86_X2APIC)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void check_x2apic(void)
{
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void enable_x2apic(void)
{
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int x2apic_enabled(void)
{
	return 0;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void x2apic_force_phys(void)
{
}
#endif
extern void enable_IR_x2apic(void);
extern int get_physical_broadcast(void);
extern void apic_disable(void);
extern int lapic_get_maxlvt(void);
extern void clear_local_APIC(void);
extern void connect_bsp_APIC(void);
extern void disconnect_bsp_APIC(int virt_wire_setup);
extern void disable_local_APIC(void);
extern void lapic_shutdown(void);
extern int verify_local_APIC(void);
extern void cache_APIC_registers(void);
extern void sync_Arb_IDs(void);
extern void init_bsp_APIC(void);
extern void setup_local_APIC(void);
extern void end_local_APIC_setup(void);
extern void init_apic_mappings(void);
extern void setup_boot_APIC_clock(void);
extern void setup_secondary_APIC_clock(void);
extern int APIC_init_uniprocessor(void);
extern void enable_NMI_through_LVT0(void);
/*
 * On 32bit this is mach-xxx local
 */
#if definedEx(CONFIG_X86_64)
extern void early_init_lapic_mapping(void);
extern int apic_is_clustered_box(void);
#endif
#if !definedEx(CONFIG_X86_64)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int apic_is_clustered_box(void)
{
	return 0;
}
#endif
extern u8 setup_APIC_eilvt_mce(u8 vector, u8 msg_type, u8 mask);
extern u8 setup_APIC_eilvt_ibs(u8 vector, u8 msg_type, u8 mask);
#if definedEx(CONFIG_X86_64)
#endif
#if !definedEx(CONFIG_X86_64)
#endif
/*
 * Copyright 2004 James Cleverdon, IBM.
 * Subject to the GNU Public License, v.2
 *
 * Generic APIC sub-arch data struct.
 *
 * Hacked for x86-64 by James Cleverdon from i386 architecture code by
 * Martin Bligh, Andi Kleen, James Bottomley, John Stultz, and
 * James Cleverdon.
 */
struct apic {
	char *name;
	int (*probe)(void);
	int (*acpi_madt_oem_check)(char *oem_id, char *oem_table_id);
	int (*apic_id_registered)(void);
	u32 irq_delivery_mode;
	u32 irq_dest_mode;
	const struct cpumask *(*target_cpus)(void);
	int disable_esr;
	int dest_logical;
	unsigned long (*check_apicid_used)(physid_mask_t *map, int apicid);
	unsigned long (*check_apicid_present)(int apicid);
	void (*vector_allocation_domain)(int cpu, struct cpumask *retmask);
	void (*init_apic_ldr)(void);
	void (*ioapic_phys_id_map)(physid_mask_t *phys_map, physid_mask_t *retmap);
	void (*setup_apic_routing)(void);
	int (*multi_timer_check)(int apic, int irq);
	int (*apicid_to_node)(int logical_apicid);
	int (*cpu_to_logical_apicid)(int cpu);
	int (*cpu_present_to_apicid)(int mps_cpu);
	void (*apicid_to_cpu_present)(int phys_apicid, physid_mask_t *retmap);
	void (*setup_portio_remap)(void);
	int (*check_phys_apicid_present)(int phys_apicid);
	void (*enable_apic_mode)(void);
	int (*phys_pkg_id)(int cpuid_apic, int index_msb);
	/*
	 * When one of the next two hooks returns 1 the apic
	 * is switched to this. Essentially they are additional
	 * probe functions:
	 */
	int (*mps_oem_check)(struct mpc_table *mpc, char *oem, char *productid);
	unsigned int (*get_apic_id)(unsigned long x);
	unsigned long (*set_apic_id)(unsigned int id);
	unsigned long apic_id_mask;
	unsigned int (*cpu_mask_to_apicid)(const struct cpumask *cpumask);
	unsigned int (*cpu_mask_to_apicid_and)(const struct cpumask *cpumask,
					       const struct cpumask *andmask);
	/* ipi */
	void (*send_IPI_mask)(const struct cpumask *mask, int vector);
	void (*send_IPI_mask_allbutself)(const struct cpumask *mask,
					 int vector);
	void (*send_IPI_allbutself)(int vector);
	void (*send_IPI_all)(int vector);
	void (*send_IPI_self)(int vector);
	/* wakeup_secondary_cpu */
	int (*wakeup_secondary_cpu)(int apicid, unsigned long start_eip);
	int trampoline_phys_low;
	int trampoline_phys_high;
	void (*wait_for_init_deassert)(atomic_t *deassert);
	void (*smp_callin_clear_local_apic)(void);
	void (*inquire_remote_apic)(int apicid);
	/* apic ops */
	u32 (*read)(u32 reg);
	void (*write)(u32 reg, u32 v);
	u64 (*icr_read)(void);
	void (*icr_write)(u32 low, u32 high);
	void (*wait_icr_idle)(void);
	u32 (*safe_wait_icr_idle)(void);
};
/*
 * Pointer to the local APIC driver in use on this system (there's
 * always just one such driver in use - the kernel decides via an
 * early probing process which one it picks - and then sticks to it):
 */
extern struct apic *apic;
/*
 * APIC functionality to boot other CPUs - only used on SMP:
 */
extern atomic_t init_deasserted;
extern int wakeup_secondary_cpu_via_nmi(int apicid, unsigned long start_eip);
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 u32 apic_read(u32 reg)
{
	return apic->read(reg);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void apic_write(u32 reg, u32 val)
{
	apic->write(reg, val);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 u64 apic_icr_read(void)
{
	return apic->icr_read();
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void apic_icr_write(u32 low, u32 high)
{
	apic->icr_write(low, high);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void apic_wait_icr_idle(void)
{
	apic->wait_icr_idle();
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 u32 safe_apic_wait_icr_idle(void)
{
	return apic->safe_wait_icr_idle();
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void ack_APIC_irq(void)
{
	/*
	 * ack_APIC_irq() actually gets compiled as a single instruction
	 * ... yummie.
	 */
	/* Docs say use 0 for future compatibility */
	apic_write(0xB0, 0);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned default_get_apic_id(unsigned long x)
{
	unsigned int ver = ((apic_read(0x30)) & 0xFFu);
	if (((ver) >= 0x14) || (__builtin_constant_p((3*32+26)) && ( ((((3*32+26))>>5)==0 && (1UL<<(((3*32+26))&31) & (
#if !definedEx(CONFIG_MATH_EMULATION)
(1<<((0*32+ 0) & 31))
#endif
#if definedEx(CONFIG_MATH_EMULATION)
0
#endif
|
#if !definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_64) && definedEx(CONFIG_PARAVIRT)
0
#endif
#if definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT)
(1<<((0*32+ 3)) & 31)
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+ 5) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if !definedEx(CONFIG_X86_PAE) && definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_PAE)
(1<<((0*32+ 6) & 31))
#endif
#if !definedEx(CONFIG_X86_PAE) && !definedEx(CONFIG_X86_64)
0
#endif
| 
#if definedEx(CONFIG_X86_CMPXCHG64)
(1<<((0*32+ 8) & 31))
#endif
#if !definedEx(CONFIG_X86_CMPXCHG64)
0
#endif
|
#if !definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_64) && definedEx(CONFIG_PARAVIRT)
0
#endif
#if definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT)
(1<<((0*32+13)) & 31)
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+24) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if !definedEx(CONFIG_X86_64) && definedEx(CONFIG_X86_CMOV) || definedEx(CONFIG_X86_64)
(1<<((0*32+15) & 31))
#endif
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_X86_CMOV)
0
#endif
| 
#if definedEx(CONFIG_X86_64)
(1<<((0*32+25) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+26) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
))) || ((((3*32+26))>>5)==1 && (1UL<<(((3*32+26))&31) & (
#if definedEx(CONFIG_X86_64)
(1<<((1*32+29) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if definedEx(CONFIG_X86_USE_3DNOW)
(1<<((1*32+31) & 31))
#endif
#if !definedEx(CONFIG_X86_USE_3DNOW)
0
#endif
))) || ((((3*32+26))>>5)==2 && (1UL<<(((3*32+26))&31) & 0)) || ((((3*32+26))>>5)==3 && (1UL<<(((3*32+26))&31) & (
#if !definedEx(CONFIG_X86_64) && definedEx(CONFIG_X86_P6_NOP) || definedEx(CONFIG_X86_64)
(1<<((3*32+20) & 31))
#endif
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_X86_P6_NOP)
0
#endif
))) || ((((3*32+26))>>5)==4 && (1UL<<(((3*32+26))&31) & 0)) || ((((3*32+26))>>5)==5 && (1UL<<(((3*32+26))&31) & 0)) || ((((3*32+26))>>5)==6 && (1UL<<(((3*32+26))&31) & 0)) || ((((3*32+26))>>5)==7 && (1UL<<(((3*32+26))&31) & 0)) ) ? 1 : (__builtin_constant_p(((3*32+26))) ? constant_test_bit(((3*32+26)), ((unsigned long *)((&boot_cpu_data)->x86_capability))) : variable_test_bit(((3*32+26)), ((unsigned long *)((&boot_cpu_data)->x86_capability))))))
		return (x >> 24) & 0xFF;
	else
		return (x >> 24) & 0x0F;
}
/*
 * Warm reset vector default position:
 */
#if definedEx(CONFIG_X86_64)
extern struct apic apic_flat;
extern struct apic apic_physflat;
extern struct apic apic_x2apic_cluster;
extern struct apic apic_x2apic_phys;
extern int default_acpi_madt_oem_check(char *, char *);
extern void apic_send_IPI_self(int vector);
extern struct apic apic_x2apic_uv_x;
#if definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(".discard"), unused)) char __pcpu_scope_x2apic_extra_bits; extern __attribute__((section(".data.percpu" "")))  __typeof__(int) per_cpu__x2apic_extra_bits
#endif
#if !definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(".data.percpu" "")))  __typeof__(int) per_cpu__x2apic_extra_bits
#endif
;
extern int default_cpu_present_to_apicid(int mps_cpu);
extern int default_check_phys_apicid_present(int phys_apicid);
#endif
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void default_wait_for_init_deassert(atomic_t *deassert)
{
	while (!atomic_read(deassert))
		cpu_relax();
	return;
}
extern void generic_bigsmp_probe(void);
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/smp.h" 1
#line 466 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/apic.h" 2
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 const struct cpumask *default_target_cpus(void)
{
	return cpu_online_mask;
}
#if definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(".discard"), unused)) char __pcpu_scope_x86_bios_cpu_apicid; extern __attribute__((section(".data.percpu" "")))  __typeof__(u16) per_cpu__x86_bios_cpu_apicid
#endif
#if !definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(".data.percpu" "")))  __typeof__(u16) per_cpu__x86_bios_cpu_apicid
#endif
; extern __typeof__(u16) *x86_bios_cpu_apicid_early_ptr; extern __typeof__(u16) x86_bios_cpu_apicid_early_map[];
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned int read_apic_id(void)
{
	unsigned int reg;
	reg = apic_read(0x20);
	return apic->get_apic_id(reg);
}
extern void default_setup_apic_routing(void);
extern struct apic apic_noop;
extern struct apic apic_default;
/*
 * Set up the logical destination ID.
 *
 * Intel recommends to set DFR, LDR and TPR before enabling
 * an APIC.  See e.g. "AP-388 82489DX User's Manual" (Intel
 * document number 292116).  So here it goes...
 */
extern void default_init_apic_ldr(void);
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int default_apic_id_registered(void)
{
	return (__builtin_constant_p((read_apic_id())) ? constant_test_bit((read_apic_id()), ((phys_cpu_present_map).mask)) : variable_test_bit((read_apic_id()), ((phys_cpu_present_map).mask)));
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int default_phys_pkg_id(int cpuid_apic, int index_msb)
{
	return cpuid_apic >> index_msb;
}
extern int default_apicid_to_node(int logical_apicid);
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned int
default_cpu_mask_to_apicid(const struct cpumask *cpumask)
{
	return ((cpumask)->bits)[0] & 0xFFu;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned int
default_cpu_mask_to_apicid_and(const struct cpumask *cpumask,
			       const struct cpumask *andmask)
{
	unsigned long mask1 = ((cpumask)->bits)[0];
	unsigned long mask2 = ((andmask)->bits)[0];
	unsigned long mask3 = ((cpu_online_mask)->bits)[0];
	return (unsigned int)(mask1 & mask2 & mask3);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long default_check_apicid_used(physid_mask_t *map, int apicid)
{
	return (__builtin_constant_p((apicid)) ? constant_test_bit((apicid), ((*map).mask)) : variable_test_bit((apicid), ((*map).mask)));
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long default_check_apicid_present(int bit)
{
	return (__builtin_constant_p((bit)) ? constant_test_bit((bit), ((phys_cpu_present_map).mask)) : variable_test_bit((bit), ((phys_cpu_present_map).mask)));
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void default_ioapic_phys_id_map(physid_mask_t *phys_map, physid_mask_t *retmap)
{
	*retmap = *phys_map;
}
/* Mapping from cpu number to logical apicid */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int default_cpu_to_logical_apicid(int cpu)
{
	return 1 << cpu;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int __default_cpu_present_to_apicid(int mps_cpu)
{
	if (mps_cpu < nr_cpu_ids && (__builtin_constant_p((cpumask_check((mps_cpu)))) ? constant_test_bit((cpumask_check((mps_cpu))), ((((cpu_present_mask))->bits))) : variable_test_bit((cpumask_check((mps_cpu))), ((((cpu_present_mask))->bits)))))
		return (int)(*({ unsigned long __ptr; __asm__ ("" : "=r"(__ptr) : "0"((&per_cpu__x86_bios_cpu_apicid))); (typeof((&per_cpu__x86_bios_cpu_apicid))) (__ptr + (((__per_cpu_offset[mps_cpu])))); }));
	else
		return 0xFFu;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int
__default_check_phys_apicid_present(int phys_apicid)
{
	return (__builtin_constant_p((phys_apicid)) ? constant_test_bit((phys_apicid), ((phys_cpu_present_map).mask)) : variable_test_bit((phys_apicid), ((phys_cpu_present_map).mask)));
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int default_cpu_present_to_apicid(int mps_cpu)
{
	return __default_cpu_present_to_apicid(mps_cpu);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int
default_check_phys_apicid_present(int phys_apicid)
{
	return __default_check_phys_apicid_present(phys_apicid);
}
extern u8 cpu_2_logical_apicid[8];
#line 15 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/smp.h" 2
#if definedEx(CONFIG_X86_IO_APIC)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/io_apic.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/io_apic.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/mpspec.h" 1
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/io_apic.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/apicdef.h" 1
#line 8 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/io_apic.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/irq_vectors.h" 1
/*
 * Linux IRQ vector layout.
 *
 * There are 256 IDT entries (per CPU - each entry is 8 bytes) which can
 * be defined by Linux. They are used as a jump table by the CPU when a
 * given vector is triggered - by a CPU-external, CPU-internal or
 * software-triggered event.
 *
 * Linux sets the kernel code address each entry jumps to early during
 * bootup, and never changes them. This is the general layout of the
 * IDT entries:
 *
 *  Vectors   0 ...  31 : system traps and exceptions - hardcoded events
 *  Vectors  32 ... 127 : device interrupts
 *  Vector  128         : legacy int80 syscall interface
 *  Vectors 129 ... 237 : device interrupts
 *  Vectors 238 ... 255 : special interrupts
 *
 * 64-bit x86 has per CPU IDT tables, 32-bit has one shared IDT table.
 *
 * This file enumerates the exact layout of them:
 */
/*
 * IDT vectors usable for external interrupt sources start
 * at 0x20:
 */
/*
 * Reserve the lowest usable priority level 0x20 - 0x2f for triggering
 * cleanup after irq migration.
 */
/*
 * Vectors 0x30-0x3f are used for ISA interrupts.
 */
/*
 * Special IRQ vectors used by the SMP architecture, 0xf0-0xff
 *
 *  some of the following vectors are 'rare', they are merged
 *  into a single vector (CALL_FUNCTION_VECTOR) to save vector space.
 *  TLB, reschedule and local APIC vectors are performance-critical.
 */
/*
 * Sanity check
 */
/* f0-f7 used for spreading out TLB flushes: */
/*
 * Local APIC timer IRQ vector is on a different priority level,
 * to work around the 'lost local interrupt if more than 2 IRQ
 * sources per level' errata.
 */
/*
 * Generic system vector for platform specific use
 */
/*
 * Performance monitoring pending work vector:
 */
/*
 * Self IPI vector for machine checks
 */
/*
 * First APIC vector available to drivers: (vectors 0x30-0xee) we
 * start at 0x31(0x41) to spread out vectors evenly between priority
 * levels. (0x80 is the syscall vector)
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int invalid_vm86_irq(int irq)
{
	return irq < 3 || irq > 15;
}
/*
 * Size the maximum number of interrupts.
 *
 * If the irq_desc[] array has a sparse layout, we can size things
 * generously - it scales up linearly with the maximum number of CPUs,
 * and the maximum number of IO-APICs, whichever is higher.
 *
 * In other cases we size more conservatively, to not create too large
 * static arrays.
 */
#if definedEx(CONFIG_SPARSE_IRQ)
#endif
#if !definedEx(CONFIG_SPARSE_IRQ)
#endif
#line 9 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/io_apic.h" 2
/*
 * Intel IO-APIC support for SMP and UP systems.
 *
 * Copyright (C) 1997, 1998, 1999, 2000 Ingo Molnar
 */
/* I/O Unit Redirection Table */
/*
 * The structure of the IO-APIC:
 */
union IO_APIC_reg_00 {
	u32	raw;
	struct {
		u32	__reserved_2	: 14,
			LTS		:  1,
			delivery_type	:  1,
			__reserved_1	:  8,
			ID		:  8;
	} __attribute__ ((packed)) bits;
};
union IO_APIC_reg_01 {
	u32	raw;
	struct {
		u32	version		:  8,
			__reserved_2	:  7,
			PRQ		:  1,
			entries		:  8,
			__reserved_1	:  8;
	} __attribute__ ((packed)) bits;
};
union IO_APIC_reg_02 {
	u32	raw;
	struct {
		u32	__reserved_2	: 24,
			arbitration	:  4,
			__reserved_1	:  4;
	} __attribute__ ((packed)) bits;
};
union IO_APIC_reg_03 {
	u32	raw;
	struct {
		u32	boot_DT		:  1,
			__reserved_1	: 31;
	} __attribute__ ((packed)) bits;
};
enum ioapic_irq_destination_types {
	dest_Fixed = 0,
	dest_LowestPrio = 1,
	dest_SMI = 2,
	dest__reserved_1 = 3,
	dest_NMI = 4,
	dest_INIT = 5,
	dest__reserved_2 = 6,
	dest_ExtINT = 7
};
struct IO_APIC_route_entry {
	__u32	vector		:  8,
		delivery_mode	:  3,	/* 000: FIXED
					 * 001: lowest prio
					 * 111: ExtINT
					 */
		dest_mode	:  1,	/* 0: physical, 1: logical */
		delivery_status	:  1,
		polarity	:  1,
		irr		:  1,
		trigger		:  1,	/* 0: edge, 1: level */
		mask		:  1,	/* 0: enabled, 1: disabled */
		__reserved_2	: 15;
	__u32	__reserved_3	: 24,
		dest		:  8;
} __attribute__ ((packed));
struct IR_IO_APIC_route_entry {
	__u64	vector		: 8,
		zero		: 3,
		index2		: 1,
		delivery_status : 1,
		polarity	: 1,
		irr		: 1,
		trigger		: 1,
		mask		: 1,
		reserved	: 31,
		format		: 1,
		index		: 15;
} __attribute__ ((packed));
/*
 * # of IO-APICs and # of IRQ routing registers
 */
extern int nr_ioapics;
extern int nr_ioapic_registers[64];
/* I/O APIC entries */
extern struct mpc_ioapic mp_ioapics[64];
/* # of MP IRQ source entries */
extern int mp_irq_entries;
/* MP IRQ source entries */
extern struct mpc_intsrc mp_irqs[256];
/* non-0 if default (table-less) MP configuration */
extern int mpc_default_type;
/* Older SiS APIC requires we rewrite the index register */
extern int sis_apic_bug;
/* 1 if "noapic" boot option passed */
extern int skip_ioapic_setup;
/* 1 if "noapic" boot option passed */
extern int noioapicquirk;
/* -1 if "noapic" boot option passed */
extern int noioapicreroute;
/* 1 if the timer IRQ uses the '8259A Virtual Wire' mode */
extern int timer_through_8259;
extern void io_apic_disable_legacy(void);
/*
 * If we use the IO-APIC for IRQ routing, disable automatic
 * assignment of PCI IRQ's.
 */
extern u8 io_apic_unique_id(u8 id);
extern int io_apic_get_unique_id(int ioapic, int apic_id);
extern int io_apic_get_version(int ioapic);
extern int io_apic_get_redir_entries(int ioapic);
struct io_apic_irq_attr;
extern int io_apic_set_pci_routing(struct device *dev, int irq,
		 struct io_apic_irq_attr *irq_attr);
void setup_IO_APIC_irq_extra(u32 gsi);
extern int (*ioapic_renumber_irq)(int ioapic, int irq);
extern void ioapic_init_mappings(void);
extern void ioapic_insert_resources(void);
extern struct IO_APIC_route_entry **alloc_ioapic_entries(void);
extern void free_ioapic_entries(struct IO_APIC_route_entry **ioapic_entries);
extern int save_IO_APIC_setup(struct IO_APIC_route_entry **ioapic_entries);
extern void mask_IO_APIC_setup(struct IO_APIC_route_entry **ioapic_entries);
extern int restore_IO_APIC_setup(struct IO_APIC_route_entry **ioapic_entries);
extern void probe_nr_irqs_gsi(void);
extern int setup_ioapic_entry(int apic, int irq,
			      struct IO_APIC_route_entry *entry,
			      unsigned int destination, int trigger,
			      int polarity, int vector, int pin);
extern void ioapic_write_entry(int apic, int pin,
			       struct IO_APIC_route_entry e);
extern void setup_ioapic_ids_from_mpc(void);
struct mp_ioapic_gsi{
	int gsi_base;
	int gsi_end;
};
extern struct mp_ioapic_gsi  mp_gsi_routing[];
int mp_find_ioapic(int gsi);
int mp_find_ioapic_pin(int ioapic, int gsi);
void __attribute__ ((__section__(".init.text"))) __attribute__((__cold__)) __attribute__((no_instrument_function)) mp_register_ioapic(int id, u32 address, u32 gsi_base);
#line 17 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/smp.h" 2
#endif
#endif
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/thread_info.h" 1
/* thread_info.h: low-level thread information
 *
 * Copyright (C) 2002  David Howells (dhowells@redhat.com)
 * - Incorporating suggestions made by Linus Torvalds and Dave Miller
 */
#line 20 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/smp.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/cpumask.h" 1
#line 21 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/smp.h" 2
extern int smp_num_siblings;
extern unsigned int num_processors;
#if definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(".discard"), unused)) char __pcpu_scope_cpu_sibling_map; extern __attribute__((section(".data.percpu" "")))  __typeof__(cpumask_var_t) per_cpu__cpu_sibling_map
#endif
#if !definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(".data.percpu" "")))  __typeof__(cpumask_var_t) per_cpu__cpu_sibling_map
#endif
;
#if definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(".discard"), unused)) char __pcpu_scope_cpu_core_map; extern __attribute__((section(".data.percpu" "")))  __typeof__(cpumask_var_t) per_cpu__cpu_core_map
#endif
#if !definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(".data.percpu" "")))  __typeof__(cpumask_var_t) per_cpu__cpu_core_map
#endif
;
#if definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(".discard"), unused)) char __pcpu_scope_cpu_llc_id; extern __attribute__((section(".data.percpu" "")))  __typeof__(u16) per_cpu__cpu_llc_id
#endif
#if !definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(".data.percpu" "")))  __typeof__(u16) per_cpu__cpu_llc_id
#endif
;
#if definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(".discard"), unused)) char __pcpu_scope_cpu_number; extern __attribute__((section(".data.percpu" "")))  __typeof__(int) per_cpu__cpu_number
#endif
#if !definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(".data.percpu" "")))  __typeof__(int) per_cpu__cpu_number
#endif
;
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 struct cpumask *cpu_sibling_mask(int cpu)
{
	return (*({ unsigned long __ptr; __asm__ ("" : "=r"(__ptr) : "0"((&per_cpu__cpu_sibling_map))); (typeof((&per_cpu__cpu_sibling_map))) (__ptr + (((__per_cpu_offset[cpu])))); }));
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 struct cpumask *cpu_core_mask(int cpu)
{
	return (*({ unsigned long __ptr; __asm__ ("" : "=r"(__ptr) : "0"((&per_cpu__cpu_core_map))); (typeof((&per_cpu__cpu_core_map))) (__ptr + (((__per_cpu_offset[cpu])))); }));
}
#if definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(".discard"), unused)) char __pcpu_scope_x86_cpu_to_apicid; extern __attribute__((section(".data.percpu" "")))  __typeof__(u16) per_cpu__x86_cpu_to_apicid
#endif
#if !definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(".data.percpu" "")))  __typeof__(u16) per_cpu__x86_cpu_to_apicid
#endif
; extern __typeof__(u16) *x86_cpu_to_apicid_early_ptr; extern __typeof__(u16) x86_cpu_to_apicid_early_map[];
#if definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(".discard"), unused)) char __pcpu_scope_x86_bios_cpu_apicid; extern __attribute__((section(".data.percpu" "")))  __typeof__(u16) per_cpu__x86_bios_cpu_apicid
#endif
#if !definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(".data.percpu" "")))  __typeof__(u16) per_cpu__x86_bios_cpu_apicid
#endif
; extern __typeof__(u16) *x86_bios_cpu_apicid_early_ptr; extern __typeof__(u16) x86_bios_cpu_apicid_early_map[];
/* Static state in head.S used to set up a CPU */
extern struct {
	void *sp;
	unsigned short ss;
} stack_start;
struct smp_ops {
	void (*smp_prepare_boot_cpu)(void);
	void (*smp_prepare_cpus)(unsigned max_cpus);
	void (*smp_cpus_done)(unsigned max_cpus);
	void (*smp_send_stop)(void);
	void (*smp_send_reschedule)(int cpu);
	int (*cpu_up)(unsigned cpu);
	int (*cpu_disable)(void);
	void (*cpu_die)(unsigned int cpu);
	void (*play_dead)(void);
	void (*send_call_func_ipi)(const struct cpumask *mask);
	void (*send_call_func_single_ipi)(int cpu);
};
/* Globals due to paravirt */
extern void set_cpu_sibling_map(int cpu);
#if !definedEx(CONFIG_PARAVIRT)
#endif
extern struct smp_ops smp_ops;
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void smp_send_stop(void)
{
	smp_ops.smp_send_stop();
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void smp_prepare_boot_cpu(void)
{
	smp_ops.smp_prepare_boot_cpu();
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void smp_prepare_cpus(unsigned int max_cpus)
{
	smp_ops.smp_prepare_cpus(max_cpus);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void smp_cpus_done(unsigned int max_cpus)
{
	smp_ops.smp_cpus_done(max_cpus);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int __cpu_up(unsigned int cpu)
{
	return smp_ops.cpu_up(cpu);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int __cpu_disable(void)
{
	return smp_ops.cpu_disable();
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __cpu_die(unsigned int cpu)
{
	smp_ops.cpu_die(cpu);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void play_dead(void)
{
	smp_ops.play_dead();
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void smp_send_reschedule(int cpu)
{
	smp_ops.smp_send_reschedule(cpu);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void arch_send_call_function_single_ipi(int cpu)
{
	smp_ops.send_call_func_single_ipi(cpu);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void arch_send_call_function_ipi_mask(const struct cpumask *mask)
{
	smp_ops.send_call_func_ipi(mask);
}
void cpu_disable_common(void);
void native_smp_prepare_boot_cpu(void);
void native_smp_prepare_cpus(unsigned int max_cpus);
void native_smp_cpus_done(unsigned int max_cpus);
int native_cpu_up(unsigned int cpunum);
int native_cpu_disable(void);
void native_cpu_die(unsigned int cpu);
void native_play_dead(void);
void play_dead_common(void);
void wbinvd_on_cpu(int cpu);
int wbinvd_on_all_cpus(void);
void native_send_call_func_ipi(const struct cpumask *mask);
void native_send_call_func_single_ipi(int cpu);
void smp_store_cpu_info(int id);
/* We don't mark CPUs online until __cpu_up(), so we need another measure */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int num_booting_cpus(void)
{
	return cpumask_weight(cpu_callout_mask);
}
extern unsigned disabled_cpus __attribute__ ((__section__(".cpuinit.data")));
#if definedEx(CONFIG_X86_32_SMP)
/*
 * This function is needed by all SMP systems. It must _always_ be valid
 * from the initial startup. We map APIC_BASE very early in page_setup(),
 * so this is correct in the x86 case.
 */
extern int safe_smp_processor_id(void);
#endif
#if !definedEx(CONFIG_X86_32_SMP) && definedEx(CONFIG_X86_64_SMP)
#endif
#if definedEx(CONFIG_X86_LOCAL_APIC)
#if !definedEx(CONFIG_X86_64)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int logical_smp_processor_id(void)
{
	/* we don't want to mark this access volatile - bad code generation */
	return (((apic_read(0xD0)) >> 24) & 0xFFu);
}
#endif
extern int hard_smp_processor_id(void);
#endif
#if !definedEx(CONFIG_X86_LOCAL_APIC)
#endif
#line 11 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/mmzone_32.h" 2
#if definedEx(CONFIG_NUMA)
extern struct pglist_data *node_data[];
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/numaq.h" 1
/*
 * Written by: Patricia Gaughen, IBM Corporation
 *
 * Copyright (C) 2002, IBM Corp.
 *
 * All rights reserved.
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation; either version 2 of the License, or
 * (at your option) any later version.
 *
 * This program is distributed in the hope that it will be useful, but
 * WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or
 * NON INFRINGEMENT.  See the GNU General Public License for more
 * details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program; if not, write to the Free Software
 * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
 *
 * Send feedback to <gone@us.ibm.com>
 */
#if definedEx(CONFIG_X86_NUMAQ)
extern int found_numaq;
extern int get_memcfg_numaq(void);
extern void *xquad_portio;
/*
 * SYS_CFG_DATA_PRIV_ADDR, struct eachquadmem, and struct sys_cfg_data are the
 */
/*
 * Communication area for each processor on lynxer-processor tests.
 *
 * NOTE: If you change the size of this eachproc structure you need
 *       to change the definition for EACH_QUAD_SIZE.
 */
struct eachquadmem {
	unsigned int	priv_mem_start;		/* Starting address of this */
						/* quad's private memory. */
						/* This is always 0. */
						/* In MB. */
	unsigned int	priv_mem_size;		/* Size of this quad's */
						/* private memory. */
						/* In MB. */
	unsigned int	low_shrd_mem_strp_start;/* Starting address of this */
						/* quad's low shared block */
						/* (untranslated). */
						/* In MB. */
	unsigned int	low_shrd_mem_start;	/* Starting address of this */
						/* quad's low shared memory */
						/* (untranslated). */
						/* In MB. */
	unsigned int	low_shrd_mem_size;	/* Size of this quad's low */
						/* shared memory. */
						/* In MB. */
	unsigned int	lmmio_copb_start;	/* Starting address of this */
						/* quad's local memory */
						/* mapped I/O in the */
						/* compatibility OPB. */
						/* In MB. */
	unsigned int	lmmio_copb_size;	/* Size of this quad's local */
						/* memory mapped I/O in the */
						/* compatibility OPB. */
						/* In MB. */
	unsigned int	lmmio_nopb_start;	/* Starting address of this */
						/* quad's local memory */
						/* mapped I/O in the */
						/* non-compatibility OPB. */
						/* In MB. */
	unsigned int	lmmio_nopb_size;	/* Size of this quad's local */
						/* memory mapped I/O in the */
						/* non-compatibility OPB. */
						/* In MB. */
	unsigned int	io_apic_0_start;	/* Starting address of I/O */
						/* APIC 0. */
	unsigned int	io_apic_0_sz;		/* Size I/O APIC 0. */
	unsigned int	io_apic_1_start;	/* Starting address of I/O */
						/* APIC 1. */
	unsigned int	io_apic_1_sz;		/* Size I/O APIC 1. */
	unsigned int	hi_shrd_mem_start;	/* Starting address of this */
						/* quad's high shared memory.*/
						/* In MB. */
	unsigned int	hi_shrd_mem_size;	/* Size of this quad's high */
						/* shared memory. */
						/* In MB. */
	unsigned int	mps_table_addr;		/* Address of this quad's */
						/* MPS tables from BIOS, */
						/* in system space.*/
	unsigned int	lcl_MDC_pio_addr;	/* Port-I/O address for */
						/* local access of MDC. */
	unsigned int	rmt_MDC_mmpio_addr;	/* MM-Port-I/O address for */
						/* remote access of MDC. */
	unsigned int	mm_port_io_start;	/* Starting address of this */
						/* quad's memory mapped Port */
						/* I/O space. */
	unsigned int	mm_port_io_size;	/* Size of this quad's memory*/
						/* mapped Port I/O space. */
	unsigned int	mm_rmt_io_apic_start;	/* Starting address of this */
						/* quad's memory mapped */
						/* remote I/O APIC space. */
	unsigned int	mm_rmt_io_apic_size;	/* Size of this quad's memory*/
						/* mapped remote I/O APIC */
						/* space. */
	unsigned int	mm_isa_start;		/* Starting address of this */
						/* quad's memory mapped ISA */
						/* space (contains MDC */
						/* memory space). */
	unsigned int	mm_isa_size;		/* Size of this quad's memory*/
						/* mapped ISA space (contains*/
						/* MDC memory space). */
	unsigned int	rmt_qmi_addr;		/* Remote addr to access QMI.*/
	unsigned int	lcl_qmi_addr;		/* Local addr to access QMI. */
};
/*
 * Note: This structure must be NOT be changed unless the multiproc and
 * OS are changed to reflect the new structure.
 */
struct sys_cfg_data {
	unsigned int	quad_id;
	unsigned int	bsp_proc_id; /* Boot Strap Processor in this quad. */
	unsigned int	scd_version; /* Version number of this table. */
	unsigned int	first_quad_id;
	unsigned int	quads_present31_0; /* 1 bit for each quad */
	unsigned int	quads_present63_32; /* 1 bit for each quad */
	unsigned int	config_flags;
	unsigned int	boot_flags;
	unsigned int	csr_start_addr; /* Absolute value (not in MB) */
	unsigned int	csr_size; /* Absolute value (not in MB) */
	unsigned int	lcl_apic_start_addr; /* Absolute value (not in MB) */
	unsigned int	lcl_apic_size; /* Absolute value (not in MB) */
	unsigned int	low_shrd_mem_base; /* 0 or 512MB or 1GB */
	unsigned int	low_shrd_mem_quad_offset; /* 0,128M,256M,512M,1G */
					/* may not be totally populated */
	unsigned int	split_mem_enbl; /* 0 for no low shared memory */
	unsigned int	mmio_sz; /* Size of total system memory mapped I/O */
				 /* (in MB). */
	unsigned int	quad_spin_lock; /* Spare location used for quad */
					/* bringup. */
	unsigned int	nonzero55; /* For checksumming. */
	unsigned int	nonzeroaa; /* For checksumming. */
	unsigned int	scd_magic_number;
	unsigned int	system_type;
	unsigned int	checksum;
	/*
	 *	memory configuration area for each quad
	 */
	struct		eachquadmem eq[(1 << 3)];	/* indexed by quad id */
};
void numaq_tsc_disable(void);
#endif
#if !definedEx(CONFIG_X86_NUMAQ)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int get_memcfg_numaq(void)
{
	return 0;
}
#endif
#line 17 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/mmzone_32.h" 2
/* summit or generic arch */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/srat.h" 1
/*
 * Some of the code in this file has been gleaned from the 64 bit
 * discontigmem support code base.
 *
 * Copyright (C) 2002, IBM Corp.
 *
 * All rights reserved.
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation; either version 2 of the License, or
 * (at your option) any later version.
 *
 * This program is distributed in the hope that it will be useful, but
 * WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or
 * NON INFRINGEMENT.  See the GNU General Public License for more
 * details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program; if not, write to the Free Software
 * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
 *
 * Send feedback to Pat Gaughen <gone@us.ibm.com>
 */
#if definedEx(CONFIG_ACPI_NUMA)
extern int get_memcfg_from_srat(void);
#endif
#if !definedEx(CONFIG_ACPI_NUMA)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int get_memcfg_from_srat(void)
{
	return 0;
}
#endif
#line 19 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/mmzone_32.h" 2
extern int get_memcfg_numa_flat(void);
/*
 * This allows any one NUMA architecture to be compiled
 * for, and still fall back to the flat function if it
 * fails.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void get_memcfg_numa(void)
{
	if (get_memcfg_numaq())
		return;
	if (get_memcfg_from_srat())
		return;
	get_memcfg_numa_flat();
}
extern void resume_map_numa_kva(pgd_t *pgd);
#endif
#if !definedEx(CONFIG_NUMA)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void resume_map_numa_kva(pgd_t *pgd) {}
#endif
#if definedEx(CONFIG_DISCONTIGMEM)
/*
 * generic node memory support, the following assumptions apply:
 *
 * 1) memory comes in 64Mb contiguous chunks which are either present or not
 * 2) we will not have more than 64Gb in total
 *
 * for now assume that 64Gb is max amount of RAM for whole system
 *    64Gb / 4096bytes/page = 16777216 pages
 */
extern s8 physnode_map[];
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int pfn_to_nid(unsigned long pfn)
{
#if definedEx(CONFIG_NUMA)
	return((int) physnode_map[(pfn) / (16777216/1024)]);
#endif
#if !definedEx(CONFIG_NUMA)
	return 0;
#endif
}
/*
 * Following are macros that each numa implmentation must define.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int pfn_valid(int pfn)
{
	int nid = pfn_to_nid(pfn);
	if (nid >= 0)
		return (pfn < ({ pg_data_t *__pgdat = 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_NUMA)
(node_data[nid])
#endif
#if !definedEx(CONFIG_X86_32) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_NUMA)
NODE_DATA(nid)
#endif
; __pgdat->node_start_pfn + __pgdat->node_spanned_pages; }));
	return 0;
}
#endif
/* always use node 0 for bootmem on this numa platform */
#line 4 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/mmzone.h" 2
#endif
#if !definedEx(CONFIG_X86_32)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/mmzone_64.h" 1
/* K8 NUMA support */
/* Copyright 2002,2003 by Andi Kleen, SuSE Labs */
/* 2.5 Version loosely based on the NUMAQ Code by Pat Gaughen. */
#if definedEx(CONFIG_NUMA)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/mmdebug.h" 1
#if definedEx(CONFIG_DEBUG_VM)
#endif
#if !definedEx(CONFIG_DEBUG_VM)
#endif
#if definedEx(CONFIG_DEBUG_VIRTUAL)
#endif
#if !definedEx(CONFIG_DEBUG_VIRTUAL)
#endif
#line 12 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/mmzone_64.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/smp.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/cpumask.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/smp.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/init.h" 1
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/smp.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/percpu.h" 1
#line 8 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/smp.h" 2
/*
 * We need the APIC definitions automatically as part of 'smp.h'
 */
#if definedEx(CONFIG_X86_LOCAL_APIC)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/mpspec.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/init.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/mpspec.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/mpspec_def.h" 1
/*
 * Structure definitions for SMP machines following the
 * Intel Multiprocessing Specification 1.1 and 1.4.
 */
/*
 * This tag identifies where the SMP configuration
 * information is.
 */
/* Intel MP Floating Pointer Structure */
struct mpf_intel {
	char signature[4];		/* "_MP_"			*/
	unsigned int physptr;		/* Configuration table address	*/
	unsigned char length;		/* Our length (paragraphs)	*/
	unsigned char specification;	/* Specification version	*/
	unsigned char checksum;		/* Checksum (makes sum 0)	*/
	unsigned char feature1;		/* Standard or configuration ?	*/
	unsigned char feature2;		/* Bit7 set for IMCR|PIC	*/
	unsigned char feature3;		/* Unused (0)			*/
	unsigned char feature4;		/* Unused (0)			*/
	unsigned char feature5;		/* Unused (0)			*/
};
struct mpc_table {
	char signature[4];
	unsigned short length;		/* Size of table */
	char spec;			/* 0x01 */
	char checksum;
	char oem[8];
	char productid[12];
	unsigned int oemptr;		/* 0 if not present */
	unsigned short oemsize;		/* 0 if not present */
	unsigned short oemcount;
	unsigned int lapic;		/* APIC address */
	unsigned int reserved;
};
/* Followed by entries */
/* Used by IBM NUMA-Q to describe node locality */
struct mpc_cpu {
	unsigned char type;
	unsigned char apicid;		/* Local APIC number */
	unsigned char apicver;		/* Its versions */
	unsigned char cpuflag;
	unsigned int cpufeature;
	unsigned int featureflag;	/* CPUID feature value */
	unsigned int reserved[2];
};
struct mpc_bus {
	unsigned char type;
	unsigned char busid;
	unsigned char bustype[6];
};
/* List of Bus Type string values, Intel MP Spec. */
struct mpc_ioapic {
	unsigned char type;
	unsigned char apicid;
	unsigned char apicver;
	unsigned char flags;
	unsigned int apicaddr;
};
struct mpc_intsrc {
	unsigned char type;
	unsigned char irqtype;
	unsigned short irqflag;
	unsigned char srcbus;
	unsigned char srcbusirq;
	unsigned char dstapic;
	unsigned char dstirq;
};
enum mp_irq_source_types {
	mp_INT = 0,
	mp_NMI = 1,
	mp_SMI = 2,
	mp_ExtINT = 3
};
struct mpc_lintsrc {
	unsigned char type;
	unsigned char irqtype;
	unsigned short irqflag;
	unsigned char srcbusid;
	unsigned char srcbusirq;
	unsigned char destapic;
	unsigned char destapiclint;
};
struct mpc_oemtable {
	char signature[4];
	unsigned short length;		/* Size of table */
	char  rev;			/* 0x01 */
	char  checksum;
	char  mpc[8];
};
/*
 *	Default configurations
 *
 *	1	2 CPU ISA 82489DX
 *	2	2 CPU EISA 82489DX neither IRQ 0 timer nor IRQ 13 DMA chaining
 *	3	2 CPU EISA 82489DX
 *	4	2 CPU MCA 82489DX
 *	5	2 CPU ISA+PCI
 *	6	2 CPU EISA+PCI
 *	7	2 CPU MCA+PCI
 */
enum mp_bustype {
	MP_BUS_ISA = 1,
	MP_BUS_EISA,
	MP_BUS_PCI,
	MP_BUS_MCA,
};
#line 8 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/mpspec.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/x86_init.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/pgtable_types.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/x86_init.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/bootparam.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/bootparam.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/screen_info.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/screen_info.h" 2
/*
 * These are set up by the setup-routine at boot-time:
 */
struct screen_info {
	__u8  orig_x;		/* 0x00 */
	__u8  orig_y;		/* 0x01 */
	__u16 ext_mem_k;	/* 0x02 */
	__u16 orig_video_page;	/* 0x04 */
	__u8  orig_video_mode;	/* 0x06 */
	__u8  orig_video_cols;	/* 0x07 */
	__u8  flags;		/* 0x08 */
	__u8  unused2;		/* 0x09 */
	__u16 orig_video_ega_bx;/* 0x0a */
	__u16 unused3;		/* 0x0c */
	__u8  orig_video_lines;	/* 0x0e */
	__u8  orig_video_isVGA;	/* 0x0f */
	__u16 orig_video_points;/* 0x10 */
	/* VESA graphic mode -- linear frame buffer */
	__u16 lfb_width;	/* 0x12 */
	__u16 lfb_height;	/* 0x14 */
	__u16 lfb_depth;	/* 0x16 */
	__u32 lfb_base;		/* 0x18 */
	__u32 lfb_size;		/* 0x1c */
	__u16 cl_magic, cl_offset; /* 0x20 */
	__u16 lfb_linelength;	/* 0x24 */
	__u8  red_size;		/* 0x26 */
	__u8  red_pos;		/* 0x27 */
	__u8  green_size;	/* 0x28 */
	__u8  green_pos;	/* 0x29 */
	__u8  blue_size;	/* 0x2a */
	__u8  blue_pos;		/* 0x2b */
	__u8  rsvd_size;	/* 0x2c */
	__u8  rsvd_pos;		/* 0x2d */
	__u16 vesapm_seg;	/* 0x2e */
	__u16 vesapm_off;	/* 0x30 */
	__u16 pages;		/* 0x32 */
	__u16 vesa_attributes;	/* 0x34 */
	__u32 capabilities;     /* 0x36 */
	__u8  _reserved[6];	/* 0x3a */
} __attribute__((packed));
extern struct screen_info screen_info;
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/bootparam.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/apm_bios.h" 1
/*
 * Include file for the interface to an APM BIOS
 * Copyright 1994-2001 Stephen Rothwell (sfr@canb.auug.org.au)
 *
 * This program is free software; you can redistribute it and/or modify it
 * under the terms of the GNU General Public License as published by the
 * Free Software Foundation; either version 2, or (at your option) any
 * later version.
 *
 * This program is distributed in the hope that it will be useful, but
 * WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
 * General Public License for more details.
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 21 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/apm_bios.h" 2
typedef unsigned short	apm_event_t;
typedef unsigned short	apm_eventinfo_t;
struct apm_bios_info {
	__u16	version;
	__u16	cseg;
	__u32	offset;
	__u16	cseg_16;
	__u16	dseg;
	__u16	flags;
	__u16	cseg_len;
	__u16	cseg_16_len;
	__u16	dseg_len;
};
/* Results of APM Installation Check */
/*
 * Data for APM that is persistent across module unload/load
 */
struct apm_info {
	struct apm_bios_info	bios;
	unsigned short		connection_version;
	int			get_power_status_broken;
	int			get_power_status_swabinminutes;
	int			allow_ints;
	int			forbid_idle;
	int			realmode_power_off;
	int			disabled;
};
/*
 * The APM function codes
 */
/*
 * Function code for APM_FUNC_RESUME_TIMER
 */
/*
 * Function code for APM_FUNC_RESUME_ON_RING
 */
/*
 * Function code for APM_FUNC_TIMER_STATUS
 */
/*
 * in arch/i386/kernel/setup.c
 */
extern struct apm_info	apm_info;
/*
 * Power states
 */
/*
 * Events (results of Get PM Event)
 */
/*
 * Error codes
 */
/*
 * APM Device IDs
 */
/*
 * This is the "All Devices" ID communicated to the BIOS
 */
/*
 * Battery status
 */
/*
 * APM defined capability bit flags
 */
/*
 * ioctl operations
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/ioctl.h" 1
#line 217 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/apm_bios.h" 2
#line 8 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/bootparam.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/edd.h" 1
/*
 * linux/include/linux/edd.h
 *  Copyright (C) 2002, 2003, 2004 Dell Inc.
 *  by Matt Domsch <Matt_Domsch@dell.com>
 *
 * structures and definitions for the int 13h, ax={41,48}h
 * BIOS Enhanced Disk Drive Services
 * This is based on the T13 group document D1572 Revision 0 (August 14 2002)
 * available at http://www.t13.org/docs2002/d1572r0.pdf.  It is
 * very similar to D1484 Revision 3 http://www.t13.org/docs2002/d1484r3.pdf
 *
 * In a nutshell, arch/{i386,x86_64}/boot/setup.S populates a scratch
 * table in the boot_params that contains a list of BIOS-enumerated
 * boot devices.
 * In arch/{i386,x86_64}/kernel/setup.c, this information is
 * transferred into the edd structure, and in drivers/firmware/edd.c, that
 * information is used to identify BIOS boot disk.  The code in setup.S
 * is very sensitive to the size of these structures.
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License v2.0 as published by
 * the Free Software Foundation
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 35 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/edd.h" 2
struct edd_device_params {
	__u16 length;
	__u16 info_flags;
	__u32 num_default_cylinders;
	__u32 num_default_heads;
	__u32 sectors_per_track;
	__u64 number_of_sectors;
	__u16 bytes_per_sector;
	__u32 dpte_ptr;		/* 0xFFFFFFFF for our purposes */
	__u16 key;		/* = 0xBEDD */
	__u8 device_path_info_length;	/* = 44 */
	__u8 reserved2;
	__u16 reserved3;
	__u8 host_bus_type[4];
	__u8 interface_type[8];
	union {
		struct {
			__u16 base_address;
			__u16 reserved1;
			__u32 reserved2;
		} __attribute__ ((packed)) isa;
		struct {
			__u8 bus;
			__u8 slot;
			__u8 function;
			__u8 channel;
			__u32 reserved;
		} __attribute__ ((packed)) pci;
		/* pcix is same as pci */
		struct {
			__u64 reserved;
		} __attribute__ ((packed)) ibnd;
		struct {
			__u64 reserved;
		} __attribute__ ((packed)) xprs;
		struct {
			__u64 reserved;
		} __attribute__ ((packed)) htpt;
		struct {
			__u64 reserved;
		} __attribute__ ((packed)) unknown;
	} interface_path;
	union {
		struct {
			__u8 device;
			__u8 reserved1;
			__u16 reserved2;
			__u32 reserved3;
			__u64 reserved4;
		} __attribute__ ((packed)) ata;
		struct {
			__u8 device;
			__u8 lun;
			__u8 reserved1;
			__u8 reserved2;
			__u32 reserved3;
			__u64 reserved4;
		} __attribute__ ((packed)) atapi;
		struct {
			__u16 id;
			__u64 lun;
			__u16 reserved1;
			__u32 reserved2;
		} __attribute__ ((packed)) scsi;
		struct {
			__u64 serial_number;
			__u64 reserved;
		} __attribute__ ((packed)) usb;
		struct {
			__u64 eui;
			__u64 reserved;
		} __attribute__ ((packed)) i1394;
		struct {
			__u64 wwid;
			__u64 lun;
		} __attribute__ ((packed)) fibre;
		struct {
			__u64 identity_tag;
			__u64 reserved;
		} __attribute__ ((packed)) i2o;
		struct {
			__u32 array_number;
			__u32 reserved1;
			__u64 reserved2;
		} __attribute__ ((packed)) raid;
		struct {
			__u8 device;
			__u8 reserved1;
			__u16 reserved2;
			__u32 reserved3;
			__u64 reserved4;
		} __attribute__ ((packed)) sata;
		struct {
			__u64 reserved1;
			__u64 reserved2;
		} __attribute__ ((packed)) unknown;
	} device_path;
	__u8 reserved4;
	__u8 checksum;
} __attribute__ ((packed));
struct edd_info {
	__u8 device;
	__u8 version;
	__u16 interface_support;
	__u16 legacy_max_cylinder;
	__u8 legacy_max_head;
	__u8 legacy_sectors_per_track;
	struct edd_device_params params;
} __attribute__ ((packed));
struct edd {
	unsigned int mbr_signature[16];
	struct edd_info edd_info[6];
	unsigned char mbr_signature_nr;
	unsigned char edd_info_nr;
};
extern struct edd edd;
#line 9 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/bootparam.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/e820.h" 1
/*
 * Legacy E820 BIOS limits us to 128 (E820MAX) nodes due to the
 * constrained space in the zeropage.  If we have more nodes than
 * that, and if we've booted off EFI firmware, then the EFI tables
 * passed us from the EFI firmware can list more nodes.  Size our
 * internal memory map tables to have room for these additional
 * nodes, based on up to three entries per node for which the
 * kernel was built: MAX_NUMNODES == (1 << CONFIG_NODES_SHIFT),
 * plus E820MAX, allowing space for the possible duplicate E820
 * entries that might need room in the same arrays, prior to the
 * call to sanitize_e820_map() to remove duplicates.  The allowance
 * of three memory map entries per node is "enough" entries for
 * the initial hardware platform motivating this mechanism to make
 * use of additional EFI map entries.  Future platforms may want
 * to allow more than three entries per node or otherwise refine
 * this size.
 */
/*
 * Odd: 'make headers_check' complains about numa.h if I try
 * to collapse the next two #ifdef lines to a single line:
 *	#if defined(__KERNEL__) && defined(CONFIG_EFI)
 */
#if definedEx(CONFIG_EFI)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/numa.h" 1
#line 33 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/e820.h" 2
#endif
#if !definedEx(CONFIG_EFI)
#endif
/* reserved RAM used by kernel itself */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 54 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/e820.h" 2
struct e820entry {
	__u64 addr;	/* start of memory segment */
	__u64 size;	/* size of memory segment */
	__u32 type;	/* type of memory segment */
} __attribute__((packed));
struct e820map {
	__u32 nr_map;
	struct e820entry map[
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) && definedEx(CONFIG_EFI) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_X86_LOCAL_APIC) && definedEx(CONFIG_EFI)
(128 + 3 * (1 << 3))
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) && !definedEx(CONFIG_EFI) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_X86_LOCAL_APIC) && !definedEx(CONFIG_EFI)
128
#endif
];
};
/* see comment in arch/x86/kernel/e820.c */
extern struct e820map e820;
extern struct e820map e820_saved;
extern unsigned long pci_mem_start;
extern int e820_any_mapped(u64 start, u64 end, unsigned type);
extern int e820_all_mapped(u64 start, u64 end, unsigned type);
extern void e820_add_region(u64 start, u64 size, int type);
extern void e820_print_map(char *who);
extern int
sanitize_e820_map(struct e820entry *biosmap, int max_nr_map, u32 *pnr_map);
extern u64 e820_update_range(u64 start, u64 size, unsigned old_type,
			       unsigned new_type);
extern u64 e820_remove_range(u64 start, u64 size, unsigned old_type,
			     int checktype);
extern void update_e820(void);
extern void e820_setup_gap(void);
extern int e820_search_gap(unsigned long *gapstart, unsigned long *gapsize,
			unsigned long start_addr, unsigned long long end_addr);
struct setup_data;
extern void parse_e820_ext(struct setup_data *data, unsigned long pa_data);
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_HIBERNATION) && definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_HIBERNATION)
extern void e820_mark_nosave_regions(unsigned long limit_pfn);
#endif
#if !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_HIBERNATION) && !definedEx(CONFIG_X86_64)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void e820_mark_nosave_regions(unsigned long limit_pfn)
{
}
#endif
#if definedEx(CONFIG_MEMTEST)
extern void early_memtest(unsigned long start, unsigned long end);
#endif
#if !definedEx(CONFIG_MEMTEST)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void early_memtest(unsigned long start, unsigned long end)
{
}
#endif
extern unsigned long end_user_pfn;
extern u64 find_e820_area(u64 start, u64 end, u64 size, u64 align);
extern u64 find_e820_area_size(u64 start, u64 *sizep, u64 align);
extern void reserve_early(u64 start, u64 end, char *name);
extern void reserve_early_overlap_ok(u64 start, u64 end, char *name);
extern void free_early(u64 start, u64 end);
extern void early_res_to_bootmem(u64 start, u64 end);
extern u64 early_reserve_e820(u64 startt, u64 sizet, u64 align);
extern unsigned long e820_end_of_ram_pfn(void);
extern unsigned long e820_end_of_low_ram_pfn(void);
extern int e820_find_active_region(const struct e820entry *ei,
				  unsigned long start_pfn,
				  unsigned long last_pfn,
				  unsigned long *ei_startpfn,
				  unsigned long *ei_endpfn);
extern void e820_register_active_regions(int nid, unsigned long start_pfn,
					 unsigned long end_pfn);
extern u64 e820_hole_size(u64 start, u64 end);
extern void finish_e820_parsing(void);
extern void e820_reserve_resources(void);
extern void e820_reserve_resources_late(void);
extern void setup_memory_map(void);
extern char *default_machine_specific_memory_setup(void);
/*
 * Returns true iff the specified range [s,e) is completely contained inside
 * the ISA region.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 bool is_ISA_range(u64 s, u64 e)
{
	return s >= 0xa0000 && e <= 0x100000;
}
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/ioport.h" 1
/*
 * ioport.h	Definitions of routines for detecting, reserving and
 *		allocating system resources.
 *
 * Authors:	Linus Torvalds
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/compiler.h" 1
#line 14 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/ioport.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 15 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/ioport.h" 2
/*
 * Resources are tree-like, allowing
 * nesting etc..
 */
struct resource {
	resource_size_t start;
	resource_size_t end;
	const char *name;
	unsigned long flags;
	struct resource *parent, *sibling, *child;
};
struct resource_list {
	struct resource_list *next;
	struct resource *res;
	struct pci_dev *dev;
};
/*
 * IO resources have these defined flags.
 */
/* PnP IRQ specific bits (IORESOURCE_BITS) */
/* PnP DMA specific bits (IORESOURCE_BITS) */
/* PnP memory I/O specific bits (IORESOURCE_BITS) */
/* PnP I/O specific bits (IORESOURCE_BITS) */
/* PCI ROM control bits (IORESOURCE_BITS) */
/* PCI control bits.  Shares IORESOURCE_BITS with above PCI ROM.  */
/* PC/ISA/whatever - the normal PC address spaces: IO and memory */
extern struct resource ioport_resource;
extern struct resource iomem_resource;
extern int request_resource(struct resource *root, struct resource *new);
extern int release_resource(struct resource *new);
extern void reserve_region_with_split(struct resource *root,
			     resource_size_t start, resource_size_t end,
			     const char *name);
extern int insert_resource(struct resource *parent, struct resource *new);
extern void insert_resource_expand_to_fit(struct resource *root, struct resource *new);
extern int allocate_resource(struct resource *root, struct resource *new,
			     resource_size_t size, resource_size_t min,
			     resource_size_t max, resource_size_t align,
			     void (*alignf)(void *, struct resource *,
					    resource_size_t, resource_size_t),
			     void *alignf_data);
int adjust_resource(struct resource *res, resource_size_t start,
		    resource_size_t size);
resource_size_t resource_alignment(struct resource *res);
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 resource_size_t resource_size(const struct resource *res)
{
	return res->end - res->start + 1;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long resource_type(const struct resource *res)
{
	return res->flags & 0x00000f00;
}
/* Convenience shorthand with allocation */
extern struct resource * __request_region(struct resource *,
					resource_size_t start,
					resource_size_t n,
					const char *name, int flags);
/* Compatibility cruft */
extern int __check_region(struct resource *, resource_size_t, resource_size_t);
extern void __release_region(struct resource *, resource_size_t,
				resource_size_t);
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int 
#if definedEx(CONFIG_ENABLE_WARN_DEPRECATED)
__attribute__((deprecated))
#endif
#if !definedEx(CONFIG_ENABLE_WARN_DEPRECATED)
#endif
 check_region(resource_size_t s,
						resource_size_t n)
{
	return __check_region(&ioport_resource, s, n);
}
/* Wrappers for managed devices */
struct device;
extern struct resource * __devm_request_region(struct device *dev,
				struct resource *parent, resource_size_t start,
				resource_size_t n, const char *name);
extern void __devm_release_region(struct device *dev, struct resource *parent,
				  resource_size_t start, resource_size_t n);
extern int iomem_map_sanity_check(resource_size_t addr, unsigned long size);
extern int iomem_is_exclusive(u64 addr);
extern int
walk_system_ram_range(unsigned long start_pfn, unsigned long nr_pages,
		void *arg, int (*func)(unsigned long, unsigned long, void *));
#line 151 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/e820.h" 2
#line 10 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/bootparam.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/ist.h" 1
/*
 * Include file for the interface to IST BIOS
 * Copyright 2002 Andy Grover <andrew.grover@intel.com>
 *
 * This program is free software; you can redistribute it and/or modify it
 * under the terms of the GNU General Public License as published by the
 * Free Software Foundation; either version 2, or (at your option) any
 * later version.
 *
 * This program is distributed in the hope that it will be useful, but
 * WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
 * General Public License for more details.
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 22 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/ist.h" 2
struct ist_info {
	__u32 signature;
	__u32 command;
	__u32 event;
	__u32 perf_level;
};
extern struct ist_info ist_info;
#line 11 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/bootparam.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/video/edid.h" 1
struct edid_info {
	unsigned char dummy[128];
};
extern struct edid_info edid_info;
#line 12 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/bootparam.h" 2
/* setup data types */
/* extensible setup data list node */
struct setup_data {
	__u64 next;
	__u32 type;
	__u32 len;
	__u8 data[0];
};
struct setup_header {
	__u8	setup_sects;
	__u16	root_flags;
	__u32	syssize;
	__u16	ram_size;
	__u16	vid_mode;
	__u16	root_dev;
	__u16	boot_flag;
	__u16	jump;
	__u32	header;
	__u16	version;
	__u32	realmode_swtch;
	__u16	start_sys;
	__u16	kernel_version;
	__u8	type_of_loader;
	__u8	loadflags;
	__u16	setup_move_size;
	__u32	code32_start;
	__u32	ramdisk_image;
	__u32	ramdisk_size;
	__u32	bootsect_kludge;
	__u16	heap_end_ptr;
	__u8	ext_loader_ver;
	__u8	ext_loader_type;
	__u32	cmd_line_ptr;
	__u32	initrd_addr_max;
	__u32	kernel_alignment;
	__u8	relocatable_kernel;
	__u8	_pad2[3];
	__u32	cmdline_size;
	__u32	hardware_subarch;
	__u64	hardware_subarch_data;
	__u32	payload_offset;
	__u32	payload_length;
	__u64	setup_data;
} __attribute__((packed));
struct sys_desc_table {
	__u16 length;
	__u8  table[14];
};
struct efi_info {
	__u32 efi_loader_signature;
	__u32 efi_systab;
	__u32 efi_memdesc_size;
	__u32 efi_memdesc_version;
	__u32 efi_memmap;
	__u32 efi_memmap_size;
	__u32 efi_systab_hi;
	__u32 efi_memmap_hi;
};
/* The so-called "zeropage" */
struct boot_params {
	struct screen_info screen_info;			/* 0x000 */
	struct apm_bios_info apm_bios_info;		/* 0x040 */
	__u8  _pad2[4];					/* 0x054 */
	__u64  tboot_addr;				/* 0x058 */
	struct ist_info ist_info;			/* 0x060 */
	__u8  _pad3[16];				/* 0x070 */
	__u8  hd0_info[16];	/* obsolete! */		/* 0x080 */
	__u8  hd1_info[16];	/* obsolete! */		/* 0x090 */
	struct sys_desc_table sys_desc_table;		/* 0x0a0 */
	__u8  _pad4[144];				/* 0x0b0 */
	struct edid_info edid_info;			/* 0x140 */
	struct efi_info efi_info;			/* 0x1c0 */
	__u32 alt_mem_k;				/* 0x1e0 */
	__u32 scratch;		/* Scratch field! */	/* 0x1e4 */
	__u8  e820_entries;				/* 0x1e8 */
	__u8  eddbuf_entries;				/* 0x1e9 */
	__u8  edd_mbr_sig_buf_entries;			/* 0x1ea */
	__u8  _pad6[6];					/* 0x1eb */
	struct setup_header hdr;    /* setup header */	/* 0x1f1 */
	__u8  _pad7[0x290-0x1f1-sizeof(struct setup_header)];
	__u32 edd_mbr_sig_buffer[16];	/* 0x290 */
	struct e820entry e820_map[128];		/* 0x2d0 */
	__u8  _pad8[48];				/* 0xcd0 */
	struct edd_info eddbuf[6];		/* 0xd00 */
	__u8  _pad9[276];				/* 0xeec */
} __attribute__((packed));
enum {
	X86_SUBARCH_PC = 0,
	X86_SUBARCH_LGUEST,
	X86_SUBARCH_XEN,
	X86_SUBARCH_MRST,
	X86_NR_SUBARCHS,
};
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/x86_init.h" 2
struct mpc_bus;
struct mpc_cpu;
struct mpc_table;
/**
 * struct x86_init_mpparse - platform specific mpparse ops
 * @mpc_record:			platform specific mpc record accounting
 * @setup_ioapic_ids:		platform specific ioapic id override
 * @mpc_apic_id:		platform specific mpc apic id assignment
 * @smp_read_mpc_oem:		platform specific oem mpc table setup
 * @mpc_oem_pci_bus:		platform specific pci bus setup (default NULL)
 * @mpc_oem_bus_info:		platform specific mpc bus info
 * @find_smp_config:		find the smp configuration
 * @get_smp_config:		get the smp configuration
 */
struct x86_init_mpparse {
	void (*mpc_record)(unsigned int mode);
	void (*setup_ioapic_ids)(void);
	int (*mpc_apic_id)(struct mpc_cpu *m);
	void (*smp_read_mpc_oem)(struct mpc_table *mpc);
	void (*mpc_oem_pci_bus)(struct mpc_bus *m);
	void (*mpc_oem_bus_info)(struct mpc_bus *m, char *name);
	void (*find_smp_config)(void);
	void (*get_smp_config)(unsigned int early);
};
/**
 * struct x86_init_resources - platform specific resource related ops
 * @probe_roms:			probe BIOS roms
 * @reserve_resources:		reserve the standard resources for the
 *				platform
 * @memory_setup:		platform specific memory setup
 *
 */
struct x86_init_resources {
	void (*probe_roms)(void);
	void (*reserve_resources)(void);
	char *(*memory_setup)(void);
};
/**
 * struct x86_init_irqs - platform specific interrupt setup
 * @pre_vector_init:		init code to run before interrupt vectors
 *				are set up.
 * @intr_init:			interrupt init code
 * @trap_init:			platform specific trap setup
 */
struct x86_init_irqs {
	void (*pre_vector_init)(void);
	void (*intr_init)(void);
	void (*trap_init)(void);
};
/**
 * struct x86_init_oem - oem platform specific customizing functions
 * @arch_setup:			platform specific architecure setup
 * @banner:			print a platform specific banner
 */
struct x86_init_oem {
	void (*arch_setup)(void);
	void (*banner)(void);
};
/**
 * struct x86_init_paging - platform specific paging functions
 * @pagetable_setup_start:	platform specific pre paging_init() call
 * @pagetable_setup_done:	platform specific post paging_init() call
 */
struct x86_init_paging {
	void (*pagetable_setup_start)(pgd_t *base);
	void (*pagetable_setup_done)(pgd_t *base);
};
/**
 * struct x86_init_timers - platform specific timer setup
 * @setup_perpcu_clockev:	set up the per cpu clock event device for the
 *				boot cpu
 * @tsc_pre_init:		platform function called before TSC init
 * @timer_init:			initialize the platform timer (default PIT/HPET)
 */
struct x86_init_timers {
	void (*setup_percpu_clockev)(void);
	void (*tsc_pre_init)(void);
	void (*timer_init)(void);
};
/**
 * struct x86_init_iommu - platform specific iommu setup
 * @iommu_init:			platform specific iommu setup
 */
struct x86_init_iommu {
	int (*iommu_init)(void);
};
/**
 * struct x86_init_ops - functions for platform specific setup
 *
 */
struct x86_init_ops {
	struct x86_init_resources	resources;
	struct x86_init_mpparse		mpparse;
	struct x86_init_irqs		irqs;
	struct x86_init_oem		oem;
	struct x86_init_paging		paging;
	struct x86_init_timers		timers;
	struct x86_init_iommu		iommu;
};
/**
 * struct x86_cpuinit_ops - platform specific cpu hotplug setups
 * @setup_percpu_clockev:	set up the per cpu clock event device
 */
struct x86_cpuinit_ops {
	void (*setup_percpu_clockev)(void);
};
/**
 * struct x86_platform_ops - platform specific runtime functions
 * @calibrate_tsc:		calibrate TSC
 * @get_wallclock:		get time from HW clock like RTC etc.
 * @set_wallclock:		set time back to HW clock
 * @is_untracked_pat_range	exclude from PAT logic
 */
struct x86_platform_ops {
	unsigned long (*calibrate_tsc)(void);
	unsigned long (*get_wallclock)(void);
	int (*set_wallclock)(unsigned long nowtime);
	void (*iommu_shutdown)(void);
	bool (*is_untracked_pat_range)(u64 start, u64 end);
};
extern struct x86_init_ops x86_init;
extern struct x86_cpuinit_ops x86_cpuinit;
extern struct x86_platform_ops x86_platform;
extern void x86_init_noop(void);
extern void x86_init_uint_noop(unsigned int unused);
#line 9 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/mpspec.h" 2
extern int apic_version[255];
extern int pic_mode;
/* Each PCI slot may be a combo card with its own bus.  4 IRQ pins per slot. */
#if !definedEx(CONFIG_MCA) && definedEx(CONFIG_EISA) || definedEx(CONFIG_MCA)
extern int mp_bus_id_to_type[256];
#endif
extern unsigned long mp_bus_not_pci[(((256) + (8 * sizeof(long)) - 1) / (8 * sizeof(long)))];
extern unsigned int boot_cpu_physical_apicid;
extern unsigned int max_physical_apicid;
extern int mpc_default_type;
extern unsigned long mp_lapic_addr;
extern int smp_found_config;
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void get_smp_config(void)
{
	x86_init.mpparse.get_smp_config(0);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void early_get_smp_config(void)
{
	x86_init.mpparse.get_smp_config(1);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void find_smp_config(void)
{
	x86_init.mpparse.find_smp_config();
}
#if definedEx(CONFIG_X86_MPPARSE)
extern void early_reserve_e820_mpc_new(void);
extern int enable_update_mptable;
extern int default_mpc_apic_id(struct mpc_cpu *m);
extern void default_smp_read_mpc_oem(struct mpc_table *mpc);
#if definedEx(CONFIG_X86_IO_APIC)
extern void default_mpc_oem_bus_info(struct mpc_bus *m, char *str);
#endif
#if !definedEx(CONFIG_X86_IO_APIC)
#endif
extern void default_find_smp_config(void);
extern void default_get_smp_config(unsigned int early);
#endif
#if !definedEx(CONFIG_X86_MPPARSE)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void early_reserve_e820_mpc_new(void) { }
#endif
void __attribute__ ((__section__(".cpuinit.text"))) __attribute__((__cold__)) generic_processor_info(int apicid, int version);
#if definedEx(CONFIG_ACPI)
extern void mp_register_ioapic(int id, u32 address, u32 gsi_base);
extern void mp_override_legacy_irq(u8 bus_irq, u8 polarity, u8 trigger,
				   u32 gsi);
extern void mp_config_acpi_legacy_irqs(void);
struct device;
extern int mp_register_gsi(struct device *dev, u32 gsi, int edge_level,
				 int active_high_low);
extern int acpi_probe_gsi(void);
#if definedEx(CONFIG_X86_IO_APIC)
extern int mp_find_ioapic(int gsi);
extern int mp_find_ioapic_pin(int ioapic, int gsi);
#endif
#endif
#if !definedEx(CONFIG_ACPI)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int acpi_probe_gsi(void)
{
	return 0;
}
#endif
struct physid_mask {
	unsigned long mask[(((255) + (8 * sizeof(long)) - 1) / (8 * sizeof(long)))];
};
typedef struct physid_mask physid_mask_t;
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long physids_coerce(physid_mask_t *map)
{
	return map->mask[0];
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void physids_promote(unsigned long physids, physid_mask_t *map)
{
	bitmap_zero((*map).mask, 255);
	map->mask[0] = physids;
}
/* Note: will create very large stack frames if physid_mask_t is big */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void physid_set_mask_of_physid(int physid, physid_mask_t *map)
{
	bitmap_zero((*map).mask, 255);
	set_bit(physid, (*map).mask);
}
extern physid_mask_t phys_cpu_present_map;
extern int generic_mps_oem_check(struct mpc_table *, char *, char *);
extern int default_acpi_madt_oem_check(char *, char *);
#line 14 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/smp.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/apic.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/cpumask.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/apic.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/delay.h" 1
/*
 * Copyright (C) 1993 Linus Torvalds
 *
 * Delay routines, using a pre-computed "loops_per_jiffy" value.
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/kernel.h" 1
#line 12 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/delay.h" 2
extern unsigned long loops_per_jiffy;
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/delay.h" 1
/*
 * Copyright (C) 1993 Linus Torvalds
 *
 * Delay routines calling functions in arch/x86/lib/delay.c
 */
/* Undefined functions to get compile-time errors */
extern void __bad_udelay(void);
extern void __bad_ndelay(void);
extern void __udelay(unsigned long usecs);
extern void __ndelay(unsigned long nsecs);
extern void __const_udelay(unsigned long xloops);
extern void __delay(unsigned long loops);
/* 0x10c7 is 2**32 / 1000000 (rounded up) */
/* 0x5 is 2**32 / 1000000000 (rounded up) */
void use_tsc_delay(void);
#line 16 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/delay.h" 2
/*
 * Using udelay() for intervals greater than a few milliseconds can
 * risk overflow for high loops_per_jiffy (high bogomips) machines. The
 * mdelay() provides a wrapper to prevent this.  For delays greater
 * than MAX_UDELAY_MS milliseconds, the wrapper is used.  Architecture
 * specific values can be defined in asm-???/delay.h as an override.
 * The 2nd mdelay() definition ensures GCC will optimize away the 
 * while loop for the common cases where n <= MAX_UDELAY_MS  --  Paul G.
 */
extern unsigned long lpj_fine;
void calibrate_delay(void);
void msleep(unsigned int msecs);
unsigned long msleep_interruptible(unsigned int msecs);
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void ssleep(unsigned int seconds)
{
	msleep(seconds * 1000);
}
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/apic.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/pm.h" 1
/*
 *  pm.h - Power management interface
 *
 *  Copyright (C) 2000 Andrew Henroid
 *
 *  This program is free software; you can redistribute it and/or modify
 *  it under the terms of the GNU General Public License as published by
 *  the Free Software Foundation; either version 2 of the License, or
 *  (at your option) any later version.
 *
 *  This program is distributed in the hope that it will be useful,
 *  but WITHOUT ANY WARRANTY; without even the implied warranty of
 *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *  GNU General Public License for more details.
 *
 *  You should have received a copy of the GNU General Public License
 *  along with this program; if not, write to the Free Software
 *  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/list.h" 1
#line 26 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/pm.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/workqueue.h" 1
/*
 * workqueue.h --- work queue handling for Linux.
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/timer.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/list.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/timer.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/ktime.h" 1
/*
 *  include/linux/ktime.h
 *
 *  ktime_t - nanosecond-resolution time format.
 *
 *   Copyright(C) 2005, Thomas Gleixner <tglx@linutronix.de>
 *   Copyright(C) 2005, Red Hat, Inc., Ingo Molnar
 *
 *  data type definitions, declarations, prototypes and macros.
 *
 *  Started by: Thomas Gleixner and Ingo Molnar
 *
 *  Credits:
 *
 *  	Roman Zippel provided the ideas and primary code snippets of
 *  	the ktime_t union and further simplifications of the original
 *  	code.
 *
 *  For licencing details see kernel-base/COPYING
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/time.h" 1
#line 26 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/ktime.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/jiffies.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/math64.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/jiffies.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/kernel.h" 1
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/jiffies.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 8 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/jiffies.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/time.h" 1
#line 9 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/jiffies.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/timex.h" 1
/*****************************************************************************
 *                                                                           *
 * Copyright (c) David L. Mills 1993                                         *
 *                                                                           *
 * Permission to use, copy, modify, and distribute this software and its     *
 * documentation for any purpose and without fee is hereby granted, provided *
 * that the above copyright notice appears in all copies and that both the   *
 * copyright notice and this permission notice appear in supporting          *
 * documentation, and that the name University of Delaware not be used in    *
 * advertising or publicity pertaining to distribution of the software       *
 * without specific, written prior permission.  The University of Delaware   *
 * makes no representations about the suitability this software for any      *
 * purpose.  It is provided "as is" without express or implied warranty.     *
 *                                                                           *
 *****************************************************************************/
/*
 * Modification history timex.h
 *
 * 29 Dec 97	Russell King
 *	Moved CLOCK_TICK_RATE, CLOCK_TICK_FACTOR and FINETUNE to asm/timex.h
 *	for ARM machines
 *
 *  9 Jan 97    Adrian Sun
 *      Shifted LATCH define to allow access to alpha machines.
 *
 * 26 Sep 94	David L. Mills
 *	Added defines for hybrid phase/frequency-lock loop.
 *
 * 19 Mar 94	David L. Mills
 *	Moved defines from kernel routines to header file and added new
 *	defines for PPS phase-lock loop.
 *
 * 20 Feb 94	David L. Mills
 *	Revised status codes and structures for external clock and PPS
 *	signal discipline.
 *
 * 28 Nov 93	David L. Mills
 *	Adjusted parameters to improve stability and increase poll
 *	interval.
 *
 * 17 Sep 93    David L. Mills
 *      Created file $NTP/include/sys/timex.h
 * 07 Oct 93    Torsten Duwe
 *      Derived linux/timex.h
 * 1995-08-13    Torsten Duwe
 *      kernel PLL updated to 1994-12-13 specs (rfc-1589)
 * 1997-08-30    Ulrich Windl
 *      Added new constant NTP_PHASE_LIMIT
 * 2004-08-12    Christoph Lameter
 *      Reworked time interpolation logic
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/time.h" 1
#line 58 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/timex.h" 2
/*
 * syscall interface - used (mainly by NTP daemon)
 * to discipline kernel clock oscillator
 */
struct timex {
	unsigned int modes;	/* mode selector */
	long offset;		/* time offset (usec) */
	long freq;		/* frequency offset (scaled ppm) */
	long maxerror;		/* maximum error (usec) */
	long esterror;		/* estimated error (usec) */
	int status;		/* clock command/status */
	long constant;		/* pll time constant */
	long precision;		/* clock precision (usec) (read only) */
	long tolerance;		/* clock frequency tolerance (ppm)
				 * (read only)
				 */
	struct timeval time;	/* (read only) */
	long tick;		/* (modified) usecs between clock ticks */
	long ppsfreq;           /* pps frequency (scaled ppm) (ro) */
	long jitter;            /* pps jitter (us) (ro) */
	int shift;              /* interval duration (s) (shift) (ro) */
	long stabil;            /* pps stability (scaled ppm) (ro) */
	long jitcnt;            /* jitter limit exceeded (ro) */
	long calcnt;            /* calibration intervals (ro) */
	long errcnt;            /* calibration errors (ro) */
	long stbcnt;            /* stability limit exceeded (ro) */
	int tai;		/* TAI offset (ro) */
	int  :32; int  :32; int  :32; int  :32;
	int  :32; int  :32; int  :32; int  :32;
	int  :32; int  :32; int  :32;
};
/*
 * Mode codes (timex.mode)
 */
/* NTP userland likes the MOD_ prefix better */
/*
 * Status codes (timex.status)
 */
/* read-only bits */
/*
 * Clock states (time_state)
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/compiler.h" 1
#line 171 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/timex.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 172 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/timex.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/param.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/param.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/asm-generic/param.h" 1
#line 3 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/param.h" 2
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/param.h" 2
#line 173 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/timex.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/timex.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/processor.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/timex.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/tsc.h" 1
/*
 * x86 TSC related functions
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/processor.h" 1
#line 9 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/tsc.h" 2
/*
 * Standard way to access the cycle counter.
 */
typedef unsigned long long cycles_t;
extern unsigned int cpu_khz;
extern unsigned int tsc_khz;
extern void disable_TSC(void);
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 cycles_t get_cycles(void)
{
	unsigned long long ret = 0;
#if !definedEx(CONFIG_X86_TSC)
	if (!(__builtin_constant_p((0*32+ 4)) && ( ((((0*32+ 4))>>5)==0 && (1UL<<(((0*32+ 4))&31) & (
#if !definedEx(CONFIG_MATH_EMULATION)
(1<<((0*32+ 0) & 31))
#endif
#if definedEx(CONFIG_MATH_EMULATION)
0
#endif
|
#if !definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_64) && definedEx(CONFIG_PARAVIRT)
0
#endif
#if definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT)
(1<<((0*32+ 3)) & 31)
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+ 5) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if !definedEx(CONFIG_X86_PAE) && definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_PAE)
(1<<((0*32+ 6) & 31))
#endif
#if !definedEx(CONFIG_X86_PAE) && !definedEx(CONFIG_X86_64)
0
#endif
| 
#if definedEx(CONFIG_X86_CMPXCHG64)
(1<<((0*32+ 8) & 31))
#endif
#if !definedEx(CONFIG_X86_CMPXCHG64)
0
#endif
|
#if !definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_64) && definedEx(CONFIG_PARAVIRT)
0
#endif
#if definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT)
(1<<((0*32+13)) & 31)
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+24) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if !definedEx(CONFIG_X86_64) && definedEx(CONFIG_X86_CMOV) || definedEx(CONFIG_X86_64)
(1<<((0*32+15) & 31))
#endif
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_X86_CMOV)
0
#endif
| 
#if definedEx(CONFIG_X86_64)
(1<<((0*32+25) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+26) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
))) || ((((0*32+ 4))>>5)==1 && (1UL<<(((0*32+ 4))&31) & (
#if definedEx(CONFIG_X86_64)
(1<<((1*32+29) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if definedEx(CONFIG_X86_USE_3DNOW)
(1<<((1*32+31) & 31))
#endif
#if !definedEx(CONFIG_X86_USE_3DNOW)
0
#endif
))) || ((((0*32+ 4))>>5)==2 && (1UL<<(((0*32+ 4))&31) & 0)) || ((((0*32+ 4))>>5)==3 && (1UL<<(((0*32+ 4))&31) & (
#if !definedEx(CONFIG_X86_64) && definedEx(CONFIG_X86_P6_NOP) || definedEx(CONFIG_X86_64)
(1<<((3*32+20) & 31))
#endif
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_X86_P6_NOP)
0
#endif
))) || ((((0*32+ 4))>>5)==4 && (1UL<<(((0*32+ 4))&31) & 0)) || ((((0*32+ 4))>>5)==5 && (1UL<<(((0*32+ 4))&31) & 0)) || ((((0*32+ 4))>>5)==6 && (1UL<<(((0*32+ 4))&31) & 0)) || ((((0*32+ 4))>>5)==7 && (1UL<<(((0*32+ 4))&31) & 0)) ) ? 1 : (__builtin_constant_p(((0*32+ 4))) ? constant_test_bit(((0*32+ 4)), ((unsigned long *)((&boot_cpu_data)->x86_capability))) : variable_test_bit(((0*32+ 4)), ((unsigned long *)((&boot_cpu_data)->x86_capability))))))
		return 0;
#endif
#if definedEx(CONFIG_PARAVIRT)
(ret = paravirt_read_tsc())
#endif
#if !definedEx(CONFIG_PARAVIRT)
((ret) = __native_read_tsc())
#endif
;
	return ret;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __attribute__((always_inline)) cycles_t vget_cycles(void)
{
	/*
	 * We only do VDSOs on TSC capable CPUs, so this shouldnt
	 * access boot_cpu_data (which is not VDSO-safe):
	 */
#if !definedEx(CONFIG_X86_TSC)
	if (!(__builtin_constant_p((0*32+ 4)) && ( ((((0*32+ 4))>>5)==0 && (1UL<<(((0*32+ 4))&31) & (
#if !definedEx(CONFIG_MATH_EMULATION)
(1<<((0*32+ 0) & 31))
#endif
#if definedEx(CONFIG_MATH_EMULATION)
0
#endif
|
#if !definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_64) && definedEx(CONFIG_PARAVIRT)
0
#endif
#if definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT)
(1<<((0*32+ 3)) & 31)
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+ 5) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if !definedEx(CONFIG_X86_PAE) && definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_PAE)
(1<<((0*32+ 6) & 31))
#endif
#if !definedEx(CONFIG_X86_PAE) && !definedEx(CONFIG_X86_64)
0
#endif
| 
#if definedEx(CONFIG_X86_CMPXCHG64)
(1<<((0*32+ 8) & 31))
#endif
#if !definedEx(CONFIG_X86_CMPXCHG64)
0
#endif
|
#if !definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_64) && definedEx(CONFIG_PARAVIRT)
0
#endif
#if definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT)
(1<<((0*32+13)) & 31)
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+24) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if !definedEx(CONFIG_X86_64) && definedEx(CONFIG_X86_CMOV) || definedEx(CONFIG_X86_64)
(1<<((0*32+15) & 31))
#endif
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_X86_CMOV)
0
#endif
| 
#if definedEx(CONFIG_X86_64)
(1<<((0*32+25) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+26) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
))) || ((((0*32+ 4))>>5)==1 && (1UL<<(((0*32+ 4))&31) & (
#if definedEx(CONFIG_X86_64)
(1<<((1*32+29) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if definedEx(CONFIG_X86_USE_3DNOW)
(1<<((1*32+31) & 31))
#endif
#if !definedEx(CONFIG_X86_USE_3DNOW)
0
#endif
))) || ((((0*32+ 4))>>5)==2 && (1UL<<(((0*32+ 4))&31) & 0)) || ((((0*32+ 4))>>5)==3 && (1UL<<(((0*32+ 4))&31) & (
#if !definedEx(CONFIG_X86_64) && definedEx(CONFIG_X86_P6_NOP) || definedEx(CONFIG_X86_64)
(1<<((3*32+20) & 31))
#endif
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_X86_P6_NOP)
0
#endif
))) || ((((0*32+ 4))>>5)==4 && (1UL<<(((0*32+ 4))&31) & 0)) || ((((0*32+ 4))>>5)==5 && (1UL<<(((0*32+ 4))&31) & 0)) || ((((0*32+ 4))>>5)==6 && (1UL<<(((0*32+ 4))&31) & 0)) || ((((0*32+ 4))>>5)==7 && (1UL<<(((0*32+ 4))&31) & 0)) ) ? 1 : (__builtin_constant_p(((0*32+ 4))) ? constant_test_bit(((0*32+ 4)), ((unsigned long *)((&boot_cpu_data)->x86_capability))) : variable_test_bit(((0*32+ 4)), ((unsigned long *)((&boot_cpu_data)->x86_capability))))))
		return 0;
#endif
	return (cycles_t)__native_read_tsc();
}
extern void tsc_init(void);
extern void mark_tsc_unstable(char *reason);
extern int unsynchronized_tsc(void);
extern int check_tsc_unstable(void);
extern unsigned long native_calibrate_tsc(void);
/*
 * Boot-time check whether the TSCs are synchronized across
 * all CPUs/cores:
 */
extern void check_tsc_sync_source(int cpu);
extern void check_tsc_sync_target(void);
extern int notsc_setup(char *);
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/timex.h" 2
/* Assume we use the PIT time source for the clock tick */
#line 175 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/timex.h" 2
/*
 * SHIFT_PLL is used as a dampening factor to define how much we
 * adjust the frequency correction for a given offset in PLL mode.
 * It also used in dampening the offset correction, to define how
 * much of the current value in time_offset we correct for each
 * second. Changing this value changes the stiffness of the ntp
 * adjustment code. A lower value makes it more flexible, reducing
 * NTP convergence time. A higher value makes it stiffer, increasing
 * convergence time, but making the clock more stable.
 *
 * In David Mills' nanokernel reference implementation SHIFT_PLL is 4.
 * However this seems to increase convergence time much too long.
 *
 * https://lists.ntp.org/pipermail/hackers/2008-January/003487.html
 *
 * In the above mailing list discussion, it seems the value of 4
 * was appropriate for other Unix systems with HZ=100, and that
 * SHIFT_PLL should be decreased as HZ increases. However, Linux's
 * clock steering implementation is HZ independent.
 *
 * Through experimentation, a SHIFT_PLL value of 2 was found to allow
 * for fast convergence (very similar to the NTPv3 code used prior to
 * v2.6.19), with good clock stability.
 *
 *
 * SHIFT_FLL is used as a dampening factor to define how much we
 * adjust the frequency correction for a given offset in FLL mode.
 * In David Mills' nanokernel reference implementation SHIFT_FLL is 2.
 *
 * MAXTC establishes the maximum time constant of the PLL.
 */
/*
 * SHIFT_USEC defines the scaling (shift) of the time_freq and
 * time_tolerance variables, which represent the current frequency
 * offset and maximum frequency tolerance.
 */
/*
 * kernel variables
 * Note: maximum error = NTP synch distance = dispersion + delay / 2;
 * estimated error = NTP dispersion.
 */
extern unsigned long tick_usec;		/* USER_HZ period (usec) */
extern unsigned long tick_nsec;		/* ACTHZ          period (nsec) */
extern int tickadj;			/* amount of adjustment per tick */
/*
 * phase-lock loop variables
 */
extern int time_status;		/* clock synchronization status bits */
extern long time_maxerror;	/* maximum error */
extern long time_esterror;	/* estimated error */
extern long time_adjust;	/* The amount of adjtime left */
extern void ntp_init(void);
extern void ntp_clear(void);
/**
 * ntp_synced - Returns 1 if the NTP status is not UNSYNC
 *
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int ntp_synced(void)
{
	return !(time_status & 0x0040);
}
/* Required to safely shift negative values */
/* Returns how long ticks are at present, in ns / 2^NTP_SCALE_SHIFT. */
extern u64 tick_length;
extern void second_overflow(void);
extern void update_ntp_one_tick(void);
extern int do_adjtimex(struct timex *);
/* Don't use! Compatibility define for existing users. */
int read_current_timer(unsigned long *timer_val);
/* The clock frequency of the i8253/i8254 PIT */
#line 10 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/jiffies.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/param.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/asm-generic/param.h" 1
#line 3 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/param.h" 2
#line 11 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/jiffies.h" 2
/*
 * The following defines establish the engineering parameters of the PLL
 * model. The HZ variable establishes the timer interrupt frequency, 100 Hz
 * for the SunOS kernel, 256 Hz for the Ultrix kernel and 1024 Hz for the
 * OSF/1 kernel. The SHIFT_HZ define expresses the same value as the
 * nearest power of two in order to avoid hardware multiply operations.
 */
/* LATCH is used in the interval timer and ftape setup. */
/* Suppose we want to devide two numbers NOM and DEN: NOM/DEN, then we can
 * improve accuracy by shifting LSH bits, hence calculating:
 *     (NOM << LSH) / DEN
 * This however means trouble for large NOM, because (NOM << LSH) may no
 * longer fit in 32 bits. The following way of calculating this gives us
 * some slack, under the following conditions:
 *   - (NOM / DEN) fits in (32 - LSH) bits.
 *   - (NOM % DEN) fits in (32 - LSH) bits.
 */
/* HZ is the requested value. ACTHZ is actual HZ ("<< 8" is for accuracy) */
/* TICK_NSEC is the time between ticks in nsec assuming real ACTHZ */
/* TICK_USEC is the time between ticks in usec assuming fake USER_HZ */
/* TICK_USEC_TO_NSEC is the time between ticks in nsec assuming real ACTHZ and	*/
/* a value TUSEC for TICK_USEC (can be set bij adjtimex)		*/
/* some arch's have a small-data section that can be accessed register-relative
 * but that can only take up to, say, 4-byte variables. jiffies being part of
 * an 8-byte variable may not be correctly accessed unless we force the issue
 */
/*
 * The 64-bit value is not atomic - you MUST NOT read it
 * without sampling the sequence number in xtime_lock.
 * get_jiffies_64() will do this for you as appropriate.
 */
extern u64 __attribute__((section(".data"))) jiffies_64;
extern unsigned long volatile __attribute__((section(".data"))) jiffies;
 static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 u64 get_jiffies_64(void)
{
	return (u64)jiffies;
}
/*
 *	These inlines deal with timer wrapping correctly. You are 
 *	strongly encouraged to use them
 *	1. Because people otherwise forget
 *	2. Because if the timer wrap changes in future you won't have to
 *	   alter your driver code.
 *
 * time_after(a,b) returns true if the time a is after time b.
 *
 * Do this with "<0" and ">=0" to only test the sign of the result. A
 * good compiler would generate better code (and a really good compiler
 * wouldn't care). Gcc is currently neither.
 */
/*
 * Calculate whether a is in the range of [b, c].
 */
/*
 * Calculate whether a is in the range of [b, c).
 */
/* Same as above, but does so with platform independent 64bit types.
 * These must be used when utilizing jiffies_64 (i.e. return value of
 * get_jiffies_64() */
/*
 * These four macros compare jiffies and 'a' for convenience.
 */
/* time_is_before_jiffies(a) return true if a is before jiffies */
/* time_is_after_jiffies(a) return true if a is after jiffies */
/* time_is_before_eq_jiffies(a) return true if a is before or equal to jiffies*/
/* time_is_after_eq_jiffies(a) return true if a is after or equal to jiffies*/
/*
 * Have the 32 bit jiffies value wrap 5 minutes after boot
 * so jiffies wrap bugs show up earlier.
 */
/*
 * Change timeval to jiffies, trying to avoid the
 * most obvious overflows..
 *
 * And some not so obvious.
 *
 * Note that we don't want to return LONG_MAX, because
 * for various timeout reasons we often end up having
 * to wait "jiffies+1" in order to guarantee that we wait
 * at _least_ "jiffies" - so "jiffies+1" had better still
 * be positive.
 */
extern unsigned long preset_lpj;
/*
 * We want to do realistic conversions of time so we need to use the same
 * values the update wall clock code uses as the jiffies size.  This value
 * is: TICK_NSEC (which is defined in timex.h).  This
 * is a constant and is in nanoseconds.  We will use scaled math
 * with a set of scales defined here as SEC_JIFFIE_SC,  USEC_JIFFIE_SC and
 * NSEC_JIFFIE_SC.  Note that these defines contain nothing but
 * constants and so are computed at compile time.  SHIFT_HZ (computed in
 * timex.h) adjusts the scaling for different HZ values.
 * Scaled math???  What is that?
 *
 * Scaled math is a way to do integer math on values that would,
 * otherwise, either overflow, underflow, or cause undesired div
 * instructions to appear in the execution path.  In short, we "scale"
 * up the operands so they take more bits (more precision, less
 * underflow), do the desired operation and then "scale" the result back
 * by the same amount.  If we do the scaling by shifting we avoid the
 * costly mpy and the dastardly div instructions.
 * Suppose, for example, we want to convert from seconds to jiffies
 * where jiffies is defined in nanoseconds as NSEC_PER_JIFFIE.  The
 * simple math is: jiff = (sec * NSEC_PER_SEC) / NSEC_PER_JIFFIE; We
 * observe that (NSEC_PER_SEC / NSEC_PER_JIFFIE) is a constant which we
 * might calculate at compile time, however, the result will only have
 * about 3-4 bits of precision (less for smaller values of HZ).
 *
 * So, we scale as follows:
 * jiff = (sec) * (NSEC_PER_SEC / NSEC_PER_JIFFIE);
 * jiff = ((sec) * ((NSEC_PER_SEC * SCALE)/ NSEC_PER_JIFFIE)) / SCALE;
 * Then we make SCALE a power of two so:
 * jiff = ((sec) * ((NSEC_PER_SEC << SCALE)/ NSEC_PER_JIFFIE)) >> SCALE;
 * Now we define:
 * #define SEC_CONV = ((NSEC_PER_SEC << SCALE)/ NSEC_PER_JIFFIE))
 * jiff = (sec * SEC_CONV) >> SCALE;
 *
 * Often the math we use will expand beyond 32-bits so we tell C how to
 * do this and pass the 64-bit result of the mpy through the ">> SCALE"
 * which should take the result back to 32-bits.  We want this expansion
 * to capture as much precision as possible.  At the same time we don't
 * want to overflow so we pick the SCALE to avoid this.  In this file,
 * that means using a different scale for each range of HZ values (as
 * defined in timex.h).
 *
 * For those who want to know, gcc will give a 64-bit result from a "*"
 * operator if the result is a long long AND at least one of the
 * operands is cast to long long (usually just prior to the "*" so as
 * not to confuse it into thinking it really has a 64-bit operand,
 * which, buy the way, it can do, but it takes more code and at least 2
 * mpys).
 * We also need to be aware that one second in nanoseconds is only a
 * couple of bits away from overflowing a 32-bit word, so we MUST use
 * 64-bits to get the full range time in nanoseconds.
 */
/*
 * Here are the scales we will use.  One for seconds, nanoseconds and
 * microseconds.
 *
 * Within the limits of cpp we do a rough cut at the SEC_JIFFIE_SC and
 * check if the sign bit is set.  If not, we bump the shift count by 1.
 * (Gets an extra bit of precision where we can use it.)
 * We know it is set for HZ = 1024 and HZ = 100 not for 1000.
 * Haven't tested others.
 * Limits of cpp (for #if expressions) only long (no long long), but
 * then we only need the most signicant bit.
 */
/*
 * USEC_ROUND is used in the timeval to jiffie conversion.  See there
 * for more details.  It is the scaled resolution rounding value.  Note
 * that it is a 64-bit value.  Since, when it is applied, we are already
 * in jiffies (albit scaled), it is nothing but the bits we will shift
 * off.
 */
/*
 * The maximum jiffie value is (MAX_INT >> 1).  Here we translate that
 * into seconds.  The 64-bit case will overflow if we are not careful,
 * so use the messy SH_DIV macro to do it.  Still all constants.
 */
/*
 * Convert various time units to each other:
 */
extern unsigned int jiffies_to_msecs(const unsigned long j);
extern unsigned int jiffies_to_usecs(const unsigned long j);
extern unsigned long msecs_to_jiffies(const unsigned int m);
extern unsigned long usecs_to_jiffies(const unsigned int u);
extern unsigned long timespec_to_jiffies(const struct timespec *value);
extern void jiffies_to_timespec(const unsigned long jiffies,
				struct timespec *value);
extern unsigned long timeval_to_jiffies(const struct timeval *value);
extern void jiffies_to_timeval(const unsigned long jiffies,
			       struct timeval *value);
extern clock_t jiffies_to_clock_t(long x);
extern unsigned long clock_t_to_jiffies(unsigned long x);
extern u64 jiffies_64_to_clock_t(u64 x);
extern u64 nsec_to_clock_t(u64 x);
extern unsigned long nsecs_to_jiffies(u64 n);
#line 27 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/ktime.h" 2
/*
 * ktime_t:
 *
 * On 64-bit CPUs a single 64-bit variable is used to store the hrtimers
 * internal representation of time values in scalar nanoseconds. The
 * design plays out best on 64-bit CPUs, where most conversions are
 * NOPs and most arithmetic ktime_t operations are plain arithmetic
 * operations.
 *
 * On 32-bit CPUs an optimized representation of the timespec structure
 * is used to avoid expensive conversions from and to timespecs. The
 * endian-aware order of the tv struct members is choosen to allow
 * mathematical operations on the tv64 member of the union too, which
 * for certain operations produces better code.
 *
 * For architectures with efficient support for 64/32-bit conversions the
 * plain scalar nanosecond based representation can be selected by the
 * config switch CONFIG_KTIME_SCALAR.
 */
union ktime {
	s64	tv64;
};
typedef union ktime ktime_t;		/* Kill this */
/*
 * ktime_t definitions when using the 64-bit scalar representation:
 */
/**
 * ktime_set - Set a ktime_t variable from a seconds/nanoseconds value
 * @secs:	seconds to set
 * @nsecs:	nanoseconds to set
 *
 * Return the ktime_t representation of the value
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 ktime_t ktime_set(const long secs, const unsigned long nsecs)
{
	if (__builtin_expect(!!(secs >= (((s64)~((u64)1 << 63)) / 1000000000L)), 0))
		return (ktime_t){ .tv64 = ((s64)~((u64)1 << 63)) };
	return (ktime_t) { .tv64 = (s64)secs * 1000000000L + (s64)nsecs };
}
/* Subtract two ktime_t variables. rem = lhs -rhs: */
/* Add two ktime_t variables. res = lhs + rhs: */
/*
 * Add a ktime_t variable and a scalar nanosecond value.
 * res = kt + nsval:
 */
/*
 * Subtract a scalar nanosecod from a ktime_t variable
 * res = kt - nsval:
 */
/* convert a timespec to ktime_t format: */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 ktime_t timespec_to_ktime(struct timespec ts)
{
	return ktime_set(ts.tv_sec, ts.tv_nsec);
}
/* convert a timeval to ktime_t format: */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 ktime_t timeval_to_ktime(struct timeval tv)
{
	return ktime_set(tv.tv_sec, tv.tv_usec * 1000L);
}
/* Map the ktime_t to timespec conversion to ns_to_timespec function */
/* Map the ktime_t to timeval conversion to ns_to_timeval function */
/* Convert ktime_t to nanoseconds - NOP in the scalar storage format: */
/**
 * ktime_equal - Compares two ktime_t variables to see if they are equal
 * @cmp1:	comparable1
 * @cmp2:	comparable2
 *
 * Compare two ktime_t variables, returns 1 if equal
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int ktime_equal(const ktime_t cmp1, const ktime_t cmp2)
{
	return cmp1.tv64 == cmp2.tv64;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 s64 ktime_to_us(const ktime_t kt)
{
	struct timeval tv = ns_to_timeval((kt).tv64);
	return (s64) tv.tv_sec * 1000000L + tv.tv_usec;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 s64 ktime_us_delta(const ktime_t later, const ktime_t earlier)
{
       return ktime_to_us(({ (ktime_t){ .tv64 = (later).tv64 - (earlier).tv64 }; }));
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 ktime_t ktime_add_us(const ktime_t kt, const u64 usec)
{
	return ({ (ktime_t){ .tv64 = (kt).tv64 + (usec * 1000) }; });
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 ktime_t ktime_sub_us(const ktime_t kt, const u64 usec)
{
	return ({ (ktime_t){ .tv64 = (kt).tv64 - (usec * 1000) }; });
}
extern ktime_t ktime_add_safe(const ktime_t lhs, const ktime_t rhs);
/*
 * The resolution of the clocks. The resolution value is returned in
 * the clock_getres() system call to give application programmers an
 * idea of the (in)accuracy of timers. Timer values are rounded up to
 * this resolution values.
 */
/* Get the monotonic time in timespec format: */
extern void ktime_get_ts(struct timespec *ts);
/* Get the real (wall-) time in timespec format: */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 ktime_t ns_to_ktime(u64 ns)
{
	static const ktime_t ktime_zero = { .tv64 = 0 };
	return ({ (ktime_t){ .tv64 = (ktime_zero).tv64 + (ns) }; });
}
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/timer.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/stddef.h" 1
#line 8 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/timer.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/debugobjects.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/list.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/debugobjects.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/spinlock.h" 1
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/debugobjects.h" 2
enum debug_obj_state {
	ODEBUG_STATE_NONE,
	ODEBUG_STATE_INIT,
	ODEBUG_STATE_INACTIVE,
	ODEBUG_STATE_ACTIVE,
	ODEBUG_STATE_DESTROYED,
	ODEBUG_STATE_NOTAVAILABLE,
	ODEBUG_STATE_MAX,
};
struct debug_obj_descr;
/**
 * struct debug_obj - representaion of an tracked object
 * @node:	hlist node to link the object into the tracker list
 * @state:	tracked object state
 * @object:	pointer to the real object
 * @descr:	pointer to an object type specific debug description structure
 */
struct debug_obj {
	struct hlist_node	node;
	enum debug_obj_state	state;
	void			*object;
	struct debug_obj_descr	*descr;
};
/**
 * struct debug_obj_descr - object type specific debug description structure
 * @name:		name of the object typee
 * @fixup_init:		fixup function, which is called when the init check
 *			fails
 * @fixup_activate:	fixup function, which is called when the activate check
 *			fails
 * @fixup_destroy:	fixup function, which is called when the destroy check
 *			fails
 * @fixup_free:		fixup function, which is called when the free check
 *			fails
 */
struct debug_obj_descr {
	const char		*name;
	int (*fixup_init)	(void *addr, enum debug_obj_state state);
	int (*fixup_activate)	(void *addr, enum debug_obj_state state);
	int (*fixup_destroy)	(void *addr, enum debug_obj_state state);
	int (*fixup_free)	(void *addr, enum debug_obj_state state);
};
#if definedEx(CONFIG_DEBUG_OBJECTS)
extern void debug_object_init      (void *addr, struct debug_obj_descr *descr);
extern void
debug_object_init_on_stack(void *addr, struct debug_obj_descr *descr);
extern void debug_object_activate  (void *addr, struct debug_obj_descr *descr);
extern void debug_object_deactivate(void *addr, struct debug_obj_descr *descr);
extern void debug_object_destroy   (void *addr, struct debug_obj_descr *descr);
extern void debug_object_free      (void *addr, struct debug_obj_descr *descr);
extern void debug_objects_early_init(void);
extern void debug_objects_mem_init(void);
#endif
#if !definedEx(CONFIG_DEBUG_OBJECTS)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void
debug_object_init      (void *addr, struct debug_obj_descr *descr) { }
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void
debug_object_init_on_stack(void *addr, struct debug_obj_descr *descr) { }
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void
debug_object_activate  (void *addr, struct debug_obj_descr *descr) { }
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void
debug_object_deactivate(void *addr, struct debug_obj_descr *descr) { }
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void
debug_object_destroy   (void *addr, struct debug_obj_descr *descr) { }
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void
debug_object_free      (void *addr, struct debug_obj_descr *descr) { }
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void debug_objects_early_init(void) { }
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void debug_objects_mem_init(void) { }
#endif
#if definedEx(CONFIG_DEBUG_OBJECTS_FREE)
extern void debug_check_no_obj_freed(const void *address, unsigned long size);
#endif
#if !definedEx(CONFIG_DEBUG_OBJECTS_FREE)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void
debug_check_no_obj_freed(const void *address, unsigned long size) { }
#endif
#line 9 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/timer.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/stringify.h" 1
#line 10 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/timer.h" 2
struct tvec_base;
struct timer_list {
	struct list_head entry;
	unsigned long expires;
	void (*function)(unsigned long);
	unsigned long data;
	struct tvec_base *base;
#if definedEx(CONFIG_TIMER_STATS)
	void *start_site;
	char start_comm[16];
	int start_pid;
#endif
#if definedEx(CONFIG_LOCKDEP)
	struct lockdep_map lockdep_map;
#endif
};
extern struct tvec_base boot_tvec_bases;
#if definedEx(CONFIG_LOCKDEP)
/*
 * NB: because we have to copy the lockdep_map, setting the lockdep_map key
 * (second argument) here is required, otherwise it could be initialised to
 * the copy of the lockdep_map later! We use the pointer to and the string
 * "<file>:<line>" as the key resp. the name of the lockdep_map.
 */
#endif
#if !definedEx(CONFIG_LOCKDEP)
#endif
void init_timer_key(struct timer_list *timer,
		    const char *name,
		    struct lock_class_key *key);
void init_timer_deferrable_key(struct timer_list *timer,
			       const char *name,
			       struct lock_class_key *key);
#if definedEx(CONFIG_LOCKDEP)
#endif
#if !definedEx(CONFIG_LOCKDEP)
#endif
#if definedEx(CONFIG_DEBUG_OBJECTS_TIMERS)
extern void init_timer_on_stack_key(struct timer_list *timer,
				    const char *name,
				    struct lock_class_key *key);
extern void destroy_timer_on_stack(struct timer_list *timer);
#endif
#if !definedEx(CONFIG_DEBUG_OBJECTS_TIMERS)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void destroy_timer_on_stack(struct timer_list *timer) { }
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void init_timer_on_stack_key(struct timer_list *timer,
					   const char *name,
					   struct lock_class_key *key)
{
	init_timer_key(timer, name, key);
}
#endif
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void setup_timer_key(struct timer_list * timer,
				const char *name,
				struct lock_class_key *key,
				void (*function)(unsigned long),
				unsigned long data)
{
	timer->function = function;
	timer->data = data;
	init_timer_key(timer, name, key);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void setup_timer_on_stack_key(struct timer_list *timer,
					const char *name,
					struct lock_class_key *key,
					void (*function)(unsigned long),
					unsigned long data)
{
	timer->function = function;
	timer->data = data;
	init_timer_on_stack_key(timer, name, key);
}
/**
 * timer_pending - is a timer pending?
 * @timer: the timer in question
 *
 * timer_pending will tell whether a given timer is currently pending,
 * or not. Callers must ensure serialization wrt. other operations done
 * to this timer, eg. interrupt contexts, or other CPUs on SMP.
 *
 * return value: 1 if the timer is pending, 0 if not.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int timer_pending(const struct timer_list * timer)
{
	return timer->entry.next != ((void *)0);
}
extern void add_timer_on(struct timer_list *timer, int cpu);
extern int del_timer(struct timer_list * timer);
extern int mod_timer(struct timer_list *timer, unsigned long expires);
extern int mod_timer_pending(struct timer_list *timer, unsigned long expires);
extern int mod_timer_pinned(struct timer_list *timer, unsigned long expires);
/*
 * The jiffies value which is added to now, when there is no timer
 * in the timer wheel:
 */
/*
 * Return when the next timer-wheel timeout occurs (in absolute jiffies),
 * locks the timer base and does the comparison against the given
 * jiffie.
 */
extern unsigned long get_next_timer_interrupt(unsigned long now);
/*
 * Timer-statistics info:
 */
#if definedEx(CONFIG_TIMER_STATS)
extern int timer_stats_active;
extern void init_timer_stats(void);
extern void timer_stats_update_stats(void *timer, pid_t pid, void *startf,
				     void *timerf, char *comm,
				     unsigned int timer_flag);
extern void __timer_stats_timer_set_start_info(struct timer_list *timer,
					       void *addr);
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void timer_stats_timer_set_start_info(struct timer_list *timer)
{
	if (__builtin_expect(!!(!timer_stats_active), 1))
		return;
	__timer_stats_timer_set_start_info(timer, __builtin_return_address(0));
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void timer_stats_timer_clear_start_info(struct timer_list *timer)
{
	timer->start_site = ((void *)0);
}
#endif
#if !definedEx(CONFIG_TIMER_STATS)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void init_timer_stats(void)
{
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void timer_stats_timer_set_start_info(struct timer_list *timer)
{
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void timer_stats_timer_clear_start_info(struct timer_list *timer)
{
}
#endif
extern void add_timer(struct timer_list *timer);
  extern int try_to_del_timer_sync(struct timer_list *timer);
  extern int del_timer_sync(struct timer_list *timer);
extern void init_timers(void);
extern void run_local_timers(void);
struct hrtimer;
extern enum hrtimer_restart it_real_fn(struct hrtimer *);
unsigned long __round_jiffies(unsigned long j, int cpu);
unsigned long __round_jiffies_relative(unsigned long j, int cpu);
unsigned long round_jiffies(unsigned long j);
unsigned long round_jiffies_relative(unsigned long j);
unsigned long __round_jiffies_up(unsigned long j, int cpu);
unsigned long __round_jiffies_up_relative(unsigned long j, int cpu);
unsigned long round_jiffies_up(unsigned long j);
unsigned long round_jiffies_up_relative(unsigned long j);
#line 10 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/workqueue.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/linkage.h" 1
#line 11 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/workqueue.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/bitops.h" 1
#line 12 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/workqueue.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/lockdep.h" 1
/*
 * Runtime locking correctness validator
 *
 *  Copyright (C) 2006,2007 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>
 *  Copyright (C) 2007 Red Hat, Inc., Peter Zijlstra <pzijlstr@redhat.com>
 *
 * see Documentation/lockdep-design.txt for more details.
 */
#line 13 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/workqueue.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic.h" 1
 #line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic_64.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic.h" 2
#line 14 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/workqueue.h" 2
struct workqueue_struct;
struct work_struct;
typedef void (*work_func_t)(struct work_struct *work);
/*
 * The first word is the work queue pointer and the flags rolled into
 * one
 */
struct work_struct {
	atomic_long_t data;
	struct list_head entry;
	work_func_t func;
#if definedEx(CONFIG_LOCKDEP)
	struct lockdep_map lockdep_map;
#endif
};
struct delayed_work {
	struct work_struct work;
	struct timer_list timer;
};
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 struct delayed_work *to_delayed_work(struct work_struct *work)
{
	return ({ const typeof( ((struct delayed_work *)0)->work ) *__mptr = (work); (struct delayed_work *)( (char *)__mptr - __builtin_offsetof(struct delayed_work,work) );});
}
struct execute_work {
	struct work_struct work;
};
#if definedEx(CONFIG_LOCKDEP)
/*
 * NB: because we have to copy the lockdep_map, setting _key
 * here is required, otherwise it could get initialised to the
 * copy of the lockdep_map!
 */
#endif
#if !definedEx(CONFIG_LOCKDEP)
#endif
/*
 * initialize a work item's function pointer
 */
#if definedEx(CONFIG_DEBUG_OBJECTS_WORK)
extern void __init_work(struct work_struct *work, int onstack);
extern void destroy_work_on_stack(struct work_struct *work);
#endif
#if !definedEx(CONFIG_DEBUG_OBJECTS_WORK)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __init_work(struct work_struct *work, int onstack) { }
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void destroy_work_on_stack(struct work_struct *work) { }
#endif
/*
 * initialize all of a work item in one go
 *
 * NOTE! No point in using "atomic_long_set()": using a direct
 * assignment of the work data initializer allows the compiler
 * to generate better code.
 */
#if definedEx(CONFIG_LOCKDEP)
#endif
#if !definedEx(CONFIG_LOCKDEP)
#endif
/**
 * work_pending - Find out whether a work item is currently pending
 * @work: The work item in question
 */
/**
 * delayed_work_pending - Find out whether a delayable work item is currently
 * pending
 * @work: The work item in question
 */
/**
 * work_clear_pending - for internal use only, mark a work item as not pending
 * @work: The work item in question
 */
extern struct workqueue_struct *
__create_workqueue_key(const char *name, int singlethread,
		       int freezeable, int rt, struct lock_class_key *key,
		       const char *lock_name);
#if definedEx(CONFIG_LOCKDEP)
#endif
#if !definedEx(CONFIG_LOCKDEP)
#endif
extern void destroy_workqueue(struct workqueue_struct *wq);
extern int queue_work(struct workqueue_struct *wq, struct work_struct *work);
extern int queue_work_on(int cpu, struct workqueue_struct *wq,
			struct work_struct *work);
extern int queue_delayed_work(struct workqueue_struct *wq,
			struct delayed_work *work, unsigned long delay);
extern int queue_delayed_work_on(int cpu, struct workqueue_struct *wq,
			struct delayed_work *work, unsigned long delay);
extern void flush_workqueue(struct workqueue_struct *wq);
extern void flush_scheduled_work(void);
extern void flush_delayed_work(struct delayed_work *work);
extern int schedule_work(struct work_struct *work);
extern int schedule_work_on(int cpu, struct work_struct *work);
extern int schedule_delayed_work(struct delayed_work *work, unsigned long delay);
extern int schedule_delayed_work_on(int cpu, struct delayed_work *work,
					unsigned long delay);
extern int schedule_on_each_cpu(work_func_t func);
extern int current_is_keventd(void);
extern int keventd_up(void);
extern void init_workqueues(void);
int execute_in_process_context(work_func_t fn, struct execute_work *);
extern int flush_work(struct work_struct *work);
extern int cancel_work_sync(struct work_struct *work);
/*
 * Kill off a pending schedule_delayed_work().  Note that the work callback
 * function may still be running on return from cancel_delayed_work(), unless
 * it returns 1 and the work doesn't re-arm itself. Run flush_workqueue() or
 * cancel_work_sync() to wait on it.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int cancel_delayed_work(struct delayed_work *work)
{
	int ret;
	ret = del_timer_sync(&work->timer);
	if (ret)
		clear_bit(0, ((unsigned long *)(&(&work->work)->data)));
	return ret;
}
/*
 * Like above, but uses del_timer() instead of del_timer_sync(). This means,
 * if it returns 0 the timer function may be running and the queueing is in
 * progress.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int __cancel_delayed_work(struct delayed_work *work)
{
	int ret;
	ret = del_timer(&work->timer);
	if (ret)
		clear_bit(0, ((unsigned long *)(&(&work->work)->data)));
	return ret;
}
extern int cancel_delayed_work_sync(struct delayed_work *work);
/* Obsolete. use cancel_delayed_work_sync() */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
void cancel_rearming_delayed_workqueue(struct workqueue_struct *wq,
					struct delayed_work *work)
{
	cancel_delayed_work_sync(work);
}
/* Obsolete. use cancel_delayed_work_sync() */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
void cancel_rearming_delayed_work(struct delayed_work *work)
{
	cancel_delayed_work_sync(work);
}
 long work_on_cpu(unsigned int cpu, long (*fn)(void *), void *arg);
#line 27 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/pm.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/spinlock.h" 1
#line 28 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/pm.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/wait.h" 1
#line 29 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/pm.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/timer.h" 1
#line 30 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/pm.h" 2
/*
 * Callbacks for platform drivers to implement.
 */
extern void (*pm_idle)(void);
extern void (*pm_power_off)(void);
extern void (*pm_power_off_prepare)(void);
/*
 * Device power management
 */
struct device;
typedef struct pm_message {
	int event;
} pm_message_t;
/**
 * struct dev_pm_ops - device PM callbacks
 *
 * Several driver power state transitions are externally visible, affecting
 * the state of pending I/O queues and (for drivers that touch hardware)
 * interrupts, wakeups, DMA, and other hardware state.  There may also be
 * internal transitions to various low power modes, which are transparent
 * to the rest of the driver stack (such as a driver that's ON gating off
 * clocks which are not in active use).
 *
 * The externally visible transitions are handled with the help of the following
 * callbacks included in this structure:
 *
 * @prepare: Prepare the device for the upcoming transition, but do NOT change
 *	its hardware state.  Prevent new children of the device from being
 *	registered after @prepare() returns (the driver's subsystem and
 *	generally the rest of the kernel is supposed to prevent new calls to the
 *	probe method from being made too once @prepare() has succeeded).  If
 *	@prepare() detects a situation it cannot handle (e.g. registration of a
 *	child already in progress), it may return -EAGAIN, so that the PM core
 *	can execute it once again (e.g. after the new child has been registered)
 *	to recover from the race condition.  This method is executed for all
 *	kinds of suspend transitions and is followed by one of the suspend
 *	callbacks: @suspend(), @freeze(), or @poweroff().
 *	The PM core executes @prepare() for all devices before starting to
 *	execute suspend callbacks for any of them, so drivers may assume all of
 *	the other devices to be present and functional while @prepare() is being
 *	executed.  In particular, it is safe to make GFP_KERNEL memory
 *	allocations from within @prepare().  However, drivers may NOT assume
 *	anything about the availability of the user space at that time and it
 *	is not correct to request firmware from within @prepare() (it's too
 *	late to do that).  [To work around this limitation, drivers may
 *	register suspend and hibernation notifiers that are executed before the
 *	freezing of tasks.]
 *
 * @complete: Undo the changes made by @prepare().  This method is executed for
 *	all kinds of resume transitions, following one of the resume callbacks:
 *	@resume(), @thaw(), @restore().  Also called if the state transition
 *	fails before the driver's suspend callback (@suspend(), @freeze(),
 *	@poweroff()) can be executed (e.g. if the suspend callback fails for one
 *	of the other devices that the PM core has unsuccessfully attempted to
 *	suspend earlier).
 *	The PM core executes @complete() after it has executed the appropriate
 *	resume callback for all devices.
 *
 * @suspend: Executed before putting the system into a sleep state in which the
 *	contents of main memory are preserved.  Quiesce the device, put it into
 *	a low power state appropriate for the upcoming system state (such as
 *	PCI_D3hot), and enable wakeup events as appropriate.
 *
 * @resume: Executed after waking the system up from a sleep state in which the
 *	contents of main memory were preserved.  Put the device into the
 *	appropriate state, according to the information saved in memory by the
 *	preceding @suspend().  The driver starts working again, responding to
 *	hardware events and software requests.  The hardware may have gone
 *	through a power-off reset, or it may have maintained state from the
 *	previous suspend() which the driver may rely on while resuming.  On most
 *	platforms, there are no restrictions on availability of resources like
 *	clocks during @resume().
 *
 * @freeze: Hibernation-specific, executed before creating a hibernation image.
 *	Quiesce operations so that a consistent image can be created, but do NOT
 *	otherwise put the device into a low power device state and do NOT emit
 *	system wakeup events.  Save in main memory the device settings to be
 *	used by @restore() during the subsequent resume from hibernation or by
 *	the subsequent @thaw(), if the creation of the image or the restoration
 *	of main memory contents from it fails.
 *
 * @thaw: Hibernation-specific, executed after creating a hibernation image OR
 *	if the creation of the image fails.  Also executed after a failing
 *	attempt to restore the contents of main memory from such an image.
 *	Undo the changes made by the preceding @freeze(), so the device can be
 *	operated in the same way as immediately before the call to @freeze().
 *
 * @poweroff: Hibernation-specific, executed after saving a hibernation image.
 *	Quiesce the device, put it into a low power state appropriate for the
 *	upcoming system state (such as PCI_D3hot), and enable wakeup events as
 *	appropriate.
 *
 * @restore: Hibernation-specific, executed after restoring the contents of main
 *	memory from a hibernation image.  Driver starts working again,
 *	responding to hardware events and software requests.  Drivers may NOT
 *	make ANY assumptions about the hardware state right prior to @restore().
 *	On most platforms, there are no restrictions on availability of
 *	resources like clocks during @restore().
 *
 * @suspend_noirq: Complete the operations of ->suspend() by carrying out any
 *	actions required for suspending the device that need interrupts to be
 *	disabled
 *
 * @resume_noirq: Prepare for the execution of ->resume() by carrying out any
 *	actions required for resuming the device that need interrupts to be
 *	disabled
 *
 * @freeze_noirq: Complete the operations of ->freeze() by carrying out any
 *	actions required for freezing the device that need interrupts to be
 *	disabled
 *
 * @thaw_noirq: Prepare for the execution of ->thaw() by carrying out any
 *	actions required for thawing the device that need interrupts to be
 *	disabled
 *
 * @poweroff_noirq: Complete the operations of ->poweroff() by carrying out any
 *	actions required for handling the device that need interrupts to be
 *	disabled
 *
 * @restore_noirq: Prepare for the execution of ->restore() by carrying out any
 *	actions required for restoring the operations of the device that need
 *	interrupts to be disabled
 *
 * All of the above callbacks, except for @complete(), return error codes.
 * However, the error codes returned by the resume operations, @resume(),
 * @thaw(), @restore(), @resume_noirq(), @thaw_noirq(), and @restore_noirq() do
 * not cause the PM core to abort the resume transition during which they are
 * returned.  The error codes returned in that cases are only printed by the PM
 * core to the system logs for debugging purposes.  Still, it is recommended
 * that drivers only return error codes from their resume methods in case of an
 * unrecoverable failure (i.e. when the device being handled refuses to resume
 * and becomes unusable) to allow us to modify the PM core in the future, so
 * that it can avoid attempting to handle devices that failed to resume and
 * their children.
 *
 * It is allowed to unregister devices while the above callbacks are being
 * executed.  However, it is not allowed to unregister a device from within any
 * of its own callbacks.
 *
 * There also are the following callbacks related to run-time power management
 * of devices:
 *
 * @runtime_suspend: Prepare the device for a condition in which it won't be
 *	able to communicate with the CPU(s) and RAM due to power management.
 *	This need not mean that the device should be put into a low power state.
 *	For example, if the device is behind a link which is about to be turned
 *	off, the device may remain at full power.  If the device does go to low
 *	power and is capable of generating run-time wake-up events, remote
 *	wake-up (i.e., a hardware mechanism allowing the device to request a
 *	change of its power state via a wake-up event, such as PCI PME) should
 *	be enabled for it.
 *
 * @runtime_resume: Put the device into the fully active state in response to a
 *	wake-up event generated by hardware or at the request of software.  If
 *	necessary, put the device into the full power state and restore its
 *	registers, so that it is fully operational.
 *
 * @runtime_idle: Device appears to be inactive and it might be put into a low
 *	power state if all of the necessary conditions are satisfied.  Check
 *	these conditions and handle the device as appropriate, possibly queueing
 *	a suspend request for it.  The return value is ignored by the PM core.
 */
struct dev_pm_ops {
	int (*prepare)(struct device *dev);
	void (*complete)(struct device *dev);
	int (*suspend)(struct device *dev);
	int (*resume)(struct device *dev);
	int (*freeze)(struct device *dev);
	int (*thaw)(struct device *dev);
	int (*poweroff)(struct device *dev);
	int (*restore)(struct device *dev);
	int (*suspend_noirq)(struct device *dev);
	int (*resume_noirq)(struct device *dev);
	int (*freeze_noirq)(struct device *dev);
	int (*thaw_noirq)(struct device *dev);
	int (*poweroff_noirq)(struct device *dev);
	int (*restore_noirq)(struct device *dev);
	int (*runtime_suspend)(struct device *dev);
	int (*runtime_resume)(struct device *dev);
	int (*runtime_idle)(struct device *dev);
};
/*
 * Use this if you want to use the same suspend and resume callbacks for suspend
 * to RAM and hibernation.
 */
/**
 * PM_EVENT_ messages
 *
 * The following PM_EVENT_ messages are defined for the internal use of the PM
 * core, in order to provide a mechanism allowing the high level suspend and
 * hibernation code to convey the necessary information to the device PM core
 * code:
 *
 * ON		No transition.
 *
 * FREEZE 	System is going to hibernate, call ->prepare() and ->freeze()
 *		for all devices.
 *
 * SUSPEND	System is going to suspend, call ->prepare() and ->suspend()
 *		for all devices.
 *
 * HIBERNATE	Hibernation image has been saved, call ->prepare() and
 *		->poweroff() for all devices.
 *
 * QUIESCE	Contents of main memory are going to be restored from a (loaded)
 *		hibernation image, call ->prepare() and ->freeze() for all
 *		devices.
 *
 * RESUME	System is resuming, call ->resume() and ->complete() for all
 *		devices.
 *
 * THAW		Hibernation image has been created, call ->thaw() and
 *		->complete() for all devices.
 *
 * RESTORE	Contents of main memory have been restored from a hibernation
 *		image, call ->restore() and ->complete() for all devices.
 *
 * RECOVER	Creation of a hibernation image or restoration of the main
 *		memory contents from a hibernation image has failed, call
 *		->thaw() and ->complete() for all devices.
 *
 * The following PM_EVENT_ messages are defined for internal use by
 * kernel subsystems.  They are never issued by the PM core.
 *
 * USER_SUSPEND		Manual selective suspend was issued by userspace.
 *
 * USER_RESUME		Manual selective resume was issued by userspace.
 *
 * REMOTE_WAKEUP	Remote-wakeup request was received from the device.
 *
 * AUTO_SUSPEND		Automatic (device idle) runtime suspend was
 *			initiated by the subsystem.
 *
 * AUTO_RESUME		Automatic (device needed) runtime resume was
 *			requested by a driver.
 */
/**
 * Device power management states
 *
 * These state labels are used internally by the PM core to indicate the current
 * status of a device with respect to the PM core operations.
 *
 * DPM_ON		Device is regarded as operational.  Set this way
 *			initially and when ->complete() is about to be called.
 *			Also set when ->prepare() fails.
 *
 * DPM_PREPARING	Device is going to be prepared for a PM transition.  Set
 *			when ->prepare() is about to be called.
 *
 * DPM_RESUMING		Device is going to be resumed.  Set when ->resume(),
 *			->thaw(), or ->restore() is about to be called.
 *
 * DPM_SUSPENDING	Device has been prepared for a power transition.  Set
 *			when ->prepare() has just succeeded.
 *
 * DPM_OFF		Device is regarded as inactive.  Set immediately after
 *			->suspend(), ->freeze(), or ->poweroff() has succeeded.
 *			Also set when ->resume()_noirq, ->thaw_noirq(), or
 *			->restore_noirq() is about to be called.
 *
 * DPM_OFF_IRQ		Device is in a "deep sleep".  Set immediately after
 *			->suspend_noirq(), ->freeze_noirq(), or
 *			->poweroff_noirq() has just succeeded.
 */
enum dpm_state {
	DPM_INVALID,
	DPM_ON,
	DPM_PREPARING,
	DPM_RESUMING,
	DPM_SUSPENDING,
	DPM_OFF,
	DPM_OFF_IRQ,
};
/**
 * Device run-time power management status.
 *
 * These status labels are used internally by the PM core to indicate the
 * current status of a device with respect to the PM core operations.  They do
 * not reflect the actual power state of the device or its status as seen by the
 * driver.
 *
 * RPM_ACTIVE		Device is fully operational.  Indicates that the device
 *			bus type's ->runtime_resume() callback has completed
 *			successfully.
 *
 * RPM_SUSPENDED	Device bus type's ->runtime_suspend() callback has
 *			completed successfully.  The device is regarded as
 *			suspended.
 *
 * RPM_RESUMING		Device bus type's ->runtime_resume() callback is being
 *			executed.
 *
 * RPM_SUSPENDING	Device bus type's ->runtime_suspend() callback is being
 *			executed.
 */
enum rpm_status {
	RPM_ACTIVE = 0,
	RPM_RESUMING,
	RPM_SUSPENDED,
	RPM_SUSPENDING,
};
/**
 * Device run-time power management request types.
 *
 * RPM_REQ_NONE		Do nothing.
 *
 * RPM_REQ_IDLE		Run the device bus type's ->runtime_idle() callback
 *
 * RPM_REQ_SUSPEND	Run the device bus type's ->runtime_suspend() callback
 *
 * RPM_REQ_RESUME	Run the device bus type's ->runtime_resume() callback
 */
enum rpm_request {
	RPM_REQ_NONE = 0,
	RPM_REQ_IDLE,
	RPM_REQ_SUSPEND,
	RPM_REQ_RESUME,
};
struct dev_pm_info {
	pm_message_t		power_state;
	unsigned int		can_wakeup:1;
	unsigned int		should_wakeup:1;
	enum dpm_state		status;		/* Owned by the PM core */
#if definedEx(CONFIG_PM_SLEEP)
	struct list_head	entry;
#endif
#if definedEx(CONFIG_PM_RUNTIME)
	struct timer_list	suspend_timer;
	unsigned long		timer_expires;
	struct work_struct	work;
	wait_queue_head_t	wait_queue;
	spinlock_t		lock;
	atomic_t		usage_count;
	atomic_t		child_count;
	unsigned int		disable_depth:3;
	unsigned int		ignore_children:1;
	unsigned int		idle_notification:1;
	unsigned int		request_pending:1;
	unsigned int		deferred_resume:1;
	unsigned int		run_wake:1;
	enum rpm_request	request;
	enum rpm_status		runtime_status;
	int			runtime_error;
#endif
};
/*
 * The PM_EVENT_ messages are also used by drivers implementing the legacy
 * suspend framework, based on the ->suspend() and ->resume() callbacks common
 * for suspend and hibernation transitions, according to the rules below.
 */
/* Necessary, because several drivers use PM_EVENT_PRETHAW */
/*
 * One transition is triggered by resume(), after a suspend() call; the
 * message is implicit:
 *
 * ON		Driver starts working again, responding to hardware events
 * 		and software requests.  The hardware may have gone through
 * 		a power-off reset, or it may have maintained state from the
 * 		previous suspend() which the driver will rely on while
 * 		resuming.  On most platforms, there are no restrictions on
 * 		availability of resources like clocks during resume().
 *
 * Other transitions are triggered by messages sent using suspend().  All
 * these transitions quiesce the driver, so that I/O queues are inactive.
 * That commonly entails turning off IRQs and DMA; there may be rules
 * about how to quiesce that are specific to the bus or the device's type.
 * (For example, network drivers mark the link state.)  Other details may
 * differ according to the message:
 *
 * SUSPEND	Quiesce, enter a low power device state appropriate for
 * 		the upcoming system state (such as PCI_D3hot), and enable
 * 		wakeup events as appropriate.
 *
 * HIBERNATE	Enter a low power device state appropriate for the hibernation
 * 		state (eg. ACPI S4) and enable wakeup events as appropriate.
 *
 * FREEZE	Quiesce operations so that a consistent image can be saved;
 * 		but do NOT otherwise enter a low power device state, and do
 * 		NOT emit system wakeup events.
 *
 * PRETHAW	Quiesce as if for FREEZE; additionally, prepare for restoring
 * 		the system from a snapshot taken after an earlier FREEZE.
 * 		Some drivers will need to reset their hardware state instead
 * 		of preserving it, to ensure that it's never mistaken for the
 * 		state which that earlier snapshot had set up.
 *
 * A minimally power-aware driver treats all messages as SUSPEND, fully
 * reinitializes its device during resume() -- whether or not it was reset
 * during the suspend/resume cycle -- and can't issue wakeup events.
 *
 * More power-aware drivers may also use low power states at runtime as
 * well as during system sleep states like PM_SUSPEND_STANDBY.  They may
 * be able to use wakeup events to exit from runtime low-power states,
 * or from system low-power states such as standby or suspend-to-RAM.
 */
#if definedEx(CONFIG_PM_SLEEP)
extern void device_pm_lock(void);
extern int sysdev_resume(void);
extern void dpm_resume_noirq(pm_message_t state);
extern void dpm_resume_end(pm_message_t state);
extern void device_pm_unlock(void);
extern int sysdev_suspend(pm_message_t state);
extern int dpm_suspend_noirq(pm_message_t state);
extern int dpm_suspend_start(pm_message_t state);
extern void __suspend_report_result(const char *function, void *fn, int ret);
#endif
#if !definedEx(CONFIG_PM_SLEEP)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int dpm_suspend_start(pm_message_t state)
{
	return 0;
}
#endif
/* How to reorder dpm_list after device_move() */
enum dpm_order {
	DPM_ORDER_NONE,
	DPM_ORDER_DEV_AFTER_PARENT,
	DPM_ORDER_PARENT_BEFORE_DEV,
	DPM_ORDER_DEV_LAST,
};
/*
 * Global Power Management flags
 * Used to keep APM and ACPI from both being active
 */
extern unsigned int	pm_flags;
#line 8 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/apic.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/alternative.h" 1
#line 10 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/apic.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/cpufeature.h" 1
/*
 * Defines x86 CPU feature bits
 */
#line 11 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/apic.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/processor.h" 1
#line 12 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/apic.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/apicdef.h" 1
/*
 * Constants for various Intel APICs. (local APIC, IOAPIC, etc.)
 *
 * Alan Cox <Alan.Cox@linux.org>, 1995.
 * Ingo Molnar <mingo@redhat.com>, 1999, 2000
 */
/*
 * This is the IO-APIC register space as specified
 * by Intel docs:
 */
/*
 * All x86-64 systems are xAPIC compatible.
 * In the following, "apicid" is a physical APIC ID.
 */
/*
 * the local APIC register structure, memory mapped. Not terribly well
 * tested, but we might eventually use this one in the future - the
 * problem why we cannot use it right now is the P5 APIC, it has an
 * errata which cannot take 8-bit reads and writes, only 32-bit ones ...
 */
struct local_apic {
/*000*/	struct { unsigned int __reserved[4]; } __reserved_01;
/*010*/	struct { unsigned int __reserved[4]; } __reserved_02;
/*020*/	struct { /* APIC ID Register */
		unsigned int   __reserved_1	: 24,
			phys_apic_id	:  4,
			__reserved_2	:  4;
		unsigned int __reserved[3];
	} id;
/*030*/	const
	struct { /* APIC Version Register */
		unsigned int   version		:  8,
			__reserved_1	:  8,
			max_lvt		:  8,
			__reserved_2	:  8;
		unsigned int __reserved[3];
	} version;
/*040*/	struct { unsigned int __reserved[4]; } __reserved_03;
/*050*/	struct { unsigned int __reserved[4]; } __reserved_04;
/*060*/	struct { unsigned int __reserved[4]; } __reserved_05;
/*070*/	struct { unsigned int __reserved[4]; } __reserved_06;
/*080*/	struct { /* Task Priority Register */
		unsigned int   priority	:  8,
			__reserved_1	: 24;
		unsigned int __reserved_2[3];
	} tpr;
/*090*/	const
	struct { /* Arbitration Priority Register */
		unsigned int   priority	:  8,
			__reserved_1	: 24;
		unsigned int __reserved_2[3];
	} apr;
/*0A0*/	const
	struct { /* Processor Priority Register */
		unsigned int   priority	:  8,
			__reserved_1	: 24;
		unsigned int __reserved_2[3];
	} ppr;
/*0B0*/	struct { /* End Of Interrupt Register */
		unsigned int   eoi;
		unsigned int __reserved[3];
	} eoi;
/*0C0*/	struct { unsigned int __reserved[4]; } __reserved_07;
/*0D0*/	struct { /* Logical Destination Register */
		unsigned int   __reserved_1	: 24,
			logical_dest	:  8;
		unsigned int __reserved_2[3];
	} ldr;
/*0E0*/	struct { /* Destination Format Register */
		unsigned int   __reserved_1	: 28,
			model		:  4;
		unsigned int __reserved_2[3];
	} dfr;
/*0F0*/	struct { /* Spurious Interrupt Vector Register */
		unsigned int	spurious_vector	:  8,
			apic_enabled	:  1,
			focus_cpu	:  1,
			__reserved_2	: 22;
		unsigned int __reserved_3[3];
	} svr;
/*100*/	struct { /* In Service Register */
/*170*/		unsigned int bitfield;
		unsigned int __reserved[3];
	} isr [8];
/*180*/	struct { /* Trigger Mode Register */
/*1F0*/		unsigned int bitfield;
		unsigned int __reserved[3];
	} tmr [8];
/*200*/	struct { /* Interrupt Request Register */
/*270*/		unsigned int bitfield;
		unsigned int __reserved[3];
	} irr [8];
/*280*/	union { /* Error Status Register */
		struct {
			unsigned int   send_cs_error			:  1,
				receive_cs_error		:  1,
				send_accept_error		:  1,
				receive_accept_error		:  1,
				__reserved_1			:  1,
				send_illegal_vector		:  1,
				receive_illegal_vector		:  1,
				illegal_register_address	:  1,
				__reserved_2			: 24;
			unsigned int __reserved_3[3];
		} error_bits;
		struct {
			unsigned int errors;
			unsigned int __reserved_3[3];
		} all_errors;
	} esr;
/*290*/	struct { unsigned int __reserved[4]; } __reserved_08;
/*2A0*/	struct { unsigned int __reserved[4]; } __reserved_09;
/*2B0*/	struct { unsigned int __reserved[4]; } __reserved_10;
/*2C0*/	struct { unsigned int __reserved[4]; } __reserved_11;
/*2D0*/	struct { unsigned int __reserved[4]; } __reserved_12;
/*2E0*/	struct { unsigned int __reserved[4]; } __reserved_13;
/*2F0*/	struct { unsigned int __reserved[4]; } __reserved_14;
/*300*/	struct { /* Interrupt Command Register 1 */
		unsigned int   vector			:  8,
			delivery_mode		:  3,
			destination_mode	:  1,
			delivery_status		:  1,
			__reserved_1		:  1,
			level			:  1,
			trigger			:  1,
			__reserved_2		:  2,
			shorthand		:  2,
			__reserved_3		:  12;
		unsigned int __reserved_4[3];
	} icr1;
/*310*/	struct { /* Interrupt Command Register 2 */
		union {
			unsigned int   __reserved_1	: 24,
				phys_dest	:  4,
				__reserved_2	:  4;
			unsigned int   __reserved_3	: 24,
				logical_dest	:  8;
		} dest;
		unsigned int __reserved_4[3];
	} icr2;
/*320*/	struct { /* LVT - Timer */
		unsigned int   vector		:  8,
			__reserved_1	:  4,
			delivery_status	:  1,
			__reserved_2	:  3,
			mask		:  1,
			timer_mode	:  1,
			__reserved_3	: 14;
		unsigned int __reserved_4[3];
	} lvt_timer;
/*330*/	struct { /* LVT - Thermal Sensor */
		unsigned int  vector		:  8,
			delivery_mode	:  3,
			__reserved_1	:  1,
			delivery_status	:  1,
			__reserved_2	:  3,
			mask		:  1,
			__reserved_3	: 15;
		unsigned int __reserved_4[3];
	} lvt_thermal;
/*340*/	struct { /* LVT - Performance Counter */
		unsigned int   vector		:  8,
			delivery_mode	:  3,
			__reserved_1	:  1,
			delivery_status	:  1,
			__reserved_2	:  3,
			mask		:  1,
			__reserved_3	: 15;
		unsigned int __reserved_4[3];
	} lvt_pc;
/*350*/	struct { /* LVT - LINT0 */
		unsigned int   vector		:  8,
			delivery_mode	:  3,
			__reserved_1	:  1,
			delivery_status	:  1,
			polarity	:  1,
			remote_irr	:  1,
			trigger		:  1,
			mask		:  1,
			__reserved_2	: 15;
		unsigned int __reserved_3[3];
	} lvt_lint0;
/*360*/	struct { /* LVT - LINT1 */
		unsigned int   vector		:  8,
			delivery_mode	:  3,
			__reserved_1	:  1,
			delivery_status	:  1,
			polarity	:  1,
			remote_irr	:  1,
			trigger		:  1,
			mask		:  1,
			__reserved_2	: 15;
		unsigned int __reserved_3[3];
	} lvt_lint1;
/*370*/	struct { /* LVT - Error */
		unsigned int   vector		:  8,
			__reserved_1	:  4,
			delivery_status	:  1,
			__reserved_2	:  3,
			mask		:  1,
			__reserved_3	: 15;
		unsigned int __reserved_4[3];
	} lvt_error;
/*380*/	struct { /* Timer Initial Count Register */
		unsigned int   initial_count;
		unsigned int __reserved_2[3];
	} timer_icr;
/*390*/	const
	struct { /* Timer Current Count Register */
		unsigned int   curr_count;
		unsigned int __reserved_2[3];
	} timer_ccr;
/*3A0*/	struct { unsigned int __reserved[4]; } __reserved_16;
/*3B0*/	struct { unsigned int __reserved[4]; } __reserved_17;
/*3C0*/	struct { unsigned int __reserved[4]; } __reserved_18;
/*3D0*/	struct { unsigned int __reserved[4]; } __reserved_19;
/*3E0*/	struct { /* Timer Divide Configuration Register */
		unsigned int   divisor		:  4,
			__reserved_1	: 28;
		unsigned int __reserved_2[3];
	} timer_dcr;
/*3F0*/	struct { unsigned int __reserved[4]; } __reserved_20;
} __attribute__ ((packed));
#line 13 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/apic.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic.h" 1
 #line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic_64.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic.h" 2
#line 14 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/apic.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/fixmap.h" 1
/*
 * fixmap.h: compile-time virtual memory allocation
 *
 * This file is subject to the terms and conditions of the GNU General Public
 * License.  See the file "COPYING" in the main directory of this archive
 * for more details.
 *
 * Copyright (C) 1998 Ingo Molnar
 *
 * Support of BIGMEM added by Gerhard Wichert, Siemens AG, July 1999
 * x86_32 and x86_64 integration by Gustavo F. Padovan, February 2009
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/kernel.h" 1
#line 20 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/fixmap.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/acpi.h" 1
/*
 *  Copyright (C) 2001 Paul Diefenbaugh <paul.s.diefenbaugh@intel.com>
 *  Copyright (C) 2001 Patrick Mochel <mochel@osdl.org>
 *
 * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 *
 *  This program is free software; you can redistribute it and/or modify
 *  it under the terms of the GNU General Public License as published by
 *  the Free Software Foundation; either version 2 of the License, or
 *  (at your option) any later version.
 *
 *  This program is distributed in the hope that it will be useful,
 *  but WITHOUT ANY WARRANTY; without even the implied warranty of
 *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *  GNU General Public License for more details.
 *
 *  You should have received a copy of the GNU General Public License
 *  along with this program; if not, write to the Free Software
 *  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
 *
 * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/acpi/pdc_intel.h" 1
/* _PDC bit definition for Intel processors */
#line 28 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/acpi.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/numa.h" 1
 #line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/numa_64.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/nodemask.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/numa_64.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/apicdef.h" 1
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/numa_64.h" 2
struct bootnode {
	u64 start;
	u64 end;
};
extern int compute_hash_shift(struct bootnode *nodes, int numblks,
			      int *nodeids);
extern void numa_init_array(void);
extern int numa_off;
extern s16 apicid_to_node[32768];
extern unsigned long numa_free_all_bootmem(void);
extern void setup_node_bootmem(int nodeid, unsigned long start,
			       unsigned long end);
/*
 * Too small node sizes may confuse the VM badly. Usually they
 * result from BIOS bugs. So dont recognize nodes as standalone
 * NUMA entities that have less than this amount of RAM listed:
 */
extern void __attribute__ ((__section__(".init.text"))) __attribute__((__cold__)) __attribute__((no_instrument_function)) init_cpu_to_node(void);
extern void __attribute__ ((__section__(".cpuinit.text"))) __attribute__((__cold__)) numa_set_node(int cpu, int node);
extern void __attribute__ ((__section__(".cpuinit.text"))) __attribute__((__cold__)) numa_clear_node(int cpu);
extern void __attribute__ ((__section__(".cpuinit.text"))) __attribute__((__cold__)) numa_add_cpu(int cpu);
extern void __attribute__ ((__section__(".cpuinit.text"))) __attribute__((__cold__)) numa_remove_cpu(int cpu);
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/numa.h" 2
#line 30 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/acpi.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/processor.h" 1
#line 31 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/acpi.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/mmu.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/spinlock.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/mmu.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/mutex.h" 1
/*
 * Mutexes: blocking mutual exclusion locks
 *
 * started by Ingo Molnar:
 *
 *  Copyright (C) 2004, 2005, 2006 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>
 *
 * This file contains the main data structure and API definitions.
 */
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/mmu.h" 2
/*
 * The x86 doesn't have a mmu context, but
 * we put the segment information here.
 */
typedef struct {
	void *ldt;
	int size;
	struct mutex lock;
	void *vdso;
} mm_context_t;
void leave_mm(int cpu);
#line 32 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/acpi.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/mpspec.h" 1
#line 33 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/acpi.h" 2
/*
 * Calling conventions:
 *
 * ACPI_SYSTEM_XFACE        - Interfaces to host OS (handlers, threads)
 * ACPI_EXTERNAL_XFACE      - External ACPI interfaces
 * ACPI_INTERNAL_XFACE      - Internal ACPI interfaces
 * ACPI_INTERNAL_VAR_XFACE  - Internal variable-parameter list interfaces
 */
/* Asm macros */
int __acpi_acquire_global_lock(unsigned int *lock);
int __acpi_release_global_lock(unsigned int *lock);
/*
 * Math helper asm macros
 */
#if definedEx(CONFIG_ACPI)
extern int acpi_lapic;
extern int acpi_ioapic;
extern int acpi_noirq;
extern int acpi_strict;
extern int acpi_disabled;
extern int acpi_ht;
extern int acpi_pci_disabled;
extern int acpi_skip_timer_override;
extern int acpi_use_timer_override;
extern u8 acpi_sci_flags;
extern int acpi_sci_override_gsi;
void acpi_pic_sci_set_trigger(unsigned int, u16);
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void disable_acpi(void)
{
	acpi_disabled = 1;
	acpi_ht = 0;
	acpi_pci_disabled = 1;
	acpi_noirq = 1;
}
extern int acpi_gsi_to_irq(u32 gsi, unsigned int *irq);
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void acpi_noirq_set(void) { acpi_noirq = 1; }
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void acpi_disable_pci(void)
{
	acpi_pci_disabled = 1;
	acpi_noirq_set();
}
/* routines for saving/restoring kernel state */
extern int acpi_save_state_mem(void);
extern void acpi_restore_state_mem(void);
extern unsigned long acpi_wakeup_address;
/* early initialization routine */
extern void acpi_reserve_wakeup_memory(void);
/*
 * Check if the CPU can handle C2 and deeper
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned int acpi_processor_cstate_check(unsigned int max_cstate)
{
	/*
	 * Early models (<=5) of AMD Opterons are not supposed to go into
	 * C2 state.
	 *
	 * Steppings 0x0A and later are good
	 */
	if (boot_cpu_data.x86 == 0x0F &&
	    boot_cpu_data.x86_vendor == 2 &&
	    boot_cpu_data.x86_model <= 0x05 &&
	    boot_cpu_data.x86_mask < 0x0A)
		return 1;
	else if ((__builtin_constant_p((3*32+21)) && ( ((((3*32+21))>>5)==0 && (1UL<<(((3*32+21))&31) & (
#if !definedEx(CONFIG_MATH_EMULATION)
(1<<((0*32+ 0) & 31))
#endif
#if definedEx(CONFIG_MATH_EMULATION)
0
#endif
|
#if !definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_64) && definedEx(CONFIG_PARAVIRT)
0
#endif
#if definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT)
(1<<((0*32+ 3)) & 31)
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+ 5) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if !definedEx(CONFIG_X86_PAE) && definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_PAE)
(1<<((0*32+ 6) & 31))
#endif
#if !definedEx(CONFIG_X86_PAE) && !definedEx(CONFIG_X86_64)
0
#endif
| 
#if definedEx(CONFIG_X86_CMPXCHG64)
(1<<((0*32+ 8) & 31))
#endif
#if !definedEx(CONFIG_X86_CMPXCHG64)
0
#endif
|
#if !definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_64) && definedEx(CONFIG_PARAVIRT)
0
#endif
#if definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT)
(1<<((0*32+13)) & 31)
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+24) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if !definedEx(CONFIG_X86_64) && definedEx(CONFIG_X86_CMOV) || definedEx(CONFIG_X86_64)
(1<<((0*32+15) & 31))
#endif
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_X86_CMOV)
0
#endif
| 
#if definedEx(CONFIG_X86_64)
(1<<((0*32+25) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+26) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
))) || ((((3*32+21))>>5)==1 && (1UL<<(((3*32+21))&31) & (
#if definedEx(CONFIG_X86_64)
(1<<((1*32+29) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if definedEx(CONFIG_X86_USE_3DNOW)
(1<<((1*32+31) & 31))
#endif
#if !definedEx(CONFIG_X86_USE_3DNOW)
0
#endif
))) || ((((3*32+21))>>5)==2 && (1UL<<(((3*32+21))&31) & 0)) || ((((3*32+21))>>5)==3 && (1UL<<(((3*32+21))&31) & (
#if !definedEx(CONFIG_X86_64) && definedEx(CONFIG_X86_P6_NOP) || definedEx(CONFIG_X86_64)
(1<<((3*32+20) & 31))
#endif
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_X86_P6_NOP)
0
#endif
))) || ((((3*32+21))>>5)==4 && (1UL<<(((3*32+21))&31) & 0)) || ((((3*32+21))>>5)==5 && (1UL<<(((3*32+21))&31) & 0)) || ((((3*32+21))>>5)==6 && (1UL<<(((3*32+21))&31) & 0)) || ((((3*32+21))>>5)==7 && (1UL<<(((3*32+21))&31) & 0)) ) ? 1 : (__builtin_constant_p(((3*32+21))) ? constant_test_bit(((3*32+21)), ((unsigned long *)((&boot_cpu_data)->x86_capability))) : variable_test_bit(((3*32+21)), ((unsigned long *)((&boot_cpu_data)->x86_capability))))))
		return 1;
	else
		return max_cstate;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 bool arch_has_acpi_pdc(void)
{
	struct cpuinfo_x86 *c = &(*({ unsigned long __ptr; __asm__ ("" : "=r"(__ptr) : "0"((&per_cpu__cpu_info))); (typeof((&per_cpu__cpu_info))) (__ptr + (((__per_cpu_offset[0])))); }));
	return (c->x86_vendor == 0 ||
		c->x86_vendor == 5);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void arch_acpi_set_pdc_bits(u32 *buf)
{
	struct cpuinfo_x86 *c = &(*({ unsigned long __ptr; __asm__ ("" : "=r"(__ptr) : "0"((&per_cpu__cpu_info))); (typeof((&per_cpu__cpu_info))) (__ptr + (((__per_cpu_offset[0])))); }));
	buf[2] |= ((0x0010) | (0x0008) | (0x0002) | (0x0100) | (0x0200));
	if ((__builtin_constant_p((4*32+ 7)) && ( ((((4*32+ 7))>>5)==0 && (1UL<<(((4*32+ 7))&31) & (
#if !definedEx(CONFIG_MATH_EMULATION)
(1<<((0*32+ 0) & 31))
#endif
#if definedEx(CONFIG_MATH_EMULATION)
0
#endif
|
#if !definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_64) && definedEx(CONFIG_PARAVIRT)
0
#endif
#if definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT)
(1<<((0*32+ 3)) & 31)
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+ 5) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if !definedEx(CONFIG_X86_PAE) && definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_PAE)
(1<<((0*32+ 6) & 31))
#endif
#if !definedEx(CONFIG_X86_PAE) && !definedEx(CONFIG_X86_64)
0
#endif
| 
#if definedEx(CONFIG_X86_CMPXCHG64)
(1<<((0*32+ 8) & 31))
#endif
#if !definedEx(CONFIG_X86_CMPXCHG64)
0
#endif
|
#if !definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_64) && definedEx(CONFIG_PARAVIRT)
0
#endif
#if definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT)
(1<<((0*32+13)) & 31)
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+24) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if !definedEx(CONFIG_X86_64) && definedEx(CONFIG_X86_CMOV) || definedEx(CONFIG_X86_64)
(1<<((0*32+15) & 31))
#endif
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_X86_CMOV)
0
#endif
| 
#if definedEx(CONFIG_X86_64)
(1<<((0*32+25) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+26) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
))) || ((((4*32+ 7))>>5)==1 && (1UL<<(((4*32+ 7))&31) & (
#if definedEx(CONFIG_X86_64)
(1<<((1*32+29) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if definedEx(CONFIG_X86_USE_3DNOW)
(1<<((1*32+31) & 31))
#endif
#if !definedEx(CONFIG_X86_USE_3DNOW)
0
#endif
))) || ((((4*32+ 7))>>5)==2 && (1UL<<(((4*32+ 7))&31) & 0)) || ((((4*32+ 7))>>5)==3 && (1UL<<(((4*32+ 7))&31) & (
#if !definedEx(CONFIG_X86_64) && definedEx(CONFIG_X86_P6_NOP) || definedEx(CONFIG_X86_64)
(1<<((3*32+20) & 31))
#endif
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_X86_P6_NOP)
0
#endif
))) || ((((4*32+ 7))>>5)==4 && (1UL<<(((4*32+ 7))&31) & 0)) || ((((4*32+ 7))>>5)==5 && (1UL<<(((4*32+ 7))&31) & 0)) || ((((4*32+ 7))>>5)==6 && (1UL<<(((4*32+ 7))&31) & 0)) || ((((4*32+ 7))>>5)==7 && (1UL<<(((4*32+ 7))&31) & 0)) ) ? 1 : (__builtin_constant_p(((4*32+ 7))) ? constant_test_bit(((4*32+ 7)), ((unsigned long *)((c)->x86_capability))) : variable_test_bit(((4*32+ 7)), ((unsigned long *)((c)->x86_capability))))))
		buf[2] |= ((0x0008) | (0x0002) | (0x0020) | (0x0800) | (0x0001));
	if ((__builtin_constant_p((0*32+22)) && ( ((((0*32+22))>>5)==0 && (1UL<<(((0*32+22))&31) & (
#if !definedEx(CONFIG_MATH_EMULATION)
(1<<((0*32+ 0) & 31))
#endif
#if definedEx(CONFIG_MATH_EMULATION)
0
#endif
|
#if !definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_64) && definedEx(CONFIG_PARAVIRT)
0
#endif
#if definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT)
(1<<((0*32+ 3)) & 31)
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+ 5) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if !definedEx(CONFIG_X86_PAE) && definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_PAE)
(1<<((0*32+ 6) & 31))
#endif
#if !definedEx(CONFIG_X86_PAE) && !definedEx(CONFIG_X86_64)
0
#endif
| 
#if definedEx(CONFIG_X86_CMPXCHG64)
(1<<((0*32+ 8) & 31))
#endif
#if !definedEx(CONFIG_X86_CMPXCHG64)
0
#endif
|
#if !definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_64) && definedEx(CONFIG_PARAVIRT)
0
#endif
#if definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT)
(1<<((0*32+13)) & 31)
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+24) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if !definedEx(CONFIG_X86_64) && definedEx(CONFIG_X86_CMOV) || definedEx(CONFIG_X86_64)
(1<<((0*32+15) & 31))
#endif
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_X86_CMOV)
0
#endif
| 
#if definedEx(CONFIG_X86_64)
(1<<((0*32+25) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+26) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
))) || ((((0*32+22))>>5)==1 && (1UL<<(((0*32+22))&31) & (
#if definedEx(CONFIG_X86_64)
(1<<((1*32+29) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if definedEx(CONFIG_X86_USE_3DNOW)
(1<<((1*32+31) & 31))
#endif
#if !definedEx(CONFIG_X86_USE_3DNOW)
0
#endif
))) || ((((0*32+22))>>5)==2 && (1UL<<(((0*32+22))&31) & 0)) || ((((0*32+22))>>5)==3 && (1UL<<(((0*32+22))&31) & (
#if !definedEx(CONFIG_X86_64) && definedEx(CONFIG_X86_P6_NOP) || definedEx(CONFIG_X86_64)
(1<<((3*32+20) & 31))
#endif
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_X86_P6_NOP)
0
#endif
))) || ((((0*32+22))>>5)==4 && (1UL<<(((0*32+22))&31) & 0)) || ((((0*32+22))>>5)==5 && (1UL<<(((0*32+22))&31) & 0)) || ((((0*32+22))>>5)==6 && (1UL<<(((0*32+22))&31) & 0)) || ((((0*32+22))>>5)==7 && (1UL<<(((0*32+22))&31) & 0)) ) ? 1 : (__builtin_constant_p(((0*32+22))) ? constant_test_bit(((0*32+22)), ((unsigned long *)((c)->x86_capability))) : variable_test_bit(((0*32+22)), ((unsigned long *)((c)->x86_capability))))))
		buf[2] |= (0x0004);
	/*
	 * If mwait/monitor is unsupported, C2/C3_FFH will be disabled
	 */
	if (!(__builtin_constant_p((4*32+ 3)) && ( ((((4*32+ 3))>>5)==0 && (1UL<<(((4*32+ 3))&31) & (
#if !definedEx(CONFIG_MATH_EMULATION)
(1<<((0*32+ 0) & 31))
#endif
#if definedEx(CONFIG_MATH_EMULATION)
0
#endif
|
#if !definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_64) && definedEx(CONFIG_PARAVIRT)
0
#endif
#if definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT)
(1<<((0*32+ 3)) & 31)
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+ 5) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if !definedEx(CONFIG_X86_PAE) && definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_PAE)
(1<<((0*32+ 6) & 31))
#endif
#if !definedEx(CONFIG_X86_PAE) && !definedEx(CONFIG_X86_64)
0
#endif
| 
#if definedEx(CONFIG_X86_CMPXCHG64)
(1<<((0*32+ 8) & 31))
#endif
#if !definedEx(CONFIG_X86_CMPXCHG64)
0
#endif
|
#if !definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_64) && definedEx(CONFIG_PARAVIRT)
0
#endif
#if definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT)
(1<<((0*32+13)) & 31)
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+24) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if !definedEx(CONFIG_X86_64) && definedEx(CONFIG_X86_CMOV) || definedEx(CONFIG_X86_64)
(1<<((0*32+15) & 31))
#endif
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_X86_CMOV)
0
#endif
| 
#if definedEx(CONFIG_X86_64)
(1<<((0*32+25) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+26) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
))) || ((((4*32+ 3))>>5)==1 && (1UL<<(((4*32+ 3))&31) & (
#if definedEx(CONFIG_X86_64)
(1<<((1*32+29) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if definedEx(CONFIG_X86_USE_3DNOW)
(1<<((1*32+31) & 31))
#endif
#if !definedEx(CONFIG_X86_USE_3DNOW)
0
#endif
))) || ((((4*32+ 3))>>5)==2 && (1UL<<(((4*32+ 3))&31) & 0)) || ((((4*32+ 3))>>5)==3 && (1UL<<(((4*32+ 3))&31) & (
#if !definedEx(CONFIG_X86_64) && definedEx(CONFIG_X86_P6_NOP) || definedEx(CONFIG_X86_64)
(1<<((3*32+20) & 31))
#endif
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_X86_P6_NOP)
0
#endif
))) || ((((4*32+ 3))>>5)==4 && (1UL<<(((4*32+ 3))&31) & 0)) || ((((4*32+ 3))>>5)==5 && (1UL<<(((4*32+ 3))&31) & 0)) || ((((4*32+ 3))>>5)==6 && (1UL<<(((4*32+ 3))&31) & 0)) || ((((4*32+ 3))>>5)==7 && (1UL<<(((4*32+ 3))&31) & 0)) ) ? 1 : (__builtin_constant_p(((4*32+ 3))) ? constant_test_bit(((4*32+ 3)), ((unsigned long *)((c)->x86_capability))) : variable_test_bit(((4*32+ 3)), ((unsigned long *)((c)->x86_capability))))))
		buf[2] &= ~((0x0200));
}
#endif
#if !definedEx(CONFIG_ACPI)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void acpi_noirq_set(void) { }
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void acpi_disable_pci(void) { }
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void disable_acpi(void) { }
#endif
struct bootnode;
#if definedEx(CONFIG_ACPI_NUMA)
extern int acpi_numa;
extern int acpi_get_nodes(struct bootnode *physnodes);
extern int acpi_scan_nodes(unsigned long start, unsigned long end);
extern void acpi_fake_nodes(const struct bootnode *fake_nodes,
				   int num_nodes);
#endif
#if !definedEx(CONFIG_ACPI_NUMA)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void acpi_fake_nodes(const struct bootnode *fake_nodes,
				   int num_nodes)
{
}
#endif
#line 21 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/fixmap.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/apicdef.h" 1
#line 22 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/fixmap.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/page.h" 1
#line 23 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/fixmap.h" 2
 #line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/vsyscall.h" 1
enum vsyscall_num {
	__NR_vgettimeofday,
	__NR_vtime,
	__NR_vgetcpu,
};
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/seqlock.h" 1
#line 19 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/vsyscall.h" 2
/* Definitions for CONFIG_GENERIC_TIME definitions */
extern int __vgetcpu_mode;
extern volatile unsigned long __jiffies;
/* kernel space (writeable) */
extern int vgetcpu_mode;
extern struct timezone sys_tz;
extern void map_vsyscall(void);
#line 28 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/fixmap.h" 2
/*
 * We can't declare FIXADDR_TOP as variable for x86_64 because vsyscall
 * uses fixmaps that relies on FIXADDR_TOP for proper address calculation.
 * Because of this, FIXADDR_TOP x86 integration was left as later work.
 */
/* Only covers 32bit vsyscalls currently. Need another set for 64bit. */
/*
 * Here we define all the compile-time 'special' virtual
 * addresses. The point is to have a constant address at
 * compile time, but to set the physical address only
 * in the boot process.
 * for x86_32: We allocate these special addresses
 * from the end of virtual memory (0xfffff000) backwards.
 * Also this lets us do fail-safe vmalloc(), we
 * can guarantee that these special addresses and
 * vmalloc()-ed addresses never overlap.
 *
 * These 'compile-time allocated' memory buffers are
 * fixed-size 4k pages (or larger if used with an increment
 * higher than 1). Use set_fixmap(idx,phys) to associate
 * physical memory with fixmap indices.
 *
 * TLB entries of such buffers will not be flushed across
 * task switches.
 */
enum fixed_addresses {
 	VSYSCALL_LAST_PAGE,
	VSYSCALL_FIRST_PAGE = VSYSCALL_LAST_PAGE
			    + (((-2UL << 20)-(-10UL << 20)) >> 12) - 1,
	VSYSCALL_HPET,
	FIX_DBGP_BASE,
	FIX_EARLYCON_MEM_BASE,
#if definedEx(CONFIG_PROVIDE_OHCI1394_DMA_INIT)
	FIX_OHCI1394_BASE,
#endif
	FIX_APIC_BASE,	/* local (CPU) APIC) -- required for SMP or not */
#if definedEx(CONFIG_X86_IO_APIC)
	FIX_IO_APIC_BASE_0,
	FIX_IO_APIC_BASE_END = FIX_IO_APIC_BASE_0 + 128 - 1,
#endif
#if definedEx(CONFIG_X86_VISWS_APIC)
	FIX_CO_CPU,	/* Cobalt timer */
	FIX_CO_APIC,	/* Cobalt APIC Redirection Table */
	FIX_LI_PCIA,	/* Lithium PCI Bridge A */
	FIX_LI_PCIB,	/* Lithium PCI Bridge B */
#endif
#if definedEx(CONFIG_X86_F00F_BUG)
	FIX_F00F_IDT,	/* Virtual mapping for IDT */
#endif
#if definedEx(CONFIG_X86_CYCLONE_TIMER)
	FIX_CYCLONE_TIMER, /*cyclone timer register*/
#endif
#if definedEx(CONFIG_PARAVIRT)
	FIX_PARAVIRT_BOOTMAP,
#endif
	FIX_TEXT_POKE1,	/* reserve 2 pages for text_poke() */
	FIX_TEXT_POKE0, /* first page is last, because allocation is backward */
	__end_of_permanent_fixed_addresses,
	/*
	 * 256 temporary boot-time mappings, used by early_ioremap(),
	 * before ioremap() is functional.
	 *
	 * We round it up to the next 256 pages boundary so that we
	 * can have a single pgd entry and a single pte table:
	 */
	FIX_BTMAP_END = __end_of_permanent_fixed_addresses + 256 -
			(__end_of_permanent_fixed_addresses & 255),
	FIX_BTMAP_BEGIN = FIX_BTMAP_END + 64*4 - 1,
#if definedEx(CONFIG_INTEL_TXT)
	FIX_TBOOT_BASE,
#endif
	__end_of_fixed_addresses
};
extern void reserve_top_address(unsigned long reserve);
extern int fixmaps_set;
extern pte_t *kmap_pte;
extern pgprot_t kmap_prot;
extern pte_t *pkmap_page_table;
void __native_set_fixmap(enum fixed_addresses idx, pte_t pte);
void native_set_fixmap(enum fixed_addresses idx,
		       phys_addr_t phys, pgprot_t flags);
#if !definedEx(CONFIG_PARAVIRT)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __set_fixmap(enum fixed_addresses idx,
				phys_addr_t phys, pgprot_t flags)
{
	native_set_fixmap(idx, phys, flags);
}
#endif
/*
 * Some hardware wants to get fixmapped without caching.
 */
extern void __this_fixmap_does_not_exist(void);
/*
 * 'index to address' translation. If anyone tries to use the idx
 * directly without translation, we catch the bug with a NULL-deference
 * kernel oops. Illegal ranges of incoming indices are caught too.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __attribute__((always_inline)) unsigned long fix_to_virt(const unsigned int idx)
{
	/*
	 * this branch gets completely eliminated after inlining,
	 * except when someone tries to use fixaddr indices in an
	 * illegal way. (such as mixing up address types or using
	 * out-of-range indices).
	 *
	 * If it doesn't get removed, the linker will complain
	 * loudly with a reasonably clear error message..
	 */
	if (idx >= __end_of_fixed_addresses)
		__this_fixmap_does_not_exist();
	return (((-2UL << 20)-((1UL) << 12)) - ((idx) << 12));
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long virt_to_fix(const unsigned long vaddr)
{
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(vaddr >=((-2UL << 20)-((1UL) << 12)) || vaddr <(((-2UL << 20)-((1UL) << 12)) -(__end_of_permanent_fixed_addresses << 12))), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/fixmap.h"), "i" (208), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (vaddr >= ((-2UL << 20)-((1UL) << 12)) || vaddr < (((-2UL << 20)-((1UL) << 12)) - (__end_of_permanent_fixed_addresses << 12))) ; } while(0)
#endif
;
	return ((((-2UL << 20)-((1UL) << 12)) - ((vaddr)&(~(((1UL) << 12)-1)))) >> 12);
}
#line 15 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/apic.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/mpspec.h" 1
#line 16 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/apic.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/system.h" 1
#line 17 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/apic.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/msr.h" 1
#line 18 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/apic.h" 2
/*
 * Debugging macros
 */
/*
 * Define the default level of output to be very little
 * This can be turned up by using apic=verbose for more
 * information and apic=debug for _lots_ of information.
 * apic_verbosity is defined in apic.c
 */
 static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void generic_apic_probe(void)
{
}
extern unsigned int apic_verbosity;
extern int local_apic_timer_c2_ok;
extern int disable_apic;
extern void __inquire_remote_apic(int apicid);
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void default_inquire_remote_apic(int apicid)
{
	if (apic_verbosity >= 2)
		__inquire_remote_apic(apicid);
}
/*
 * With 82489DX we can't rely on apic feature bit
 * retrieved via cpuid but still have to deal with
 * such an apic chip so we assume that SMP configuration
 * is found from MP table (64bit case uses ACPI mostly
 * which set smp presence flag as well so we are safe
 * to use this helper too).
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 bool apic_from_smp_config(void)
{
	return smp_found_config && !disable_apic;
}
/*
 * Basic functions accessing APICs.
 */
#if definedEx(CONFIG_PARAVIRT)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h" 1
#line 87 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/apic.h" 2
#endif
#if definedEx(CONFIG_X86_64)
extern int is_vsmp_box(void);
#endif
#if !definedEx(CONFIG_X86_64)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int is_vsmp_box(void)
{
	return 0;
}
#endif
extern void xapic_wait_icr_idle(void);
extern u32 safe_xapic_wait_icr_idle(void);
extern void xapic_icr_write(u32, u32);
extern int setup_profiling_timer(unsigned int);
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void native_apic_mem_write(u32 reg, u32 v)
{
	volatile u32 *addr = (volatile u32 *)((fix_to_virt(FIX_APIC_BASE)) + reg);
	asm volatile ("661:\n\t" "movl %0, %1" "\n662:\n" ".section .altinstructions,\"a\"\n" " " ".balign 8" " " "\n" " " ".quad" " " "661b\n" " " ".quad" " " "663f\n" "	 .byte " "(3*32+19)" "\n" "	 .byte 662b-661b\n" "	 .byte 664f-663f\n" "	 .byte 0xff + (664f-663f) - (662b-661b)\n" ".previous\n" ".section .altinstr_replacement, \"ax\"\n" "663:\n\t" "xchgl %0, %1" "\n664:\n" ".previous" : 
 "=r"(v), "=m"(*addr) : "i" (0),"0"(v), "m"(*addr));
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 u32 native_apic_mem_read(u32 reg)
{
	return *((volatile u32 *)((fix_to_virt(FIX_APIC_BASE)) + reg));
}
extern void native_apic_wait_icr_idle(void);
extern u32 native_safe_apic_wait_icr_idle(void);
extern void native_apic_icr_write(u32 low, u32 id);
extern u64 native_apic_icr_read(void);
extern int x2apic_mode;
#if definedEx(CONFIG_X86_X2APIC)
/*
 * Make previous memory operations globally visible before
 * sending the IPI through x2apic wrmsr. We need a serializing instruction or
 * mfence for this.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void x2apic_wrmsr_fence(void)
{
	asm volatile("mfence" : : : "memory");
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void native_apic_msr_write(u32 reg, u32 v)
{
	if (reg == 0xE0 || reg == 0x20 || reg == 0xD0 ||
	    reg == 0x30)
		return;
#if definedEx(CONFIG_PARAVIRT)
do { paravirt_write_msr(0x800 +(reg >> 4), v, 0); } while (0)
#endif
#if !definedEx(CONFIG_PARAVIRT)
wrmsr(0x800 + (reg >> 4), v, 0)
#endif
;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 u32 native_apic_msr_read(u32 reg)
{
	u32 low, high;
	if (reg == 0xE0)
		return -1;
#if definedEx(CONFIG_PARAVIRT)
do { int _err; u64 _l = paravirt_read_msr(0x800 +(reg >> 4), &_err); low = (u32)_l; high = _l >> 32; } while (0)
#endif
#if !definedEx(CONFIG_PARAVIRT)
do { u64 __val = native_read_msr((0x800 +(reg >> 4))); (low) = (u32)__val; (high) = (u32)(__val >> 32); } while (0)
#endif
;
	return low;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void native_x2apic_wait_icr_idle(void)
{
	/* no need to wait for icr idle in x2apic */
	return;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 u32 native_safe_x2apic_wait_icr_idle(void)
{
	/* no need to wait for icr idle in x2apic */
	return 0;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void native_x2apic_icr_write(u32 low, u32 id)
{
#if definedEx(CONFIG_PARAVIRT)
do { paravirt_write_msr(0x800 +(0x300 >> 4), (u32)((u64)(((__u64) id) << 32 | low)), ((u64)(((__u64) id) << 32 | low))>>32); } while (0)
#endif
#if !definedEx(CONFIG_PARAVIRT)
native_write_msr((0x800 +(0x300 >> 4)), (u32)((u64)(((__u64) id) << 32 | low)), (u32)((u64)(((__u64) id) << 32 | low) >> 32))
#endif
;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 u64 native_x2apic_icr_read(void)
{
	unsigned long val;
#if definedEx(CONFIG_PARAVIRT)
do { int _err; val = paravirt_read_msr(0x800 +(0x300 >> 4), &_err); } while (0)
#endif
#if !definedEx(CONFIG_PARAVIRT)
((val) = native_read_msr((0x800 +(0x300 >> 4))))
#endif
;
	return val;
}
extern int x2apic_phys;
extern void check_x2apic(void);
extern void enable_x2apic(void);
extern void x2apic_icr_write(u32 low, u32 id);
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int x2apic_enabled(void)
{
	int msr, msr2;
	if (!(__builtin_constant_p((4*32+21)) && ( ((((4*32+21))>>5)==0 && (1UL<<(((4*32+21))&31) & (
#if !definedEx(CONFIG_MATH_EMULATION)
(1<<((0*32+ 0) & 31))
#endif
#if definedEx(CONFIG_MATH_EMULATION)
0
#endif
|
#if !definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_64) && definedEx(CONFIG_PARAVIRT)
0
#endif
#if definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT)
(1<<((0*32+ 3)) & 31)
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+ 5) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if !definedEx(CONFIG_X86_PAE) && definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_PAE)
(1<<((0*32+ 6) & 31))
#endif
#if !definedEx(CONFIG_X86_PAE) && !definedEx(CONFIG_X86_64)
0
#endif
| 
#if definedEx(CONFIG_X86_CMPXCHG64)
(1<<((0*32+ 8) & 31))
#endif
#if !definedEx(CONFIG_X86_CMPXCHG64)
0
#endif
|
#if !definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_64) && definedEx(CONFIG_PARAVIRT)
0
#endif
#if definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT)
(1<<((0*32+13)) & 31)
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+24) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if !definedEx(CONFIG_X86_64) && definedEx(CONFIG_X86_CMOV) || definedEx(CONFIG_X86_64)
(1<<((0*32+15) & 31))
#endif
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_X86_CMOV)
0
#endif
| 
#if definedEx(CONFIG_X86_64)
(1<<((0*32+25) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+26) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
))) || ((((4*32+21))>>5)==1 && (1UL<<(((4*32+21))&31) & (
#if definedEx(CONFIG_X86_64)
(1<<((1*32+29) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if definedEx(CONFIG_X86_USE_3DNOW)
(1<<((1*32+31) & 31))
#endif
#if !definedEx(CONFIG_X86_USE_3DNOW)
0
#endif
))) || ((((4*32+21))>>5)==2 && (1UL<<(((4*32+21))&31) & 0)) || ((((4*32+21))>>5)==3 && (1UL<<(((4*32+21))&31) & (
#if !definedEx(CONFIG_X86_64) && definedEx(CONFIG_X86_P6_NOP) || definedEx(CONFIG_X86_64)
(1<<((3*32+20) & 31))
#endif
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_X86_P6_NOP)
0
#endif
))) || ((((4*32+21))>>5)==4 && (1UL<<(((4*32+21))&31) & 0)) || ((((4*32+21))>>5)==5 && (1UL<<(((4*32+21))&31) & 0)) || ((((4*32+21))>>5)==6 && (1UL<<(((4*32+21))&31) & 0)) || ((((4*32+21))>>5)==7 && (1UL<<(((4*32+21))&31) & 0)) ) ? 1 : (__builtin_constant_p(((4*32+21))) ? constant_test_bit(((4*32+21)), ((unsigned long *)((&boot_cpu_data)->x86_capability))) : variable_test_bit(((4*32+21)), ((unsigned long *)((&boot_cpu_data)->x86_capability))))))
		return 0;
#if definedEx(CONFIG_PARAVIRT)
do { int _err; u64 _l = paravirt_read_msr(0x0000001b, &_err); msr = (u32)_l; msr2 = _l >> 32; } while (0)
#endif
#if !definedEx(CONFIG_PARAVIRT)
do { u64 __val = native_read_msr((0x0000001b)); (msr) = (u32)__val; (msr2) = (u32)(__val >> 32); } while (0)
#endif
;
	if (msr & (1UL << 10))
		return 1;
	return 0;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void x2apic_force_phys(void)
{
	x2apic_phys = 1;
}
#endif
#if !definedEx(CONFIG_X86_X2APIC)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void check_x2apic(void)
{
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void enable_x2apic(void)
{
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int x2apic_enabled(void)
{
	return 0;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void x2apic_force_phys(void)
{
}
#endif
extern void enable_IR_x2apic(void);
extern int get_physical_broadcast(void);
extern void apic_disable(void);
extern int lapic_get_maxlvt(void);
extern void clear_local_APIC(void);
extern void connect_bsp_APIC(void);
extern void disconnect_bsp_APIC(int virt_wire_setup);
extern void disable_local_APIC(void);
extern void lapic_shutdown(void);
extern int verify_local_APIC(void);
extern void cache_APIC_registers(void);
extern void sync_Arb_IDs(void);
extern void init_bsp_APIC(void);
extern void setup_local_APIC(void);
extern void end_local_APIC_setup(void);
extern void init_apic_mappings(void);
extern void setup_boot_APIC_clock(void);
extern void setup_secondary_APIC_clock(void);
extern int APIC_init_uniprocessor(void);
extern void enable_NMI_through_LVT0(void);
/*
 * On 32bit this is mach-xxx local
 */
#if definedEx(CONFIG_X86_64)
extern void early_init_lapic_mapping(void);
extern int apic_is_clustered_box(void);
#endif
#if !definedEx(CONFIG_X86_64)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int apic_is_clustered_box(void)
{
	return 0;
}
#endif
extern u8 setup_APIC_eilvt_mce(u8 vector, u8 msg_type, u8 mask);
extern u8 setup_APIC_eilvt_ibs(u8 vector, u8 msg_type, u8 mask);
#if definedEx(CONFIG_X86_64)
#endif
#if !definedEx(CONFIG_X86_64)
#endif
/*
 * Copyright 2004 James Cleverdon, IBM.
 * Subject to the GNU Public License, v.2
 *
 * Generic APIC sub-arch data struct.
 *
 * Hacked for x86-64 by James Cleverdon from i386 architecture code by
 * Martin Bligh, Andi Kleen, James Bottomley, John Stultz, and
 * James Cleverdon.
 */
struct apic {
	char *name;
	int (*probe)(void);
	int (*acpi_madt_oem_check)(char *oem_id, char *oem_table_id);
	int (*apic_id_registered)(void);
	u32 irq_delivery_mode;
	u32 irq_dest_mode;
	const struct cpumask *(*target_cpus)(void);
	int disable_esr;
	int dest_logical;
	unsigned long (*check_apicid_used)(physid_mask_t *map, int apicid);
	unsigned long (*check_apicid_present)(int apicid);
	void (*vector_allocation_domain)(int cpu, struct cpumask *retmask);
	void (*init_apic_ldr)(void);
	void (*ioapic_phys_id_map)(physid_mask_t *phys_map, physid_mask_t *retmap);
	void (*setup_apic_routing)(void);
	int (*multi_timer_check)(int apic, int irq);
	int (*apicid_to_node)(int logical_apicid);
	int (*cpu_to_logical_apicid)(int cpu);
	int (*cpu_present_to_apicid)(int mps_cpu);
	void (*apicid_to_cpu_present)(int phys_apicid, physid_mask_t *retmap);
	void (*setup_portio_remap)(void);
	int (*check_phys_apicid_present)(int phys_apicid);
	void (*enable_apic_mode)(void);
	int (*phys_pkg_id)(int cpuid_apic, int index_msb);
	/*
	 * When one of the next two hooks returns 1 the apic
	 * is switched to this. Essentially they are additional
	 * probe functions:
	 */
	int (*mps_oem_check)(struct mpc_table *mpc, char *oem, char *productid);
	unsigned int (*get_apic_id)(unsigned long x);
	unsigned long (*set_apic_id)(unsigned int id);
	unsigned long apic_id_mask;
	unsigned int (*cpu_mask_to_apicid)(const struct cpumask *cpumask);
	unsigned int (*cpu_mask_to_apicid_and)(const struct cpumask *cpumask,
					       const struct cpumask *andmask);
	/* ipi */
	void (*send_IPI_mask)(const struct cpumask *mask, int vector);
	void (*send_IPI_mask_allbutself)(const struct cpumask *mask,
					 int vector);
	void (*send_IPI_allbutself)(int vector);
	void (*send_IPI_all)(int vector);
	void (*send_IPI_self)(int vector);
	/* wakeup_secondary_cpu */
	int (*wakeup_secondary_cpu)(int apicid, unsigned long start_eip);
	int trampoline_phys_low;
	int trampoline_phys_high;
	void (*wait_for_init_deassert)(atomic_t *deassert);
	void (*smp_callin_clear_local_apic)(void);
	void (*inquire_remote_apic)(int apicid);
	/* apic ops */
	u32 (*read)(u32 reg);
	void (*write)(u32 reg, u32 v);
	u64 (*icr_read)(void);
	void (*icr_write)(u32 low, u32 high);
	void (*wait_icr_idle)(void);
	u32 (*safe_wait_icr_idle)(void);
};
/*
 * Pointer to the local APIC driver in use on this system (there's
 * always just one such driver in use - the kernel decides via an
 * early probing process which one it picks - and then sticks to it):
 */
extern struct apic *apic;
/*
 * APIC functionality to boot other CPUs - only used on SMP:
 */
extern atomic_t init_deasserted;
extern int wakeup_secondary_cpu_via_nmi(int apicid, unsigned long start_eip);
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 u32 apic_read(u32 reg)
{
	return apic->read(reg);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void apic_write(u32 reg, u32 val)
{
	apic->write(reg, val);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 u64 apic_icr_read(void)
{
	return apic->icr_read();
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void apic_icr_write(u32 low, u32 high)
{
	apic->icr_write(low, high);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void apic_wait_icr_idle(void)
{
	apic->wait_icr_idle();
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 u32 safe_apic_wait_icr_idle(void)
{
	return apic->safe_wait_icr_idle();
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void ack_APIC_irq(void)
{
	/*
	 * ack_APIC_irq() actually gets compiled as a single instruction
	 * ... yummie.
	 */
	/* Docs say use 0 for future compatibility */
	apic_write(0xB0, 0);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned default_get_apic_id(unsigned long x)
{
	unsigned int ver = ((apic_read(0x30)) & 0xFFu);
	if (((ver) >= 0x14) || (__builtin_constant_p((3*32+26)) && ( ((((3*32+26))>>5)==0 && (1UL<<(((3*32+26))&31) & (
#if !definedEx(CONFIG_MATH_EMULATION)
(1<<((0*32+ 0) & 31))
#endif
#if definedEx(CONFIG_MATH_EMULATION)
0
#endif
|
#if !definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_64) && definedEx(CONFIG_PARAVIRT)
0
#endif
#if definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT)
(1<<((0*32+ 3)) & 31)
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+ 5) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if !definedEx(CONFIG_X86_PAE) && definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_PAE)
(1<<((0*32+ 6) & 31))
#endif
#if !definedEx(CONFIG_X86_PAE) && !definedEx(CONFIG_X86_64)
0
#endif
| 
#if definedEx(CONFIG_X86_CMPXCHG64)
(1<<((0*32+ 8) & 31))
#endif
#if !definedEx(CONFIG_X86_CMPXCHG64)
0
#endif
|
#if !definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_64) && definedEx(CONFIG_PARAVIRT)
0
#endif
#if definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT)
(1<<((0*32+13)) & 31)
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+24) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if !definedEx(CONFIG_X86_64) && definedEx(CONFIG_X86_CMOV) || definedEx(CONFIG_X86_64)
(1<<((0*32+15) & 31))
#endif
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_X86_CMOV)
0
#endif
| 
#if definedEx(CONFIG_X86_64)
(1<<((0*32+25) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+26) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
))) || ((((3*32+26))>>5)==1 && (1UL<<(((3*32+26))&31) & (
#if definedEx(CONFIG_X86_64)
(1<<((1*32+29) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if definedEx(CONFIG_X86_USE_3DNOW)
(1<<((1*32+31) & 31))
#endif
#if !definedEx(CONFIG_X86_USE_3DNOW)
0
#endif
))) || ((((3*32+26))>>5)==2 && (1UL<<(((3*32+26))&31) & 0)) || ((((3*32+26))>>5)==3 && (1UL<<(((3*32+26))&31) & (
#if !definedEx(CONFIG_X86_64) && definedEx(CONFIG_X86_P6_NOP) || definedEx(CONFIG_X86_64)
(1<<((3*32+20) & 31))
#endif
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_X86_P6_NOP)
0
#endif
))) || ((((3*32+26))>>5)==4 && (1UL<<(((3*32+26))&31) & 0)) || ((((3*32+26))>>5)==5 && (1UL<<(((3*32+26))&31) & 0)) || ((((3*32+26))>>5)==6 && (1UL<<(((3*32+26))&31) & 0)) || ((((3*32+26))>>5)==7 && (1UL<<(((3*32+26))&31) & 0)) ) ? 1 : (__builtin_constant_p(((3*32+26))) ? constant_test_bit(((3*32+26)), ((unsigned long *)((&boot_cpu_data)->x86_capability))) : variable_test_bit(((3*32+26)), ((unsigned long *)((&boot_cpu_data)->x86_capability))))))
		return (x >> 24) & 0xFF;
	else
		return (x >> 24) & 0x0F;
}
/*
 * Warm reset vector default position:
 */
#if definedEx(CONFIG_X86_64)
extern struct apic apic_flat;
extern struct apic apic_physflat;
extern struct apic apic_x2apic_cluster;
extern struct apic apic_x2apic_phys;
extern int default_acpi_madt_oem_check(char *, char *);
extern void apic_send_IPI_self(int vector);
extern struct apic apic_x2apic_uv_x;
#if definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(".discard"), unused)) char __pcpu_scope_x2apic_extra_bits; extern __attribute__((section(".data.percpu" "")))  __typeof__(int) per_cpu__x2apic_extra_bits
#endif
#if !definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(".data.percpu" "")))  __typeof__(int) per_cpu__x2apic_extra_bits
#endif
;
extern int default_cpu_present_to_apicid(int mps_cpu);
extern int default_check_phys_apicid_present(int phys_apicid);
#endif
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void default_wait_for_init_deassert(atomic_t *deassert)
{
	while (!atomic_read(deassert))
		cpu_relax();
	return;
}
extern void generic_bigsmp_probe(void);
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/smp.h" 1
#line 466 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/apic.h" 2
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 const struct cpumask *default_target_cpus(void)
{
	return cpu_online_mask;
}
#if definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(".discard"), unused)) char __pcpu_scope_x86_bios_cpu_apicid; extern __attribute__((section(".data.percpu" "")))  __typeof__(u16) per_cpu__x86_bios_cpu_apicid
#endif
#if !definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(".data.percpu" "")))  __typeof__(u16) per_cpu__x86_bios_cpu_apicid
#endif
; extern __typeof__(u16) *x86_bios_cpu_apicid_early_ptr; extern __typeof__(u16) x86_bios_cpu_apicid_early_map[];
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned int read_apic_id(void)
{
	unsigned int reg;
	reg = apic_read(0x20);
	return apic->get_apic_id(reg);
}
extern void default_setup_apic_routing(void);
extern struct apic apic_noop;
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned int
default_cpu_mask_to_apicid(const struct cpumask *cpumask)
{
	return ((cpumask)->bits)[0] & 0xFFu;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned int
default_cpu_mask_to_apicid_and(const struct cpumask *cpumask,
			       const struct cpumask *andmask)
{
	unsigned long mask1 = ((cpumask)->bits)[0];
	unsigned long mask2 = ((andmask)->bits)[0];
	unsigned long mask3 = ((cpu_online_mask)->bits)[0];
	return (unsigned int)(mask1 & mask2 & mask3);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long default_check_apicid_used(physid_mask_t *map, int apicid)
{
	return (__builtin_constant_p((apicid)) ? constant_test_bit((apicid), ((*map).mask)) : variable_test_bit((apicid), ((*map).mask)));
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long default_check_apicid_present(int bit)
{
	return (__builtin_constant_p((bit)) ? constant_test_bit((bit), ((phys_cpu_present_map).mask)) : variable_test_bit((bit), ((phys_cpu_present_map).mask)));
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void default_ioapic_phys_id_map(physid_mask_t *phys_map, physid_mask_t *retmap)
{
	*retmap = *phys_map;
}
/* Mapping from cpu number to logical apicid */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int default_cpu_to_logical_apicid(int cpu)
{
	return 1 << cpu;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int __default_cpu_present_to_apicid(int mps_cpu)
{
	if (mps_cpu < nr_cpu_ids && (__builtin_constant_p((cpumask_check((mps_cpu)))) ? constant_test_bit((cpumask_check((mps_cpu))), ((((cpu_present_mask))->bits))) : variable_test_bit((cpumask_check((mps_cpu))), ((((cpu_present_mask))->bits)))))
		return (int)(*({ unsigned long __ptr; __asm__ ("" : "=r"(__ptr) : "0"((&per_cpu__x86_bios_cpu_apicid))); (typeof((&per_cpu__x86_bios_cpu_apicid))) (__ptr + (((__per_cpu_offset[mps_cpu])))); }));
	else
		return 0xFFFFu;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int
__default_check_phys_apicid_present(int phys_apicid)
{
	return (__builtin_constant_p((phys_apicid)) ? constant_test_bit((phys_apicid), ((phys_cpu_present_map).mask)) : variable_test_bit((phys_apicid), ((phys_cpu_present_map).mask)));
}
 extern int default_cpu_present_to_apicid(int mps_cpu);
extern int default_check_phys_apicid_present(int phys_apicid);
#line 15 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/smp.h" 2
#if definedEx(CONFIG_X86_IO_APIC)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/io_apic.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/io_apic.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/mpspec.h" 1
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/io_apic.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/apicdef.h" 1
#line 8 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/io_apic.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/irq_vectors.h" 1
/*
 * Linux IRQ vector layout.
 *
 * There are 256 IDT entries (per CPU - each entry is 8 bytes) which can
 * be defined by Linux. They are used as a jump table by the CPU when a
 * given vector is triggered - by a CPU-external, CPU-internal or
 * software-triggered event.
 *
 * Linux sets the kernel code address each entry jumps to early during
 * bootup, and never changes them. This is the general layout of the
 * IDT entries:
 *
 *  Vectors   0 ...  31 : system traps and exceptions - hardcoded events
 *  Vectors  32 ... 127 : device interrupts
 *  Vector  128         : legacy int80 syscall interface
 *  Vectors 129 ... 237 : device interrupts
 *  Vectors 238 ... 255 : special interrupts
 *
 * 64-bit x86 has per CPU IDT tables, 32-bit has one shared IDT table.
 *
 * This file enumerates the exact layout of them:
 */
/*
 * IDT vectors usable for external interrupt sources start
 * at 0x20:
 */
/*
 * Reserve the lowest usable priority level 0x20 - 0x2f for triggering
 * cleanup after irq migration.
 */
/*
 * Vectors 0x30-0x3f are used for ISA interrupts.
 */
/*
 * Special IRQ vectors used by the SMP architecture, 0xf0-0xff
 *
 *  some of the following vectors are 'rare', they are merged
 *  into a single vector (CALL_FUNCTION_VECTOR) to save vector space.
 *  TLB, reschedule and local APIC vectors are performance-critical.
 */
/*
 * Sanity check
 */
/* f0-f7 used for spreading out TLB flushes: */
/*
 * Local APIC timer IRQ vector is on a different priority level,
 * to work around the 'lost local interrupt if more than 2 IRQ
 * sources per level' errata.
 */
/*
 * Generic system vector for platform specific use
 */
/*
 * Performance monitoring pending work vector:
 */
/*
 * Self IPI vector for machine checks
 */
/*
 * First APIC vector available to drivers: (vectors 0x30-0xee) we
 * start at 0x31(0x41) to spread out vectors evenly between priority
 * levels. (0x80 is the syscall vector)
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int invalid_vm86_irq(int irq)
{
	return irq < 3 || irq > 15;
}
/*
 * Size the maximum number of interrupts.
 *
 * If the irq_desc[] array has a sparse layout, we can size things
 * generously - it scales up linearly with the maximum number of CPUs,
 * and the maximum number of IO-APICs, whichever is higher.
 *
 * In other cases we size more conservatively, to not create too large
 * static arrays.
 */
#if definedEx(CONFIG_SPARSE_IRQ)
#endif
#if !definedEx(CONFIG_SPARSE_IRQ)
#endif
#line 9 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/io_apic.h" 2
/*
 * Intel IO-APIC support for SMP and UP systems.
 *
 * Copyright (C) 1997, 1998, 1999, 2000 Ingo Molnar
 */
/* I/O Unit Redirection Table */
/*
 * The structure of the IO-APIC:
 */
union IO_APIC_reg_00 {
	u32	raw;
	struct {
		u32	__reserved_2	: 14,
			LTS		:  1,
			delivery_type	:  1,
			__reserved_1	:  8,
			ID		:  8;
	} __attribute__ ((packed)) bits;
};
union IO_APIC_reg_01 {
	u32	raw;
	struct {
		u32	version		:  8,
			__reserved_2	:  7,
			PRQ		:  1,
			entries		:  8,
			__reserved_1	:  8;
	} __attribute__ ((packed)) bits;
};
union IO_APIC_reg_02 {
	u32	raw;
	struct {
		u32	__reserved_2	: 24,
			arbitration	:  4,
			__reserved_1	:  4;
	} __attribute__ ((packed)) bits;
};
union IO_APIC_reg_03 {
	u32	raw;
	struct {
		u32	boot_DT		:  1,
			__reserved_1	: 31;
	} __attribute__ ((packed)) bits;
};
enum ioapic_irq_destination_types {
	dest_Fixed = 0,
	dest_LowestPrio = 1,
	dest_SMI = 2,
	dest__reserved_1 = 3,
	dest_NMI = 4,
	dest_INIT = 5,
	dest__reserved_2 = 6,
	dest_ExtINT = 7
};
struct IO_APIC_route_entry {
	__u32	vector		:  8,
		delivery_mode	:  3,	/* 000: FIXED
					 * 001: lowest prio
					 * 111: ExtINT
					 */
		dest_mode	:  1,	/* 0: physical, 1: logical */
		delivery_status	:  1,
		polarity	:  1,
		irr		:  1,
		trigger		:  1,	/* 0: edge, 1: level */
		mask		:  1,	/* 0: enabled, 1: disabled */
		__reserved_2	: 15;
	__u32	__reserved_3	: 24,
		dest		:  8;
} __attribute__ ((packed));
struct IR_IO_APIC_route_entry {
	__u64	vector		: 8,
		zero		: 3,
		index2		: 1,
		delivery_status : 1,
		polarity	: 1,
		irr		: 1,
		trigger		: 1,
		mask		: 1,
		reserved	: 31,
		format		: 1,
		index		: 15;
} __attribute__ ((packed));
/*
 * # of IO-APICs and # of IRQ routing registers
 */
extern int nr_ioapics;
extern int nr_ioapic_registers[128];
/* I/O APIC entries */
extern struct mpc_ioapic mp_ioapics[128];
/* # of MP IRQ source entries */
extern int mp_irq_entries;
/* MP IRQ source entries */
extern struct mpc_intsrc mp_irqs[(256 * 4)];
/* non-0 if default (table-less) MP configuration */
extern int mpc_default_type;
/* Older SiS APIC requires we rewrite the index register */
extern int sis_apic_bug;
/* 1 if "noapic" boot option passed */
extern int skip_ioapic_setup;
/* 1 if "noapic" boot option passed */
extern int noioapicquirk;
/* -1 if "noapic" boot option passed */
extern int noioapicreroute;
/* 1 if the timer IRQ uses the '8259A Virtual Wire' mode */
extern int timer_through_8259;
extern void io_apic_disable_legacy(void);
/*
 * If we use the IO-APIC for IRQ routing, disable automatic
 * assignment of PCI IRQ's.
 */
extern u8 io_apic_unique_id(u8 id);
extern int io_apic_get_unique_id(int ioapic, int apic_id);
extern int io_apic_get_version(int ioapic);
extern int io_apic_get_redir_entries(int ioapic);
struct io_apic_irq_attr;
extern int io_apic_set_pci_routing(struct device *dev, int irq,
		 struct io_apic_irq_attr *irq_attr);
void setup_IO_APIC_irq_extra(u32 gsi);
extern int (*ioapic_renumber_irq)(int ioapic, int irq);
extern void ioapic_init_mappings(void);
extern void ioapic_insert_resources(void);
extern struct IO_APIC_route_entry **alloc_ioapic_entries(void);
extern void free_ioapic_entries(struct IO_APIC_route_entry **ioapic_entries);
extern int save_IO_APIC_setup(struct IO_APIC_route_entry **ioapic_entries);
extern void mask_IO_APIC_setup(struct IO_APIC_route_entry **ioapic_entries);
extern int restore_IO_APIC_setup(struct IO_APIC_route_entry **ioapic_entries);
extern void probe_nr_irqs_gsi(void);
extern int setup_ioapic_entry(int apic, int irq,
			      struct IO_APIC_route_entry *entry,
			      unsigned int destination, int trigger,
			      int polarity, int vector, int pin);
extern void ioapic_write_entry(int apic, int pin,
			       struct IO_APIC_route_entry e);
extern void setup_ioapic_ids_from_mpc(void);
struct mp_ioapic_gsi{
	int gsi_base;
	int gsi_end;
};
extern struct mp_ioapic_gsi  mp_gsi_routing[];
int mp_find_ioapic(int gsi);
int mp_find_ioapic_pin(int ioapic, int gsi);
void __attribute__ ((__section__(".init.text"))) __attribute__((__cold__)) __attribute__((no_instrument_function)) mp_register_ioapic(int id, u32 address, u32 gsi_base);
#line 17 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/smp.h" 2
#endif
#endif
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/thread_info.h" 1
/* thread_info.h: low-level thread information
 *
 * Copyright (C) 2002  David Howells (dhowells@redhat.com)
 * - Incorporating suggestions made by Linus Torvalds and Dave Miller
 */
#line 20 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/smp.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/cpumask.h" 1
#line 21 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/smp.h" 2
extern int smp_num_siblings;
extern unsigned int num_processors;
#if definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(".discard"), unused)) char __pcpu_scope_cpu_sibling_map; extern __attribute__((section(".data.percpu" "")))  __typeof__(cpumask_var_t) per_cpu__cpu_sibling_map
#endif
#if !definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(".data.percpu" "")))  __typeof__(cpumask_var_t) per_cpu__cpu_sibling_map
#endif
;
#if definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(".discard"), unused)) char __pcpu_scope_cpu_core_map; extern __attribute__((section(".data.percpu" "")))  __typeof__(cpumask_var_t) per_cpu__cpu_core_map
#endif
#if !definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(".data.percpu" "")))  __typeof__(cpumask_var_t) per_cpu__cpu_core_map
#endif
;
#if definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(".discard"), unused)) char __pcpu_scope_cpu_llc_id; extern __attribute__((section(".data.percpu" "")))  __typeof__(u16) per_cpu__cpu_llc_id
#endif
#if !definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(".data.percpu" "")))  __typeof__(u16) per_cpu__cpu_llc_id
#endif
;
#if definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(".discard"), unused)) char __pcpu_scope_cpu_number; extern __attribute__((section(".data.percpu" "")))  __typeof__(int) per_cpu__cpu_number
#endif
#if !definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(".data.percpu" "")))  __typeof__(int) per_cpu__cpu_number
#endif
;
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 struct cpumask *cpu_sibling_mask(int cpu)
{
	return (*({ unsigned long __ptr; __asm__ ("" : "=r"(__ptr) : "0"((&per_cpu__cpu_sibling_map))); (typeof((&per_cpu__cpu_sibling_map))) (__ptr + (((__per_cpu_offset[cpu])))); }));
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 struct cpumask *cpu_core_mask(int cpu)
{
	return (*({ unsigned long __ptr; __asm__ ("" : "=r"(__ptr) : "0"((&per_cpu__cpu_core_map))); (typeof((&per_cpu__cpu_core_map))) (__ptr + (((__per_cpu_offset[cpu])))); }));
}
#if definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(".discard"), unused)) char __pcpu_scope_x86_cpu_to_apicid; extern __attribute__((section(".data.percpu" "")))  __typeof__(u16) per_cpu__x86_cpu_to_apicid
#endif
#if !definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(".data.percpu" "")))  __typeof__(u16) per_cpu__x86_cpu_to_apicid
#endif
; extern __typeof__(u16) *x86_cpu_to_apicid_early_ptr; extern __typeof__(u16) x86_cpu_to_apicid_early_map[];
#if definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(".discard"), unused)) char __pcpu_scope_x86_bios_cpu_apicid; extern __attribute__((section(".data.percpu" "")))  __typeof__(u16) per_cpu__x86_bios_cpu_apicid
#endif
#if !definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(".data.percpu" "")))  __typeof__(u16) per_cpu__x86_bios_cpu_apicid
#endif
; extern __typeof__(u16) *x86_bios_cpu_apicid_early_ptr; extern __typeof__(u16) x86_bios_cpu_apicid_early_map[];
/* Static state in head.S used to set up a CPU */
extern struct {
	void *sp;
	unsigned short ss;
} stack_start;
struct smp_ops {
	void (*smp_prepare_boot_cpu)(void);
	void (*smp_prepare_cpus)(unsigned max_cpus);
	void (*smp_cpus_done)(unsigned max_cpus);
	void (*smp_send_stop)(void);
	void (*smp_send_reschedule)(int cpu);
	int (*cpu_up)(unsigned cpu);
	int (*cpu_disable)(void);
	void (*cpu_die)(unsigned int cpu);
	void (*play_dead)(void);
	void (*send_call_func_ipi)(const struct cpumask *mask);
	void (*send_call_func_single_ipi)(int cpu);
};
/* Globals due to paravirt */
extern void set_cpu_sibling_map(int cpu);
#if !definedEx(CONFIG_PARAVIRT)
#endif
extern struct smp_ops smp_ops;
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void smp_send_stop(void)
{
	smp_ops.smp_send_stop();
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void smp_prepare_boot_cpu(void)
{
	smp_ops.smp_prepare_boot_cpu();
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void smp_prepare_cpus(unsigned int max_cpus)
{
	smp_ops.smp_prepare_cpus(max_cpus);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void smp_cpus_done(unsigned int max_cpus)
{
	smp_ops.smp_cpus_done(max_cpus);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int __cpu_up(unsigned int cpu)
{
	return smp_ops.cpu_up(cpu);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int __cpu_disable(void)
{
	return smp_ops.cpu_disable();
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __cpu_die(unsigned int cpu)
{
	smp_ops.cpu_die(cpu);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void play_dead(void)
{
	smp_ops.play_dead();
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void smp_send_reschedule(int cpu)
{
	smp_ops.smp_send_reschedule(cpu);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void arch_send_call_function_single_ipi(int cpu)
{
	smp_ops.send_call_func_single_ipi(cpu);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void arch_send_call_function_ipi_mask(const struct cpumask *mask)
{
	smp_ops.send_call_func_ipi(mask);
}
void cpu_disable_common(void);
void native_smp_prepare_boot_cpu(void);
void native_smp_prepare_cpus(unsigned int max_cpus);
void native_smp_cpus_done(unsigned int max_cpus);
int native_cpu_up(unsigned int cpunum);
int native_cpu_disable(void);
void native_cpu_die(unsigned int cpu);
void native_play_dead(void);
void play_dead_common(void);
void wbinvd_on_cpu(int cpu);
int wbinvd_on_all_cpus(void);
void native_send_call_func_ipi(const struct cpumask *mask);
void native_send_call_func_single_ipi(int cpu);
void smp_store_cpu_info(int id);
/* We don't mark CPUs online until __cpu_up(), so we need another measure */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int num_booting_cpus(void)
{
	return cpumask_weight(cpu_callout_mask);
}
extern unsigned disabled_cpus __attribute__ ((__section__(".cpuinit.data")));
#if definedEx(CONFIG_X86_32_SMP)
/*
 * This function is needed by all SMP systems. It must _always_ be valid
 * from the initial startup. We map APIC_BASE very early in page_setup(),
 * so this is correct in the x86 case.
 */
extern int safe_smp_processor_id(void);
#endif
#if !definedEx(CONFIG_X86_32_SMP) && definedEx(CONFIG_X86_64_SMP)
#endif
#if definedEx(CONFIG_X86_LOCAL_APIC)
#if !definedEx(CONFIG_X86_64)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int logical_smp_processor_id(void)
{
	/* we don't want to mark this access volatile - bad code generation */
	return (((apic_read(0xD0)) >> 24) & 0xFFu);
}
#endif
extern int hard_smp_processor_id(void);
#endif
#if !definedEx(CONFIG_X86_LOCAL_APIC)
#endif
#line 14 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/mmzone_64.h" 2
/* Simple perfect hash to map physical addresses to node numbers */
struct memnode {
	int shift;
	unsigned int mapsize;
	s16 *map;
	s16 embedded_map[64 - 8];
} __attribute__((__aligned__((1 << (5))))); /* total size = 128 bytes */
extern struct memnode memnode;
extern struct pglist_data *node_data[];
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __attribute__((pure)) int phys_to_nid(unsigned long addr)
{
	unsigned nid;
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_NUMA) && definedEx(CONFIG_DEBUG_VIRTUAL)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(!memnode.map), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/mmzone_64.h"), "i" (31), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (!memnode.map) ; } while(0)
#endif
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_NUMA) && !definedEx(CONFIG_DEBUG_VIRTUAL)
do { } while (0)
#endif
;
	nid = memnode.map[addr >> memnode.shift];
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_NUMA) && definedEx(CONFIG_DEBUG_VIRTUAL)
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(nid >=(1 << 3) || !node_data[nid]), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" "2:\t.long 1b - 2b, %c0 - 2b\n" "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/mmzone_64.h"), "i" (33), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (nid >=(1 << 3) || !node_data[nid]) ; } while(0)
#endif
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_NUMA) && !definedEx(CONFIG_DEBUG_VIRTUAL)
do { } while (0)
#endif
;
	return nid;
}
#if definedEx(CONFIG_NUMA_EMU)
#endif
#endif
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/mmzone.h" 2
#endif
#line 785 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/mmzone.h" 2
#endif
extern struct pglist_data *first_online_pgdat(void);
extern struct pglist_data *next_online_pgdat(struct pglist_data *pgdat);
extern struct zone *next_zone(struct zone *zone);
/**
 * for_each_online_pgdat - helper macro to iterate over all online nodes
 * @pgdat - pointer to a pg_data_t variable
 */
/**
 * for_each_zone - helper macro to iterate over all memory zones
 * @zone - pointer to struct zone variable
 *
 * The user only needs to declare the zone variable, for_each_zone
 * fills it in.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 struct zone *zonelist_zone(struct zoneref *zoneref)
{
	return zoneref->zone;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int zonelist_zone_idx(struct zoneref *zoneref)
{
	return zoneref->zone_idx;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int zonelist_node_idx(struct zoneref *zoneref)
{
#if definedEx(CONFIG_NUMA)
	/* zone_to_nid not available in this context */
	return zoneref->zone->node;
#endif
#if !definedEx(CONFIG_NUMA)
	return 0;
#endif
}
/**
 * next_zones_zonelist - Returns the next zone at or below highest_zoneidx within the allowed nodemask using a cursor within a zonelist as a starting point
 * @z - The cursor used as a starting point for the search
 * @highest_zoneidx - The zone index of the highest zone to return
 * @nodes - An optional nodemask to filter the zonelist with
 * @zone - The first suitable zone found is returned via this parameter
 *
 * This function returns the next zone at or below a given zone index that is
 * within the allowed nodemask using a cursor as the starting point for the
 * search. The zoneref returned is a cursor that represents the current zone
 * being examined. It should be advanced by one before calling
 * next_zones_zonelist again.
 */
struct zoneref *next_zones_zonelist(struct zoneref *z,
					enum zone_type highest_zoneidx,
					nodemask_t *nodes,
					struct zone **zone);
/**
 * first_zones_zonelist - Returns the first zone at or below highest_zoneidx within the allowed nodemask in a zonelist
 * @zonelist - The zonelist to search for a suitable zone
 * @highest_zoneidx - The zone index of the highest zone to return
 * @nodes - An optional nodemask to filter the zonelist with
 * @zone - The first suitable zone found is returned via this parameter
 *
 * This function returns the first zone at or below a given zone index that is
 * within the allowed nodemask. The zoneref returned is a cursor that can be
 * used to iterate the zonelist with next_zones_zonelist by advancing it by
 * one before calling.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 struct zoneref *first_zones_zonelist(struct zonelist *zonelist,
					enum zone_type highest_zoneidx,
					nodemask_t *nodes,
					struct zone **zone)
{
	return next_zones_zonelist(zonelist->_zonerefs, highest_zoneidx, nodes,
								zone);
}
/**
 * for_each_zone_zonelist_nodemask - helper macro to iterate over valid zones in a zonelist at or below a given zone index and within a nodemask
 * @zone - The current zone in the iterator
 * @z - The current pointer within zonelist->zones being iterated
 * @zlist - The zonelist being iterated
 * @highidx - The zone index of the highest zone to return
 * @nodemask - Nodemask allowed by the allocator
 *
 * This iterator iterates though all zones at or below a given zone index and
 * within a given nodemask
 */
/**
 * for_each_zone_zonelist - helper macro to iterate over valid zones in a zonelist at or below a given zone index
 * @zone - The current zone in the iterator
 * @z - The current pointer within zonelist->zones being iterated
 * @zlist - The zonelist being iterated
 * @highidx - The zone index of the highest zone to return
 *
 * This iterator iterates though all zones at or below a given zone index.
 */
#if definedEx(CONFIG_SPARSEMEM)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/sparsemem.h" 1
/*
 * generic non-linear memory support:
 *
 * 1) we will not split memory into more chunks than will fit into the flags
 *    field of the struct page
 *
 * SECTION_SIZE_BITS		2^n: size of each section
 * MAX_PHYSADDR_BITS		2^n: max size of physical address space
 * MAX_PHYSMEM_BITS		2^n: how much memory we can have in that space
 *
 */
#if definedEx(CONFIG_X86_32)
#if definedEx(CONFIG_X86_PAE)
#endif
#if !definedEx(CONFIG_X86_PAE)
#endif
#endif
#if !definedEx(CONFIG_X86_32)
#endif
#line 909 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/mmzone.h" 2
#endif
#if definedEx(CONFIG_FLATMEM)
#endif
#if definedEx(CONFIG_SPARSEMEM)
/*
 * SECTION_SHIFT    		#bits space required to store a section #
 *
 * PA_SECTION_SHIFT		physical address to/from section number
 * PFN_SECTION_SHIFT		pfn to/from section number
 */
struct page;
struct page_cgroup;
struct mem_section {
	/*
	 * This is, logically, a pointer to an array of struct
	 * pages.  However, it is stored with some other magic.
	 * (see sparse.c::sparse_init_one_section())
	 *
	 * Additionally during early boot we encode node id of
	 * the location of the section here to guide allocation.
	 * (see sparse.c::memory_present())
	 *
	 * Making it a UL at least makes someone do a cast
	 * before using it wrong.
	 */
	unsigned long section_mem_map;
	/* See declaration of similar field in struct zone */
	unsigned long *pageblock_flags;
#if definedEx(CONFIG_CGROUP_MEM_RES_CTLR)
	/*
	 * If !SPARSEMEM, pgdat doesn't have page_cgroup pointer. We use
	 * section. (see memcontrol.h/page_cgroup.h about this.)
	 */
	struct page_cgroup *page_cgroup;
	unsigned long pad;
#endif
};
#if definedEx(CONFIG_SPARSEMEM_EXTREME)
#endif
#if !definedEx(CONFIG_SPARSEMEM_EXTREME)
#endif
#if definedEx(CONFIG_SPARSEMEM_EXTREME)
extern struct mem_section *mem_section[((1UL << (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_SPARSEMEM) && definedEx(CONFIG_X86_PAE)
36
#endif
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_SPARSEMEM) && !definedEx(CONFIG_X86_PAE)
32
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_SPARSEMEM)
46
#endif
 - 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_SPARSEMEM) && definedEx(CONFIG_X86_PAE)
29
#endif
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_SPARSEMEM) && !definedEx(CONFIG_X86_PAE)
26
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_SPARSEMEM)
27
#endif
)) / (((1UL) << 12) / sizeof (struct mem_section)))];
#endif
#if !definedEx(CONFIG_SPARSEMEM_EXTREME)
extern struct mem_section mem_section[((1UL << (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_SPARSEMEM) && definedEx(CONFIG_X86_PAE)
36
#endif
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_SPARSEMEM) && !definedEx(CONFIG_X86_PAE)
32
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_SPARSEMEM)
46
#endif
 - 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_SPARSEMEM) && definedEx(CONFIG_X86_PAE)
29
#endif
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_SPARSEMEM) && !definedEx(CONFIG_X86_PAE)
26
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_SPARSEMEM)
27
#endif
)) / 1)][1];
#endif
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 struct mem_section *__nr_to_section(unsigned long nr)
{
	if (!mem_section[((nr) / 
#if definedEx(CONFIG_SPARSEMEM) && definedEx(CONFIG_SPARSEMEM_EXTREME)
(((1UL) << 12) / sizeof (struct mem_section))
#endif
#if definedEx(CONFIG_SPARSEMEM) && !definedEx(CONFIG_SPARSEMEM_EXTREME)
1
#endif
)])
		return ((void *)0);
	return &mem_section[((nr) / 
#if definedEx(CONFIG_SPARSEMEM) && definedEx(CONFIG_SPARSEMEM_EXTREME)
(((1UL) << 12) / sizeof (struct mem_section))
#endif
#if definedEx(CONFIG_SPARSEMEM) && !definedEx(CONFIG_SPARSEMEM_EXTREME)
1
#endif
)][nr & (
#if definedEx(CONFIG_SPARSEMEM) && definedEx(CONFIG_SPARSEMEM_EXTREME)
(((1UL) << 12) / sizeof (struct mem_section))
#endif
#if definedEx(CONFIG_SPARSEMEM) && !definedEx(CONFIG_SPARSEMEM_EXTREME)
1
#endif
 - 1)];
}
extern int __section_nr(struct mem_section* ms);
extern unsigned long usemap_size(void);
/*
 * We use the lower bits of the mem_map pointer to store
 * a little bit of information.  There should be at least
 * 3 bits here due to 32-bit alignment.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 struct page *__section_mem_map_addr(struct mem_section *section)
{
	unsigned long map = section->section_mem_map;
	map &= (~((1UL<<2)-1));
	return (struct page *)map;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int present_section(struct mem_section *section)
{
	return (section && (section->section_mem_map & (1UL<<0)));
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int present_section_nr(unsigned long nr)
{
	return present_section(__nr_to_section(nr));
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int valid_section(struct mem_section *section)
{
	return (section && (section->section_mem_map & (1UL<<1)));
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int valid_section_nr(unsigned long nr)
{
	return valid_section(__nr_to_section(nr));
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 struct mem_section *__pfn_to_section(unsigned long pfn)
{
	return __nr_to_section(((pfn) >> (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_SPARSEMEM) && definedEx(CONFIG_X86_PAE)
29
#endif
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_SPARSEMEM) && !definedEx(CONFIG_X86_PAE)
26
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_SPARSEMEM)
27
#endif
 - 12)));
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int pfn_valid(unsigned long pfn)
{
	if (((pfn) >> (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_SPARSEMEM) && definedEx(CONFIG_X86_PAE)
29
#endif
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_SPARSEMEM) && !definedEx(CONFIG_X86_PAE)
26
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_SPARSEMEM)
27
#endif
 - 12)) >= (1UL << (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_SPARSEMEM) && definedEx(CONFIG_X86_PAE)
36
#endif
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_SPARSEMEM) && !definedEx(CONFIG_X86_PAE)
32
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_SPARSEMEM)
46
#endif
 - 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_SPARSEMEM) && definedEx(CONFIG_X86_PAE)
29
#endif
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_SPARSEMEM) && !definedEx(CONFIG_X86_PAE)
26
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_SPARSEMEM)
27
#endif
)))
		return 0;
	return valid_section(__nr_to_section(((pfn) >> (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_SPARSEMEM) && definedEx(CONFIG_X86_PAE)
29
#endif
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_SPARSEMEM) && !definedEx(CONFIG_X86_PAE)
26
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_SPARSEMEM)
27
#endif
 - 12))));
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int pfn_present(unsigned long pfn)
{
	if (((pfn) >> (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_SPARSEMEM) && definedEx(CONFIG_X86_PAE)
29
#endif
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_SPARSEMEM) && !definedEx(CONFIG_X86_PAE)
26
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_SPARSEMEM)
27
#endif
 - 12)) >= (1UL << (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_SPARSEMEM) && definedEx(CONFIG_X86_PAE)
36
#endif
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_SPARSEMEM) && !definedEx(CONFIG_X86_PAE)
32
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_SPARSEMEM)
46
#endif
 - 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_SPARSEMEM) && definedEx(CONFIG_X86_PAE)
29
#endif
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_SPARSEMEM) && !definedEx(CONFIG_X86_PAE)
26
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_SPARSEMEM)
27
#endif
)))
		return 0;
	return present_section(__nr_to_section(((pfn) >> (
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_SPARSEMEM) && definedEx(CONFIG_X86_PAE)
29
#endif
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_SPARSEMEM) && !definedEx(CONFIG_X86_PAE)
26
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_SPARSEMEM)
27
#endif
 - 12))));
}
/*
 * These are _only_ used during initialisation, therefore they
 * can use __initdata ...  They could have names to indicate
 * this restriction.
 */
#if definedEx(CONFIG_NUMA)
#endif
#if !definedEx(CONFIG_NUMA)
#endif
void sparse_init(void);
#endif
#if !definedEx(CONFIG_SPARSEMEM)
#endif
#if definedEx(CONFIG_NODES_SPAN_OTHER_NODES)
bool early_pfn_in_nid(unsigned long pfn, int nid);
#endif
#if !definedEx(CONFIG_NODES_SPAN_OTHER_NODES)
#endif
#if !definedEx(CONFIG_SPARSEMEM)
#endif
void memory_present(int nid, unsigned long start, unsigned long end);
unsigned long __attribute__ ((__section__(".init.text"))) __attribute__((__cold__)) __attribute__((no_instrument_function)) node_memmap_size_bytes(int, unsigned long, unsigned long);
/*
 * If it is possible to have holes within a MAX_ORDER_NR_PAGES, then we
 * need to check pfn validility within that MAX_ORDER_NR_PAGES block.
 * pfn_valid_within() should be used in this case; we optimise this away
 * when we have no holes within a MAX_ORDER_NR_PAGES block.
 */
 static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int memmap_valid_within(unsigned long pfn,
					struct page *page, struct zone *zone)
{
	return 1;
}
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/gfp.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/stddef.h" 1
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/gfp.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/linkage.h" 1
#line 8 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/gfp.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/topology.h" 1
/*
 * include/linux/topology.h
 *
 * Written by: Matthew Dobson, IBM Corporation
 *
 * Copyright (C) 2002, IBM Corp.
 *
 * All rights reserved.
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation; either version 2 of the License, or
 * (at your option) any later version.
 *
 * This program is distributed in the hope that it will be useful, but
 * WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or
 * NON INFRINGEMENT.  See the GNU General Public License for more
 * details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program; if not, write to the Free Software
 * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
 *
 * Send feedback to <colpatch@us.ibm.com>
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/cpumask.h" 1
#line 32 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/topology.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/bitops.h" 1
#line 33 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/topology.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/mmzone.h" 1
#line 34 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/topology.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/smp.h" 1
/*
 *	Generic SMP support
 *		Alan Cox. <alan@redhat.com>
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/errno.h" 1
#line 11 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/smp.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 12 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/smp.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/list.h" 1
#line 13 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/smp.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/cpumask.h" 1
#line 14 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/smp.h" 2
extern void cpu_idle(void);
struct call_single_data {
	struct list_head list;
	void (*func) (void *info);
	void *info;
	u16 flags;
	u16 priv;
};
/* total number of cpus in this system (may exceed NR_CPUS) */
extern unsigned int total_cpus;
int smp_call_function_single(int cpuid, void (*func) (void *info), void *info,
				int wait);
#if definedEx(CONFIG_SMP)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/preempt.h" 1
#line 34 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/smp.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/kernel.h" 1
#line 35 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/smp.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/compiler.h" 1
#line 36 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/smp.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/thread_info.h" 1
/* thread_info.h: common low-level thread information accessors
 *
 * Copyright (C) 2002  David Howells (dhowells@redhat.com)
 * - Incorporating suggestions made by Linus Torvalds
 */
#line 37 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/smp.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/smp.h" 1
#if !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_NUMA) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/cpumask.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/smp.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/init.h" 1
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/smp.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/percpu.h" 1
#line 8 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/smp.h" 2
/*
 * We need the APIC definitions automatically as part of 'smp.h'
 */
#if definedEx(CONFIG_X86_LOCAL_APIC)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/mpspec.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/init.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/mpspec.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/mpspec_def.h" 1
/*
 * Structure definitions for SMP machines following the
 * Intel Multiprocessing Specification 1.1 and 1.4.
 */
/*
 * This tag identifies where the SMP configuration
 * information is.
 */
#if definedEx(CONFIG_X86_32)
#endif
#if !definedEx(CONFIG_X86_32)
#endif
/* Intel MP Floating Pointer Structure */
struct mpf_intel {
	char signature[4];		/* "_MP_"			*/
	unsigned int physptr;		/* Configuration table address	*/
	unsigned char length;		/* Our length (paragraphs)	*/
	unsigned char specification;	/* Specification version	*/
	unsigned char checksum;		/* Checksum (makes sum 0)	*/
	unsigned char feature1;		/* Standard or configuration ?	*/
	unsigned char feature2;		/* Bit7 set for IMCR|PIC	*/
	unsigned char feature3;		/* Unused (0)			*/
	unsigned char feature4;		/* Unused (0)			*/
	unsigned char feature5;		/* Unused (0)			*/
};
struct mpc_table {
	char signature[4];
	unsigned short length;		/* Size of table */
	char spec;			/* 0x01 */
	char checksum;
	char oem[8];
	char productid[12];
	unsigned int oemptr;		/* 0 if not present */
	unsigned short oemsize;		/* 0 if not present */
	unsigned short oemcount;
	unsigned int lapic;		/* APIC address */
	unsigned int reserved;
};
/* Followed by entries */
/* Used by IBM NUMA-Q to describe node locality */
struct mpc_cpu {
	unsigned char type;
	unsigned char apicid;		/* Local APIC number */
	unsigned char apicver;		/* Its versions */
	unsigned char cpuflag;
	unsigned int cpufeature;
	unsigned int featureflag;	/* CPUID feature value */
	unsigned int reserved[2];
};
struct mpc_bus {
	unsigned char type;
	unsigned char busid;
	unsigned char bustype[6];
};
/* List of Bus Type string values, Intel MP Spec. */
struct mpc_ioapic {
	unsigned char type;
	unsigned char apicid;
	unsigned char apicver;
	unsigned char flags;
	unsigned int apicaddr;
};
struct mpc_intsrc {
	unsigned char type;
	unsigned char irqtype;
	unsigned short irqflag;
	unsigned char srcbus;
	unsigned char srcbusirq;
	unsigned char dstapic;
	unsigned char dstirq;
};
enum mp_irq_source_types {
	mp_INT = 0,
	mp_NMI = 1,
	mp_SMI = 2,
	mp_ExtINT = 3
};
struct mpc_lintsrc {
	unsigned char type;
	unsigned char irqtype;
	unsigned short irqflag;
	unsigned char srcbusid;
	unsigned char srcbusirq;
	unsigned char destapic;
	unsigned char destapiclint;
};
struct mpc_oemtable {
	char signature[4];
	unsigned short length;		/* Size of table */
	char  rev;			/* 0x01 */
	char  checksum;
	char  mpc[8];
};
/*
 *	Default configurations
 *
 *	1	2 CPU ISA 82489DX
 *	2	2 CPU EISA 82489DX neither IRQ 0 timer nor IRQ 13 DMA chaining
 *	3	2 CPU EISA 82489DX
 *	4	2 CPU MCA 82489DX
 *	5	2 CPU ISA+PCI
 *	6	2 CPU EISA+PCI
 *	7	2 CPU MCA+PCI
 */
enum mp_bustype {
	MP_BUS_ISA = 1,
	MP_BUS_EISA,
	MP_BUS_PCI,
	MP_BUS_MCA,
};
#line 8 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/mpspec.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/x86_init.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/pgtable_types.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/x86_init.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/bootparam.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/bootparam.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/screen_info.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/screen_info.h" 2
/*
 * These are set up by the setup-routine at boot-time:
 */
struct screen_info {
	__u8  orig_x;		/* 0x00 */
	__u8  orig_y;		/* 0x01 */
	__u16 ext_mem_k;	/* 0x02 */
	__u16 orig_video_page;	/* 0x04 */
	__u8  orig_video_mode;	/* 0x06 */
	__u8  orig_video_cols;	/* 0x07 */
	__u8  flags;		/* 0x08 */
	__u8  unused2;		/* 0x09 */
	__u16 orig_video_ega_bx;/* 0x0a */
	__u16 unused3;		/* 0x0c */
	__u8  orig_video_lines;	/* 0x0e */
	__u8  orig_video_isVGA;	/* 0x0f */
	__u16 orig_video_points;/* 0x10 */
	/* VESA graphic mode -- linear frame buffer */
	__u16 lfb_width;	/* 0x12 */
	__u16 lfb_height;	/* 0x14 */
	__u16 lfb_depth;	/* 0x16 */
	__u32 lfb_base;		/* 0x18 */
	__u32 lfb_size;		/* 0x1c */
	__u16 cl_magic, cl_offset; /* 0x20 */
	__u16 lfb_linelength;	/* 0x24 */
	__u8  red_size;		/* 0x26 */
	__u8  red_pos;		/* 0x27 */
	__u8  green_size;	/* 0x28 */
	__u8  green_pos;	/* 0x29 */
	__u8  blue_size;	/* 0x2a */
	__u8  blue_pos;		/* 0x2b */
	__u8  rsvd_size;	/* 0x2c */
	__u8  rsvd_pos;		/* 0x2d */
	__u16 vesapm_seg;	/* 0x2e */
	__u16 vesapm_off;	/* 0x30 */
	__u16 pages;		/* 0x32 */
	__u16 vesa_attributes;	/* 0x34 */
	__u32 capabilities;     /* 0x36 */
	__u8  _reserved[6];	/* 0x3a */
} __attribute__((packed));
extern struct screen_info screen_info;
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/bootparam.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/apm_bios.h" 1
/*
 * Include file for the interface to an APM BIOS
 * Copyright 1994-2001 Stephen Rothwell (sfr@canb.auug.org.au)
 *
 * This program is free software; you can redistribute it and/or modify it
 * under the terms of the GNU General Public License as published by the
 * Free Software Foundation; either version 2, or (at your option) any
 * later version.
 *
 * This program is distributed in the hope that it will be useful, but
 * WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
 * General Public License for more details.
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 21 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/apm_bios.h" 2
typedef unsigned short	apm_event_t;
typedef unsigned short	apm_eventinfo_t;
struct apm_bios_info {
	__u16	version;
	__u16	cseg;
	__u32	offset;
	__u16	cseg_16;
	__u16	dseg;
	__u16	flags;
	__u16	cseg_len;
	__u16	cseg_16_len;
	__u16	dseg_len;
};
/* Results of APM Installation Check */
/*
 * Data for APM that is persistent across module unload/load
 */
struct apm_info {
	struct apm_bios_info	bios;
	unsigned short		connection_version;
	int			get_power_status_broken;
	int			get_power_status_swabinminutes;
	int			allow_ints;
	int			forbid_idle;
	int			realmode_power_off;
	int			disabled;
};
/*
 * The APM function codes
 */
/*
 * Function code for APM_FUNC_RESUME_TIMER
 */
/*
 * Function code for APM_FUNC_RESUME_ON_RING
 */
/*
 * Function code for APM_FUNC_TIMER_STATUS
 */
/*
 * in arch/i386/kernel/setup.c
 */
extern struct apm_info	apm_info;
/*
 * Power states
 */
/*
 * Events (results of Get PM Event)
 */
/*
 * Error codes
 */
/*
 * APM Device IDs
 */
/*
 * This is the "All Devices" ID communicated to the BIOS
 */
/*
 * Battery status
 */
/*
 * APM defined capability bit flags
 */
/*
 * ioctl operations
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/ioctl.h" 1
#line 217 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/apm_bios.h" 2
#line 8 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/bootparam.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/edd.h" 1
/*
 * linux/include/linux/edd.h
 *  Copyright (C) 2002, 2003, 2004 Dell Inc.
 *  by Matt Domsch <Matt_Domsch@dell.com>
 *
 * structures and definitions for the int 13h, ax={41,48}h
 * BIOS Enhanced Disk Drive Services
 * This is based on the T13 group document D1572 Revision 0 (August 14 2002)
 * available at http://www.t13.org/docs2002/d1572r0.pdf.  It is
 * very similar to D1484 Revision 3 http://www.t13.org/docs2002/d1484r3.pdf
 *
 * In a nutshell, arch/{i386,x86_64}/boot/setup.S populates a scratch
 * table in the boot_params that contains a list of BIOS-enumerated
 * boot devices.
 * In arch/{i386,x86_64}/kernel/setup.c, this information is
 * transferred into the edd structure, and in drivers/firmware/edd.c, that
 * information is used to identify BIOS boot disk.  The code in setup.S
 * is very sensitive to the size of these structures.
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License v2.0 as published by
 * the Free Software Foundation
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 35 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/edd.h" 2
struct edd_device_params {
	__u16 length;
	__u16 info_flags;
	__u32 num_default_cylinders;
	__u32 num_default_heads;
	__u32 sectors_per_track;
	__u64 number_of_sectors;
	__u16 bytes_per_sector;
	__u32 dpte_ptr;		/* 0xFFFFFFFF for our purposes */
	__u16 key;		/* = 0xBEDD */
	__u8 device_path_info_length;	/* = 44 */
	__u8 reserved2;
	__u16 reserved3;
	__u8 host_bus_type[4];
	__u8 interface_type[8];
	union {
		struct {
			__u16 base_address;
			__u16 reserved1;
			__u32 reserved2;
		} __attribute__ ((packed)) isa;
		struct {
			__u8 bus;
			__u8 slot;
			__u8 function;
			__u8 channel;
			__u32 reserved;
		} __attribute__ ((packed)) pci;
		/* pcix is same as pci */
		struct {
			__u64 reserved;
		} __attribute__ ((packed)) ibnd;
		struct {
			__u64 reserved;
		} __attribute__ ((packed)) xprs;
		struct {
			__u64 reserved;
		} __attribute__ ((packed)) htpt;
		struct {
			__u64 reserved;
		} __attribute__ ((packed)) unknown;
	} interface_path;
	union {
		struct {
			__u8 device;
			__u8 reserved1;
			__u16 reserved2;
			__u32 reserved3;
			__u64 reserved4;
		} __attribute__ ((packed)) ata;
		struct {
			__u8 device;
			__u8 lun;
			__u8 reserved1;
			__u8 reserved2;
			__u32 reserved3;
			__u64 reserved4;
		} __attribute__ ((packed)) atapi;
		struct {
			__u16 id;
			__u64 lun;
			__u16 reserved1;
			__u32 reserved2;
		} __attribute__ ((packed)) scsi;
		struct {
			__u64 serial_number;
			__u64 reserved;
		} __attribute__ ((packed)) usb;
		struct {
			__u64 eui;
			__u64 reserved;
		} __attribute__ ((packed)) i1394;
		struct {
			__u64 wwid;
			__u64 lun;
		} __attribute__ ((packed)) fibre;
		struct {
			__u64 identity_tag;
			__u64 reserved;
		} __attribute__ ((packed)) i2o;
		struct {
			__u32 array_number;
			__u32 reserved1;
			__u64 reserved2;
		} __attribute__ ((packed)) raid;
		struct {
			__u8 device;
			__u8 reserved1;
			__u16 reserved2;
			__u32 reserved3;
			__u64 reserved4;
		} __attribute__ ((packed)) sata;
		struct {
			__u64 reserved1;
			__u64 reserved2;
		} __attribute__ ((packed)) unknown;
	} device_path;
	__u8 reserved4;
	__u8 checksum;
} __attribute__ ((packed));
struct edd_info {
	__u8 device;
	__u8 version;
	__u16 interface_support;
	__u16 legacy_max_cylinder;
	__u8 legacy_max_head;
	__u8 legacy_sectors_per_track;
	struct edd_device_params params;
} __attribute__ ((packed));
struct edd {
	unsigned int mbr_signature[16];
	struct edd_info edd_info[6];
	unsigned char mbr_signature_nr;
	unsigned char edd_info_nr;
};
extern struct edd edd;
#line 9 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/bootparam.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/e820.h" 1
/*
 * Legacy E820 BIOS limits us to 128 (E820MAX) nodes due to the
 * constrained space in the zeropage.  If we have more nodes than
 * that, and if we've booted off EFI firmware, then the EFI tables
 * passed us from the EFI firmware can list more nodes.  Size our
 * internal memory map tables to have room for these additional
 * nodes, based on up to three entries per node for which the
 * kernel was built: MAX_NUMNODES == (1 << CONFIG_NODES_SHIFT),
 * plus E820MAX, allowing space for the possible duplicate E820
 * entries that might need room in the same arrays, prior to the
 * call to sanitize_e820_map() to remove duplicates.  The allowance
 * of three memory map entries per node is "enough" entries for
 * the initial hardware platform motivating this mechanism to make
 * use of additional EFI map entries.  Future platforms may want
 * to allow more than three entries per node or otherwise refine
 * this size.
 */
/*
 * Odd: 'make headers_check' complains about numa.h if I try
 * to collapse the next two #ifdef lines to a single line:
 *	#if defined(__KERNEL__) && defined(CONFIG_EFI)
 */
#if definedEx(CONFIG_EFI)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/numa.h" 1
#line 33 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/e820.h" 2
#endif
#if !definedEx(CONFIG_EFI)
#endif
/* reserved RAM used by kernel itself */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 54 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/e820.h" 2
struct e820entry {
	__u64 addr;	/* start of memory segment */
	__u64 size;	/* size of memory segment */
	__u32 type;	/* type of memory segment */
} __attribute__((packed));
struct e820map {
	__u32 nr_map;
	struct e820entry map[
#if !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) && definedEx(CONFIG_EFI) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) && definedEx(CONFIG_EFI) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) && definedEx(CONFIG_EFI) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) && definedEx(CONFIG_EFI) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_X86_LOCAL_APIC) && definedEx(CONFIG_EFI)
(128 + 3 * (1 << 
#if !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_NODES_SHIFT) || definedEx(CONFIG_NEED_MULTIPLE_NODES)
#if definedEx(CONFIG_NEED_MULTIPLE_NODES)
3
#endif
#if !definedEx(CONFIG_NEED_MULTIPLE_NODES)
CONFIG_NODES_SHIFT
#endif
#endif
#if !definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_NODES_SHIFT)
0
#endif
))
#endif
#if !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) && !definedEx(CONFIG_EFI) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) && !definedEx(CONFIG_EFI) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) && !definedEx(CONFIG_EFI) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) && !definedEx(CONFIG_EFI) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_X86_LOCAL_APIC) && !definedEx(CONFIG_EFI)
128
#endif
];
};
/* see comment in arch/x86/kernel/e820.c */
extern struct e820map e820;
extern struct e820map e820_saved;
extern unsigned long pci_mem_start;
extern int e820_any_mapped(u64 start, u64 end, unsigned type);
extern int e820_all_mapped(u64 start, u64 end, unsigned type);
extern void e820_add_region(u64 start, u64 size, int type);
extern void e820_print_map(char *who);
extern int
sanitize_e820_map(struct e820entry *biosmap, int max_nr_map, u32 *pnr_map);
extern u64 e820_update_range(u64 start, u64 size, unsigned old_type,
			       unsigned new_type);
extern u64 e820_remove_range(u64 start, u64 size, unsigned old_type,
			     int checktype);
extern void update_e820(void);
extern void e820_setup_gap(void);
extern int e820_search_gap(unsigned long *gapstart, unsigned long *gapsize,
			unsigned long start_addr, unsigned long long end_addr);
struct setup_data;
extern void parse_e820_ext(struct setup_data *data, unsigned long pa_data);
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_HIBERNATION) && definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_HIBERNATION)
extern void e820_mark_nosave_regions(unsigned long limit_pfn);
#endif
#if !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_HIBERNATION) && !definedEx(CONFIG_X86_64)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void e820_mark_nosave_regions(unsigned long limit_pfn)
{
}
#endif
#if definedEx(CONFIG_MEMTEST)
extern void early_memtest(unsigned long start, unsigned long end);
#endif
#if !definedEx(CONFIG_MEMTEST)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void early_memtest(unsigned long start, unsigned long end)
{
}
#endif
extern unsigned long end_user_pfn;
extern u64 find_e820_area(u64 start, u64 end, u64 size, u64 align);
extern u64 find_e820_area_size(u64 start, u64 *sizep, u64 align);
extern void reserve_early(u64 start, u64 end, char *name);
extern void reserve_early_overlap_ok(u64 start, u64 end, char *name);
extern void free_early(u64 start, u64 end);
extern void early_res_to_bootmem(u64 start, u64 end);
extern u64 early_reserve_e820(u64 startt, u64 sizet, u64 align);
extern unsigned long e820_end_of_ram_pfn(void);
extern unsigned long e820_end_of_low_ram_pfn(void);
extern int e820_find_active_region(const struct e820entry *ei,
				  unsigned long start_pfn,
				  unsigned long last_pfn,
				  unsigned long *ei_startpfn,
				  unsigned long *ei_endpfn);
extern void e820_register_active_regions(int nid, unsigned long start_pfn,
					 unsigned long end_pfn);
extern u64 e820_hole_size(u64 start, u64 end);
extern void finish_e820_parsing(void);
extern void e820_reserve_resources(void);
extern void e820_reserve_resources_late(void);
extern void setup_memory_map(void);
extern char *default_machine_specific_memory_setup(void);
/*
 * Returns true iff the specified range [s,e) is completely contained inside
 * the ISA region.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 bool is_ISA_range(u64 s, u64 e)
{
	return s >= 0xa0000 && e <= 0x100000;
}
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/ioport.h" 1
/*
 * ioport.h	Definitions of routines for detecting, reserving and
 *		allocating system resources.
 *
 * Authors:	Linus Torvalds
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/compiler.h" 1
#line 14 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/ioport.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 15 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/ioport.h" 2
/*
 * Resources are tree-like, allowing
 * nesting etc..
 */
struct resource {
	resource_size_t start;
	resource_size_t end;
	const char *name;
	unsigned long flags;
	struct resource *parent, *sibling, *child;
};
struct resource_list {
	struct resource_list *next;
	struct resource *res;
	struct pci_dev *dev;
};
/*
 * IO resources have these defined flags.
 */
/* PnP IRQ specific bits (IORESOURCE_BITS) */
/* PnP DMA specific bits (IORESOURCE_BITS) */
/* PnP memory I/O specific bits (IORESOURCE_BITS) */
/* PnP I/O specific bits (IORESOURCE_BITS) */
/* PCI ROM control bits (IORESOURCE_BITS) */
/* PCI control bits.  Shares IORESOURCE_BITS with above PCI ROM.  */
/* PC/ISA/whatever - the normal PC address spaces: IO and memory */
extern struct resource ioport_resource;
extern struct resource iomem_resource;
extern int request_resource(struct resource *root, struct resource *new);
extern int release_resource(struct resource *new);
extern void reserve_region_with_split(struct resource *root,
			     resource_size_t start, resource_size_t end,
			     const char *name);
extern int insert_resource(struct resource *parent, struct resource *new);
extern void insert_resource_expand_to_fit(struct resource *root, struct resource *new);
extern int allocate_resource(struct resource *root, struct resource *new,
			     resource_size_t size, resource_size_t min,
			     resource_size_t max, resource_size_t align,
			     void (*alignf)(void *, struct resource *,
					    resource_size_t, resource_size_t),
			     void *alignf_data);
int adjust_resource(struct resource *res, resource_size_t start,
		    resource_size_t size);
resource_size_t resource_alignment(struct resource *res);
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 resource_size_t resource_size(const struct resource *res)
{
	return res->end - res->start + 1;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long resource_type(const struct resource *res)
{
	return res->flags & 0x00000f00;
}
/* Convenience shorthand with allocation */
extern struct resource * __request_region(struct resource *,
					resource_size_t start,
					resource_size_t n,
					const char *name, int flags);
/* Compatibility cruft */
extern int __check_region(struct resource *, resource_size_t, resource_size_t);
extern void __release_region(struct resource *, resource_size_t,
				resource_size_t);
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int 
#if definedEx(CONFIG_ENABLE_WARN_DEPRECATED)
__attribute__((deprecated))
#endif
#if !definedEx(CONFIG_ENABLE_WARN_DEPRECATED)
#endif
 check_region(resource_size_t s,
						resource_size_t n)
{
	return __check_region(&ioport_resource, s, n);
}
/* Wrappers for managed devices */
struct device;
extern struct resource * __devm_request_region(struct device *dev,
				struct resource *parent, resource_size_t start,
				resource_size_t n, const char *name);
extern void __devm_release_region(struct device *dev, struct resource *parent,
				  resource_size_t start, resource_size_t n);
extern int iomem_map_sanity_check(resource_size_t addr, unsigned long size);
extern int iomem_is_exclusive(u64 addr);
extern int
walk_system_ram_range(unsigned long start_pfn, unsigned long nr_pages,
		void *arg, int (*func)(unsigned long, unsigned long, void *));
#line 151 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/e820.h" 2
#line 10 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/bootparam.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/ist.h" 1
/*
 * Include file for the interface to IST BIOS
 * Copyright 2002 Andy Grover <andrew.grover@intel.com>
 *
 * This program is free software; you can redistribute it and/or modify it
 * under the terms of the GNU General Public License as published by the
 * Free Software Foundation; either version 2, or (at your option) any
 * later version.
 *
 * This program is distributed in the hope that it will be useful, but
 * WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
 * General Public License for more details.
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 22 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/ist.h" 2
struct ist_info {
	__u32 signature;
	__u32 command;
	__u32 event;
	__u32 perf_level;
};
extern struct ist_info ist_info;
#line 11 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/bootparam.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/video/edid.h" 1
struct edid_info {
	unsigned char dummy[128];
};
extern struct edid_info edid_info;
#line 12 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/bootparam.h" 2
/* setup data types */
/* extensible setup data list node */
struct setup_data {
	__u64 next;
	__u32 type;
	__u32 len;
	__u8 data[0];
};
struct setup_header {
	__u8	setup_sects;
	__u16	root_flags;
	__u32	syssize;
	__u16	ram_size;
	__u16	vid_mode;
	__u16	root_dev;
	__u16	boot_flag;
	__u16	jump;
	__u32	header;
	__u16	version;
	__u32	realmode_swtch;
	__u16	start_sys;
	__u16	kernel_version;
	__u8	type_of_loader;
	__u8	loadflags;
	__u16	setup_move_size;
	__u32	code32_start;
	__u32	ramdisk_image;
	__u32	ramdisk_size;
	__u32	bootsect_kludge;
	__u16	heap_end_ptr;
	__u8	ext_loader_ver;
	__u8	ext_loader_type;
	__u32	cmd_line_ptr;
	__u32	initrd_addr_max;
	__u32	kernel_alignment;
	__u8	relocatable_kernel;
	__u8	_pad2[3];
	__u32	cmdline_size;
	__u32	hardware_subarch;
	__u64	hardware_subarch_data;
	__u32	payload_offset;
	__u32	payload_length;
	__u64	setup_data;
} __attribute__((packed));
struct sys_desc_table {
	__u16 length;
	__u8  table[14];
};
struct efi_info {
	__u32 efi_loader_signature;
	__u32 efi_systab;
	__u32 efi_memdesc_size;
	__u32 efi_memdesc_version;
	__u32 efi_memmap;
	__u32 efi_memmap_size;
	__u32 efi_systab_hi;
	__u32 efi_memmap_hi;
};
/* The so-called "zeropage" */
struct boot_params {
	struct screen_info screen_info;			/* 0x000 */
	struct apm_bios_info apm_bios_info;		/* 0x040 */
	__u8  _pad2[4];					/* 0x054 */
	__u64  tboot_addr;				/* 0x058 */
	struct ist_info ist_info;			/* 0x060 */
	__u8  _pad3[16];				/* 0x070 */
	__u8  hd0_info[16];	/* obsolete! */		/* 0x080 */
	__u8  hd1_info[16];	/* obsolete! */		/* 0x090 */
	struct sys_desc_table sys_desc_table;		/* 0x0a0 */
	__u8  _pad4[144];				/* 0x0b0 */
	struct edid_info edid_info;			/* 0x140 */
	struct efi_info efi_info;			/* 0x1c0 */
	__u32 alt_mem_k;				/* 0x1e0 */
	__u32 scratch;		/* Scratch field! */	/* 0x1e4 */
	__u8  e820_entries;				/* 0x1e8 */
	__u8  eddbuf_entries;				/* 0x1e9 */
	__u8  edd_mbr_sig_buf_entries;			/* 0x1ea */
	__u8  _pad6[6];					/* 0x1eb */
	struct setup_header hdr;    /* setup header */	/* 0x1f1 */
	__u8  _pad7[0x290-0x1f1-sizeof(struct setup_header)];
	__u32 edd_mbr_sig_buffer[16];	/* 0x290 */
	struct e820entry e820_map[128];		/* 0x2d0 */
	__u8  _pad8[48];				/* 0xcd0 */
	struct edd_info eddbuf[6];		/* 0xd00 */
	__u8  _pad9[276];				/* 0xeec */
} __attribute__((packed));
enum {
	X86_SUBARCH_PC = 0,
	X86_SUBARCH_LGUEST,
	X86_SUBARCH_XEN,
	X86_SUBARCH_MRST,
	X86_NR_SUBARCHS,
};
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/x86_init.h" 2
struct mpc_bus;
struct mpc_cpu;
struct mpc_table;
/**
 * struct x86_init_mpparse - platform specific mpparse ops
 * @mpc_record:			platform specific mpc record accounting
 * @setup_ioapic_ids:		platform specific ioapic id override
 * @mpc_apic_id:		platform specific mpc apic id assignment
 * @smp_read_mpc_oem:		platform specific oem mpc table setup
 * @mpc_oem_pci_bus:		platform specific pci bus setup (default NULL)
 * @mpc_oem_bus_info:		platform specific mpc bus info
 * @find_smp_config:		find the smp configuration
 * @get_smp_config:		get the smp configuration
 */
struct x86_init_mpparse {
	void (*mpc_record)(unsigned int mode);
	void (*setup_ioapic_ids)(void);
	int (*mpc_apic_id)(struct mpc_cpu *m);
	void (*smp_read_mpc_oem)(struct mpc_table *mpc);
	void (*mpc_oem_pci_bus)(struct mpc_bus *m);
	void (*mpc_oem_bus_info)(struct mpc_bus *m, char *name);
	void (*find_smp_config)(void);
	void (*get_smp_config)(unsigned int early);
};
/**
 * struct x86_init_resources - platform specific resource related ops
 * @probe_roms:			probe BIOS roms
 * @reserve_resources:		reserve the standard resources for the
 *				platform
 * @memory_setup:		platform specific memory setup
 *
 */
struct x86_init_resources {
	void (*probe_roms)(void);
	void (*reserve_resources)(void);
	char *(*memory_setup)(void);
};
/**
 * struct x86_init_irqs - platform specific interrupt setup
 * @pre_vector_init:		init code to run before interrupt vectors
 *				are set up.
 * @intr_init:			interrupt init code
 * @trap_init:			platform specific trap setup
 */
struct x86_init_irqs {
	void (*pre_vector_init)(void);
	void (*intr_init)(void);
	void (*trap_init)(void);
};
/**
 * struct x86_init_oem - oem platform specific customizing functions
 * @arch_setup:			platform specific architecure setup
 * @banner:			print a platform specific banner
 */
struct x86_init_oem {
	void (*arch_setup)(void);
	void (*banner)(void);
};
/**
 * struct x86_init_paging - platform specific paging functions
 * @pagetable_setup_start:	platform specific pre paging_init() call
 * @pagetable_setup_done:	platform specific post paging_init() call
 */
struct x86_init_paging {
	void (*pagetable_setup_start)(pgd_t *base);
	void (*pagetable_setup_done)(pgd_t *base);
};
/**
 * struct x86_init_timers - platform specific timer setup
 * @setup_perpcu_clockev:	set up the per cpu clock event device for the
 *				boot cpu
 * @tsc_pre_init:		platform function called before TSC init
 * @timer_init:			initialize the platform timer (default PIT/HPET)
 */
struct x86_init_timers {
	void (*setup_percpu_clockev)(void);
	void (*tsc_pre_init)(void);
	void (*timer_init)(void);
};
/**
 * struct x86_init_iommu - platform specific iommu setup
 * @iommu_init:			platform specific iommu setup
 */
struct x86_init_iommu {
	int (*iommu_init)(void);
};
/**
 * struct x86_init_ops - functions for platform specific setup
 *
 */
struct x86_init_ops {
	struct x86_init_resources	resources;
	struct x86_init_mpparse		mpparse;
	struct x86_init_irqs		irqs;
	struct x86_init_oem		oem;
	struct x86_init_paging		paging;
	struct x86_init_timers		timers;
	struct x86_init_iommu		iommu;
};
/**
 * struct x86_cpuinit_ops - platform specific cpu hotplug setups
 * @setup_percpu_clockev:	set up the per cpu clock event device
 */
struct x86_cpuinit_ops {
	void (*setup_percpu_clockev)(void);
};
/**
 * struct x86_platform_ops - platform specific runtime functions
 * @calibrate_tsc:		calibrate TSC
 * @get_wallclock:		get time from HW clock like RTC etc.
 * @set_wallclock:		set time back to HW clock
 * @is_untracked_pat_range	exclude from PAT logic
 */
struct x86_platform_ops {
	unsigned long (*calibrate_tsc)(void);
	unsigned long (*get_wallclock)(void);
	int (*set_wallclock)(unsigned long nowtime);
	void (*iommu_shutdown)(void);
	bool (*is_untracked_pat_range)(u64 start, u64 end);
};
extern struct x86_init_ops x86_init;
extern struct x86_cpuinit_ops x86_cpuinit;
extern struct x86_platform_ops x86_platform;
extern void x86_init_noop(void);
extern void x86_init_uint_noop(unsigned int unused);
#line 9 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/mpspec.h" 2
extern int apic_version[
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_X86_LOCAL_APIC)
256
#endif
#if !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC)
255
#endif
];
extern int pic_mode;
#if definedEx(CONFIG_X86_32)
/*
 * Summit or generic (i.e. installer) kernels need lots of bus entries.
 * Maximum 256 PCI busses, plus 1 ISA bus in each of 4 cabinets.
 */
extern unsigned int def_to_bigsmp;
extern u8 apicid_2_node[];
#if definedEx(CONFIG_X86_NUMAQ)
extern int mp_bus_id_to_node[260];
extern int mp_bus_id_to_local[260];
extern int quad_local_to_mp_bus_id [8/4][4];
#endif
#endif
#if !definedEx(CONFIG_X86_32)
/* Each PCI slot may be a combo card with its own bus.  4 IRQ pins per slot. */
#endif
#if !definedEx(CONFIG_MCA) && definedEx(CONFIG_EISA) || definedEx(CONFIG_MCA)
extern int mp_bus_id_to_type[
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_X86_LOCAL_APIC)
260
#endif
#if !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC)
256
#endif
];
#endif
extern unsigned long mp_bus_not_pci[(((
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_X86_LOCAL_APIC)
260
#endif
#if !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC)
256
#endif
) + (8 * sizeof(long)) - 1) / (8 * sizeof(long)))];
extern unsigned int boot_cpu_physical_apicid;
extern unsigned int max_physical_apicid;
extern int mpc_default_type;
extern unsigned long mp_lapic_addr;
extern int smp_found_config;
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void get_smp_config(void)
{
	x86_init.mpparse.get_smp_config(0);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void early_get_smp_config(void)
{
	x86_init.mpparse.get_smp_config(1);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void find_smp_config(void)
{
	x86_init.mpparse.find_smp_config();
}
#if definedEx(CONFIG_X86_MPPARSE)
extern void early_reserve_e820_mpc_new(void);
extern int enable_update_mptable;
extern int default_mpc_apic_id(struct mpc_cpu *m);
extern void default_smp_read_mpc_oem(struct mpc_table *mpc);
#if definedEx(CONFIG_X86_IO_APIC)
extern void default_mpc_oem_bus_info(struct mpc_bus *m, char *str);
#endif
#if !definedEx(CONFIG_X86_IO_APIC)
#endif
extern void default_find_smp_config(void);
extern void default_get_smp_config(unsigned int early);
#endif
#if !definedEx(CONFIG_X86_MPPARSE)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void early_reserve_e820_mpc_new(void) { }
#endif
void __attribute__ ((__section__(".cpuinit.text"))) __attribute__((__cold__)) generic_processor_info(int apicid, int version);
#if definedEx(CONFIG_ACPI)
extern void mp_register_ioapic(int id, u32 address, u32 gsi_base);
extern void mp_override_legacy_irq(u8 bus_irq, u8 polarity, u8 trigger,
				   u32 gsi);
extern void mp_config_acpi_legacy_irqs(void);
struct device;
extern int mp_register_gsi(struct device *dev, u32 gsi, int edge_level,
				 int active_high_low);
extern int acpi_probe_gsi(void);
#if definedEx(CONFIG_X86_IO_APIC)
extern int mp_find_ioapic(int gsi);
extern int mp_find_ioapic_pin(int ioapic, int gsi);
#endif
#endif
#if !definedEx(CONFIG_ACPI)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int acpi_probe_gsi(void)
{
	return 0;
}
#endif
struct physid_mask {
	unsigned long mask[(((
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_X86_LOCAL_APIC)
256
#endif
#if !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC)
255
#endif
) + (8 * sizeof(long)) - 1) / (8 * sizeof(long)))];
};
typedef struct physid_mask physid_mask_t;
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long physids_coerce(physid_mask_t *map)
{
	return map->mask[0];
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void physids_promote(unsigned long physids, physid_mask_t *map)
{
	bitmap_zero((*map).mask, 
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_X86_LOCAL_APIC)
256
#endif
#if !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC)
255
#endif
);
	map->mask[0] = physids;
}
/* Note: will create very large stack frames if physid_mask_t is big */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void physid_set_mask_of_physid(int physid, physid_mask_t *map)
{
	bitmap_zero((*map).mask, 
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_X86_LOCAL_APIC)
256
#endif
#if !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC)
255
#endif
);
	set_bit(physid, (*map).mask);
}
extern physid_mask_t phys_cpu_present_map;
extern int generic_mps_oem_check(struct mpc_table *, char *, char *);
extern int default_acpi_madt_oem_check(char *, char *);
#line 14 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/smp.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/apic.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/cpumask.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/apic.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/delay.h" 1
/*
 * Copyright (C) 1993 Linus Torvalds
 *
 * Delay routines, using a pre-computed "loops_per_jiffy" value.
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/kernel.h" 1
#line 12 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/delay.h" 2
extern unsigned long loops_per_jiffy;
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/delay.h" 1
/*
 * Copyright (C) 1993 Linus Torvalds
 *
 * Delay routines calling functions in arch/x86/lib/delay.c
 */
/* Undefined functions to get compile-time errors */
extern void __bad_udelay(void);
extern void __bad_ndelay(void);
extern void __udelay(unsigned long usecs);
extern void __ndelay(unsigned long nsecs);
extern void __const_udelay(unsigned long xloops);
extern void __delay(unsigned long loops);
/* 0x10c7 is 2**32 / 1000000 (rounded up) */
/* 0x5 is 2**32 / 1000000000 (rounded up) */
void use_tsc_delay(void);
#line 16 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/delay.h" 2
/*
 * Using udelay() for intervals greater than a few milliseconds can
 * risk overflow for high loops_per_jiffy (high bogomips) machines. The
 * mdelay() provides a wrapper to prevent this.  For delays greater
 * than MAX_UDELAY_MS milliseconds, the wrapper is used.  Architecture
 * specific values can be defined in asm-???/delay.h as an override.
 * The 2nd mdelay() definition ensures GCC will optimize away the 
 * while loop for the common cases where n <= MAX_UDELAY_MS  --  Paul G.
 */
extern unsigned long lpj_fine;
void calibrate_delay(void);
void msleep(unsigned int msecs);
unsigned long msleep_interruptible(unsigned int msecs);
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void ssleep(unsigned int seconds)
{
	msleep(seconds * 1000);
}
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/apic.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/pm.h" 1
/*
 *  pm.h - Power management interface
 *
 *  Copyright (C) 2000 Andrew Henroid
 *
 *  This program is free software; you can redistribute it and/or modify
 *  it under the terms of the GNU General Public License as published by
 *  the Free Software Foundation; either version 2 of the License, or
 *  (at your option) any later version.
 *
 *  This program is distributed in the hope that it will be useful,
 *  but WITHOUT ANY WARRANTY; without even the implied warranty of
 *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *  GNU General Public License for more details.
 *
 *  You should have received a copy of the GNU General Public License
 *  along with this program; if not, write to the Free Software
 *  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/list.h" 1
#line 26 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/pm.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/workqueue.h" 1
/*
 * workqueue.h --- work queue handling for Linux.
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/timer.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/list.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/timer.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/ktime.h" 1
/*
 *  include/linux/ktime.h
 *
 *  ktime_t - nanosecond-resolution time format.
 *
 *   Copyright(C) 2005, Thomas Gleixner <tglx@linutronix.de>
 *   Copyright(C) 2005, Red Hat, Inc., Ingo Molnar
 *
 *  data type definitions, declarations, prototypes and macros.
 *
 *  Started by: Thomas Gleixner and Ingo Molnar
 *
 *  Credits:
 *
 *  	Roman Zippel provided the ideas and primary code snippets of
 *  	the ktime_t union and further simplifications of the original
 *  	code.
 *
 *  For licencing details see kernel-base/COPYING
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/time.h" 1
#line 26 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/ktime.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/jiffies.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/math64.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/jiffies.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/kernel.h" 1
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/jiffies.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 8 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/jiffies.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/time.h" 1
#line 9 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/jiffies.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/timex.h" 1
/*****************************************************************************
 *                                                                           *
 * Copyright (c) David L. Mills 1993                                         *
 *                                                                           *
 * Permission to use, copy, modify, and distribute this software and its     *
 * documentation for any purpose and without fee is hereby granted, provided *
 * that the above copyright notice appears in all copies and that both the   *
 * copyright notice and this permission notice appear in supporting          *
 * documentation, and that the name University of Delaware not be used in    *
 * advertising or publicity pertaining to distribution of the software       *
 * without specific, written prior permission.  The University of Delaware   *
 * makes no representations about the suitability this software for any      *
 * purpose.  It is provided "as is" without express or implied warranty.     *
 *                                                                           *
 *****************************************************************************/
/*
 * Modification history timex.h
 *
 * 29 Dec 97	Russell King
 *	Moved CLOCK_TICK_RATE, CLOCK_TICK_FACTOR and FINETUNE to asm/timex.h
 *	for ARM machines
 *
 *  9 Jan 97    Adrian Sun
 *      Shifted LATCH define to allow access to alpha machines.
 *
 * 26 Sep 94	David L. Mills
 *	Added defines for hybrid phase/frequency-lock loop.
 *
 * 19 Mar 94	David L. Mills
 *	Moved defines from kernel routines to header file and added new
 *	defines for PPS phase-lock loop.
 *
 * 20 Feb 94	David L. Mills
 *	Revised status codes and structures for external clock and PPS
 *	signal discipline.
 *
 * 28 Nov 93	David L. Mills
 *	Adjusted parameters to improve stability and increase poll
 *	interval.
 *
 * 17 Sep 93    David L. Mills
 *      Created file $NTP/include/sys/timex.h
 * 07 Oct 93    Torsten Duwe
 *      Derived linux/timex.h
 * 1995-08-13    Torsten Duwe
 *      kernel PLL updated to 1994-12-13 specs (rfc-1589)
 * 1997-08-30    Ulrich Windl
 *      Added new constant NTP_PHASE_LIMIT
 * 2004-08-12    Christoph Lameter
 *      Reworked time interpolation logic
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/time.h" 1
#line 58 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/timex.h" 2
/*
 * syscall interface - used (mainly by NTP daemon)
 * to discipline kernel clock oscillator
 */
struct timex {
	unsigned int modes;	/* mode selector */
	long offset;		/* time offset (usec) */
	long freq;		/* frequency offset (scaled ppm) */
	long maxerror;		/* maximum error (usec) */
	long esterror;		/* estimated error (usec) */
	int status;		/* clock command/status */
	long constant;		/* pll time constant */
	long precision;		/* clock precision (usec) (read only) */
	long tolerance;		/* clock frequency tolerance (ppm)
				 * (read only)
				 */
	struct timeval time;	/* (read only) */
	long tick;		/* (modified) usecs between clock ticks */
	long ppsfreq;           /* pps frequency (scaled ppm) (ro) */
	long jitter;            /* pps jitter (us) (ro) */
	int shift;              /* interval duration (s) (shift) (ro) */
	long stabil;            /* pps stability (scaled ppm) (ro) */
	long jitcnt;            /* jitter limit exceeded (ro) */
	long calcnt;            /* calibration intervals (ro) */
	long errcnt;            /* calibration errors (ro) */
	long stbcnt;            /* stability limit exceeded (ro) */
	int tai;		/* TAI offset (ro) */
	int  :32; int  :32; int  :32; int  :32;
	int  :32; int  :32; int  :32; int  :32;
	int  :32; int  :32; int  :32;
};
/*
 * Mode codes (timex.mode)
 */
/* NTP userland likes the MOD_ prefix better */
/*
 * Status codes (timex.status)
 */
/* read-only bits */
/*
 * Clock states (time_state)
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/compiler.h" 1
#line 171 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/timex.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 172 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/timex.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/param.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/param.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/asm-generic/param.h" 1
#line 3 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/param.h" 2
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/param.h" 2
#line 173 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/timex.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/timex.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/processor.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/timex.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/tsc.h" 1
/*
 * x86 TSC related functions
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/processor.h" 1
#line 9 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/tsc.h" 2
/*
 * Standard way to access the cycle counter.
 */
typedef unsigned long long cycles_t;
extern unsigned int cpu_khz;
extern unsigned int tsc_khz;
extern void disable_TSC(void);
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 cycles_t get_cycles(void)
{
	unsigned long long ret = 0;
#if !definedEx(CONFIG_X86_TSC)
	if (!(__builtin_constant_p((0*32+ 4)) && ( ((((0*32+ 4))>>5)==0 && (1UL<<(((0*32+ 4))&31) & (
#if !definedEx(CONFIG_MATH_EMULATION)
(1<<((0*32+ 0) & 31))
#endif
#if definedEx(CONFIG_MATH_EMULATION)
0
#endif
|
#if !definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_64) && definedEx(CONFIG_PARAVIRT)
0
#endif
#if definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT)
(1<<((0*32+ 3)) & 31)
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+ 5) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if !definedEx(CONFIG_X86_PAE) && definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_PAE)
(1<<((0*32+ 6) & 31))
#endif
#if !definedEx(CONFIG_X86_PAE) && !definedEx(CONFIG_X86_64)
0
#endif
| 
#if definedEx(CONFIG_X86_CMPXCHG64)
(1<<((0*32+ 8) & 31))
#endif
#if !definedEx(CONFIG_X86_CMPXCHG64)
0
#endif
|
#if !definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_64) && definedEx(CONFIG_PARAVIRT)
0
#endif
#if definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT)
(1<<((0*32+13)) & 31)
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+24) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if !definedEx(CONFIG_X86_64) && definedEx(CONFIG_X86_CMOV) || definedEx(CONFIG_X86_64)
(1<<((0*32+15) & 31))
#endif
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_X86_CMOV)
0
#endif
| 
#if definedEx(CONFIG_X86_64)
(1<<((0*32+25) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+26) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
))) || ((((0*32+ 4))>>5)==1 && (1UL<<(((0*32+ 4))&31) & (
#if definedEx(CONFIG_X86_64)
(1<<((1*32+29) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if definedEx(CONFIG_X86_USE_3DNOW)
(1<<((1*32+31) & 31))
#endif
#if !definedEx(CONFIG_X86_USE_3DNOW)
0
#endif
))) || ((((0*32+ 4))>>5)==2 && (1UL<<(((0*32+ 4))&31) & 0)) || ((((0*32+ 4))>>5)==3 && (1UL<<(((0*32+ 4))&31) & (
#if !definedEx(CONFIG_X86_64) && definedEx(CONFIG_X86_P6_NOP) || definedEx(CONFIG_X86_64)
(1<<((3*32+20) & 31))
#endif
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_X86_P6_NOP)
0
#endif
))) || ((((0*32+ 4))>>5)==4 && (1UL<<(((0*32+ 4))&31) & 0)) || ((((0*32+ 4))>>5)==5 && (1UL<<(((0*32+ 4))&31) & 0)) || ((((0*32+ 4))>>5)==6 && (1UL<<(((0*32+ 4))&31) & 0)) || ((((0*32+ 4))>>5)==7 && (1UL<<(((0*32+ 4))&31) & 0)) ) ? 1 : (__builtin_constant_p(((0*32+ 4))) ? constant_test_bit(((0*32+ 4)), ((unsigned long *)((&boot_cpu_data)->x86_capability))) : variable_test_bit(((0*32+ 4)), ((unsigned long *)((&boot_cpu_data)->x86_capability))))))
		return 0;
#endif
#if definedEx(CONFIG_PARAVIRT)
(ret = paravirt_read_tsc())
#endif
#if !definedEx(CONFIG_PARAVIRT)
((ret) = __native_read_tsc())
#endif
;
	return ret;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __attribute__((always_inline)) cycles_t vget_cycles(void)
{
	/*
	 * We only do VDSOs on TSC capable CPUs, so this shouldnt
	 * access boot_cpu_data (which is not VDSO-safe):
	 */
#if !definedEx(CONFIG_X86_TSC)
	if (!(__builtin_constant_p((0*32+ 4)) && ( ((((0*32+ 4))>>5)==0 && (1UL<<(((0*32+ 4))&31) & (
#if !definedEx(CONFIG_MATH_EMULATION)
(1<<((0*32+ 0) & 31))
#endif
#if definedEx(CONFIG_MATH_EMULATION)
0
#endif
|
#if !definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_64) && definedEx(CONFIG_PARAVIRT)
0
#endif
#if definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT)
(1<<((0*32+ 3)) & 31)
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+ 5) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if !definedEx(CONFIG_X86_PAE) && definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_PAE)
(1<<((0*32+ 6) & 31))
#endif
#if !definedEx(CONFIG_X86_PAE) && !definedEx(CONFIG_X86_64)
0
#endif
| 
#if definedEx(CONFIG_X86_CMPXCHG64)
(1<<((0*32+ 8) & 31))
#endif
#if !definedEx(CONFIG_X86_CMPXCHG64)
0
#endif
|
#if !definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_64) && definedEx(CONFIG_PARAVIRT)
0
#endif
#if definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT)
(1<<((0*32+13)) & 31)
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+24) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if !definedEx(CONFIG_X86_64) && definedEx(CONFIG_X86_CMOV) || definedEx(CONFIG_X86_64)
(1<<((0*32+15) & 31))
#endif
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_X86_CMOV)
0
#endif
| 
#if definedEx(CONFIG_X86_64)
(1<<((0*32+25) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+26) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
))) || ((((0*32+ 4))>>5)==1 && (1UL<<(((0*32+ 4))&31) & (
#if definedEx(CONFIG_X86_64)
(1<<((1*32+29) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if definedEx(CONFIG_X86_USE_3DNOW)
(1<<((1*32+31) & 31))
#endif
#if !definedEx(CONFIG_X86_USE_3DNOW)
0
#endif
))) || ((((0*32+ 4))>>5)==2 && (1UL<<(((0*32+ 4))&31) & 0)) || ((((0*32+ 4))>>5)==3 && (1UL<<(((0*32+ 4))&31) & (
#if !definedEx(CONFIG_X86_64) && definedEx(CONFIG_X86_P6_NOP) || definedEx(CONFIG_X86_64)
(1<<((3*32+20) & 31))
#endif
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_X86_P6_NOP)
0
#endif
))) || ((((0*32+ 4))>>5)==4 && (1UL<<(((0*32+ 4))&31) & 0)) || ((((0*32+ 4))>>5)==5 && (1UL<<(((0*32+ 4))&31) & 0)) || ((((0*32+ 4))>>5)==6 && (1UL<<(((0*32+ 4))&31) & 0)) || ((((0*32+ 4))>>5)==7 && (1UL<<(((0*32+ 4))&31) & 0)) ) ? 1 : (__builtin_constant_p(((0*32+ 4))) ? constant_test_bit(((0*32+ 4)), ((unsigned long *)((&boot_cpu_data)->x86_capability))) : variable_test_bit(((0*32+ 4)), ((unsigned long *)((&boot_cpu_data)->x86_capability))))))
		return 0;
#endif
	return (cycles_t)__native_read_tsc();
}
extern void tsc_init(void);
extern void mark_tsc_unstable(char *reason);
extern int unsynchronized_tsc(void);
extern int check_tsc_unstable(void);
extern unsigned long native_calibrate_tsc(void);
/*
 * Boot-time check whether the TSCs are synchronized across
 * all CPUs/cores:
 */
extern void check_tsc_sync_source(int cpu);
extern void check_tsc_sync_target(void);
extern int notsc_setup(char *);
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/timex.h" 2
/* Assume we use the PIT time source for the clock tick */
#line 175 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/timex.h" 2
/*
 * SHIFT_PLL is used as a dampening factor to define how much we
 * adjust the frequency correction for a given offset in PLL mode.
 * It also used in dampening the offset correction, to define how
 * much of the current value in time_offset we correct for each
 * second. Changing this value changes the stiffness of the ntp
 * adjustment code. A lower value makes it more flexible, reducing
 * NTP convergence time. A higher value makes it stiffer, increasing
 * convergence time, but making the clock more stable.
 *
 * In David Mills' nanokernel reference implementation SHIFT_PLL is 4.
 * However this seems to increase convergence time much too long.
 *
 * https://lists.ntp.org/pipermail/hackers/2008-January/003487.html
 *
 * In the above mailing list discussion, it seems the value of 4
 * was appropriate for other Unix systems with HZ=100, and that
 * SHIFT_PLL should be decreased as HZ increases. However, Linux's
 * clock steering implementation is HZ independent.
 *
 * Through experimentation, a SHIFT_PLL value of 2 was found to allow
 * for fast convergence (very similar to the NTPv3 code used prior to
 * v2.6.19), with good clock stability.
 *
 *
 * SHIFT_FLL is used as a dampening factor to define how much we
 * adjust the frequency correction for a given offset in FLL mode.
 * In David Mills' nanokernel reference implementation SHIFT_FLL is 2.
 *
 * MAXTC establishes the maximum time constant of the PLL.
 */
/*
 * SHIFT_USEC defines the scaling (shift) of the time_freq and
 * time_tolerance variables, which represent the current frequency
 * offset and maximum frequency tolerance.
 */
/*
 * kernel variables
 * Note: maximum error = NTP synch distance = dispersion + delay / 2;
 * estimated error = NTP dispersion.
 */
extern unsigned long tick_usec;		/* USER_HZ period (usec) */
extern unsigned long tick_nsec;		/* ACTHZ          period (nsec) */
extern int tickadj;			/* amount of adjustment per tick */
/*
 * phase-lock loop variables
 */
extern int time_status;		/* clock synchronization status bits */
extern long time_maxerror;	/* maximum error */
extern long time_esterror;	/* estimated error */
extern long time_adjust;	/* The amount of adjtime left */
extern void ntp_init(void);
extern void ntp_clear(void);
/**
 * ntp_synced - Returns 1 if the NTP status is not UNSYNC
 *
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int ntp_synced(void)
{
	return !(time_status & 0x0040);
}
/* Required to safely shift negative values */
/* Returns how long ticks are at present, in ns / 2^NTP_SCALE_SHIFT. */
extern u64 tick_length;
extern void second_overflow(void);
extern void update_ntp_one_tick(void);
extern int do_adjtimex(struct timex *);
/* Don't use! Compatibility define for existing users. */
int read_current_timer(unsigned long *timer_val);
/* The clock frequency of the i8253/i8254 PIT */
#line 10 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/jiffies.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/param.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/asm-generic/param.h" 1
#line 3 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/param.h" 2
#line 11 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/jiffies.h" 2
/*
 * The following defines establish the engineering parameters of the PLL
 * model. The HZ variable establishes the timer interrupt frequency, 100 Hz
 * for the SunOS kernel, 256 Hz for the Ultrix kernel and 1024 Hz for the
 * OSF/1 kernel. The SHIFT_HZ define expresses the same value as the
 * nearest power of two in order to avoid hardware multiply operations.
 */
/* LATCH is used in the interval timer and ftape setup. */
/* Suppose we want to devide two numbers NOM and DEN: NOM/DEN, then we can
 * improve accuracy by shifting LSH bits, hence calculating:
 *     (NOM << LSH) / DEN
 * This however means trouble for large NOM, because (NOM << LSH) may no
 * longer fit in 32 bits. The following way of calculating this gives us
 * some slack, under the following conditions:
 *   - (NOM / DEN) fits in (32 - LSH) bits.
 *   - (NOM % DEN) fits in (32 - LSH) bits.
 */
/* HZ is the requested value. ACTHZ is actual HZ ("<< 8" is for accuracy) */
/* TICK_NSEC is the time between ticks in nsec assuming real ACTHZ */
/* TICK_USEC is the time between ticks in usec assuming fake USER_HZ */
/* TICK_USEC_TO_NSEC is the time between ticks in nsec assuming real ACTHZ and	*/
/* a value TUSEC for TICK_USEC (can be set bij adjtimex)		*/
/* some arch's have a small-data section that can be accessed register-relative
 * but that can only take up to, say, 4-byte variables. jiffies being part of
 * an 8-byte variable may not be correctly accessed unless we force the issue
 */
/*
 * The 64-bit value is not atomic - you MUST NOT read it
 * without sampling the sequence number in xtime_lock.
 * get_jiffies_64() will do this for you as appropriate.
 */
extern u64 __attribute__((section(".data"))) jiffies_64;
extern unsigned long volatile __attribute__((section(".data"))) jiffies;
#if !definedEx(CONFIG_64BIT)
u64 get_jiffies_64(void);
#endif
#if definedEx(CONFIG_64BIT)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 u64 get_jiffies_64(void)
{
	return (u64)jiffies;
}
#endif
/*
 *	These inlines deal with timer wrapping correctly. You are 
 *	strongly encouraged to use them
 *	1. Because people otherwise forget
 *	2. Because if the timer wrap changes in future you won't have to
 *	   alter your driver code.
 *
 * time_after(a,b) returns true if the time a is after time b.
 *
 * Do this with "<0" and ">=0" to only test the sign of the result. A
 * good compiler would generate better code (and a really good compiler
 * wouldn't care). Gcc is currently neither.
 */
/*
 * Calculate whether a is in the range of [b, c].
 */
/*
 * Calculate whether a is in the range of [b, c).
 */
/* Same as above, but does so with platform independent 64bit types.
 * These must be used when utilizing jiffies_64 (i.e. return value of
 * get_jiffies_64() */
/*
 * These four macros compare jiffies and 'a' for convenience.
 */
/* time_is_before_jiffies(a) return true if a is before jiffies */
/* time_is_after_jiffies(a) return true if a is after jiffies */
/* time_is_before_eq_jiffies(a) return true if a is before or equal to jiffies*/
/* time_is_after_eq_jiffies(a) return true if a is after or equal to jiffies*/
/*
 * Have the 32 bit jiffies value wrap 5 minutes after boot
 * so jiffies wrap bugs show up earlier.
 */
/*
 * Change timeval to jiffies, trying to avoid the
 * most obvious overflows..
 *
 * And some not so obvious.
 *
 * Note that we don't want to return LONG_MAX, because
 * for various timeout reasons we often end up having
 * to wait "jiffies+1" in order to guarantee that we wait
 * at _least_ "jiffies" - so "jiffies+1" had better still
 * be positive.
 */
extern unsigned long preset_lpj;
/*
 * We want to do realistic conversions of time so we need to use the same
 * values the update wall clock code uses as the jiffies size.  This value
 * is: TICK_NSEC (which is defined in timex.h).  This
 * is a constant and is in nanoseconds.  We will use scaled math
 * with a set of scales defined here as SEC_JIFFIE_SC,  USEC_JIFFIE_SC and
 * NSEC_JIFFIE_SC.  Note that these defines contain nothing but
 * constants and so are computed at compile time.  SHIFT_HZ (computed in
 * timex.h) adjusts the scaling for different HZ values.
 * Scaled math???  What is that?
 *
 * Scaled math is a way to do integer math on values that would,
 * otherwise, either overflow, underflow, or cause undesired div
 * instructions to appear in the execution path.  In short, we "scale"
 * up the operands so they take more bits (more precision, less
 * underflow), do the desired operation and then "scale" the result back
 * by the same amount.  If we do the scaling by shifting we avoid the
 * costly mpy and the dastardly div instructions.
 * Suppose, for example, we want to convert from seconds to jiffies
 * where jiffies is defined in nanoseconds as NSEC_PER_JIFFIE.  The
 * simple math is: jiff = (sec * NSEC_PER_SEC) / NSEC_PER_JIFFIE; We
 * observe that (NSEC_PER_SEC / NSEC_PER_JIFFIE) is a constant which we
 * might calculate at compile time, however, the result will only have
 * about 3-4 bits of precision (less for smaller values of HZ).
 *
 * So, we scale as follows:
 * jiff = (sec) * (NSEC_PER_SEC / NSEC_PER_JIFFIE);
 * jiff = ((sec) * ((NSEC_PER_SEC * SCALE)/ NSEC_PER_JIFFIE)) / SCALE;
 * Then we make SCALE a power of two so:
 * jiff = ((sec) * ((NSEC_PER_SEC << SCALE)/ NSEC_PER_JIFFIE)) >> SCALE;
 * Now we define:
 * #define SEC_CONV = ((NSEC_PER_SEC << SCALE)/ NSEC_PER_JIFFIE))
 * jiff = (sec * SEC_CONV) >> SCALE;
 *
 * Often the math we use will expand beyond 32-bits so we tell C how to
 * do this and pass the 64-bit result of the mpy through the ">> SCALE"
 * which should take the result back to 32-bits.  We want this expansion
 * to capture as much precision as possible.  At the same time we don't
 * want to overflow so we pick the SCALE to avoid this.  In this file,
 * that means using a different scale for each range of HZ values (as
 * defined in timex.h).
 *
 * For those who want to know, gcc will give a 64-bit result from a "*"
 * operator if the result is a long long AND at least one of the
 * operands is cast to long long (usually just prior to the "*" so as
 * not to confuse it into thinking it really has a 64-bit operand,
 * which, buy the way, it can do, but it takes more code and at least 2
 * mpys).
 * We also need to be aware that one second in nanoseconds is only a
 * couple of bits away from overflowing a 32-bit word, so we MUST use
 * 64-bits to get the full range time in nanoseconds.
 */
/*
 * Here are the scales we will use.  One for seconds, nanoseconds and
 * microseconds.
 *
 * Within the limits of cpp we do a rough cut at the SEC_JIFFIE_SC and
 * check if the sign bit is set.  If not, we bump the shift count by 1.
 * (Gets an extra bit of precision where we can use it.)
 * We know it is set for HZ = 1024 and HZ = 100 not for 1000.
 * Haven't tested others.
 * Limits of cpp (for #if expressions) only long (no long long), but
 * then we only need the most signicant bit.
 */
/*
 * USEC_ROUND is used in the timeval to jiffie conversion.  See there
 * for more details.  It is the scaled resolution rounding value.  Note
 * that it is a 64-bit value.  Since, when it is applied, we are already
 * in jiffies (albit scaled), it is nothing but the bits we will shift
 * off.
 */
/*
 * The maximum jiffie value is (MAX_INT >> 1).  Here we translate that
 * into seconds.  The 64-bit case will overflow if we are not careful,
 * so use the messy SH_DIV macro to do it.  Still all constants.
 */
#if !definedEx(CONFIG_64BIT)
#endif
#if definedEx(CONFIG_64BIT)
#endif
/*
 * Convert various time units to each other:
 */
extern unsigned int jiffies_to_msecs(const unsigned long j);
extern unsigned int jiffies_to_usecs(const unsigned long j);
extern unsigned long msecs_to_jiffies(const unsigned int m);
extern unsigned long usecs_to_jiffies(const unsigned int u);
extern unsigned long timespec_to_jiffies(const struct timespec *value);
extern void jiffies_to_timespec(const unsigned long jiffies,
				struct timespec *value);
extern unsigned long timeval_to_jiffies(const struct timeval *value);
extern void jiffies_to_timeval(const unsigned long jiffies,
			       struct timeval *value);
extern clock_t jiffies_to_clock_t(long x);
extern unsigned long clock_t_to_jiffies(unsigned long x);
extern u64 jiffies_64_to_clock_t(u64 x);
extern u64 nsec_to_clock_t(u64 x);
extern unsigned long nsecs_to_jiffies(u64 n);
#line 27 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/ktime.h" 2
/*
 * ktime_t:
 *
 * On 64-bit CPUs a single 64-bit variable is used to store the hrtimers
 * internal representation of time values in scalar nanoseconds. The
 * design plays out best on 64-bit CPUs, where most conversions are
 * NOPs and most arithmetic ktime_t operations are plain arithmetic
 * operations.
 *
 * On 32-bit CPUs an optimized representation of the timespec structure
 * is used to avoid expensive conversions from and to timespecs. The
 * endian-aware order of the tv struct members is choosen to allow
 * mathematical operations on the tv64 member of the union too, which
 * for certain operations produces better code.
 *
 * For architectures with efficient support for 64/32-bit conversions the
 * plain scalar nanosecond based representation can be selected by the
 * config switch CONFIG_KTIME_SCALAR.
 */
union ktime {
	s64	tv64;
#if !definedEx(CONFIG_64BIT) && !definedEx(CONFIG_KTIME_SCALAR)
	struct {
 	s32	nsec, sec;
	} tv;
#endif
};
typedef union ktime ktime_t;		/* Kill this */
#if definedEx(CONFIG_64BIT)
#endif
#if !definedEx(CONFIG_64BIT)
#endif
/*
 * ktime_t definitions when using the 64-bit scalar representation:
 */
#if !definedEx(CONFIG_64BIT) && definedEx(CONFIG_KTIME_SCALAR) || definedEx(CONFIG_64BIT)
/**
 * ktime_set - Set a ktime_t variable from a seconds/nanoseconds value
 * @secs:	seconds to set
 * @nsecs:	nanoseconds to set
 *
 * Return the ktime_t representation of the value
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 ktime_t ktime_set(const long secs, const unsigned long nsecs)
{
#if definedEx(CONFIG_64BIT)
	if (__builtin_expect(!!(secs >= (((s64)~((u64)1 << 63)) / 1000000000L)), 0))
		return (ktime_t){ .tv64 = ((s64)~((u64)1 << 63)) };
#endif
	return (ktime_t) { .tv64 = (s64)secs * 1000000000L + (s64)nsecs };
}
/* Subtract two ktime_t variables. rem = lhs -rhs: */
/* Add two ktime_t variables. res = lhs + rhs: */
/*
 * Add a ktime_t variable and a scalar nanosecond value.
 * res = kt + nsval:
 */
/*
 * Subtract a scalar nanosecod from a ktime_t variable
 * res = kt - nsval:
 */
/* convert a timespec to ktime_t format: */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 ktime_t timespec_to_ktime(struct timespec ts)
{
	return ktime_set(ts.tv_sec, ts.tv_nsec);
}
/* convert a timeval to ktime_t format: */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 ktime_t timeval_to_ktime(struct timeval tv)
{
	return ktime_set(tv.tv_sec, tv.tv_usec * 1000L);
}
/* Map the ktime_t to timespec conversion to ns_to_timespec function */
/* Map the ktime_t to timeval conversion to ns_to_timeval function */
/* Convert ktime_t to nanoseconds - NOP in the scalar storage format: */
#endif
#if !definedEx(CONFIG_64BIT) && !definedEx(CONFIG_KTIME_SCALAR)
/*
 * Helper macros/inlines to get the ktime_t math right in the timespec
 * representation. The macros are sometimes ugly - their actual use is
 * pretty okay-ish, given the circumstances. We do all this for
 * performance reasons. The pure scalar nsec_t based code was nice and
 * simple, but created too many 64-bit / 32-bit conversions and divisions.
 *
 * Be especially aware that negative values are represented in a way
 * that the tv.sec field is negative and the tv.nsec field is greater
 * or equal to zero but less than nanoseconds per second. This is the
 * same representation which is used by timespecs.
 *
 *   tv.sec < 0 and 0 >= tv.nsec < NSEC_PER_SEC
 */
/* Set a ktime_t variable to a value in sec/nsec representation: */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 ktime_t ktime_set(const long secs, const unsigned long nsecs)
{
	return (ktime_t) { .tv = { .sec = secs, .nsec = nsecs } };
}
/**
 * ktime_sub - subtract two ktime_t variables
 * @lhs:	minuend
 * @rhs:	subtrahend
 *
 * Returns the remainder of the substraction
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 ktime_t ktime_sub(const ktime_t lhs, const ktime_t rhs)
{
	ktime_t res;
	res.tv64 = lhs.tv64 - rhs.tv64;
	if (res.tv.nsec < 0)
		res.tv.nsec += 1000000000L;
	return res;
}
/**
 * ktime_add - add two ktime_t variables
 * @add1:	addend1
 * @add2:	addend2
 *
 * Returns the sum of @add1 and @add2.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 ktime_t ktime_add(const ktime_t add1, const ktime_t add2)
{
	ktime_t res;
	res.tv64 = add1.tv64 + add2.tv64;
	/*
	 * performance trick: the (u32) -NSEC gives 0x00000000Fxxxxxxx
	 * so we subtract NSEC_PER_SEC and add 1 to the upper 32 bit.
	 *
	 * it's equivalent to:
	 *   tv.nsec -= NSEC_PER_SEC
	 *   tv.sec ++;
	 */
	if (res.tv.nsec >= 1000000000L)
		res.tv64 += (u32)-1000000000L;
	return res;
}
/**
 * ktime_add_ns - Add a scalar nanoseconds value to a ktime_t variable
 * @kt:		addend
 * @nsec:	the scalar nsec value to add
 *
 * Returns the sum of @kt and @nsec in ktime_t format
 */
extern ktime_t ktime_add_ns(const ktime_t kt, u64 nsec);
/**
 * ktime_sub_ns - Subtract a scalar nanoseconds value from a ktime_t variable
 * @kt:		minuend
 * @nsec:	the scalar nsec value to subtract
 *
 * Returns the subtraction of @nsec from @kt in ktime_t format
 */
extern ktime_t ktime_sub_ns(const ktime_t kt, u64 nsec);
/**
 * timespec_to_ktime - convert a timespec to ktime_t format
 * @ts:		the timespec variable to convert
 *
 * Returns a ktime_t variable with the converted timespec value
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 ktime_t timespec_to_ktime(const struct timespec ts)
{
	return (ktime_t) { .tv = { .sec = (s32)ts.tv_sec,
			   	   .nsec = (s32)ts.tv_nsec } };
}
/**
 * timeval_to_ktime - convert a timeval to ktime_t format
 * @tv:		the timeval variable to convert
 *
 * Returns a ktime_t variable with the converted timeval value
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 ktime_t timeval_to_ktime(const struct timeval tv)
{
	return (ktime_t) { .tv = { .sec = (s32)tv.tv_sec,
				   .nsec = (s32)tv.tv_usec * 1000 } };
}
/**
 * ktime_to_timespec - convert a ktime_t variable to timespec format
 * @kt:		the ktime_t variable to convert
 *
 * Returns the timespec representation of the ktime value
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 struct timespec ktime_to_timespec(const ktime_t kt)
{
	return (struct timespec) { .tv_sec = (time_t) kt.tv.sec,
				   .tv_nsec = (long) kt.tv.nsec };
}
/**
 * ktime_to_timeval - convert a ktime_t variable to timeval format
 * @kt:		the ktime_t variable to convert
 *
 * Returns the timeval representation of the ktime value
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 struct timeval ktime_to_timeval(const ktime_t kt)
{
	return (struct timeval) {
		.tv_sec = (time_t) kt.tv.sec,
		.tv_usec = (suseconds_t) (kt.tv.nsec / 1000L) };
}
/**
 * ktime_to_ns - convert a ktime_t variable to scalar nanoseconds
 * @kt:		the ktime_t variable to convert
 *
 * Returns the scalar nanoseconds representation of @kt
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 s64 ktime_to_ns(const ktime_t kt)
{
	return (s64) kt.tv.sec * 1000000000L + kt.tv.nsec;
}
#endif
/**
 * ktime_equal - Compares two ktime_t variables to see if they are equal
 * @cmp1:	comparable1
 * @cmp2:	comparable2
 *
 * Compare two ktime_t variables, returns 1 if equal
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int ktime_equal(const ktime_t cmp1, const ktime_t cmp2)
{
	return cmp1.tv64 == cmp2.tv64;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 s64 ktime_to_us(const ktime_t kt)
{
	struct timeval tv = 
#if !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) && definedEx(CONFIG_KTIME_SCALAR) || !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) && definedEx(CONFIG_KTIME_SCALAR) || !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) && definedEx(CONFIG_KTIME_SCALAR) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_X86_LOCAL_APIC) && definedEx(CONFIG_KTIME_SCALAR) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_X86_LOCAL_APIC) && definedEx(CONFIG_KTIME_SCALAR)
ns_to_timeval((kt).tv64)
#endif
#if !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) || !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) && !definedEx(CONFIG_KTIME_SCALAR) || !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) && !definedEx(CONFIG_NUMA) || !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) && !definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_NUMA) && !definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) && !definedEx(CONFIG_KTIME_SCALAR) || !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) && !definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) && !definedEx(CONFIG_NUMA) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) && !definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) && !definedEx(CONFIG_KTIME_SCALAR) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_X86_LOCAL_APIC) && !definedEx(CONFIG_KTIME_SCALAR) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_X86_LOCAL_APIC) && !definedEx(CONFIG_KTIME_SCALAR)
ktime_to_timeval(kt)
#endif
;
	return (s64) tv.tv_sec * 1000000L + tv.tv_usec;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 s64 ktime_us_delta(const ktime_t later, const ktime_t earlier)
{
       return ktime_to_us(
#if !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) && definedEx(CONFIG_KTIME_SCALAR) || !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) && definedEx(CONFIG_KTIME_SCALAR) || !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) && definedEx(CONFIG_KTIME_SCALAR) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_X86_LOCAL_APIC) && definedEx(CONFIG_KTIME_SCALAR) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_X86_LOCAL_APIC) && definedEx(CONFIG_KTIME_SCALAR)
({ (ktime_t){ .tv64 = (later).tv64 - (earlier).tv64 }; })
#endif
#if !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) || !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) && !definedEx(CONFIG_KTIME_SCALAR) || !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) && !definedEx(CONFIG_NUMA) || !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) && !definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_NUMA) && !definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) && !definedEx(CONFIG_KTIME_SCALAR) || !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) && !definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) && !definedEx(CONFIG_NUMA) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) && !definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) && !definedEx(CONFIG_KTIME_SCALAR) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_X86_LOCAL_APIC) && !definedEx(CONFIG_KTIME_SCALAR) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_X86_LOCAL_APIC) && !definedEx(CONFIG_KTIME_SCALAR)
ktime_sub(later, earlier)
#endif
);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 ktime_t ktime_add_us(const ktime_t kt, const u64 usec)
{
	return 
#if !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) && definedEx(CONFIG_KTIME_SCALAR) || !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) && definedEx(CONFIG_KTIME_SCALAR) || !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) && definedEx(CONFIG_KTIME_SCALAR) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_X86_LOCAL_APIC) && definedEx(CONFIG_KTIME_SCALAR) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_X86_LOCAL_APIC) && definedEx(CONFIG_KTIME_SCALAR)
({ (ktime_t){ .tv64 = (kt).tv64 + (usec * 1000) }; })
#endif
#if !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) || !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) && !definedEx(CONFIG_KTIME_SCALAR) || !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) && !definedEx(CONFIG_NUMA) || !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) && !definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_NUMA) && !definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) && !definedEx(CONFIG_KTIME_SCALAR) || !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) && !definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) && !definedEx(CONFIG_NUMA) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) && !definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) && !definedEx(CONFIG_KTIME_SCALAR) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_X86_LOCAL_APIC) && !definedEx(CONFIG_KTIME_SCALAR) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_X86_LOCAL_APIC) && !definedEx(CONFIG_KTIME_SCALAR)
ktime_add_ns(kt, usec * 1000)
#endif
;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 ktime_t ktime_sub_us(const ktime_t kt, const u64 usec)
{
	return 
#if !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) && definedEx(CONFIG_KTIME_SCALAR) || !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) && definedEx(CONFIG_KTIME_SCALAR) || !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) && definedEx(CONFIG_KTIME_SCALAR) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_X86_LOCAL_APIC) && definedEx(CONFIG_KTIME_SCALAR) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_X86_LOCAL_APIC) && definedEx(CONFIG_KTIME_SCALAR)
({ (ktime_t){ .tv64 = (kt).tv64 - (usec * 1000) }; })
#endif
#if !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) || !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) && !definedEx(CONFIG_KTIME_SCALAR) || !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) && !definedEx(CONFIG_NUMA) || !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) && !definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_NUMA) && !definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) && !definedEx(CONFIG_KTIME_SCALAR) || !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) && !definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) && !definedEx(CONFIG_NUMA) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) && !definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) && !definedEx(CONFIG_KTIME_SCALAR) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_X86_LOCAL_APIC) && !definedEx(CONFIG_KTIME_SCALAR) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_X86_LOCAL_APIC) && !definedEx(CONFIG_KTIME_SCALAR)
ktime_sub_ns(kt, usec * 1000)
#endif
;
}
extern ktime_t ktime_add_safe(const ktime_t lhs, const ktime_t rhs);
/*
 * The resolution of the clocks. The resolution value is returned in
 * the clock_getres() system call to give application programmers an
 * idea of the (in)accuracy of timers. Timer values are rounded up to
 * this resolution values.
 */
/* Get the monotonic time in timespec format: */
extern void ktime_get_ts(struct timespec *ts);
/* Get the real (wall-) time in timespec format: */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 ktime_t ns_to_ktime(u64 ns)
{
	static const ktime_t ktime_zero = { .tv64 = 0 };
	return 
#if !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) && definedEx(CONFIG_KTIME_SCALAR) || !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) && definedEx(CONFIG_KTIME_SCALAR) || !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) && definedEx(CONFIG_KTIME_SCALAR) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_X86_LOCAL_APIC) && definedEx(CONFIG_KTIME_SCALAR) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_X86_LOCAL_APIC) && definedEx(CONFIG_KTIME_SCALAR)
({ (ktime_t){ .tv64 = (ktime_zero).tv64 + (ns) }; })
#endif
#if !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) || !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) && !definedEx(CONFIG_KTIME_SCALAR) || !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) && !definedEx(CONFIG_NUMA) || !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) && !definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_NUMA) && !definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) && !definedEx(CONFIG_KTIME_SCALAR) || !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) && !definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) && !definedEx(CONFIG_NUMA) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) && !definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) && !definedEx(CONFIG_KTIME_SCALAR) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_X86_LOCAL_APIC) && !definedEx(CONFIG_KTIME_SCALAR) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_64BIT) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_X86_LOCAL_APIC) && !definedEx(CONFIG_KTIME_SCALAR)
ktime_add_ns(ktime_zero, ns)
#endif
;
}
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/timer.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/stddef.h" 1
#line 8 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/timer.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/debugobjects.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/list.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/debugobjects.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/spinlock.h" 1
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/debugobjects.h" 2
enum debug_obj_state {
	ODEBUG_STATE_NONE,
	ODEBUG_STATE_INIT,
	ODEBUG_STATE_INACTIVE,
	ODEBUG_STATE_ACTIVE,
	ODEBUG_STATE_DESTROYED,
	ODEBUG_STATE_NOTAVAILABLE,
	ODEBUG_STATE_MAX,
};
struct debug_obj_descr;
/**
 * struct debug_obj - representaion of an tracked object
 * @node:	hlist node to link the object into the tracker list
 * @state:	tracked object state
 * @object:	pointer to the real object
 * @descr:	pointer to an object type specific debug description structure
 */
struct debug_obj {
	struct hlist_node	node;
	enum debug_obj_state	state;
	void			*object;
	struct debug_obj_descr	*descr;
};
/**
 * struct debug_obj_descr - object type specific debug description structure
 * @name:		name of the object typee
 * @fixup_init:		fixup function, which is called when the init check
 *			fails
 * @fixup_activate:	fixup function, which is called when the activate check
 *			fails
 * @fixup_destroy:	fixup function, which is called when the destroy check
 *			fails
 * @fixup_free:		fixup function, which is called when the free check
 *			fails
 */
struct debug_obj_descr {
	const char		*name;
	int (*fixup_init)	(void *addr, enum debug_obj_state state);
	int (*fixup_activate)	(void *addr, enum debug_obj_state state);
	int (*fixup_destroy)	(void *addr, enum debug_obj_state state);
	int (*fixup_free)	(void *addr, enum debug_obj_state state);
};
#if definedEx(CONFIG_DEBUG_OBJECTS)
extern void debug_object_init      (void *addr, struct debug_obj_descr *descr);
extern void
debug_object_init_on_stack(void *addr, struct debug_obj_descr *descr);
extern void debug_object_activate  (void *addr, struct debug_obj_descr *descr);
extern void debug_object_deactivate(void *addr, struct debug_obj_descr *descr);
extern void debug_object_destroy   (void *addr, struct debug_obj_descr *descr);
extern void debug_object_free      (void *addr, struct debug_obj_descr *descr);
extern void debug_objects_early_init(void);
extern void debug_objects_mem_init(void);
#endif
#if !definedEx(CONFIG_DEBUG_OBJECTS)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void
debug_object_init      (void *addr, struct debug_obj_descr *descr) { }
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void
debug_object_init_on_stack(void *addr, struct debug_obj_descr *descr) { }
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void
debug_object_activate  (void *addr, struct debug_obj_descr *descr) { }
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void
debug_object_deactivate(void *addr, struct debug_obj_descr *descr) { }
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void
debug_object_destroy   (void *addr, struct debug_obj_descr *descr) { }
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void
debug_object_free      (void *addr, struct debug_obj_descr *descr) { }
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void debug_objects_early_init(void) { }
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void debug_objects_mem_init(void) { }
#endif
#if definedEx(CONFIG_DEBUG_OBJECTS_FREE)
extern void debug_check_no_obj_freed(const void *address, unsigned long size);
#endif
#if !definedEx(CONFIG_DEBUG_OBJECTS_FREE)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void
debug_check_no_obj_freed(const void *address, unsigned long size) { }
#endif
#line 9 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/timer.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/stringify.h" 1
#line 10 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/timer.h" 2
struct tvec_base;
struct timer_list {
	struct list_head entry;
	unsigned long expires;
	void (*function)(unsigned long);
	unsigned long data;
	struct tvec_base *base;
#if definedEx(CONFIG_TIMER_STATS)
	void *start_site;
	char start_comm[16];
	int start_pid;
#endif
#if definedEx(CONFIG_LOCKDEP)
	struct lockdep_map lockdep_map;
#endif
};
extern struct tvec_base boot_tvec_bases;
#if definedEx(CONFIG_LOCKDEP)
/*
 * NB: because we have to copy the lockdep_map, setting the lockdep_map key
 * (second argument) here is required, otherwise it could be initialised to
 * the copy of the lockdep_map later! We use the pointer to and the string
 * "<file>:<line>" as the key resp. the name of the lockdep_map.
 */
#endif
#if !definedEx(CONFIG_LOCKDEP)
#endif
void init_timer_key(struct timer_list *timer,
		    const char *name,
		    struct lock_class_key *key);
void init_timer_deferrable_key(struct timer_list *timer,
			       const char *name,
			       struct lock_class_key *key);
#if definedEx(CONFIG_LOCKDEP)
#endif
#if !definedEx(CONFIG_LOCKDEP)
#endif
#if definedEx(CONFIG_DEBUG_OBJECTS_TIMERS)
extern void init_timer_on_stack_key(struct timer_list *timer,
				    const char *name,
				    struct lock_class_key *key);
extern void destroy_timer_on_stack(struct timer_list *timer);
#endif
#if !definedEx(CONFIG_DEBUG_OBJECTS_TIMERS)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void destroy_timer_on_stack(struct timer_list *timer) { }
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void init_timer_on_stack_key(struct timer_list *timer,
					   const char *name,
					   struct lock_class_key *key)
{
	init_timer_key(timer, name, key);
}
#endif
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void setup_timer_key(struct timer_list * timer,
				const char *name,
				struct lock_class_key *key,
				void (*function)(unsigned long),
				unsigned long data)
{
	timer->function = function;
	timer->data = data;
	init_timer_key(timer, name, key);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void setup_timer_on_stack_key(struct timer_list *timer,
					const char *name,
					struct lock_class_key *key,
					void (*function)(unsigned long),
					unsigned long data)
{
	timer->function = function;
	timer->data = data;
	init_timer_on_stack_key(timer, name, key);
}
/**
 * timer_pending - is a timer pending?
 * @timer: the timer in question
 *
 * timer_pending will tell whether a given timer is currently pending,
 * or not. Callers must ensure serialization wrt. other operations done
 * to this timer, eg. interrupt contexts, or other CPUs on SMP.
 *
 * return value: 1 if the timer is pending, 0 if not.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int timer_pending(const struct timer_list * timer)
{
	return timer->entry.next != ((void *)0);
}
extern void add_timer_on(struct timer_list *timer, int cpu);
extern int del_timer(struct timer_list * timer);
extern int mod_timer(struct timer_list *timer, unsigned long expires);
extern int mod_timer_pending(struct timer_list *timer, unsigned long expires);
extern int mod_timer_pinned(struct timer_list *timer, unsigned long expires);
/*
 * The jiffies value which is added to now, when there is no timer
 * in the timer wheel:
 */
/*
 * Return when the next timer-wheel timeout occurs (in absolute jiffies),
 * locks the timer base and does the comparison against the given
 * jiffie.
 */
extern unsigned long get_next_timer_interrupt(unsigned long now);
/*
 * Timer-statistics info:
 */
#if definedEx(CONFIG_TIMER_STATS)
extern int timer_stats_active;
extern void init_timer_stats(void);
extern void timer_stats_update_stats(void *timer, pid_t pid, void *startf,
				     void *timerf, char *comm,
				     unsigned int timer_flag);
extern void __timer_stats_timer_set_start_info(struct timer_list *timer,
					       void *addr);
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void timer_stats_timer_set_start_info(struct timer_list *timer)
{
	if (__builtin_expect(!!(!timer_stats_active), 1))
		return;
	__timer_stats_timer_set_start_info(timer, __builtin_return_address(0));
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void timer_stats_timer_clear_start_info(struct timer_list *timer)
{
	timer->start_site = ((void *)0);
}
#endif
#if !definedEx(CONFIG_TIMER_STATS)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void init_timer_stats(void)
{
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void timer_stats_timer_set_start_info(struct timer_list *timer)
{
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void timer_stats_timer_clear_start_info(struct timer_list *timer)
{
}
#endif
extern void add_timer(struct timer_list *timer);
  extern int try_to_del_timer_sync(struct timer_list *timer);
  extern int del_timer_sync(struct timer_list *timer);
extern void init_timers(void);
extern void run_local_timers(void);
struct hrtimer;
extern enum hrtimer_restart it_real_fn(struct hrtimer *);
unsigned long __round_jiffies(unsigned long j, int cpu);
unsigned long __round_jiffies_relative(unsigned long j, int cpu);
unsigned long round_jiffies(unsigned long j);
unsigned long round_jiffies_relative(unsigned long j);
unsigned long __round_jiffies_up(unsigned long j, int cpu);
unsigned long __round_jiffies_up_relative(unsigned long j, int cpu);
unsigned long round_jiffies_up(unsigned long j);
unsigned long round_jiffies_up_relative(unsigned long j);
#line 10 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/workqueue.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/linkage.h" 1
#line 11 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/workqueue.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/bitops.h" 1
#line 12 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/workqueue.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/lockdep.h" 1
/*
 * Runtime locking correctness validator
 *
 *  Copyright (C) 2006,2007 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>
 *  Copyright (C) 2007 Red Hat, Inc., Peter Zijlstra <pzijlstr@redhat.com>
 *
 * see Documentation/lockdep-design.txt for more details.
 */
#line 13 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/workqueue.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic.h" 1
#if definedEx(CONFIG_X86_32)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic_32.h" 1
#line 4 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic.h" 2
#endif
#if !definedEx(CONFIG_X86_32)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic_64.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic.h" 2
#endif
#line 14 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/workqueue.h" 2
struct workqueue_struct;
struct work_struct;
typedef void (*work_func_t)(struct work_struct *work);
/*
 * The first word is the work queue pointer and the flags rolled into
 * one
 */
struct work_struct {
	atomic_long_t data;
	struct list_head entry;
	work_func_t func;
#if definedEx(CONFIG_LOCKDEP)
	struct lockdep_map lockdep_map;
#endif
};
struct delayed_work {
	struct work_struct work;
	struct timer_list timer;
};
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 struct delayed_work *to_delayed_work(struct work_struct *work)
{
	return ({ const typeof( ((struct delayed_work *)0)->work ) *__mptr = (work); (struct delayed_work *)( (char *)__mptr - __builtin_offsetof(struct delayed_work,work) );});
}
struct execute_work {
	struct work_struct work;
};
#if definedEx(CONFIG_LOCKDEP)
/*
 * NB: because we have to copy the lockdep_map, setting _key
 * here is required, otherwise it could get initialised to the
 * copy of the lockdep_map!
 */
#endif
#if !definedEx(CONFIG_LOCKDEP)
#endif
/*
 * initialize a work item's function pointer
 */
#if definedEx(CONFIG_DEBUG_OBJECTS_WORK)
extern void __init_work(struct work_struct *work, int onstack);
extern void destroy_work_on_stack(struct work_struct *work);
#endif
#if !definedEx(CONFIG_DEBUG_OBJECTS_WORK)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __init_work(struct work_struct *work, int onstack) { }
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void destroy_work_on_stack(struct work_struct *work) { }
#endif
/*
 * initialize all of a work item in one go
 *
 * NOTE! No point in using "atomic_long_set()": using a direct
 * assignment of the work data initializer allows the compiler
 * to generate better code.
 */
#if definedEx(CONFIG_LOCKDEP)
#endif
#if !definedEx(CONFIG_LOCKDEP)
#endif
/**
 * work_pending - Find out whether a work item is currently pending
 * @work: The work item in question
 */
/**
 * delayed_work_pending - Find out whether a delayable work item is currently
 * pending
 * @work: The work item in question
 */
/**
 * work_clear_pending - for internal use only, mark a work item as not pending
 * @work: The work item in question
 */
extern struct workqueue_struct *
__create_workqueue_key(const char *name, int singlethread,
		       int freezeable, int rt, struct lock_class_key *key,
		       const char *lock_name);
#if definedEx(CONFIG_LOCKDEP)
#endif
#if !definedEx(CONFIG_LOCKDEP)
#endif
extern void destroy_workqueue(struct workqueue_struct *wq);
extern int queue_work(struct workqueue_struct *wq, struct work_struct *work);
extern int queue_work_on(int cpu, struct workqueue_struct *wq,
			struct work_struct *work);
extern int queue_delayed_work(struct workqueue_struct *wq,
			struct delayed_work *work, unsigned long delay);
extern int queue_delayed_work_on(int cpu, struct workqueue_struct *wq,
			struct delayed_work *work, unsigned long delay);
extern void flush_workqueue(struct workqueue_struct *wq);
extern void flush_scheduled_work(void);
extern void flush_delayed_work(struct delayed_work *work);
extern int schedule_work(struct work_struct *work);
extern int schedule_work_on(int cpu, struct work_struct *work);
extern int schedule_delayed_work(struct delayed_work *work, unsigned long delay);
extern int schedule_delayed_work_on(int cpu, struct delayed_work *work,
					unsigned long delay);
extern int schedule_on_each_cpu(work_func_t func);
extern int current_is_keventd(void);
extern int keventd_up(void);
extern void init_workqueues(void);
int execute_in_process_context(work_func_t fn, struct execute_work *);
extern int flush_work(struct work_struct *work);
extern int cancel_work_sync(struct work_struct *work);
/*
 * Kill off a pending schedule_delayed_work().  Note that the work callback
 * function may still be running on return from cancel_delayed_work(), unless
 * it returns 1 and the work doesn't re-arm itself. Run flush_workqueue() or
 * cancel_work_sync() to wait on it.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int cancel_delayed_work(struct delayed_work *work)
{
	int ret;
	ret = del_timer_sync(&work->timer);
	if (ret)
		clear_bit(0, ((unsigned long *)(&(&work->work)->data)));
	return ret;
}
/*
 * Like above, but uses del_timer() instead of del_timer_sync(). This means,
 * if it returns 0 the timer function may be running and the queueing is in
 * progress.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int __cancel_delayed_work(struct delayed_work *work)
{
	int ret;
	ret = del_timer(&work->timer);
	if (ret)
		clear_bit(0, ((unsigned long *)(&(&work->work)->data)));
	return ret;
}
extern int cancel_delayed_work_sync(struct delayed_work *work);
/* Obsolete. use cancel_delayed_work_sync() */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
void cancel_rearming_delayed_workqueue(struct workqueue_struct *wq,
					struct delayed_work *work)
{
	cancel_delayed_work_sync(work);
}
/* Obsolete. use cancel_delayed_work_sync() */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
void cancel_rearming_delayed_work(struct delayed_work *work)
{
	cancel_delayed_work_sync(work);
}
 long work_on_cpu(unsigned int cpu, long (*fn)(void *), void *arg);
#line 27 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/pm.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/spinlock.h" 1
#line 28 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/pm.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/wait.h" 1
#line 29 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/pm.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/timer.h" 1
#line 30 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/pm.h" 2
/*
 * Callbacks for platform drivers to implement.
 */
extern void (*pm_idle)(void);
extern void (*pm_power_off)(void);
extern void (*pm_power_off_prepare)(void);
/*
 * Device power management
 */
struct device;
typedef struct pm_message {
	int event;
} pm_message_t;
/**
 * struct dev_pm_ops - device PM callbacks
 *
 * Several driver power state transitions are externally visible, affecting
 * the state of pending I/O queues and (for drivers that touch hardware)
 * interrupts, wakeups, DMA, and other hardware state.  There may also be
 * internal transitions to various low power modes, which are transparent
 * to the rest of the driver stack (such as a driver that's ON gating off
 * clocks which are not in active use).
 *
 * The externally visible transitions are handled with the help of the following
 * callbacks included in this structure:
 *
 * @prepare: Prepare the device for the upcoming transition, but do NOT change
 *	its hardware state.  Prevent new children of the device from being
 *	registered after @prepare() returns (the driver's subsystem and
 *	generally the rest of the kernel is supposed to prevent new calls to the
 *	probe method from being made too once @prepare() has succeeded).  If
 *	@prepare() detects a situation it cannot handle (e.g. registration of a
 *	child already in progress), it may return -EAGAIN, so that the PM core
 *	can execute it once again (e.g. after the new child has been registered)
 *	to recover from the race condition.  This method is executed for all
 *	kinds of suspend transitions and is followed by one of the suspend
 *	callbacks: @suspend(), @freeze(), or @poweroff().
 *	The PM core executes @prepare() for all devices before starting to
 *	execute suspend callbacks for any of them, so drivers may assume all of
 *	the other devices to be present and functional while @prepare() is being
 *	executed.  In particular, it is safe to make GFP_KERNEL memory
 *	allocations from within @prepare().  However, drivers may NOT assume
 *	anything about the availability of the user space at that time and it
 *	is not correct to request firmware from within @prepare() (it's too
 *	late to do that).  [To work around this limitation, drivers may
 *	register suspend and hibernation notifiers that are executed before the
 *	freezing of tasks.]
 *
 * @complete: Undo the changes made by @prepare().  This method is executed for
 *	all kinds of resume transitions, following one of the resume callbacks:
 *	@resume(), @thaw(), @restore().  Also called if the state transition
 *	fails before the driver's suspend callback (@suspend(), @freeze(),
 *	@poweroff()) can be executed (e.g. if the suspend callback fails for one
 *	of the other devices that the PM core has unsuccessfully attempted to
 *	suspend earlier).
 *	The PM core executes @complete() after it has executed the appropriate
 *	resume callback for all devices.
 *
 * @suspend: Executed before putting the system into a sleep state in which the
 *	contents of main memory are preserved.  Quiesce the device, put it into
 *	a low power state appropriate for the upcoming system state (such as
 *	PCI_D3hot), and enable wakeup events as appropriate.
 *
 * @resume: Executed after waking the system up from a sleep state in which the
 *	contents of main memory were preserved.  Put the device into the
 *	appropriate state, according to the information saved in memory by the
 *	preceding @suspend().  The driver starts working again, responding to
 *	hardware events and software requests.  The hardware may have gone
 *	through a power-off reset, or it may have maintained state from the
 *	previous suspend() which the driver may rely on while resuming.  On most
 *	platforms, there are no restrictions on availability of resources like
 *	clocks during @resume().
 *
 * @freeze: Hibernation-specific, executed before creating a hibernation image.
 *	Quiesce operations so that a consistent image can be created, but do NOT
 *	otherwise put the device into a low power device state and do NOT emit
 *	system wakeup events.  Save in main memory the device settings to be
 *	used by @restore() during the subsequent resume from hibernation or by
 *	the subsequent @thaw(), if the creation of the image or the restoration
 *	of main memory contents from it fails.
 *
 * @thaw: Hibernation-specific, executed after creating a hibernation image OR
 *	if the creation of the image fails.  Also executed after a failing
 *	attempt to restore the contents of main memory from such an image.
 *	Undo the changes made by the preceding @freeze(), so the device can be
 *	operated in the same way as immediately before the call to @freeze().
 *
 * @poweroff: Hibernation-specific, executed after saving a hibernation image.
 *	Quiesce the device, put it into a low power state appropriate for the
 *	upcoming system state (such as PCI_D3hot), and enable wakeup events as
 *	appropriate.
 *
 * @restore: Hibernation-specific, executed after restoring the contents of main
 *	memory from a hibernation image.  Driver starts working again,
 *	responding to hardware events and software requests.  Drivers may NOT
 *	make ANY assumptions about the hardware state right prior to @restore().
 *	On most platforms, there are no restrictions on availability of
 *	resources like clocks during @restore().
 *
 * @suspend_noirq: Complete the operations of ->suspend() by carrying out any
 *	actions required for suspending the device that need interrupts to be
 *	disabled
 *
 * @resume_noirq: Prepare for the execution of ->resume() by carrying out any
 *	actions required for resuming the device that need interrupts to be
 *	disabled
 *
 * @freeze_noirq: Complete the operations of ->freeze() by carrying out any
 *	actions required for freezing the device that need interrupts to be
 *	disabled
 *
 * @thaw_noirq: Prepare for the execution of ->thaw() by carrying out any
 *	actions required for thawing the device that need interrupts to be
 *	disabled
 *
 * @poweroff_noirq: Complete the operations of ->poweroff() by carrying out any
 *	actions required for handling the device that need interrupts to be
 *	disabled
 *
 * @restore_noirq: Prepare for the execution of ->restore() by carrying out any
 *	actions required for restoring the operations of the device that need
 *	interrupts to be disabled
 *
 * All of the above callbacks, except for @complete(), return error codes.
 * However, the error codes returned by the resume operations, @resume(),
 * @thaw(), @restore(), @resume_noirq(), @thaw_noirq(), and @restore_noirq() do
 * not cause the PM core to abort the resume transition during which they are
 * returned.  The error codes returned in that cases are only printed by the PM
 * core to the system logs for debugging purposes.  Still, it is recommended
 * that drivers only return error codes from their resume methods in case of an
 * unrecoverable failure (i.e. when the device being handled refuses to resume
 * and becomes unusable) to allow us to modify the PM core in the future, so
 * that it can avoid attempting to handle devices that failed to resume and
 * their children.
 *
 * It is allowed to unregister devices while the above callbacks are being
 * executed.  However, it is not allowed to unregister a device from within any
 * of its own callbacks.
 *
 * There also are the following callbacks related to run-time power management
 * of devices:
 *
 * @runtime_suspend: Prepare the device for a condition in which it won't be
 *	able to communicate with the CPU(s) and RAM due to power management.
 *	This need not mean that the device should be put into a low power state.
 *	For example, if the device is behind a link which is about to be turned
 *	off, the device may remain at full power.  If the device does go to low
 *	power and is capable of generating run-time wake-up events, remote
 *	wake-up (i.e., a hardware mechanism allowing the device to request a
 *	change of its power state via a wake-up event, such as PCI PME) should
 *	be enabled for it.
 *
 * @runtime_resume: Put the device into the fully active state in response to a
 *	wake-up event generated by hardware or at the request of software.  If
 *	necessary, put the device into the full power state and restore its
 *	registers, so that it is fully operational.
 *
 * @runtime_idle: Device appears to be inactive and it might be put into a low
 *	power state if all of the necessary conditions are satisfied.  Check
 *	these conditions and handle the device as appropriate, possibly queueing
 *	a suspend request for it.  The return value is ignored by the PM core.
 */
struct dev_pm_ops {
	int (*prepare)(struct device *dev);
	void (*complete)(struct device *dev);
	int (*suspend)(struct device *dev);
	int (*resume)(struct device *dev);
	int (*freeze)(struct device *dev);
	int (*thaw)(struct device *dev);
	int (*poweroff)(struct device *dev);
	int (*restore)(struct device *dev);
	int (*suspend_noirq)(struct device *dev);
	int (*resume_noirq)(struct device *dev);
	int (*freeze_noirq)(struct device *dev);
	int (*thaw_noirq)(struct device *dev);
	int (*poweroff_noirq)(struct device *dev);
	int (*restore_noirq)(struct device *dev);
	int (*runtime_suspend)(struct device *dev);
	int (*runtime_resume)(struct device *dev);
	int (*runtime_idle)(struct device *dev);
};
/*
 * Use this if you want to use the same suspend and resume callbacks for suspend
 * to RAM and hibernation.
 */
/**
 * PM_EVENT_ messages
 *
 * The following PM_EVENT_ messages are defined for the internal use of the PM
 * core, in order to provide a mechanism allowing the high level suspend and
 * hibernation code to convey the necessary information to the device PM core
 * code:
 *
 * ON		No transition.
 *
 * FREEZE 	System is going to hibernate, call ->prepare() and ->freeze()
 *		for all devices.
 *
 * SUSPEND	System is going to suspend, call ->prepare() and ->suspend()
 *		for all devices.
 *
 * HIBERNATE	Hibernation image has been saved, call ->prepare() and
 *		->poweroff() for all devices.
 *
 * QUIESCE	Contents of main memory are going to be restored from a (loaded)
 *		hibernation image, call ->prepare() and ->freeze() for all
 *		devices.
 *
 * RESUME	System is resuming, call ->resume() and ->complete() for all
 *		devices.
 *
 * THAW		Hibernation image has been created, call ->thaw() and
 *		->complete() for all devices.
 *
 * RESTORE	Contents of main memory have been restored from a hibernation
 *		image, call ->restore() and ->complete() for all devices.
 *
 * RECOVER	Creation of a hibernation image or restoration of the main
 *		memory contents from a hibernation image has failed, call
 *		->thaw() and ->complete() for all devices.
 *
 * The following PM_EVENT_ messages are defined for internal use by
 * kernel subsystems.  They are never issued by the PM core.
 *
 * USER_SUSPEND		Manual selective suspend was issued by userspace.
 *
 * USER_RESUME		Manual selective resume was issued by userspace.
 *
 * REMOTE_WAKEUP	Remote-wakeup request was received from the device.
 *
 * AUTO_SUSPEND		Automatic (device idle) runtime suspend was
 *			initiated by the subsystem.
 *
 * AUTO_RESUME		Automatic (device needed) runtime resume was
 *			requested by a driver.
 */
/**
 * Device power management states
 *
 * These state labels are used internally by the PM core to indicate the current
 * status of a device with respect to the PM core operations.
 *
 * DPM_ON		Device is regarded as operational.  Set this way
 *			initially and when ->complete() is about to be called.
 *			Also set when ->prepare() fails.
 *
 * DPM_PREPARING	Device is going to be prepared for a PM transition.  Set
 *			when ->prepare() is about to be called.
 *
 * DPM_RESUMING		Device is going to be resumed.  Set when ->resume(),
 *			->thaw(), or ->restore() is about to be called.
 *
 * DPM_SUSPENDING	Device has been prepared for a power transition.  Set
 *			when ->prepare() has just succeeded.
 *
 * DPM_OFF		Device is regarded as inactive.  Set immediately after
 *			->suspend(), ->freeze(), or ->poweroff() has succeeded.
 *			Also set when ->resume()_noirq, ->thaw_noirq(), or
 *			->restore_noirq() is about to be called.
 *
 * DPM_OFF_IRQ		Device is in a "deep sleep".  Set immediately after
 *			->suspend_noirq(), ->freeze_noirq(), or
 *			->poweroff_noirq() has just succeeded.
 */
enum dpm_state {
	DPM_INVALID,
	DPM_ON,
	DPM_PREPARING,
	DPM_RESUMING,
	DPM_SUSPENDING,
	DPM_OFF,
	DPM_OFF_IRQ,
};
/**
 * Device run-time power management status.
 *
 * These status labels are used internally by the PM core to indicate the
 * current status of a device with respect to the PM core operations.  They do
 * not reflect the actual power state of the device or its status as seen by the
 * driver.
 *
 * RPM_ACTIVE		Device is fully operational.  Indicates that the device
 *			bus type's ->runtime_resume() callback has completed
 *			successfully.
 *
 * RPM_SUSPENDED	Device bus type's ->runtime_suspend() callback has
 *			completed successfully.  The device is regarded as
 *			suspended.
 *
 * RPM_RESUMING		Device bus type's ->runtime_resume() callback is being
 *			executed.
 *
 * RPM_SUSPENDING	Device bus type's ->runtime_suspend() callback is being
 *			executed.
 */
enum rpm_status {
	RPM_ACTIVE = 0,
	RPM_RESUMING,
	RPM_SUSPENDED,
	RPM_SUSPENDING,
};
/**
 * Device run-time power management request types.
 *
 * RPM_REQ_NONE		Do nothing.
 *
 * RPM_REQ_IDLE		Run the device bus type's ->runtime_idle() callback
 *
 * RPM_REQ_SUSPEND	Run the device bus type's ->runtime_suspend() callback
 *
 * RPM_REQ_RESUME	Run the device bus type's ->runtime_resume() callback
 */
enum rpm_request {
	RPM_REQ_NONE = 0,
	RPM_REQ_IDLE,
	RPM_REQ_SUSPEND,
	RPM_REQ_RESUME,
};
struct dev_pm_info {
	pm_message_t		power_state;
	unsigned int		can_wakeup:1;
	unsigned int		should_wakeup:1;
	enum dpm_state		status;		/* Owned by the PM core */
#if definedEx(CONFIG_PM_SLEEP)
	struct list_head	entry;
#endif
#if definedEx(CONFIG_PM_RUNTIME)
	struct timer_list	suspend_timer;
	unsigned long		timer_expires;
	struct work_struct	work;
	wait_queue_head_t	wait_queue;
	spinlock_t		lock;
	atomic_t		usage_count;
	atomic_t		child_count;
	unsigned int		disable_depth:3;
	unsigned int		ignore_children:1;
	unsigned int		idle_notification:1;
	unsigned int		request_pending:1;
	unsigned int		deferred_resume:1;
	unsigned int		run_wake:1;
	enum rpm_request	request;
	enum rpm_status		runtime_status;
	int			runtime_error;
#endif
};
/*
 * The PM_EVENT_ messages are also used by drivers implementing the legacy
 * suspend framework, based on the ->suspend() and ->resume() callbacks common
 * for suspend and hibernation transitions, according to the rules below.
 */
/* Necessary, because several drivers use PM_EVENT_PRETHAW */
/*
 * One transition is triggered by resume(), after a suspend() call; the
 * message is implicit:
 *
 * ON		Driver starts working again, responding to hardware events
 * 		and software requests.  The hardware may have gone through
 * 		a power-off reset, or it may have maintained state from the
 * 		previous suspend() which the driver will rely on while
 * 		resuming.  On most platforms, there are no restrictions on
 * 		availability of resources like clocks during resume().
 *
 * Other transitions are triggered by messages sent using suspend().  All
 * these transitions quiesce the driver, so that I/O queues are inactive.
 * That commonly entails turning off IRQs and DMA; there may be rules
 * about how to quiesce that are specific to the bus or the device's type.
 * (For example, network drivers mark the link state.)  Other details may
 * differ according to the message:
 *
 * SUSPEND	Quiesce, enter a low power device state appropriate for
 * 		the upcoming system state (such as PCI_D3hot), and enable
 * 		wakeup events as appropriate.
 *
 * HIBERNATE	Enter a low power device state appropriate for the hibernation
 * 		state (eg. ACPI S4) and enable wakeup events as appropriate.
 *
 * FREEZE	Quiesce operations so that a consistent image can be saved;
 * 		but do NOT otherwise enter a low power device state, and do
 * 		NOT emit system wakeup events.
 *
 * PRETHAW	Quiesce as if for FREEZE; additionally, prepare for restoring
 * 		the system from a snapshot taken after an earlier FREEZE.
 * 		Some drivers will need to reset their hardware state instead
 * 		of preserving it, to ensure that it's never mistaken for the
 * 		state which that earlier snapshot had set up.
 *
 * A minimally power-aware driver treats all messages as SUSPEND, fully
 * reinitializes its device during resume() -- whether or not it was reset
 * during the suspend/resume cycle -- and can't issue wakeup events.
 *
 * More power-aware drivers may also use low power states at runtime as
 * well as during system sleep states like PM_SUSPEND_STANDBY.  They may
 * be able to use wakeup events to exit from runtime low-power states,
 * or from system low-power states such as standby or suspend-to-RAM.
 */
#if definedEx(CONFIG_PM_SLEEP)
extern void device_pm_lock(void);
extern int sysdev_resume(void);
extern void dpm_resume_noirq(pm_message_t state);
extern void dpm_resume_end(pm_message_t state);
extern void device_pm_unlock(void);
extern int sysdev_suspend(pm_message_t state);
extern int dpm_suspend_noirq(pm_message_t state);
extern int dpm_suspend_start(pm_message_t state);
extern void __suspend_report_result(const char *function, void *fn, int ret);
#endif
#if !definedEx(CONFIG_PM_SLEEP)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int dpm_suspend_start(pm_message_t state)
{
	return 0;
}
#endif
/* How to reorder dpm_list after device_move() */
enum dpm_order {
	DPM_ORDER_NONE,
	DPM_ORDER_DEV_AFTER_PARENT,
	DPM_ORDER_PARENT_BEFORE_DEV,
	DPM_ORDER_DEV_LAST,
};
/*
 * Global Power Management flags
 * Used to keep APM and ACPI from both being active
 */
extern unsigned int	pm_flags;
#line 8 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/apic.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/alternative.h" 1
#line 10 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/apic.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/cpufeature.h" 1
/*
 * Defines x86 CPU feature bits
 */
#line 11 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/apic.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/processor.h" 1
#line 12 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/apic.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/apicdef.h" 1
/*
 * Constants for various Intel APICs. (local APIC, IOAPIC, etc.)
 *
 * Alan Cox <Alan.Cox@linux.org>, 1995.
 * Ingo Molnar <mingo@redhat.com>, 1999, 2000
 */
/*
 * This is the IO-APIC register space as specified
 * by Intel docs:
 */
#if definedEx(CONFIG_X86_32)
#endif
#if !definedEx(CONFIG_X86_32)
#endif
#if definedEx(CONFIG_X86_32)
#endif
#if !definedEx(CONFIG_X86_32)
#endif
/*
 * All x86-64 systems are xAPIC compatible.
 * In the following, "apicid" is a physical APIC ID.
 */
/*
 * the local APIC register structure, memory mapped. Not terribly well
 * tested, but we might eventually use this one in the future - the
 * problem why we cannot use it right now is the P5 APIC, it has an
 * errata which cannot take 8-bit reads and writes, only 32-bit ones ...
 */
struct local_apic {
/*000*/	struct { unsigned int __reserved[4]; } __reserved_01;
/*010*/	struct { unsigned int __reserved[4]; } __reserved_02;
/*020*/	struct { /* APIC ID Register */
		unsigned int   __reserved_1	: 24,
			phys_apic_id	:  4,
			__reserved_2	:  4;
		unsigned int __reserved[3];
	} id;
/*030*/	const
	struct { /* APIC Version Register */
		unsigned int   version		:  8,
			__reserved_1	:  8,
			max_lvt		:  8,
			__reserved_2	:  8;
		unsigned int __reserved[3];
	} version;
/*040*/	struct { unsigned int __reserved[4]; } __reserved_03;
/*050*/	struct { unsigned int __reserved[4]; } __reserved_04;
/*060*/	struct { unsigned int __reserved[4]; } __reserved_05;
/*070*/	struct { unsigned int __reserved[4]; } __reserved_06;
/*080*/	struct { /* Task Priority Register */
		unsigned int   priority	:  8,
			__reserved_1	: 24;
		unsigned int __reserved_2[3];
	} tpr;
/*090*/	const
	struct { /* Arbitration Priority Register */
		unsigned int   priority	:  8,
			__reserved_1	: 24;
		unsigned int __reserved_2[3];
	} apr;
/*0A0*/	const
	struct { /* Processor Priority Register */
		unsigned int   priority	:  8,
			__reserved_1	: 24;
		unsigned int __reserved_2[3];
	} ppr;
/*0B0*/	struct { /* End Of Interrupt Register */
		unsigned int   eoi;
		unsigned int __reserved[3];
	} eoi;
/*0C0*/	struct { unsigned int __reserved[4]; } __reserved_07;
/*0D0*/	struct { /* Logical Destination Register */
		unsigned int   __reserved_1	: 24,
			logical_dest	:  8;
		unsigned int __reserved_2[3];
	} ldr;
/*0E0*/	struct { /* Destination Format Register */
		unsigned int   __reserved_1	: 28,
			model		:  4;
		unsigned int __reserved_2[3];
	} dfr;
/*0F0*/	struct { /* Spurious Interrupt Vector Register */
		unsigned int	spurious_vector	:  8,
			apic_enabled	:  1,
			focus_cpu	:  1,
			__reserved_2	: 22;
		unsigned int __reserved_3[3];
	} svr;
/*100*/	struct { /* In Service Register */
/*170*/		unsigned int bitfield;
		unsigned int __reserved[3];
	} isr [8];
/*180*/	struct { /* Trigger Mode Register */
/*1F0*/		unsigned int bitfield;
		unsigned int __reserved[3];
	} tmr [8];
/*200*/	struct { /* Interrupt Request Register */
/*270*/		unsigned int bitfield;
		unsigned int __reserved[3];
	} irr [8];
/*280*/	union { /* Error Status Register */
		struct {
			unsigned int   send_cs_error			:  1,
				receive_cs_error		:  1,
				send_accept_error		:  1,
				receive_accept_error		:  1,
				__reserved_1			:  1,
				send_illegal_vector		:  1,
				receive_illegal_vector		:  1,
				illegal_register_address	:  1,
				__reserved_2			: 24;
			unsigned int __reserved_3[3];
		} error_bits;
		struct {
			unsigned int errors;
			unsigned int __reserved_3[3];
		} all_errors;
	} esr;
/*290*/	struct { unsigned int __reserved[4]; } __reserved_08;
/*2A0*/	struct { unsigned int __reserved[4]; } __reserved_09;
/*2B0*/	struct { unsigned int __reserved[4]; } __reserved_10;
/*2C0*/	struct { unsigned int __reserved[4]; } __reserved_11;
/*2D0*/	struct { unsigned int __reserved[4]; } __reserved_12;
/*2E0*/	struct { unsigned int __reserved[4]; } __reserved_13;
/*2F0*/	struct { unsigned int __reserved[4]; } __reserved_14;
/*300*/	struct { /* Interrupt Command Register 1 */
		unsigned int   vector			:  8,
			delivery_mode		:  3,
			destination_mode	:  1,
			delivery_status		:  1,
			__reserved_1		:  1,
			level			:  1,
			trigger			:  1,
			__reserved_2		:  2,
			shorthand		:  2,
			__reserved_3		:  12;
		unsigned int __reserved_4[3];
	} icr1;
/*310*/	struct { /* Interrupt Command Register 2 */
		union {
			unsigned int   __reserved_1	: 24,
				phys_dest	:  4,
				__reserved_2	:  4;
			unsigned int   __reserved_3	: 24,
				logical_dest	:  8;
		} dest;
		unsigned int __reserved_4[3];
	} icr2;
/*320*/	struct { /* LVT - Timer */
		unsigned int   vector		:  8,
			__reserved_1	:  4,
			delivery_status	:  1,
			__reserved_2	:  3,
			mask		:  1,
			timer_mode	:  1,
			__reserved_3	: 14;
		unsigned int __reserved_4[3];
	} lvt_timer;
/*330*/	struct { /* LVT - Thermal Sensor */
		unsigned int  vector		:  8,
			delivery_mode	:  3,
			__reserved_1	:  1,
			delivery_status	:  1,
			__reserved_2	:  3,
			mask		:  1,
			__reserved_3	: 15;
		unsigned int __reserved_4[3];
	} lvt_thermal;
/*340*/	struct { /* LVT - Performance Counter */
		unsigned int   vector		:  8,
			delivery_mode	:  3,
			__reserved_1	:  1,
			delivery_status	:  1,
			__reserved_2	:  3,
			mask		:  1,
			__reserved_3	: 15;
		unsigned int __reserved_4[3];
	} lvt_pc;
/*350*/	struct { /* LVT - LINT0 */
		unsigned int   vector		:  8,
			delivery_mode	:  3,
			__reserved_1	:  1,
			delivery_status	:  1,
			polarity	:  1,
			remote_irr	:  1,
			trigger		:  1,
			mask		:  1,
			__reserved_2	: 15;
		unsigned int __reserved_3[3];
	} lvt_lint0;
/*360*/	struct { /* LVT - LINT1 */
		unsigned int   vector		:  8,
			delivery_mode	:  3,
			__reserved_1	:  1,
			delivery_status	:  1,
			polarity	:  1,
			remote_irr	:  1,
			trigger		:  1,
			mask		:  1,
			__reserved_2	: 15;
		unsigned int __reserved_3[3];
	} lvt_lint1;
/*370*/	struct { /* LVT - Error */
		unsigned int   vector		:  8,
			__reserved_1	:  4,
			delivery_status	:  1,
			__reserved_2	:  3,
			mask		:  1,
			__reserved_3	: 15;
		unsigned int __reserved_4[3];
	} lvt_error;
/*380*/	struct { /* Timer Initial Count Register */
		unsigned int   initial_count;
		unsigned int __reserved_2[3];
	} timer_icr;
/*390*/	const
	struct { /* Timer Current Count Register */
		unsigned int   curr_count;
		unsigned int __reserved_2[3];
	} timer_ccr;
/*3A0*/	struct { unsigned int __reserved[4]; } __reserved_16;
/*3B0*/	struct { unsigned int __reserved[4]; } __reserved_17;
/*3C0*/	struct { unsigned int __reserved[4]; } __reserved_18;
/*3D0*/	struct { unsigned int __reserved[4]; } __reserved_19;
/*3E0*/	struct { /* Timer Divide Configuration Register */
		unsigned int   divisor		:  4,
			__reserved_1	: 28;
		unsigned int __reserved_2[3];
	} timer_dcr;
/*3F0*/	struct { unsigned int __reserved[4]; } __reserved_20;
} __attribute__ ((packed));
#if definedEx(CONFIG_X86_32)
#endif
#if !definedEx(CONFIG_X86_32)
#endif
#line 13 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/apic.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic.h" 1
#if definedEx(CONFIG_X86_32)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic_32.h" 1
#line 4 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic.h" 2
#endif
#if !definedEx(CONFIG_X86_32)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic_64.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/atomic.h" 2
#endif
#line 14 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/apic.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/fixmap.h" 1
/*
 * fixmap.h: compile-time virtual memory allocation
 *
 * This file is subject to the terms and conditions of the GNU General Public
 * License.  See the file "COPYING" in the main directory of this archive
 * for more details.
 *
 * Copyright (C) 1998 Ingo Molnar
 *
 * Support of BIGMEM added by Gerhard Wichert, Siemens AG, July 1999
 * x86_32 and x86_64 integration by Gustavo F. Padovan, February 2009
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/kernel.h" 1
#line 20 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/fixmap.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/acpi.h" 1
/*
 *  Copyright (C) 2001 Paul Diefenbaugh <paul.s.diefenbaugh@intel.com>
 *  Copyright (C) 2001 Patrick Mochel <mochel@osdl.org>
 *
 * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 *
 *  This program is free software; you can redistribute it and/or modify
 *  it under the terms of the GNU General Public License as published by
 *  the Free Software Foundation; either version 2 of the License, or
 *  (at your option) any later version.
 *
 *  This program is distributed in the hope that it will be useful,
 *  but WITHOUT ANY WARRANTY; without even the implied warranty of
 *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *  GNU General Public License for more details.
 *
 *  You should have received a copy of the GNU General Public License
 *  along with this program; if not, write to the Free Software
 *  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
 *
 * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/acpi/pdc_intel.h" 1
/* _PDC bit definition for Intel processors */
#line 28 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/acpi.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/numa.h" 1
#if definedEx(CONFIG_X86_32)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/numa_32.h" 1
extern int pxm_to_nid(int pxm);
extern void numa_remove_cpu(int cpu);
#if definedEx(CONFIG_HIGHMEM)
extern void set_highmem_pages_init(void);
#endif
#if !definedEx(CONFIG_HIGHMEM)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void set_highmem_pages_init(void)
{
}
#endif
#line 4 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/numa.h" 2
#endif
#if !definedEx(CONFIG_X86_32)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/numa_64.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/nodemask.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/numa_64.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/apicdef.h" 1
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/numa_64.h" 2
struct bootnode {
	u64 start;
	u64 end;
};
extern int compute_hash_shift(struct bootnode *nodes, int numblks,
			      int *nodeids);
extern void numa_init_array(void);
extern int numa_off;
extern s16 apicid_to_node[32768];
extern unsigned long numa_free_all_bootmem(void);
extern void setup_node_bootmem(int nodeid, unsigned long start,
			       unsigned long end);
#if definedEx(CONFIG_NUMA)
/*
 * Too small node sizes may confuse the VM badly. Usually they
 * result from BIOS bugs. So dont recognize nodes as standalone
 * NUMA entities that have less than this amount of RAM listed:
 */
extern void __attribute__ ((__section__(".init.text"))) __attribute__((__cold__)) __attribute__((no_instrument_function)) init_cpu_to_node(void);
extern void __attribute__ ((__section__(".cpuinit.text"))) __attribute__((__cold__)) numa_set_node(int cpu, int node);
extern void __attribute__ ((__section__(".cpuinit.text"))) __attribute__((__cold__)) numa_clear_node(int cpu);
extern void __attribute__ ((__section__(".cpuinit.text"))) __attribute__((__cold__)) numa_add_cpu(int cpu);
extern void __attribute__ ((__section__(".cpuinit.text"))) __attribute__((__cold__)) numa_remove_cpu(int cpu);
#endif
#if !definedEx(CONFIG_NUMA)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void init_cpu_to_node(void)		{ }
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void numa_set_node(int cpu, int node)	{ }
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void numa_clear_node(int cpu)		{ }
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void numa_add_cpu(int cpu, int node)	{ }
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void numa_remove_cpu(int cpu)		{ }
#endif
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/numa.h" 2
#endif
#line 30 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/acpi.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/processor.h" 1
#line 31 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/acpi.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/mmu.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/spinlock.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/mmu.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/mutex.h" 1
/*
 * Mutexes: blocking mutual exclusion locks
 *
 * started by Ingo Molnar:
 *
 *  Copyright (C) 2004, 2005, 2006 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>
 *
 * This file contains the main data structure and API definitions.
 */
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/mmu.h" 2
/*
 * The x86 doesn't have a mmu context, but
 * we put the segment information here.
 */
typedef struct {
	void *ldt;
	int size;
	struct mutex lock;
	void *vdso;
} mm_context_t;
void leave_mm(int cpu);
#line 32 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/acpi.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/mpspec.h" 1
#line 33 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/acpi.h" 2
/*
 * Calling conventions:
 *
 * ACPI_SYSTEM_XFACE        - Interfaces to host OS (handlers, threads)
 * ACPI_EXTERNAL_XFACE      - External ACPI interfaces
 * ACPI_INTERNAL_XFACE      - Internal ACPI interfaces
 * ACPI_INTERNAL_VAR_XFACE  - Internal variable-parameter list interfaces
 */
/* Asm macros */
int __acpi_acquire_global_lock(unsigned int *lock);
int __acpi_release_global_lock(unsigned int *lock);
/*
 * Math helper asm macros
 */
#if definedEx(CONFIG_ACPI)
extern int acpi_lapic;
extern int acpi_ioapic;
extern int acpi_noirq;
extern int acpi_strict;
extern int acpi_disabled;
extern int acpi_ht;
extern int acpi_pci_disabled;
extern int acpi_skip_timer_override;
extern int acpi_use_timer_override;
extern u8 acpi_sci_flags;
extern int acpi_sci_override_gsi;
void acpi_pic_sci_set_trigger(unsigned int, u16);
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void disable_acpi(void)
{
	acpi_disabled = 1;
	acpi_ht = 0;
	acpi_pci_disabled = 1;
	acpi_noirq = 1;
}
extern int acpi_gsi_to_irq(u32 gsi, unsigned int *irq);
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void acpi_noirq_set(void) { acpi_noirq = 1; }
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void acpi_disable_pci(void)
{
	acpi_pci_disabled = 1;
	acpi_noirq_set();
}
/* routines for saving/restoring kernel state */
extern int acpi_save_state_mem(void);
extern void acpi_restore_state_mem(void);
extern unsigned long acpi_wakeup_address;
/* early initialization routine */
extern void acpi_reserve_wakeup_memory(void);
/*
 * Check if the CPU can handle C2 and deeper
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned int acpi_processor_cstate_check(unsigned int max_cstate)
{
	/*
	 * Early models (<=5) of AMD Opterons are not supposed to go into
	 * C2 state.
	 *
	 * Steppings 0x0A and later are good
	 */
	if (boot_cpu_data.x86 == 0x0F &&
	    boot_cpu_data.x86_vendor == 2 &&
	    boot_cpu_data.x86_model <= 0x05 &&
	    boot_cpu_data.x86_mask < 0x0A)
		return 1;
	else if ((__builtin_constant_p((3*32+21)) && ( ((((3*32+21))>>5)==0 && (1UL<<(((3*32+21))&31) & (
#if !definedEx(CONFIG_MATH_EMULATION)
(1<<((0*32+ 0) & 31))
#endif
#if definedEx(CONFIG_MATH_EMULATION)
0
#endif
|
#if !definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_64) && definedEx(CONFIG_PARAVIRT)
0
#endif
#if definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT)
(1<<((0*32+ 3)) & 31)
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+ 5) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if !definedEx(CONFIG_X86_PAE) && definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_PAE)
(1<<((0*32+ 6) & 31))
#endif
#if !definedEx(CONFIG_X86_PAE) && !definedEx(CONFIG_X86_64)
0
#endif
| 
#if definedEx(CONFIG_X86_CMPXCHG64)
(1<<((0*32+ 8) & 31))
#endif
#if !definedEx(CONFIG_X86_CMPXCHG64)
0
#endif
|
#if !definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_64) && definedEx(CONFIG_PARAVIRT)
0
#endif
#if definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT)
(1<<((0*32+13)) & 31)
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+24) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if !definedEx(CONFIG_X86_64) && definedEx(CONFIG_X86_CMOV) || definedEx(CONFIG_X86_64)
(1<<((0*32+15) & 31))
#endif
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_X86_CMOV)
0
#endif
| 
#if definedEx(CONFIG_X86_64)
(1<<((0*32+25) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+26) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
))) || ((((3*32+21))>>5)==1 && (1UL<<(((3*32+21))&31) & (
#if definedEx(CONFIG_X86_64)
(1<<((1*32+29) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if definedEx(CONFIG_X86_USE_3DNOW)
(1<<((1*32+31) & 31))
#endif
#if !definedEx(CONFIG_X86_USE_3DNOW)
0
#endif
))) || ((((3*32+21))>>5)==2 && (1UL<<(((3*32+21))&31) & 0)) || ((((3*32+21))>>5)==3 && (1UL<<(((3*32+21))&31) & (
#if !definedEx(CONFIG_X86_64) && definedEx(CONFIG_X86_P6_NOP) || definedEx(CONFIG_X86_64)
(1<<((3*32+20) & 31))
#endif
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_X86_P6_NOP)
0
#endif
))) || ((((3*32+21))>>5)==4 && (1UL<<(((3*32+21))&31) & 0)) || ((((3*32+21))>>5)==5 && (1UL<<(((3*32+21))&31) & 0)) || ((((3*32+21))>>5)==6 && (1UL<<(((3*32+21))&31) & 0)) || ((((3*32+21))>>5)==7 && (1UL<<(((3*32+21))&31) & 0)) ) ? 1 : (__builtin_constant_p(((3*32+21))) ? constant_test_bit(((3*32+21)), ((unsigned long *)((&boot_cpu_data)->x86_capability))) : variable_test_bit(((3*32+21)), ((unsigned long *)((&boot_cpu_data)->x86_capability))))))
		return 1;
	else
		return max_cstate;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 bool arch_has_acpi_pdc(void)
{
	struct cpuinfo_x86 *c = &(*({ unsigned long __ptr; __asm__ ("" : "=r"(__ptr) : "0"((&per_cpu__cpu_info))); (typeof((&per_cpu__cpu_info))) (__ptr + (((__per_cpu_offset[0])))); }));
	return (c->x86_vendor == 0 ||
		c->x86_vendor == 5);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void arch_acpi_set_pdc_bits(u32 *buf)
{
	struct cpuinfo_x86 *c = &(*({ unsigned long __ptr; __asm__ ("" : "=r"(__ptr) : "0"((&per_cpu__cpu_info))); (typeof((&per_cpu__cpu_info))) (__ptr + (((__per_cpu_offset[0])))); }));
	buf[2] |= ((0x0010) | (0x0008) | (0x0002) | (0x0100) | (0x0200));
	if ((__builtin_constant_p((4*32+ 7)) && ( ((((4*32+ 7))>>5)==0 && (1UL<<(((4*32+ 7))&31) & (
#if !definedEx(CONFIG_MATH_EMULATION)
(1<<((0*32+ 0) & 31))
#endif
#if definedEx(CONFIG_MATH_EMULATION)
0
#endif
|
#if !definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_64) && definedEx(CONFIG_PARAVIRT)
0
#endif
#if definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT)
(1<<((0*32+ 3)) & 31)
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+ 5) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if !definedEx(CONFIG_X86_PAE) && definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_PAE)
(1<<((0*32+ 6) & 31))
#endif
#if !definedEx(CONFIG_X86_PAE) && !definedEx(CONFIG_X86_64)
0
#endif
| 
#if definedEx(CONFIG_X86_CMPXCHG64)
(1<<((0*32+ 8) & 31))
#endif
#if !definedEx(CONFIG_X86_CMPXCHG64)
0
#endif
|
#if !definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_64) && definedEx(CONFIG_PARAVIRT)
0
#endif
#if definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT)
(1<<((0*32+13)) & 31)
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+24) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if !definedEx(CONFIG_X86_64) && definedEx(CONFIG_X86_CMOV) || definedEx(CONFIG_X86_64)
(1<<((0*32+15) & 31))
#endif
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_X86_CMOV)
0
#endif
| 
#if definedEx(CONFIG_X86_64)
(1<<((0*32+25) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+26) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
))) || ((((4*32+ 7))>>5)==1 && (1UL<<(((4*32+ 7))&31) & (
#if definedEx(CONFIG_X86_64)
(1<<((1*32+29) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if definedEx(CONFIG_X86_USE_3DNOW)
(1<<((1*32+31) & 31))
#endif
#if !definedEx(CONFIG_X86_USE_3DNOW)
0
#endif
))) || ((((4*32+ 7))>>5)==2 && (1UL<<(((4*32+ 7))&31) & 0)) || ((((4*32+ 7))>>5)==3 && (1UL<<(((4*32+ 7))&31) & (
#if !definedEx(CONFIG_X86_64) && definedEx(CONFIG_X86_P6_NOP) || definedEx(CONFIG_X86_64)
(1<<((3*32+20) & 31))
#endif
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_X86_P6_NOP)
0
#endif
))) || ((((4*32+ 7))>>5)==4 && (1UL<<(((4*32+ 7))&31) & 0)) || ((((4*32+ 7))>>5)==5 && (1UL<<(((4*32+ 7))&31) & 0)) || ((((4*32+ 7))>>5)==6 && (1UL<<(((4*32+ 7))&31) & 0)) || ((((4*32+ 7))>>5)==7 && (1UL<<(((4*32+ 7))&31) & 0)) ) ? 1 : (__builtin_constant_p(((4*32+ 7))) ? constant_test_bit(((4*32+ 7)), ((unsigned long *)((c)->x86_capability))) : variable_test_bit(((4*32+ 7)), ((unsigned long *)((c)->x86_capability))))))
		buf[2] |= ((0x0008) | (0x0002) | (0x0020) | (0x0800) | (0x0001));
	if ((__builtin_constant_p((0*32+22)) && ( ((((0*32+22))>>5)==0 && (1UL<<(((0*32+22))&31) & (
#if !definedEx(CONFIG_MATH_EMULATION)
(1<<((0*32+ 0) & 31))
#endif
#if definedEx(CONFIG_MATH_EMULATION)
0
#endif
|
#if !definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_64) && definedEx(CONFIG_PARAVIRT)
0
#endif
#if definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT)
(1<<((0*32+ 3)) & 31)
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+ 5) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if !definedEx(CONFIG_X86_PAE) && definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_PAE)
(1<<((0*32+ 6) & 31))
#endif
#if !definedEx(CONFIG_X86_PAE) && !definedEx(CONFIG_X86_64)
0
#endif
| 
#if definedEx(CONFIG_X86_CMPXCHG64)
(1<<((0*32+ 8) & 31))
#endif
#if !definedEx(CONFIG_X86_CMPXCHG64)
0
#endif
|
#if !definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_64) && definedEx(CONFIG_PARAVIRT)
0
#endif
#if definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT)
(1<<((0*32+13)) & 31)
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+24) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if !definedEx(CONFIG_X86_64) && definedEx(CONFIG_X86_CMOV) || definedEx(CONFIG_X86_64)
(1<<((0*32+15) & 31))
#endif
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_X86_CMOV)
0
#endif
| 
#if definedEx(CONFIG_X86_64)
(1<<((0*32+25) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+26) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
))) || ((((0*32+22))>>5)==1 && (1UL<<(((0*32+22))&31) & (
#if definedEx(CONFIG_X86_64)
(1<<((1*32+29) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if definedEx(CONFIG_X86_USE_3DNOW)
(1<<((1*32+31) & 31))
#endif
#if !definedEx(CONFIG_X86_USE_3DNOW)
0
#endif
))) || ((((0*32+22))>>5)==2 && (1UL<<(((0*32+22))&31) & 0)) || ((((0*32+22))>>5)==3 && (1UL<<(((0*32+22))&31) & (
#if !definedEx(CONFIG_X86_64) && definedEx(CONFIG_X86_P6_NOP) || definedEx(CONFIG_X86_64)
(1<<((3*32+20) & 31))
#endif
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_X86_P6_NOP)
0
#endif
))) || ((((0*32+22))>>5)==4 && (1UL<<(((0*32+22))&31) & 0)) || ((((0*32+22))>>5)==5 && (1UL<<(((0*32+22))&31) & 0)) || ((((0*32+22))>>5)==6 && (1UL<<(((0*32+22))&31) & 0)) || ((((0*32+22))>>5)==7 && (1UL<<(((0*32+22))&31) & 0)) ) ? 1 : (__builtin_constant_p(((0*32+22))) ? constant_test_bit(((0*32+22)), ((unsigned long *)((c)->x86_capability))) : variable_test_bit(((0*32+22)), ((unsigned long *)((c)->x86_capability))))))
		buf[2] |= (0x0004);
	/*
	 * If mwait/monitor is unsupported, C2/C3_FFH will be disabled
	 */
	if (!(__builtin_constant_p((4*32+ 3)) && ( ((((4*32+ 3))>>5)==0 && (1UL<<(((4*32+ 3))&31) & (
#if !definedEx(CONFIG_MATH_EMULATION)
(1<<((0*32+ 0) & 31))
#endif
#if definedEx(CONFIG_MATH_EMULATION)
0
#endif
|
#if !definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_64) && definedEx(CONFIG_PARAVIRT)
0
#endif
#if definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT)
(1<<((0*32+ 3)) & 31)
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+ 5) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if !definedEx(CONFIG_X86_PAE) && definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_PAE)
(1<<((0*32+ 6) & 31))
#endif
#if !definedEx(CONFIG_X86_PAE) && !definedEx(CONFIG_X86_64)
0
#endif
| 
#if definedEx(CONFIG_X86_CMPXCHG64)
(1<<((0*32+ 8) & 31))
#endif
#if !definedEx(CONFIG_X86_CMPXCHG64)
0
#endif
|
#if !definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_64) && definedEx(CONFIG_PARAVIRT)
0
#endif
#if definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT)
(1<<((0*32+13)) & 31)
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+24) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if !definedEx(CONFIG_X86_64) && definedEx(CONFIG_X86_CMOV) || definedEx(CONFIG_X86_64)
(1<<((0*32+15) & 31))
#endif
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_X86_CMOV)
0
#endif
| 
#if definedEx(CONFIG_X86_64)
(1<<((0*32+25) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+26) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
))) || ((((4*32+ 3))>>5)==1 && (1UL<<(((4*32+ 3))&31) & (
#if definedEx(CONFIG_X86_64)
(1<<((1*32+29) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if definedEx(CONFIG_X86_USE_3DNOW)
(1<<((1*32+31) & 31))
#endif
#if !definedEx(CONFIG_X86_USE_3DNOW)
0
#endif
))) || ((((4*32+ 3))>>5)==2 && (1UL<<(((4*32+ 3))&31) & 0)) || ((((4*32+ 3))>>5)==3 && (1UL<<(((4*32+ 3))&31) & (
#if !definedEx(CONFIG_X86_64) && definedEx(CONFIG_X86_P6_NOP) || definedEx(CONFIG_X86_64)
(1<<((3*32+20) & 31))
#endif
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_X86_P6_NOP)
0
#endif
))) || ((((4*32+ 3))>>5)==4 && (1UL<<(((4*32+ 3))&31) & 0)) || ((((4*32+ 3))>>5)==5 && (1UL<<(((4*32+ 3))&31) & 0)) || ((((4*32+ 3))>>5)==6 && (1UL<<(((4*32+ 3))&31) & 0)) || ((((4*32+ 3))>>5)==7 && (1UL<<(((4*32+ 3))&31) & 0)) ) ? 1 : (__builtin_constant_p(((4*32+ 3))) ? constant_test_bit(((4*32+ 3)), ((unsigned long *)((c)->x86_capability))) : variable_test_bit(((4*32+ 3)), ((unsigned long *)((c)->x86_capability))))))
		buf[2] &= ~((0x0200));
}
#endif
#if !definedEx(CONFIG_ACPI)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void acpi_noirq_set(void) { }
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void acpi_disable_pci(void) { }
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void disable_acpi(void) { }
#endif
struct bootnode;
#if definedEx(CONFIG_ACPI_NUMA)
extern int acpi_numa;
extern int acpi_get_nodes(struct bootnode *physnodes);
extern int acpi_scan_nodes(unsigned long start, unsigned long end);
extern void acpi_fake_nodes(const struct bootnode *fake_nodes,
				   int num_nodes);
#endif
#if !definedEx(CONFIG_ACPI_NUMA)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void acpi_fake_nodes(const struct bootnode *fake_nodes,
				   int num_nodes)
{
}
#endif
#line 21 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/fixmap.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/apicdef.h" 1
#line 22 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/fixmap.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/page.h" 1
#line 23 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/fixmap.h" 2
#if definedEx(CONFIG_X86_32)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/threads.h" 1
#line 25 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/fixmap.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/kmap_types.h" 1
#if !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_X86_LOCAL_APIC)
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_DEBUG_HIGHMEM)
#endif
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/asm-generic/kmap_types.h" 1
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
#endif
#if !definedEx(CONFIG_X86_32) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_DEBUG_HIGHMEM) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_DEBUG_HIGHMEM) && !definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_PARAVIRT) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES)
#endif
enum km_type {
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
__KM_FENCE_0 ,
#endif
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
#endif
	KM_BOUNCE_READ,
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
__KM_FENCE_1 ,
#endif
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
#endif
	KM_SKB_SUNRPC_DATA,
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
__KM_FENCE_2 ,
#endif
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
#endif
	KM_SKB_DATA_SOFTIRQ,
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
__KM_FENCE_3 ,
#endif
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
#endif
	KM_USER0,
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
__KM_FENCE_4 ,
#endif
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
#endif
	KM_USER1,
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
__KM_FENCE_5 ,
#endif
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
#endif
	KM_BIO_SRC_IRQ,
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
__KM_FENCE_6 ,
#endif
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
#endif
	KM_BIO_DST_IRQ,
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
__KM_FENCE_7 ,
#endif
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
#endif
	KM_PTE0,
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
__KM_FENCE_8 ,
#endif
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
#endif
	KM_PTE1,
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
__KM_FENCE_9 ,
#endif
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
#endif
	KM_IRQ0,
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
__KM_FENCE_10 ,
#endif
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
#endif
	KM_IRQ1,
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
__KM_FENCE_11 ,
#endif
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
#endif
	KM_SOFTIRQ0,
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
__KM_FENCE_12 ,
#endif
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
#endif
	KM_SOFTIRQ1,
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
__KM_FENCE_13 ,
#endif
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
#endif
	KM_SYNC_ICACHE,
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
__KM_FENCE_14 ,
#endif
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
#endif
	KM_SYNC_DCACHE,
/* UML specific, for copy_*_user - used in do_op_one_page */
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
__KM_FENCE_15 ,
#endif
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
#endif
	KM_UML_USERCOPY,
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
__KM_FENCE_16 ,
#endif
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
#endif
	KM_IRQ_PTE,
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
__KM_FENCE_17 ,
#endif
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
#endif
	KM_NMI,
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
__KM_FENCE_18 ,
#endif
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
#endif
	KM_NMI_PTE,
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) && definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
__KM_FENCE_19 ,
#endif
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_PARAVIRT) && !definedEx(CONFIG_DEBUG_HIGHMEM) && definedEx(CONFIG_X86_LOCAL_APIC)
#endif
	KM_TYPE_NR
};
#line 10 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/kmap_types.h" 2
#endif
#line 26 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/fixmap.h" 2
#endif
#if !definedEx(CONFIG_X86_32)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/vsyscall.h" 1
enum vsyscall_num {
	__NR_vgettimeofday,
	__NR_vtime,
	__NR_vgetcpu,
};
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/seqlock.h" 1
#line 19 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/vsyscall.h" 2
/* Definitions for CONFIG_GENERIC_TIME definitions */
extern int __vgetcpu_mode;
extern volatile unsigned long __jiffies;
/* kernel space (writeable) */
extern int vgetcpu_mode;
extern struct timezone sys_tz;
extern void map_vsyscall(void);
#line 28 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/fixmap.h" 2
#endif
/*
 * We can't declare FIXADDR_TOP as variable for x86_64 because vsyscall
 * uses fixmaps that relies on FIXADDR_TOP for proper address calculation.
 * Because of this, FIXADDR_TOP x86 integration was left as later work.
 */
#if definedEx(CONFIG_X86_32)
/* used by vmalloc.c, vsyscall.lds.S.
 *
 * Leave one empty page between vmalloc'ed areas and
 * the start of the fixmap.
 */
extern unsigned long __FIXADDR_TOP;
#endif
#if !definedEx(CONFIG_X86_32)
/* Only covers 32bit vsyscalls currently. Need another set for 64bit. */
#endif
/*
 * Here we define all the compile-time 'special' virtual
 * addresses. The point is to have a constant address at
 * compile time, but to set the physical address only
 * in the boot process.
 * for x86_32: We allocate these special addresses
 * from the end of virtual memory (0xfffff000) backwards.
 * Also this lets us do fail-safe vmalloc(), we
 * can guarantee that these special addresses and
 * vmalloc()-ed addresses never overlap.
 *
 * These 'compile-time allocated' memory buffers are
 * fixed-size 4k pages (or larger if used with an increment
 * higher than 1). Use set_fixmap(idx,phys) to associate
 * physical memory with fixmap indices.
 *
 * TLB entries of such buffers will not be flushed across
 * task switches.
 */
enum fixed_addresses {
#if definedEx(CONFIG_X86_32)
	FIX_HOLE,
	FIX_VDSO,
#endif
#if !definedEx(CONFIG_X86_32)
	VSYSCALL_LAST_PAGE,
	VSYSCALL_FIRST_PAGE = VSYSCALL_LAST_PAGE
			    + (((-2UL << 20)-(-10UL << 20)) >> 12) - 1,
	VSYSCALL_HPET,
#endif
	FIX_DBGP_BASE,
	FIX_EARLYCON_MEM_BASE,
#if definedEx(CONFIG_PROVIDE_OHCI1394_DMA_INIT)
	FIX_OHCI1394_BASE,
#endif
	FIX_APIC_BASE,	/* local (CPU) APIC) -- required for SMP or not */
#if definedEx(CONFIG_X86_IO_APIC)
	FIX_IO_APIC_BASE_0,
	FIX_IO_APIC_BASE_END = FIX_IO_APIC_BASE_0 + 
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_X86_LOCAL_APIC)
64
#endif
#if !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC)
128
#endif
 - 1,
#endif
#if definedEx(CONFIG_X86_VISWS_APIC)
	FIX_CO_CPU,	/* Cobalt timer */
	FIX_CO_APIC,	/* Cobalt APIC Redirection Table */
	FIX_LI_PCIA,	/* Lithium PCI Bridge A */
	FIX_LI_PCIB,	/* Lithium PCI Bridge B */
#endif
#if definedEx(CONFIG_X86_F00F_BUG)
	FIX_F00F_IDT,	/* Virtual mapping for IDT */
#endif
#if definedEx(CONFIG_X86_CYCLONE_TIMER)
	FIX_CYCLONE_TIMER, /*cyclone timer register*/
#endif
#if definedEx(CONFIG_X86_32)
	FIX_KMAP_BEGIN,	/* reserved pte's for temporary kernel mappings */
	FIX_KMAP_END = FIX_KMAP_BEGIN+(KM_TYPE_NR*8)-1,
#if definedEx(CONFIG_PCI_MMCONFIG)
	FIX_PCIE_MCFG,
#endif
#endif
#if definedEx(CONFIG_PARAVIRT)
	FIX_PARAVIRT_BOOTMAP,
#endif
	FIX_TEXT_POKE1,	/* reserve 2 pages for text_poke() */
	FIX_TEXT_POKE0, /* first page is last, because allocation is backward */
	__end_of_permanent_fixed_addresses,
	/*
	 * 256 temporary boot-time mappings, used by early_ioremap(),
	 * before ioremap() is functional.
	 *
	 * We round it up to the next 256 pages boundary so that we
	 * can have a single pgd entry and a single pte table:
	 */
	FIX_BTMAP_END = __end_of_permanent_fixed_addresses + 256 -
			(__end_of_permanent_fixed_addresses & 255),
	FIX_BTMAP_BEGIN = FIX_BTMAP_END + 64*4 - 1,
#if definedEx(CONFIG_X86_32)
	FIX_WP_TEST,
#endif
#if definedEx(CONFIG_INTEL_TXT)
	FIX_TBOOT_BASE,
#endif
	__end_of_fixed_addresses
};
extern void reserve_top_address(unsigned long reserve);
extern int fixmaps_set;
extern pte_t *kmap_pte;
extern pgprot_t kmap_prot;
extern pte_t *pkmap_page_table;
void __native_set_fixmap(enum fixed_addresses idx, pte_t pte);
void native_set_fixmap(enum fixed_addresses idx,
		       phys_addr_t phys, pgprot_t flags);
#if !definedEx(CONFIG_PARAVIRT)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __set_fixmap(enum fixed_addresses idx,
				phys_addr_t phys, pgprot_t flags)
{
	native_set_fixmap(idx, phys, flags);
}
#endif
/*
 * Some hardware wants to get fixmapped without caching.
 */
extern void __this_fixmap_does_not_exist(void);
/*
 * 'index to address' translation. If anyone tries to use the idx
 * directly without translation, we catch the bug with a NULL-deference
 * kernel oops. Illegal ranges of incoming indices are caught too.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 __attribute__((always_inline)) unsigned long fix_to_virt(const unsigned int idx)
{
	/*
	 * this branch gets completely eliminated after inlining,
	 * except when someone tries to use fixaddr indices in an
	 * illegal way. (such as mixing up address types or using
	 * out-of-range indices).
	 *
	 * If it doesn't get removed, the linker will complain
	 * loudly with a reasonably clear error message..
	 */
	if (idx >= __end_of_fixed_addresses)
		__this_fixmap_does_not_exist();
	return (
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_X86_LOCAL_APIC)
((unsigned long)__FIXADDR_TOP)
#endif
#if !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC)
((-2UL << 20)-((1UL) << 12))
#endif
 - ((idx) << 12));
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long virt_to_fix(const unsigned long vaddr)
{
#if definedEx(CONFIG_BUG)
do { if (__builtin_expect(!!(vaddr >= 
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_X86_LOCAL_APIC)
((unsigned long)__FIXADDR_TOP)
#endif
#if !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC)
(
(-2UL << 20)
-((1UL) << 12))
#endif
 || vaddr <(
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_X86_LOCAL_APIC)
((unsigned long)__FIXADDR_TOP)
#endif
#if !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC)
(
(-2UL << 20)
-((1UL) << 12))
#endif
 -(__end_of_permanent_fixed_addresses << 12))), 0)) 
#if definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("1:\tud2\n" ".pushsection __bug_table,\"a\"\n" 
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b, %c0\n"
#endif
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_BUG) && definedEx(CONFIG_DEBUG_BUGVERBOSE)
"2:\t.long 1b - 2b, %c0 - 2b\n"
#endif
 "\t.word %c1, 0\n" "\t.org 2b+%c2\n" ".popsection" : : "i" ("/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/fixmap.h"), "i" (208), "i" (sizeof(struct bug_entry))); __builtin_unreachable(); } while (0)
#endif
#if definedEx(CONFIG_BUG) && !definedEx(CONFIG_DEBUG_BUGVERBOSE)
do { asm volatile("ud2"); __builtin_unreachable(); } while (0)
#endif
; } while(0)
#endif
#if !definedEx(CONFIG_BUG)
do { if (vaddr >= 
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_X86_LOCAL_APIC)
((unsigned long)__FIXADDR_TOP)
#endif
#if !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC)
(
(-2UL << 20)
-((1UL) << 12))
#endif
 || vaddr < (
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_X86_LOCAL_APIC)
((unsigned long)__FIXADDR_TOP)
#endif
#if !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC)
(
(-2UL << 20)
-((1UL) << 12))
#endif
 - (__end_of_permanent_fixed_addresses << 12))) ; } while(0)
#endif
;
	return ((
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_X86_LOCAL_APIC)
((unsigned long)__FIXADDR_TOP)
#endif
#if !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC)
((-2UL << 20)-((1UL) << 12))
#endif
 - ((vaddr)&(~(((1UL) << 12)-1)))) >> 12);
}
#line 15 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/apic.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/mpspec.h" 1
#line 16 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/apic.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/system.h" 1
#line 17 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/apic.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/msr.h" 1
#line 18 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/apic.h" 2
/*
 * Debugging macros
 */
/*
 * Define the default level of output to be very little
 * This can be turned up by using apic=verbose for more
 * information and apic=debug for _lots_ of information.
 * apic_verbosity is defined in apic.c
 */
#if definedEx(CONFIG_X86_32) && definedEx(CONFIG_X86_LOCAL_APIC)
extern void generic_apic_probe(void);
#endif
#if !definedEx(CONFIG_X86_32) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_X86_LOCAL_APIC)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void generic_apic_probe(void)
{
}
#endif
extern unsigned int apic_verbosity;
extern int local_apic_timer_c2_ok;
extern int disable_apic;
extern void __inquire_remote_apic(int apicid);
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void default_inquire_remote_apic(int apicid)
{
	if (apic_verbosity >= 2)
		__inquire_remote_apic(apicid);
}
/*
 * With 82489DX we can't rely on apic feature bit
 * retrieved via cpuid but still have to deal with
 * such an apic chip so we assume that SMP configuration
 * is found from MP table (64bit case uses ACPI mostly
 * which set smp presence flag as well so we are safe
 * to use this helper too).
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 bool apic_from_smp_config(void)
{
	return smp_found_config && !disable_apic;
}
/*
 * Basic functions accessing APICs.
 */
#if definedEx(CONFIG_PARAVIRT)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/paravirt.h" 1
#line 87 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/apic.h" 2
#endif
#if definedEx(CONFIG_X86_64)
extern int is_vsmp_box(void);
#endif
#if !definedEx(CONFIG_X86_64)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int is_vsmp_box(void)
{
	return 0;
}
#endif
extern void xapic_wait_icr_idle(void);
extern u32 safe_xapic_wait_icr_idle(void);
extern void xapic_icr_write(u32, u32);
extern int setup_profiling_timer(unsigned int);
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void native_apic_mem_write(u32 reg, u32 v)
{
	volatile u32 *addr = (volatile u32 *)((fix_to_virt(FIX_APIC_BASE)) + reg);
	asm volatile ("661:\n\t" "movl %0, %1" "\n662:\n" ".section .altinstructions,\"a\"\n" 
#if definedEx(CONFIG_X86_32)
" " ".balign 4" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".balign 8" " "
#endif
 "\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 "661b\n" 
#if definedEx(CONFIG_X86_32)
" " ".long" " "
#endif
#if !definedEx(CONFIG_X86_32)
" " ".quad" " "
#endif
 "663f\n" "	 .byte " "(3*32+19)" "\n" "	 .byte 662b-661b\n" "	 .byte 664f-663f\n" "	 .byte 0xff + (664f-663f) - (662b-661b)\n" ".previous\n" ".section .altinstr_replacement, \"ax\"\n" "663:\n\t" "xchgl %0, %1" "\n664:\n" ".previous" : 
 "=r"(v), "=m"(*addr) : "i" (0),"0"(v), "m"(*addr));
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 u32 native_apic_mem_read(u32 reg)
{
	return *((volatile u32 *)((fix_to_virt(FIX_APIC_BASE)) + reg));
}
extern void native_apic_wait_icr_idle(void);
extern u32 native_safe_apic_wait_icr_idle(void);
extern void native_apic_icr_write(u32 low, u32 id);
extern u64 native_apic_icr_read(void);
extern int x2apic_mode;
#if definedEx(CONFIG_X86_X2APIC)
/*
 * Make previous memory operations globally visible before
 * sending the IPI through x2apic wrmsr. We need a serializing instruction or
 * mfence for this.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void x2apic_wrmsr_fence(void)
{
	asm volatile("mfence" : : : "memory");
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void native_apic_msr_write(u32 reg, u32 v)
{
	if (reg == 0xE0 || reg == 0x20 || reg == 0xD0 ||
	    reg == 0x30)
		return;
#if definedEx(CONFIG_PARAVIRT)
do { paravirt_write_msr(0x800 +(reg >> 4), v, 0); } while (0)
#endif
#if !definedEx(CONFIG_PARAVIRT)
wrmsr(0x800 + (reg >> 4), v, 0)
#endif
;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 u32 native_apic_msr_read(u32 reg)
{
	u32 low, high;
	if (reg == 0xE0)
		return -1;
#if definedEx(CONFIG_PARAVIRT)
do { int _err; u64 _l = paravirt_read_msr(0x800 +(reg >> 4), &_err); low = (u32)_l; high = _l >> 32; } while (0)
#endif
#if !definedEx(CONFIG_PARAVIRT)
do { u64 __val = native_read_msr((0x800 +(reg >> 4))); (low) = (u32)__val; (high) = (u32)(__val >> 32); } while (0)
#endif
;
	return low;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void native_x2apic_wait_icr_idle(void)
{
	/* no need to wait for icr idle in x2apic */
	return;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 u32 native_safe_x2apic_wait_icr_idle(void)
{
	/* no need to wait for icr idle in x2apic */
	return 0;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void native_x2apic_icr_write(u32 low, u32 id)
{
#if definedEx(CONFIG_PARAVIRT)
do { paravirt_write_msr(0x800 +(0x300 >> 4), (u32)((u64)(((__u64) id) << 32 | low)), ((u64)(((__u64) id) << 32 | low))>>32); } while (0)
#endif
#if !definedEx(CONFIG_PARAVIRT)
native_write_msr((0x800 +(0x300 >> 4)), (u32)((u64)(((__u64) id) << 32 | low)), (u32)((u64)(((__u64) id) << 32 | low) >> 32))
#endif
;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 u64 native_x2apic_icr_read(void)
{
	unsigned long val;
#if definedEx(CONFIG_PARAVIRT)
do { int _err; val = paravirt_read_msr(0x800 +(0x300 >> 4), &_err); } while (0)
#endif
#if !definedEx(CONFIG_PARAVIRT)
((val) = native_read_msr((0x800 +(0x300 >> 4))))
#endif
;
	return val;
}
extern int x2apic_phys;
extern void check_x2apic(void);
extern void enable_x2apic(void);
extern void x2apic_icr_write(u32 low, u32 id);
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int x2apic_enabled(void)
{
	int msr, msr2;
	if (!(__builtin_constant_p((4*32+21)) && ( ((((4*32+21))>>5)==0 && (1UL<<(((4*32+21))&31) & (
#if !definedEx(CONFIG_MATH_EMULATION)
(1<<((0*32+ 0) & 31))
#endif
#if definedEx(CONFIG_MATH_EMULATION)
0
#endif
|
#if !definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_64) && definedEx(CONFIG_PARAVIRT)
0
#endif
#if definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT)
(1<<((0*32+ 3)) & 31)
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+ 5) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if !definedEx(CONFIG_X86_PAE) && definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_PAE)
(1<<((0*32+ 6) & 31))
#endif
#if !definedEx(CONFIG_X86_PAE) && !definedEx(CONFIG_X86_64)
0
#endif
| 
#if definedEx(CONFIG_X86_CMPXCHG64)
(1<<((0*32+ 8) & 31))
#endif
#if !definedEx(CONFIG_X86_CMPXCHG64)
0
#endif
|
#if !definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_64) && definedEx(CONFIG_PARAVIRT)
0
#endif
#if definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT)
(1<<((0*32+13)) & 31)
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+24) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if !definedEx(CONFIG_X86_64) && definedEx(CONFIG_X86_CMOV) || definedEx(CONFIG_X86_64)
(1<<((0*32+15) & 31))
#endif
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_X86_CMOV)
0
#endif
| 
#if definedEx(CONFIG_X86_64)
(1<<((0*32+25) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+26) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
))) || ((((4*32+21))>>5)==1 && (1UL<<(((4*32+21))&31) & (
#if definedEx(CONFIG_X86_64)
(1<<((1*32+29) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if definedEx(CONFIG_X86_USE_3DNOW)
(1<<((1*32+31) & 31))
#endif
#if !definedEx(CONFIG_X86_USE_3DNOW)
0
#endif
))) || ((((4*32+21))>>5)==2 && (1UL<<(((4*32+21))&31) & 0)) || ((((4*32+21))>>5)==3 && (1UL<<(((4*32+21))&31) & (
#if !definedEx(CONFIG_X86_64) && definedEx(CONFIG_X86_P6_NOP) || definedEx(CONFIG_X86_64)
(1<<((3*32+20) & 31))
#endif
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_X86_P6_NOP)
0
#endif
))) || ((((4*32+21))>>5)==4 && (1UL<<(((4*32+21))&31) & 0)) || ((((4*32+21))>>5)==5 && (1UL<<(((4*32+21))&31) & 0)) || ((((4*32+21))>>5)==6 && (1UL<<(((4*32+21))&31) & 0)) || ((((4*32+21))>>5)==7 && (1UL<<(((4*32+21))&31) & 0)) ) ? 1 : (__builtin_constant_p(((4*32+21))) ? constant_test_bit(((4*32+21)), ((unsigned long *)((&boot_cpu_data)->x86_capability))) : variable_test_bit(((4*32+21)), ((unsigned long *)((&boot_cpu_data)->x86_capability))))))
		return 0;
#if definedEx(CONFIG_PARAVIRT)
do { int _err; u64 _l = paravirt_read_msr(0x0000001b, &_err); msr = (u32)_l; msr2 = _l >> 32; } while (0)
#endif
#if !definedEx(CONFIG_PARAVIRT)
do { u64 __val = native_read_msr((0x0000001b)); (msr) = (u32)__val; (msr2) = (u32)(__val >> 32); } while (0)
#endif
;
	if (msr & (1UL << 10))
		return 1;
	return 0;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void x2apic_force_phys(void)
{
	x2apic_phys = 1;
}
#endif
#if !definedEx(CONFIG_X86_X2APIC)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void check_x2apic(void)
{
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void enable_x2apic(void)
{
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int x2apic_enabled(void)
{
	return 0;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void x2apic_force_phys(void)
{
}
#endif
extern void enable_IR_x2apic(void);
extern int get_physical_broadcast(void);
extern void apic_disable(void);
extern int lapic_get_maxlvt(void);
extern void clear_local_APIC(void);
extern void connect_bsp_APIC(void);
extern void disconnect_bsp_APIC(int virt_wire_setup);
extern void disable_local_APIC(void);
extern void lapic_shutdown(void);
extern int verify_local_APIC(void);
extern void cache_APIC_registers(void);
extern void sync_Arb_IDs(void);
extern void init_bsp_APIC(void);
extern void setup_local_APIC(void);
extern void end_local_APIC_setup(void);
extern void init_apic_mappings(void);
extern void setup_boot_APIC_clock(void);
extern void setup_secondary_APIC_clock(void);
extern int APIC_init_uniprocessor(void);
extern void enable_NMI_through_LVT0(void);
/*
 * On 32bit this is mach-xxx local
 */
#if definedEx(CONFIG_X86_64)
extern void early_init_lapic_mapping(void);
extern int apic_is_clustered_box(void);
#endif
#if !definedEx(CONFIG_X86_64)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int apic_is_clustered_box(void)
{
	return 0;
}
#endif
extern u8 setup_APIC_eilvt_mce(u8 vector, u8 msg_type, u8 mask);
extern u8 setup_APIC_eilvt_ibs(u8 vector, u8 msg_type, u8 mask);
#if definedEx(CONFIG_X86_64)
#endif
#if !definedEx(CONFIG_X86_64)
#endif
/*
 * Copyright 2004 James Cleverdon, IBM.
 * Subject to the GNU Public License, v.2
 *
 * Generic APIC sub-arch data struct.
 *
 * Hacked for x86-64 by James Cleverdon from i386 architecture code by
 * Martin Bligh, Andi Kleen, James Bottomley, John Stultz, and
 * James Cleverdon.
 */
struct apic {
	char *name;
	int (*probe)(void);
	int (*acpi_madt_oem_check)(char *oem_id, char *oem_table_id);
	int (*apic_id_registered)(void);
	u32 irq_delivery_mode;
	u32 irq_dest_mode;
	const struct cpumask *(*target_cpus)(void);
	int disable_esr;
	int dest_logical;
	unsigned long (*check_apicid_used)(physid_mask_t *map, int apicid);
	unsigned long (*check_apicid_present)(int apicid);
	void (*vector_allocation_domain)(int cpu, struct cpumask *retmask);
	void (*init_apic_ldr)(void);
	void (*ioapic_phys_id_map)(physid_mask_t *phys_map, physid_mask_t *retmap);
	void (*setup_apic_routing)(void);
	int (*multi_timer_check)(int apic, int irq);
	int (*apicid_to_node)(int logical_apicid);
	int (*cpu_to_logical_apicid)(int cpu);
	int (*cpu_present_to_apicid)(int mps_cpu);
	void (*apicid_to_cpu_present)(int phys_apicid, physid_mask_t *retmap);
	void (*setup_portio_remap)(void);
	int (*check_phys_apicid_present)(int phys_apicid);
	void (*enable_apic_mode)(void);
	int (*phys_pkg_id)(int cpuid_apic, int index_msb);
	/*
	 * When one of the next two hooks returns 1 the apic
	 * is switched to this. Essentially they are additional
	 * probe functions:
	 */
	int (*mps_oem_check)(struct mpc_table *mpc, char *oem, char *productid);
	unsigned int (*get_apic_id)(unsigned long x);
	unsigned long (*set_apic_id)(unsigned int id);
	unsigned long apic_id_mask;
	unsigned int (*cpu_mask_to_apicid)(const struct cpumask *cpumask);
	unsigned int (*cpu_mask_to_apicid_and)(const struct cpumask *cpumask,
					       const struct cpumask *andmask);
	/* ipi */
	void (*send_IPI_mask)(const struct cpumask *mask, int vector);
	void (*send_IPI_mask_allbutself)(const struct cpumask *mask,
					 int vector);
	void (*send_IPI_allbutself)(int vector);
	void (*send_IPI_all)(int vector);
	void (*send_IPI_self)(int vector);
	/* wakeup_secondary_cpu */
	int (*wakeup_secondary_cpu)(int apicid, unsigned long start_eip);
	int trampoline_phys_low;
	int trampoline_phys_high;
	void (*wait_for_init_deassert)(atomic_t *deassert);
	void (*smp_callin_clear_local_apic)(void);
	void (*inquire_remote_apic)(int apicid);
	/* apic ops */
	u32 (*read)(u32 reg);
	void (*write)(u32 reg, u32 v);
	u64 (*icr_read)(void);
	void (*icr_write)(u32 low, u32 high);
	void (*wait_icr_idle)(void);
	u32 (*safe_wait_icr_idle)(void);
};
/*
 * Pointer to the local APIC driver in use on this system (there's
 * always just one such driver in use - the kernel decides via an
 * early probing process which one it picks - and then sticks to it):
 */
extern struct apic *apic;
/*
 * APIC functionality to boot other CPUs - only used on SMP:
 */
extern atomic_t init_deasserted;
extern int wakeup_secondary_cpu_via_nmi(int apicid, unsigned long start_eip);
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 u32 apic_read(u32 reg)
{
	return apic->read(reg);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void apic_write(u32 reg, u32 val)
{
	apic->write(reg, val);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 u64 apic_icr_read(void)
{
	return apic->icr_read();
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void apic_icr_write(u32 low, u32 high)
{
	apic->icr_write(low, high);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void apic_wait_icr_idle(void)
{
	apic->wait_icr_idle();
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 u32 safe_apic_wait_icr_idle(void)
{
	return apic->safe_wait_icr_idle();
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void ack_APIC_irq(void)
{
	/*
	 * ack_APIC_irq() actually gets compiled as a single instruction
	 * ... yummie.
	 */
	/* Docs say use 0 for future compatibility */
	apic_write(0xB0, 0);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned default_get_apic_id(unsigned long x)
{
	unsigned int ver = ((apic_read(0x30)) & 0xFFu);
	if (((ver) >= 0x14) || (__builtin_constant_p((3*32+26)) && ( ((((3*32+26))>>5)==0 && (1UL<<(((3*32+26))&31) & (
#if !definedEx(CONFIG_MATH_EMULATION)
(1<<((0*32+ 0) & 31))
#endif
#if definedEx(CONFIG_MATH_EMULATION)
0
#endif
|
#if !definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_64) && definedEx(CONFIG_PARAVIRT)
0
#endif
#if definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT)
(1<<((0*32+ 3)) & 31)
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+ 5) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if !definedEx(CONFIG_X86_PAE) && definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_PAE)
(1<<((0*32+ 6) & 31))
#endif
#if !definedEx(CONFIG_X86_PAE) && !definedEx(CONFIG_X86_64)
0
#endif
| 
#if definedEx(CONFIG_X86_CMPXCHG64)
(1<<((0*32+ 8) & 31))
#endif
#if !definedEx(CONFIG_X86_CMPXCHG64)
0
#endif
|
#if !definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_64) && definedEx(CONFIG_PARAVIRT)
0
#endif
#if definedEx(CONFIG_X86_64) && !definedEx(CONFIG_PARAVIRT)
(1<<((0*32+13)) & 31)
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+24) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if !definedEx(CONFIG_X86_64) && definedEx(CONFIG_X86_CMOV) || definedEx(CONFIG_X86_64)
(1<<((0*32+15) & 31))
#endif
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_X86_CMOV)
0
#endif
| 
#if definedEx(CONFIG_X86_64)
(1<<((0*32+25) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if definedEx(CONFIG_X86_64)
(1<<((0*32+26) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
))) || ((((3*32+26))>>5)==1 && (1UL<<(((3*32+26))&31) & (
#if definedEx(CONFIG_X86_64)
(1<<((1*32+29) & 31))
#endif
#if !definedEx(CONFIG_X86_64)
0
#endif
|
#if definedEx(CONFIG_X86_USE_3DNOW)
(1<<((1*32+31) & 31))
#endif
#if !definedEx(CONFIG_X86_USE_3DNOW)
0
#endif
))) || ((((3*32+26))>>5)==2 && (1UL<<(((3*32+26))&31) & 0)) || ((((3*32+26))>>5)==3 && (1UL<<(((3*32+26))&31) & (
#if !definedEx(CONFIG_X86_64) && definedEx(CONFIG_X86_P6_NOP) || definedEx(CONFIG_X86_64)
(1<<((3*32+20) & 31))
#endif
#if !definedEx(CONFIG_X86_64) && !definedEx(CONFIG_X86_P6_NOP)
0
#endif
))) || ((((3*32+26))>>5)==4 && (1UL<<(((3*32+26))&31) & 0)) || ((((3*32+26))>>5)==5 && (1UL<<(((3*32+26))&31) & 0)) || ((((3*32+26))>>5)==6 && (1UL<<(((3*32+26))&31) & 0)) || ((((3*32+26))>>5)==7 && (1UL<<(((3*32+26))&31) & 0)) ) ? 1 : (__builtin_constant_p(((3*32+26))) ? constant_test_bit(((3*32+26)), ((unsigned long *)((&boot_cpu_data)->x86_capability))) : variable_test_bit(((3*32+26)), ((unsigned long *)((&boot_cpu_data)->x86_capability))))))
		return (x >> 24) & 0xFF;
	else
		return (x >> 24) & 0x0F;
}
/*
 * Warm reset vector default position:
 */
#if definedEx(CONFIG_X86_64)
extern struct apic apic_flat;
extern struct apic apic_physflat;
extern struct apic apic_x2apic_cluster;
extern struct apic apic_x2apic_phys;
extern int default_acpi_madt_oem_check(char *, char *);
extern void apic_send_IPI_self(int vector);
extern struct apic apic_x2apic_uv_x;
#if definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(".discard"), unused)) char __pcpu_scope_x2apic_extra_bits; extern __attribute__((section(".data.percpu" "")))  __typeof__(int) per_cpu__x2apic_extra_bits
#endif
#if !definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(".data.percpu" "")))  __typeof__(int) per_cpu__x2apic_extra_bits
#endif
;
extern int default_cpu_present_to_apicid(int mps_cpu);
extern int default_check_phys_apicid_present(int phys_apicid);
#endif
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void default_wait_for_init_deassert(atomic_t *deassert)
{
	while (!atomic_read(deassert))
		cpu_relax();
	return;
}
extern void generic_bigsmp_probe(void);
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/smp.h" 1
#line 466 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/apic.h" 2
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 const struct cpumask *default_target_cpus(void)
{
	return cpu_online_mask;
}
#if definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(".discard"), unused)) char __pcpu_scope_x86_bios_cpu_apicid; extern __attribute__((section(".data.percpu" "")))  __typeof__(u16) per_cpu__x86_bios_cpu_apicid
#endif
#if !definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(".data.percpu" "")))  __typeof__(u16) per_cpu__x86_bios_cpu_apicid
#endif
; extern __typeof__(u16) *x86_bios_cpu_apicid_early_ptr; extern __typeof__(u16) x86_bios_cpu_apicid_early_map[];
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned int read_apic_id(void)
{
	unsigned int reg;
	reg = apic_read(0x20);
	return apic->get_apic_id(reg);
}
extern void default_setup_apic_routing(void);
extern struct apic apic_noop;
#if definedEx(CONFIG_X86_32)
extern struct apic apic_default;
/*
 * Set up the logical destination ID.
 *
 * Intel recommends to set DFR, LDR and TPR before enabling
 * an APIC.  See e.g. "AP-388 82489DX User's Manual" (Intel
 * document number 292116).  So here it goes...
 */
extern void default_init_apic_ldr(void);
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int default_apic_id_registered(void)
{
	return (__builtin_constant_p((read_apic_id())) ? constant_test_bit((read_apic_id()), ((phys_cpu_present_map).mask)) : variable_test_bit((read_apic_id()), ((phys_cpu_present_map).mask)));
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int default_phys_pkg_id(int cpuid_apic, int index_msb)
{
	return cpuid_apic >> index_msb;
}
extern int default_apicid_to_node(int logical_apicid);
#endif
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned int
default_cpu_mask_to_apicid(const struct cpumask *cpumask)
{
	return ((cpumask)->bits)[0] & 0xFFu;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned int
default_cpu_mask_to_apicid_and(const struct cpumask *cpumask,
			       const struct cpumask *andmask)
{
	unsigned long mask1 = ((cpumask)->bits)[0];
	unsigned long mask2 = ((andmask)->bits)[0];
	unsigned long mask3 = ((cpu_online_mask)->bits)[0];
	return (unsigned int)(mask1 & mask2 & mask3);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long default_check_apicid_used(physid_mask_t *map, int apicid)
{
	return (__builtin_constant_p((apicid)) ? constant_test_bit((apicid), ((*map).mask)) : variable_test_bit((apicid), ((*map).mask)));
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long default_check_apicid_present(int bit)
{
	return (__builtin_constant_p((bit)) ? constant_test_bit((bit), ((phys_cpu_present_map).mask)) : variable_test_bit((bit), ((phys_cpu_present_map).mask)));
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void default_ioapic_phys_id_map(physid_mask_t *phys_map, physid_mask_t *retmap)
{
	*retmap = *phys_map;
}
/* Mapping from cpu number to logical apicid */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int default_cpu_to_logical_apicid(int cpu)
{
	return 1 << cpu;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int __default_cpu_present_to_apicid(int mps_cpu)
{
	if (mps_cpu < nr_cpu_ids && (__builtin_constant_p((cpumask_check((mps_cpu)))) ? constant_test_bit((cpumask_check((mps_cpu))), ((((cpu_present_mask))->bits))) : variable_test_bit((cpumask_check((mps_cpu))), ((((cpu_present_mask))->bits)))))
		return (int)(*({ unsigned long __ptr; __asm__ ("" : "=r"(__ptr) : "0"((&per_cpu__x86_bios_cpu_apicid))); (typeof((&per_cpu__x86_bios_cpu_apicid))) (__ptr + (((__per_cpu_offset[mps_cpu])))); }));
	else
		return 
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_X86_LOCAL_APIC)
0xFFu
#endif
#if !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC)
0xFFFFu
#endif
;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int
__default_check_phys_apicid_present(int phys_apicid)
{
	return (__builtin_constant_p((phys_apicid)) ? constant_test_bit((phys_apicid), ((phys_cpu_present_map).mask)) : variable_test_bit((phys_apicid), ((phys_cpu_present_map).mask)));
}
#if definedEx(CONFIG_X86_32)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int default_cpu_present_to_apicid(int mps_cpu)
{
	return __default_cpu_present_to_apicid(mps_cpu);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int
default_check_phys_apicid_present(int phys_apicid)
{
	return __default_check_phys_apicid_present(phys_apicid);
}
#endif
#if !definedEx(CONFIG_X86_32)
extern int default_cpu_present_to_apicid(int mps_cpu);
extern int default_check_phys_apicid_present(int phys_apicid);
#endif
#if definedEx(CONFIG_X86_32)
extern u8 cpu_2_logical_apicid[8];
#endif
#line 15 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/smp.h" 2
#if definedEx(CONFIG_X86_IO_APIC)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/io_apic.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/io_apic.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/mpspec.h" 1
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/io_apic.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/apicdef.h" 1
#line 8 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/io_apic.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/irq_vectors.h" 1
/*
 * Linux IRQ vector layout.
 *
 * There are 256 IDT entries (per CPU - each entry is 8 bytes) which can
 * be defined by Linux. They are used as a jump table by the CPU when a
 * given vector is triggered - by a CPU-external, CPU-internal or
 * software-triggered event.
 *
 * Linux sets the kernel code address each entry jumps to early during
 * bootup, and never changes them. This is the general layout of the
 * IDT entries:
 *
 *  Vectors   0 ...  31 : system traps and exceptions - hardcoded events
 *  Vectors  32 ... 127 : device interrupts
 *  Vector  128         : legacy int80 syscall interface
 *  Vectors 129 ... 237 : device interrupts
 *  Vectors 238 ... 255 : special interrupts
 *
 * 64-bit x86 has per CPU IDT tables, 32-bit has one shared IDT table.
 *
 * This file enumerates the exact layout of them:
 */
/*
 * IDT vectors usable for external interrupt sources start
 * at 0x20:
 */
#if definedEx(CONFIG_X86_32)
#endif
#if !definedEx(CONFIG_X86_32)
#endif
/*
 * Reserve the lowest usable priority level 0x20 - 0x2f for triggering
 * cleanup after irq migration.
 */
/*
 * Vectors 0x30-0x3f are used for ISA interrupts.
 */
/*
 * Special IRQ vectors used by the SMP architecture, 0xf0-0xff
 *
 *  some of the following vectors are 'rare', they are merged
 *  into a single vector (CALL_FUNCTION_VECTOR) to save vector space.
 *  TLB, reschedule and local APIC vectors are performance-critical.
 */
/*
 * Sanity check
 */
/* f0-f7 used for spreading out TLB flushes: */
/*
 * Local APIC timer IRQ vector is on a different priority level,
 * to work around the 'lost local interrupt if more than 2 IRQ
 * sources per level' errata.
 */
/*
 * Generic system vector for platform specific use
 */
/*
 * Performance monitoring pending work vector:
 */
/*
 * Self IPI vector for machine checks
 */
/*
 * First APIC vector available to drivers: (vectors 0x30-0xee) we
 * start at 0x31(0x41) to spread out vectors evenly between priority
 * levels. (0x80 is the syscall vector)
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int invalid_vm86_irq(int irq)
{
	return irq < 3 || irq > 15;
}
/*
 * Size the maximum number of interrupts.
 *
 * If the irq_desc[] array has a sparse layout, we can size things
 * generously - it scales up linearly with the maximum number of CPUs,
 * and the maximum number of IO-APICs, whichever is higher.
 *
 * In other cases we size more conservatively, to not create too large
 * static arrays.
 */
#if definedEx(CONFIG_SPARSE_IRQ)
#endif
#if !definedEx(CONFIG_SPARSE_IRQ)
#endif
#line 9 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/io_apic.h" 2
/*
 * Intel IO-APIC support for SMP and UP systems.
 *
 * Copyright (C) 1997, 1998, 1999, 2000 Ingo Molnar
 */
/* I/O Unit Redirection Table */
/*
 * The structure of the IO-APIC:
 */
union IO_APIC_reg_00 {
	u32	raw;
	struct {
		u32	__reserved_2	: 14,
			LTS		:  1,
			delivery_type	:  1,
			__reserved_1	:  8,
			ID		:  8;
	} __attribute__ ((packed)) bits;
};
union IO_APIC_reg_01 {
	u32	raw;
	struct {
		u32	version		:  8,
			__reserved_2	:  7,
			PRQ		:  1,
			entries		:  8,
			__reserved_1	:  8;
	} __attribute__ ((packed)) bits;
};
union IO_APIC_reg_02 {
	u32	raw;
	struct {
		u32	__reserved_2	: 24,
			arbitration	:  4,
			__reserved_1	:  4;
	} __attribute__ ((packed)) bits;
};
union IO_APIC_reg_03 {
	u32	raw;
	struct {
		u32	boot_DT		:  1,
			__reserved_1	: 31;
	} __attribute__ ((packed)) bits;
};
enum ioapic_irq_destination_types {
	dest_Fixed = 0,
	dest_LowestPrio = 1,
	dest_SMI = 2,
	dest__reserved_1 = 3,
	dest_NMI = 4,
	dest_INIT = 5,
	dest__reserved_2 = 6,
	dest_ExtINT = 7
};
struct IO_APIC_route_entry {
	__u32	vector		:  8,
		delivery_mode	:  3,	/* 000: FIXED
					 * 001: lowest prio
					 * 111: ExtINT
					 */
		dest_mode	:  1,	/* 0: physical, 1: logical */
		delivery_status	:  1,
		polarity	:  1,
		irr		:  1,
		trigger		:  1,	/* 0: edge, 1: level */
		mask		:  1,	/* 0: enabled, 1: disabled */
		__reserved_2	: 15;
	__u32	__reserved_3	: 24,
		dest		:  8;
} __attribute__ ((packed));
struct IR_IO_APIC_route_entry {
	__u64	vector		: 8,
		zero		: 3,
		index2		: 1,
		delivery_status : 1,
		polarity	: 1,
		irr		: 1,
		trigger		: 1,
		mask		: 1,
		reserved	: 31,
		format		: 1,
		index		: 15;
} __attribute__ ((packed));
/*
 * # of IO-APICs and # of IRQ routing registers
 */
extern int nr_ioapics;
extern int nr_ioapic_registers[
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_X86_LOCAL_APIC)
64
#endif
#if !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC)
128
#endif
];
/* I/O APIC entries */
extern struct mpc_ioapic mp_ioapics[
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_X86_LOCAL_APIC)
64
#endif
#if !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC)
128
#endif
];
/* # of MP IRQ source entries */
extern int mp_irq_entries;
/* MP IRQ source entries */
extern struct mpc_intsrc mp_irqs[
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_X86_LOCAL_APIC)
256
#endif
#if !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_X86_LOCAL_APIC)
(256 * 4)
#endif
];
/* non-0 if default (table-less) MP configuration */
extern int mpc_default_type;
/* Older SiS APIC requires we rewrite the index register */
extern int sis_apic_bug;
/* 1 if "noapic" boot option passed */
extern int skip_ioapic_setup;
/* 1 if "noapic" boot option passed */
extern int noioapicquirk;
/* -1 if "noapic" boot option passed */
extern int noioapicreroute;
/* 1 if the timer IRQ uses the '8259A Virtual Wire' mode */
extern int timer_through_8259;
extern void io_apic_disable_legacy(void);
/*
 * If we use the IO-APIC for IRQ routing, disable automatic
 * assignment of PCI IRQ's.
 */
extern u8 io_apic_unique_id(u8 id);
extern int io_apic_get_unique_id(int ioapic, int apic_id);
extern int io_apic_get_version(int ioapic);
extern int io_apic_get_redir_entries(int ioapic);
struct io_apic_irq_attr;
extern int io_apic_set_pci_routing(struct device *dev, int irq,
		 struct io_apic_irq_attr *irq_attr);
void setup_IO_APIC_irq_extra(u32 gsi);
extern int (*ioapic_renumber_irq)(int ioapic, int irq);
extern void ioapic_init_mappings(void);
extern void ioapic_insert_resources(void);
extern struct IO_APIC_route_entry **alloc_ioapic_entries(void);
extern void free_ioapic_entries(struct IO_APIC_route_entry **ioapic_entries);
extern int save_IO_APIC_setup(struct IO_APIC_route_entry **ioapic_entries);
extern void mask_IO_APIC_setup(struct IO_APIC_route_entry **ioapic_entries);
extern int restore_IO_APIC_setup(struct IO_APIC_route_entry **ioapic_entries);
extern void probe_nr_irqs_gsi(void);
extern int setup_ioapic_entry(int apic, int irq,
			      struct IO_APIC_route_entry *entry,
			      unsigned int destination, int trigger,
			      int polarity, int vector, int pin);
extern void ioapic_write_entry(int apic, int pin,
			       struct IO_APIC_route_entry e);
extern void setup_ioapic_ids_from_mpc(void);
struct mp_ioapic_gsi{
	int gsi_base;
	int gsi_end;
};
extern struct mp_ioapic_gsi  mp_gsi_routing[];
int mp_find_ioapic(int gsi);
int mp_find_ioapic_pin(int ioapic, int gsi);
void __attribute__ ((__section__(".init.text"))) __attribute__((__cold__)) __attribute__((no_instrument_function)) mp_register_ioapic(int id, u32 address, u32 gsi_base);
#line 17 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/smp.h" 2
#endif
#endif
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/thread_info.h" 1
/* thread_info.h: low-level thread information
 *
 * Copyright (C) 2002  David Howells (dhowells@redhat.com)
 * - Incorporating suggestions made by Linus Torvalds and Dave Miller
 */
#line 20 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/smp.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/cpumask.h" 1
#line 21 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/smp.h" 2
extern int smp_num_siblings;
extern unsigned int num_processors;
#if definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(".discard"), unused)) char __pcpu_scope_cpu_sibling_map; extern __attribute__((section(".data.percpu" "")))  __typeof__(cpumask_var_t) per_cpu__cpu_sibling_map
#endif
#if !definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(".data.percpu" "")))  __typeof__(cpumask_var_t) per_cpu__cpu_sibling_map
#endif
;
#if definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(".discard"), unused)) char __pcpu_scope_cpu_core_map; extern __attribute__((section(".data.percpu" "")))  __typeof__(cpumask_var_t) per_cpu__cpu_core_map
#endif
#if !definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(".data.percpu" "")))  __typeof__(cpumask_var_t) per_cpu__cpu_core_map
#endif
;
#if definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(".discard"), unused)) char __pcpu_scope_cpu_llc_id; extern __attribute__((section(".data.percpu" "")))  __typeof__(u16) per_cpu__cpu_llc_id
#endif
#if !definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(".data.percpu" "")))  __typeof__(u16) per_cpu__cpu_llc_id
#endif
;
#if definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(".discard"), unused)) char __pcpu_scope_cpu_number; extern __attribute__((section(".data.percpu" "")))  __typeof__(int) per_cpu__cpu_number
#endif
#if !definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(".data.percpu" "")))  __typeof__(int) per_cpu__cpu_number
#endif
;
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 struct cpumask *cpu_sibling_mask(int cpu)
{
	return (*({ unsigned long __ptr; __asm__ ("" : "=r"(__ptr) : "0"((&per_cpu__cpu_sibling_map))); (typeof((&per_cpu__cpu_sibling_map))) (__ptr + (((__per_cpu_offset[cpu])))); }));
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 struct cpumask *cpu_core_mask(int cpu)
{
	return (*({ unsigned long __ptr; __asm__ ("" : "=r"(__ptr) : "0"((&per_cpu__cpu_core_map))); (typeof((&per_cpu__cpu_core_map))) (__ptr + (((__per_cpu_offset[cpu])))); }));
}
#if definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(".discard"), unused)) char __pcpu_scope_x86_cpu_to_apicid; extern __attribute__((section(".data.percpu" "")))  __typeof__(u16) per_cpu__x86_cpu_to_apicid
#endif
#if !definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(".data.percpu" "")))  __typeof__(u16) per_cpu__x86_cpu_to_apicid
#endif
; extern __typeof__(u16) *x86_cpu_to_apicid_early_ptr; extern __typeof__(u16) x86_cpu_to_apicid_early_map[];
#if definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(".discard"), unused)) char __pcpu_scope_x86_bios_cpu_apicid; extern __attribute__((section(".data.percpu" "")))  __typeof__(u16) per_cpu__x86_bios_cpu_apicid
#endif
#if !definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(".data.percpu" "")))  __typeof__(u16) per_cpu__x86_bios_cpu_apicid
#endif
; extern __typeof__(u16) *x86_bios_cpu_apicid_early_ptr; extern __typeof__(u16) x86_bios_cpu_apicid_early_map[];
/* Static state in head.S used to set up a CPU */
extern struct {
	void *sp;
	unsigned short ss;
} stack_start;
struct smp_ops {
	void (*smp_prepare_boot_cpu)(void);
	void (*smp_prepare_cpus)(unsigned max_cpus);
	void (*smp_cpus_done)(unsigned max_cpus);
	void (*smp_send_stop)(void);
	void (*smp_send_reschedule)(int cpu);
	int (*cpu_up)(unsigned cpu);
	int (*cpu_disable)(void);
	void (*cpu_die)(unsigned int cpu);
	void (*play_dead)(void);
	void (*send_call_func_ipi)(const struct cpumask *mask);
	void (*send_call_func_single_ipi)(int cpu);
};
/* Globals due to paravirt */
extern void set_cpu_sibling_map(int cpu);
#if !definedEx(CONFIG_PARAVIRT)
#endif
extern struct smp_ops smp_ops;
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void smp_send_stop(void)
{
	smp_ops.smp_send_stop();
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void smp_prepare_boot_cpu(void)
{
	smp_ops.smp_prepare_boot_cpu();
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void smp_prepare_cpus(unsigned int max_cpus)
{
	smp_ops.smp_prepare_cpus(max_cpus);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void smp_cpus_done(unsigned int max_cpus)
{
	smp_ops.smp_cpus_done(max_cpus);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int __cpu_up(unsigned int cpu)
{
	return smp_ops.cpu_up(cpu);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int __cpu_disable(void)
{
	return smp_ops.cpu_disable();
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void __cpu_die(unsigned int cpu)
{
	smp_ops.cpu_die(cpu);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void play_dead(void)
{
	smp_ops.play_dead();
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void smp_send_reschedule(int cpu)
{
	smp_ops.smp_send_reschedule(cpu);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void arch_send_call_function_single_ipi(int cpu)
{
	smp_ops.send_call_func_single_ipi(cpu);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void arch_send_call_function_ipi_mask(const struct cpumask *mask)
{
	smp_ops.send_call_func_ipi(mask);
}
void cpu_disable_common(void);
void native_smp_prepare_boot_cpu(void);
void native_smp_prepare_cpus(unsigned int max_cpus);
void native_smp_cpus_done(unsigned int max_cpus);
int native_cpu_up(unsigned int cpunum);
int native_cpu_disable(void);
void native_cpu_die(unsigned int cpu);
void native_play_dead(void);
void play_dead_common(void);
void wbinvd_on_cpu(int cpu);
int wbinvd_on_all_cpus(void);
void native_send_call_func_ipi(const struct cpumask *mask);
void native_send_call_func_single_ipi(int cpu);
void smp_store_cpu_info(int id);
/* We don't mark CPUs online until __cpu_up(), so we need another measure */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int num_booting_cpus(void)
{
	return cpumask_weight(cpu_callout_mask);
}
extern unsigned disabled_cpus __attribute__ ((__section__(".cpuinit.data")));
#if definedEx(CONFIG_X86_32_SMP)
/*
 * This function is needed by all SMP systems. It must _always_ be valid
 * from the initial startup. We map APIC_BASE very early in page_setup(),
 * so this is correct in the x86 case.
 */
extern int safe_smp_processor_id(void);
#endif
#if !definedEx(CONFIG_X86_32_SMP) && definedEx(CONFIG_X86_64_SMP)
#endif
#if definedEx(CONFIG_X86_LOCAL_APIC)
#if !definedEx(CONFIG_X86_64)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int logical_smp_processor_id(void)
{
	/* we don't want to mark this access volatile - bad code generation */
	return (((apic_read(0xD0)) >> 24) & 0xFFu);
}
#endif
extern int hard_smp_processor_id(void);
#endif
#if !definedEx(CONFIG_X86_LOCAL_APIC)
#endif
#endif
#line 38 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/smp.h" 2
/*
 * main cross-CPU interfaces, handles INIT, TLB flush, STOP, etc.
 * (defined in asm header):
 */
/*
 * stops all CPUs but the current one:
 */
extern void smp_send_stop(void);
/*
 * sends a 'reschedule' event to another CPU:
 */
extern void smp_send_reschedule(int cpu);
/*
 * Prepare machine for booting other CPUs.
 */
extern void smp_prepare_cpus(unsigned int max_cpus);
/*
 * Bring a CPU up
 */
extern int __cpu_up(unsigned int cpunum);
/*
 * Final polishing of CPUs
 */
extern void smp_cpus_done(unsigned int max_cpus);
/*
 * Call a function on all other processors
 */
int smp_call_function(void(*func)(void *info), void *info, int wait);
void smp_call_function_many(const struct cpumask *mask,
			    void (*func)(void *info), void *info, bool wait);
void __smp_call_function_single(int cpuid, struct call_single_data *data,
				int wait);
int smp_call_function_any(const struct cpumask *mask,
			  void (*func)(void *info), void *info, int wait);
/*
 * Generic and arch helpers
 */
#if definedEx(CONFIG_USE_GENERIC_SMP_HELPERS)
void generic_smp_call_function_single_interrupt(void);
void generic_smp_call_function_interrupt(void);
void ipi_call_lock(void);
void ipi_call_unlock(void);
void ipi_call_lock_irq(void);
void ipi_call_unlock_irq(void);
#endif
/*
 * Call a function on all processors
 */
int on_each_cpu(void (*func) (void *info), void *info, int wait);
/*
 * Mark the boot cpu "online" so that it can call console drivers in
 * printk() and can access its per-cpu storage.
 */
void smp_prepare_boot_cpu(void);
extern unsigned int setup_max_cpus;
#endif
#if !definedEx(CONFIG_SMP)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void smp_send_stop(void) { }
/*
 *	These macros fold the SMP functionality into a single CPU system
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int up_smp_call_function(void (*func)(void *), void *info)
{
	return 0;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void smp_send_reschedule(int cpu) { }
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void init_call_single_data(void) { }
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int
smp_call_function_any(const struct cpumask *mask, void (*func)(void *info),
		      void *info, int wait)
{
	return smp_call_function_single(0, func, info, wait);
}
#endif
/*
 * smp_processor_id(): get the current CPU ID.
 *
 * if DEBUG_PREEMPT is enabled the we check whether it is
 * used in a preemption-safe way. (smp_processor_id() is safe
 * if it's used in a preemption-off critical section, or in
 * a thread that is bound to the current CPU.)
 *
 * NOTE: raw_smp_processor_id() is for internal use only
 * (smp_processor_id() is the preferred variant), but in rare
 * instances it might also be used to turn off false positives
 * (i.e. smp_processor_id() use that the debugging code reports but
 * which use for some reason is legal). Don't use this to hack around
 * the warning message, as your code might not work under PREEMPT.
 */
#if definedEx(CONFIG_DEBUG_PREEMPT)
  extern unsigned int debug_smp_processor_id(void);
#endif
#if !definedEx(CONFIG_DEBUG_PREEMPT)
#endif
/*
 * Callback to arch code if there's nosmp or maxcpus=0 on the
 * boot command line:
 */
extern void arch_disable_smp_support(void);
void smp_setup_processor_id(void);
#line 35 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/topology.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/topology.h" 1
/*
 * Written by: Matthew Dobson, IBM Corporation
 *
 * Copyright (C) 2002, IBM Corp.
 *
 * All rights reserved.
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation; either version 2 of the License, or
 * (at your option) any later version.
 *
 * This program is distributed in the hope that it will be useful, but
 * WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or
 * NON INFRINGEMENT.  See the GNU General Public License for more
 * details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program; if not, write to the Free Software
 * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
 *
 * Send feedback to <colpatch@us.ibm.com>
 */
#if definedEx(CONFIG_X86_32)
#if definedEx(CONFIG_X86_HT)
#endif
#endif
#if !definedEx(CONFIG_X86_32)
#if definedEx(CONFIG_SMP)
#endif
#endif
/*
 * to preserve the visibility of NUMA_NO_NODE definition,
 * moved to there from here.  May be used independent of
 * CONFIG_NUMA.
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/numa.h" 1
#line 45 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/topology.h" 2
#if definedEx(CONFIG_NUMA)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/cpumask.h" 1
#line 48 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/topology.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/mpspec.h" 1
#if !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) || !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) && !definedEx(CONFIG_NUMA) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) && !definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_X86_LOCAL_APIC)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/init.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/mpspec.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/mpspec_def.h" 1
/*
 * Structure definitions for SMP machines following the
 * Intel Multiprocessing Specification 1.1 and 1.4.
 */
/*
 * This tag identifies where the SMP configuration
 * information is.
 */
#if definedEx(CONFIG_X86_32)
#endif
#if !definedEx(CONFIG_X86_32)
#endif
/* Intel MP Floating Pointer Structure */
struct mpf_intel {
	char signature[4];		/* "_MP_"			*/
	unsigned int physptr;		/* Configuration table address	*/
	unsigned char length;		/* Our length (paragraphs)	*/
	unsigned char specification;	/* Specification version	*/
	unsigned char checksum;		/* Checksum (makes sum 0)	*/
	unsigned char feature1;		/* Standard or configuration ?	*/
	unsigned char feature2;		/* Bit7 set for IMCR|PIC	*/
	unsigned char feature3;		/* Unused (0)			*/
	unsigned char feature4;		/* Unused (0)			*/
	unsigned char feature5;		/* Unused (0)			*/
};
struct mpc_table {
	char signature[4];
	unsigned short length;		/* Size of table */
	char spec;			/* 0x01 */
	char checksum;
	char oem[8];
	char productid[12];
	unsigned int oemptr;		/* 0 if not present */
	unsigned short oemsize;		/* 0 if not present */
	unsigned short oemcount;
	unsigned int lapic;		/* APIC address */
	unsigned int reserved;
};
/* Followed by entries */
/* Used by IBM NUMA-Q to describe node locality */
struct mpc_cpu {
	unsigned char type;
	unsigned char apicid;		/* Local APIC number */
	unsigned char apicver;		/* Its versions */
	unsigned char cpuflag;
	unsigned int cpufeature;
	unsigned int featureflag;	/* CPUID feature value */
	unsigned int reserved[2];
};
struct mpc_bus {
	unsigned char type;
	unsigned char busid;
	unsigned char bustype[6];
};
/* List of Bus Type string values, Intel MP Spec. */
struct mpc_ioapic {
	unsigned char type;
	unsigned char apicid;
	unsigned char apicver;
	unsigned char flags;
	unsigned int apicaddr;
};
struct mpc_intsrc {
	unsigned char type;
	unsigned char irqtype;
	unsigned short irqflag;
	unsigned char srcbus;
	unsigned char srcbusirq;
	unsigned char dstapic;
	unsigned char dstirq;
};
enum mp_irq_source_types {
	mp_INT = 0,
	mp_NMI = 1,
	mp_SMI = 2,
	mp_ExtINT = 3
};
struct mpc_lintsrc {
	unsigned char type;
	unsigned char irqtype;
	unsigned short irqflag;
	unsigned char srcbusid;
	unsigned char srcbusirq;
	unsigned char destapic;
	unsigned char destapiclint;
};
struct mpc_oemtable {
	char signature[4];
	unsigned short length;		/* Size of table */
	char  rev;			/* 0x01 */
	char  checksum;
	char  mpc[8];
};
/*
 *	Default configurations
 *
 *	1	2 CPU ISA 82489DX
 *	2	2 CPU EISA 82489DX neither IRQ 0 timer nor IRQ 13 DMA chaining
 *	3	2 CPU EISA 82489DX
 *	4	2 CPU MCA 82489DX
 *	5	2 CPU ISA+PCI
 *	6	2 CPU EISA+PCI
 *	7	2 CPU MCA+PCI
 */
enum mp_bustype {
	MP_BUS_ISA = 1,
	MP_BUS_EISA,
	MP_BUS_PCI,
	MP_BUS_MCA,
};
#line 8 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/mpspec.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/x86_init.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/pgtable_types.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/x86_init.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/bootparam.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/bootparam.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/screen_info.h" 1
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 6 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/screen_info.h" 2
/*
 * These are set up by the setup-routine at boot-time:
 */
struct screen_info {
	__u8  orig_x;		/* 0x00 */
	__u8  orig_y;		/* 0x01 */
	__u16 ext_mem_k;	/* 0x02 */
	__u16 orig_video_page;	/* 0x04 */
	__u8  orig_video_mode;	/* 0x06 */
	__u8  orig_video_cols;	/* 0x07 */
	__u8  flags;		/* 0x08 */
	__u8  unused2;		/* 0x09 */
	__u16 orig_video_ega_bx;/* 0x0a */
	__u16 unused3;		/* 0x0c */
	__u8  orig_video_lines;	/* 0x0e */
	__u8  orig_video_isVGA;	/* 0x0f */
	__u16 orig_video_points;/* 0x10 */
	/* VESA graphic mode -- linear frame buffer */
	__u16 lfb_width;	/* 0x12 */
	__u16 lfb_height;	/* 0x14 */
	__u16 lfb_depth;	/* 0x16 */
	__u32 lfb_base;		/* 0x18 */
	__u32 lfb_size;		/* 0x1c */
	__u16 cl_magic, cl_offset; /* 0x20 */
	__u16 lfb_linelength;	/* 0x24 */
	__u8  red_size;		/* 0x26 */
	__u8  red_pos;		/* 0x27 */
	__u8  green_size;	/* 0x28 */
	__u8  green_pos;	/* 0x29 */
	__u8  blue_size;	/* 0x2a */
	__u8  blue_pos;		/* 0x2b */
	__u8  rsvd_size;	/* 0x2c */
	__u8  rsvd_pos;		/* 0x2d */
	__u16 vesapm_seg;	/* 0x2e */
	__u16 vesapm_off;	/* 0x30 */
	__u16 pages;		/* 0x32 */
	__u16 vesa_attributes;	/* 0x34 */
	__u32 capabilities;     /* 0x36 */
	__u8  _reserved[6];	/* 0x3a */
} __attribute__((packed));
extern struct screen_info screen_info;
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/bootparam.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/apm_bios.h" 1
/*
 * Include file for the interface to an APM BIOS
 * Copyright 1994-2001 Stephen Rothwell (sfr@canb.auug.org.au)
 *
 * This program is free software; you can redistribute it and/or modify it
 * under the terms of the GNU General Public License as published by the
 * Free Software Foundation; either version 2, or (at your option) any
 * later version.
 *
 * This program is distributed in the hope that it will be useful, but
 * WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
 * General Public License for more details.
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 21 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/apm_bios.h" 2
typedef unsigned short	apm_event_t;
typedef unsigned short	apm_eventinfo_t;
struct apm_bios_info {
	__u16	version;
	__u16	cseg;
	__u32	offset;
	__u16	cseg_16;
	__u16	dseg;
	__u16	flags;
	__u16	cseg_len;
	__u16	cseg_16_len;
	__u16	dseg_len;
};
/* Results of APM Installation Check */
/*
 * Data for APM that is persistent across module unload/load
 */
struct apm_info {
	struct apm_bios_info	bios;
	unsigned short		connection_version;
	int			get_power_status_broken;
	int			get_power_status_swabinminutes;
	int			allow_ints;
	int			forbid_idle;
	int			realmode_power_off;
	int			disabled;
};
/*
 * The APM function codes
 */
/*
 * Function code for APM_FUNC_RESUME_TIMER
 */
/*
 * Function code for APM_FUNC_RESUME_ON_RING
 */
/*
 * Function code for APM_FUNC_TIMER_STATUS
 */
/*
 * in arch/i386/kernel/setup.c
 */
extern struct apm_info	apm_info;
/*
 * Power states
 */
/*
 * Events (results of Get PM Event)
 */
/*
 * Error codes
 */
/*
 * APM Device IDs
 */
/*
 * This is the "All Devices" ID communicated to the BIOS
 */
/*
 * Battery status
 */
/*
 * APM defined capability bit flags
 */
/*
 * ioctl operations
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/ioctl.h" 1
#line 217 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/apm_bios.h" 2
#line 8 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/bootparam.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/edd.h" 1
/*
 * linux/include/linux/edd.h
 *  Copyright (C) 2002, 2003, 2004 Dell Inc.
 *  by Matt Domsch <Matt_Domsch@dell.com>
 *
 * structures and definitions for the int 13h, ax={41,48}h
 * BIOS Enhanced Disk Drive Services
 * This is based on the T13 group document D1572 Revision 0 (August 14 2002)
 * available at http://www.t13.org/docs2002/d1572r0.pdf.  It is
 * very similar to D1484 Revision 3 http://www.t13.org/docs2002/d1484r3.pdf
 *
 * In a nutshell, arch/{i386,x86_64}/boot/setup.S populates a scratch
 * table in the boot_params that contains a list of BIOS-enumerated
 * boot devices.
 * In arch/{i386,x86_64}/kernel/setup.c, this information is
 * transferred into the edd structure, and in drivers/firmware/edd.c, that
 * information is used to identify BIOS boot disk.  The code in setup.S
 * is very sensitive to the size of these structures.
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License v2.0 as published by
 * the Free Software Foundation
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 35 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/edd.h" 2
struct edd_device_params {
	__u16 length;
	__u16 info_flags;
	__u32 num_default_cylinders;
	__u32 num_default_heads;
	__u32 sectors_per_track;
	__u64 number_of_sectors;
	__u16 bytes_per_sector;
	__u32 dpte_ptr;		/* 0xFFFFFFFF for our purposes */
	__u16 key;		/* = 0xBEDD */
	__u8 device_path_info_length;	/* = 44 */
	__u8 reserved2;
	__u16 reserved3;
	__u8 host_bus_type[4];
	__u8 interface_type[8];
	union {
		struct {
			__u16 base_address;
			__u16 reserved1;
			__u32 reserved2;
		} __attribute__ ((packed)) isa;
		struct {
			__u8 bus;
			__u8 slot;
			__u8 function;
			__u8 channel;
			__u32 reserved;
		} __attribute__ ((packed)) pci;
		/* pcix is same as pci */
		struct {
			__u64 reserved;
		} __attribute__ ((packed)) ibnd;
		struct {
			__u64 reserved;
		} __attribute__ ((packed)) xprs;
		struct {
			__u64 reserved;
		} __attribute__ ((packed)) htpt;
		struct {
			__u64 reserved;
		} __attribute__ ((packed)) unknown;
	} interface_path;
	union {
		struct {
			__u8 device;
			__u8 reserved1;
			__u16 reserved2;
			__u32 reserved3;
			__u64 reserved4;
		} __attribute__ ((packed)) ata;
		struct {
			__u8 device;
			__u8 lun;
			__u8 reserved1;
			__u8 reserved2;
			__u32 reserved3;
			__u64 reserved4;
		} __attribute__ ((packed)) atapi;
		struct {
			__u16 id;
			__u64 lun;
			__u16 reserved1;
			__u32 reserved2;
		} __attribute__ ((packed)) scsi;
		struct {
			__u64 serial_number;
			__u64 reserved;
		} __attribute__ ((packed)) usb;
		struct {
			__u64 eui;
			__u64 reserved;
		} __attribute__ ((packed)) i1394;
		struct {
			__u64 wwid;
			__u64 lun;
		} __attribute__ ((packed)) fibre;
		struct {
			__u64 identity_tag;
			__u64 reserved;
		} __attribute__ ((packed)) i2o;
		struct {
			__u32 array_number;
			__u32 reserved1;
			__u64 reserved2;
		} __attribute__ ((packed)) raid;
		struct {
			__u8 device;
			__u8 reserved1;
			__u16 reserved2;
			__u32 reserved3;
			__u64 reserved4;
		} __attribute__ ((packed)) sata;
		struct {
			__u64 reserved1;
			__u64 reserved2;
		} __attribute__ ((packed)) unknown;
	} device_path;
	__u8 reserved4;
	__u8 checksum;
} __attribute__ ((packed));
struct edd_info {
	__u8 device;
	__u8 version;
	__u16 interface_support;
	__u16 legacy_max_cylinder;
	__u8 legacy_max_head;
	__u8 legacy_sectors_per_track;
	struct edd_device_params params;
} __attribute__ ((packed));
struct edd {
	unsigned int mbr_signature[16];
	struct edd_info edd_info[6];
	unsigned char mbr_signature_nr;
	unsigned char edd_info_nr;
};
extern struct edd edd;
#line 9 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/bootparam.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/e820.h" 1
/*
 * Legacy E820 BIOS limits us to 128 (E820MAX) nodes due to the
 * constrained space in the zeropage.  If we have more nodes than
 * that, and if we've booted off EFI firmware, then the EFI tables
 * passed us from the EFI firmware can list more nodes.  Size our
 * internal memory map tables to have room for these additional
 * nodes, based on up to three entries per node for which the
 * kernel was built: MAX_NUMNODES == (1 << CONFIG_NODES_SHIFT),
 * plus E820MAX, allowing space for the possible duplicate E820
 * entries that might need room in the same arrays, prior to the
 * call to sanitize_e820_map() to remove duplicates.  The allowance
 * of three memory map entries per node is "enough" entries for
 * the initial hardware platform motivating this mechanism to make
 * use of additional EFI map entries.  Future platforms may want
 * to allow more than three entries per node or otherwise refine
 * this size.
 */
/*
 * Odd: 'make headers_check' complains about numa.h if I try
 * to collapse the next two #ifdef lines to a single line:
 *	#if defined(__KERNEL__) && defined(CONFIG_EFI)
 */
#if definedEx(CONFIG_EFI)
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/numa.h" 1
#line 33 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/e820.h" 2
#endif
#if !definedEx(CONFIG_EFI)
#endif
/* reserved RAM used by kernel itself */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 54 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/e820.h" 2
struct e820entry {
	__u64 addr;	/* start of memory segment */
	__u64 size;	/* size of memory segment */
	__u32 type;	/* type of memory segment */
} __attribute__((packed));
struct e820map {
	__u32 nr_map;
	struct e820entry map[
#if !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) && definedEx(CONFIG_EFI) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) && definedEx(CONFIG_EFI) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) && definedEx(CONFIG_EFI) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) && definedEx(CONFIG_EFI) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) && definedEx(CONFIG_EFI) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) && definedEx(CONFIG_EFI) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) && definedEx(CONFIG_EFI) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_NUMA) && definedEx(CONFIG_EFI)
(128 + 3 * (1 << 
#if !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_NODES_SHIFT) || definedEx(CONFIG_NEED_MULTIPLE_NODES)
#if definedEx(CONFIG_NEED_MULTIPLE_NODES)
3
#endif
#if !definedEx(CONFIG_NEED_MULTIPLE_NODES)
CONFIG_NODES_SHIFT
#endif
#endif
#if !definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_NODES_SHIFT)
0
#endif
))
#endif
#if !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) && !definedEx(CONFIG_EFI) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) && !definedEx(CONFIG_EFI) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) && !definedEx(CONFIG_EFI) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) && !definedEx(CONFIG_EFI) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) && !definedEx(CONFIG_EFI) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) && !definedEx(CONFIG_EFI) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) && !definedEx(CONFIG_EFI) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_NUMA) && !definedEx(CONFIG_EFI)
128
#endif
];
};
/* see comment in arch/x86/kernel/e820.c */
extern struct e820map e820;
extern struct e820map e820_saved;
extern unsigned long pci_mem_start;
extern int e820_any_mapped(u64 start, u64 end, unsigned type);
extern int e820_all_mapped(u64 start, u64 end, unsigned type);
extern void e820_add_region(u64 start, u64 size, int type);
extern void e820_print_map(char *who);
extern int
sanitize_e820_map(struct e820entry *biosmap, int max_nr_map, u32 *pnr_map);
extern u64 e820_update_range(u64 start, u64 size, unsigned old_type,
			       unsigned new_type);
extern u64 e820_remove_range(u64 start, u64 size, unsigned old_type,
			     int checktype);
extern void update_e820(void);
extern void e820_setup_gap(void);
extern int e820_search_gap(unsigned long *gapstart, unsigned long *gapsize,
			unsigned long start_addr, unsigned long long end_addr);
struct setup_data;
extern void parse_e820_ext(struct setup_data *data, unsigned long pa_data);
#if !definedEx(CONFIG_X86_32) && definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_HIBERNATION) && definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_HIBERNATION)
extern void e820_mark_nosave_regions(unsigned long limit_pfn);
#endif
#if !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_X86_64) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_HIBERNATION) && !definedEx(CONFIG_X86_64)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void e820_mark_nosave_regions(unsigned long limit_pfn)
{
}
#endif
#if definedEx(CONFIG_MEMTEST)
extern void early_memtest(unsigned long start, unsigned long end);
#endif
#if !definedEx(CONFIG_MEMTEST)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void early_memtest(unsigned long start, unsigned long end)
{
}
#endif
extern unsigned long end_user_pfn;
extern u64 find_e820_area(u64 start, u64 end, u64 size, u64 align);
extern u64 find_e820_area_size(u64 start, u64 *sizep, u64 align);
extern void reserve_early(u64 start, u64 end, char *name);
extern void reserve_early_overlap_ok(u64 start, u64 end, char *name);
extern void free_early(u64 start, u64 end);
extern void early_res_to_bootmem(u64 start, u64 end);
extern u64 early_reserve_e820(u64 startt, u64 sizet, u64 align);
extern unsigned long e820_end_of_ram_pfn(void);
extern unsigned long e820_end_of_low_ram_pfn(void);
extern int e820_find_active_region(const struct e820entry *ei,
				  unsigned long start_pfn,
				  unsigned long last_pfn,
				  unsigned long *ei_startpfn,
				  unsigned long *ei_endpfn);
extern void e820_register_active_regions(int nid, unsigned long start_pfn,
					 unsigned long end_pfn);
extern u64 e820_hole_size(u64 start, u64 end);
extern void finish_e820_parsing(void);
extern void e820_reserve_resources(void);
extern void e820_reserve_resources_late(void);
extern void setup_memory_map(void);
extern char *default_machine_specific_memory_setup(void);
/*
 * Returns true iff the specified range [s,e) is completely contained inside
 * the ISA region.
 */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 bool is_ISA_range(u64 s, u64 e)
{
	return s >= 0xa0000 && e <= 0x100000;
}
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/ioport.h" 1
/*
 * ioport.h	Definitions of routines for detecting, reserving and
 *		allocating system resources.
 *
 * Authors:	Linus Torvalds
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/compiler.h" 1
#line 14 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/ioport.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 15 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/ioport.h" 2
/*
 * Resources are tree-like, allowing
 * nesting etc..
 */
struct resource {
	resource_size_t start;
	resource_size_t end;
	const char *name;
	unsigned long flags;
	struct resource *parent, *sibling, *child;
};
struct resource_list {
	struct resource_list *next;
	struct resource *res;
	struct pci_dev *dev;
};
/*
 * IO resources have these defined flags.
 */
/* PnP IRQ specific bits (IORESOURCE_BITS) */
/* PnP DMA specific bits (IORESOURCE_BITS) */
/* PnP memory I/O specific bits (IORESOURCE_BITS) */
/* PnP I/O specific bits (IORESOURCE_BITS) */
/* PCI ROM control bits (IORESOURCE_BITS) */
/* PCI control bits.  Shares IORESOURCE_BITS with above PCI ROM.  */
/* PC/ISA/whatever - the normal PC address spaces: IO and memory */
extern struct resource ioport_resource;
extern struct resource iomem_resource;
extern int request_resource(struct resource *root, struct resource *new);
extern int release_resource(struct resource *new);
extern void reserve_region_with_split(struct resource *root,
			     resource_size_t start, resource_size_t end,
			     const char *name);
extern int insert_resource(struct resource *parent, struct resource *new);
extern void insert_resource_expand_to_fit(struct resource *root, struct resource *new);
extern int allocate_resource(struct resource *root, struct resource *new,
			     resource_size_t size, resource_size_t min,
			     resource_size_t max, resource_size_t align,
			     void (*alignf)(void *, struct resource *,
					    resource_size_t, resource_size_t),
			     void *alignf_data);
int adjust_resource(struct resource *res, resource_size_t start,
		    resource_size_t size);
resource_size_t resource_alignment(struct resource *res);
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 resource_size_t resource_size(const struct resource *res)
{
	return res->end - res->start + 1;
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long resource_type(const struct resource *res)
{
	return res->flags & 0x00000f00;
}
/* Convenience shorthand with allocation */
extern struct resource * __request_region(struct resource *,
					resource_size_t start,
					resource_size_t n,
					const char *name, int flags);
/* Compatibility cruft */
extern int __check_region(struct resource *, resource_size_t, resource_size_t);
extern void __release_region(struct resource *, resource_size_t,
				resource_size_t);
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int 
#if definedEx(CONFIG_ENABLE_WARN_DEPRECATED)
__attribute__((deprecated))
#endif
#if !definedEx(CONFIG_ENABLE_WARN_DEPRECATED)
#endif
 check_region(resource_size_t s,
						resource_size_t n)
{
	return __check_region(&ioport_resource, s, n);
}
/* Wrappers for managed devices */
struct device;
extern struct resource * __devm_request_region(struct device *dev,
				struct resource *parent, resource_size_t start,
				resource_size_t n, const char *name);
extern void __devm_release_region(struct device *dev, struct resource *parent,
				  resource_size_t start, resource_size_t n);
extern int iomem_map_sanity_check(resource_size_t addr, unsigned long size);
extern int iomem_is_exclusive(u64 addr);
extern int
walk_system_ram_range(unsigned long start_pfn, unsigned long nr_pages,
		void *arg, int (*func)(unsigned long, unsigned long, void *));
#line 151 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/e820.h" 2
#line 10 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/bootparam.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/ist.h" 1
/*
 * Include file for the interface to IST BIOS
 * Copyright 2002 Andy Grover <andrew.grover@intel.com>
 *
 * This program is free software; you can redistribute it and/or modify it
 * under the terms of the GNU General Public License as published by the
 * Free Software Foundation; either version 2, or (at your option) any
 * later version.
 *
 * This program is distributed in the hope that it will be useful, but
 * WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
 * General Public License for more details.
 */
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/linux/types.h" 1
#line 22 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/ist.h" 2
struct ist_info {
	__u32 signature;
	__u32 command;
	__u32 event;
	__u32 perf_level;
};
extern struct ist_info ist_info;
#line 11 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/bootparam.h" 2
#line 1 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/include/video/edid.h" 1
struct edid_info {
	unsigned char dummy[128];
};
extern struct edid_info edid_info;
#line 12 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/bootparam.h" 2
/* setup data types */
/* extensible setup data list node */
struct setup_data {
	__u64 next;
	__u32 type;
	__u32 len;
	__u8 data[0];
};
struct setup_header {
	__u8	setup_sects;
	__u16	root_flags;
	__u32	syssize;
	__u16	ram_size;
	__u16	vid_mode;
	__u16	root_dev;
	__u16	boot_flag;
	__u16	jump;
	__u32	header;
	__u16	version;
	__u32	realmode_swtch;
	__u16	start_sys;
	__u16	kernel_version;
	__u8	type_of_loader;
	__u8	loadflags;
	__u16	setup_move_size;
	__u32	code32_start;
	__u32	ramdisk_image;
	__u32	ramdisk_size;
	__u32	bootsect_kludge;
	__u16	heap_end_ptr;
	__u8	ext_loader_ver;
	__u8	ext_loader_type;
	__u32	cmd_line_ptr;
	__u32	initrd_addr_max;
	__u32	kernel_alignment;
	__u8	relocatable_kernel;
	__u8	_pad2[3];
	__u32	cmdline_size;
	__u32	hardware_subarch;
	__u64	hardware_subarch_data;
	__u32	payload_offset;
	__u32	payload_length;
	__u64	setup_data;
} __attribute__((packed));
struct sys_desc_table {
	__u16 length;
	__u8  table[14];
};
struct efi_info {
	__u32 efi_loader_signature;
	__u32 efi_systab;
	__u32 efi_memdesc_size;
	__u32 efi_memdesc_version;
	__u32 efi_memmap;
	__u32 efi_memmap_size;
	__u32 efi_systab_hi;
	__u32 efi_memmap_hi;
};
/* The so-called "zeropage" */
struct boot_params {
	struct screen_info screen_info;			/* 0x000 */
	struct apm_bios_info apm_bios_info;		/* 0x040 */
	__u8  _pad2[4];					/* 0x054 */
	__u64  tboot_addr;				/* 0x058 */
	struct ist_info ist_info;			/* 0x060 */
	__u8  _pad3[16];				/* 0x070 */
	__u8  hd0_info[16];	/* obsolete! */		/* 0x080 */
	__u8  hd1_info[16];	/* obsolete! */		/* 0x090 */
	struct sys_desc_table sys_desc_table;		/* 0x0a0 */
	__u8  _pad4[144];				/* 0x0b0 */
	struct edid_info edid_info;			/* 0x140 */
	struct efi_info efi_info;			/* 0x1c0 */
	__u32 alt_mem_k;				/* 0x1e0 */
	__u32 scratch;		/* Scratch field! */	/* 0x1e4 */
	__u8  e820_entries;				/* 0x1e8 */
	__u8  eddbuf_entries;				/* 0x1e9 */
	__u8  edd_mbr_sig_buf_entries;			/* 0x1ea */
	__u8  _pad6[6];					/* 0x1eb */
	struct setup_header hdr;    /* setup header */	/* 0x1f1 */
	__u8  _pad7[0x290-0x1f1-sizeof(struct setup_header)];
	__u32 edd_mbr_sig_buffer[16];	/* 0x290 */
	struct e820entry e820_map[128];		/* 0x2d0 */
	__u8  _pad8[48];				/* 0xcd0 */
	struct edd_info eddbuf[6];		/* 0xd00 */
	__u8  _pad9[276];				/* 0xeec */
} __attribute__((packed));
enum {
	X86_SUBARCH_PC = 0,
	X86_SUBARCH_LGUEST,
	X86_SUBARCH_XEN,
	X86_SUBARCH_MRST,
	X86_NR_SUBARCHS,
};
#line 7 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/x86_init.h" 2
struct mpc_bus;
struct mpc_cpu;
struct mpc_table;
/**
 * struct x86_init_mpparse - platform specific mpparse ops
 * @mpc_record:			platform specific mpc record accounting
 * @setup_ioapic_ids:		platform specific ioapic id override
 * @mpc_apic_id:		platform specific mpc apic id assignment
 * @smp_read_mpc_oem:		platform specific oem mpc table setup
 * @mpc_oem_pci_bus:		platform specific pci bus setup (default NULL)
 * @mpc_oem_bus_info:		platform specific mpc bus info
 * @find_smp_config:		find the smp configuration
 * @get_smp_config:		get the smp configuration
 */
struct x86_init_mpparse {
	void (*mpc_record)(unsigned int mode);
	void (*setup_ioapic_ids)(void);
	int (*mpc_apic_id)(struct mpc_cpu *m);
	void (*smp_read_mpc_oem)(struct mpc_table *mpc);
	void (*mpc_oem_pci_bus)(struct mpc_bus *m);
	void (*mpc_oem_bus_info)(struct mpc_bus *m, char *name);
	void (*find_smp_config)(void);
	void (*get_smp_config)(unsigned int early);
};
/**
 * struct x86_init_resources - platform specific resource related ops
 * @probe_roms:			probe BIOS roms
 * @reserve_resources:		reserve the standard resources for the
 *				platform
 * @memory_setup:		platform specific memory setup
 *
 */
struct x86_init_resources {
	void (*probe_roms)(void);
	void (*reserve_resources)(void);
	char *(*memory_setup)(void);
};
/**
 * struct x86_init_irqs - platform specific interrupt setup
 * @pre_vector_init:		init code to run before interrupt vectors
 *				are set up.
 * @intr_init:			interrupt init code
 * @trap_init:			platform specific trap setup
 */
struct x86_init_irqs {
	void (*pre_vector_init)(void);
	void (*intr_init)(void);
	void (*trap_init)(void);
};
/**
 * struct x86_init_oem - oem platform specific customizing functions
 * @arch_setup:			platform specific architecure setup
 * @banner:			print a platform specific banner
 */
struct x86_init_oem {
	void (*arch_setup)(void);
	void (*banner)(void);
};
/**
 * struct x86_init_paging - platform specific paging functions
 * @pagetable_setup_start:	platform specific pre paging_init() call
 * @pagetable_setup_done:	platform specific post paging_init() call
 */
struct x86_init_paging {
	void (*pagetable_setup_start)(pgd_t *base);
	void (*pagetable_setup_done)(pgd_t *base);
};
/**
 * struct x86_init_timers - platform specific timer setup
 * @setup_perpcu_clockev:	set up the per cpu clock event device for the
 *				boot cpu
 * @tsc_pre_init:		platform function called before TSC init
 * @timer_init:			initialize the platform timer (default PIT/HPET)
 */
struct x86_init_timers {
	void (*setup_percpu_clockev)(void);
	void (*tsc_pre_init)(void);
	void (*timer_init)(void);
};
/**
 * struct x86_init_iommu - platform specific iommu setup
 * @iommu_init:			platform specific iommu setup
 */
struct x86_init_iommu {
	int (*iommu_init)(void);
};
/**
 * struct x86_init_ops - functions for platform specific setup
 *
 */
struct x86_init_ops {
	struct x86_init_resources	resources;
	struct x86_init_mpparse		mpparse;
	struct x86_init_irqs		irqs;
	struct x86_init_oem		oem;
	struct x86_init_paging		paging;
	struct x86_init_timers		timers;
	struct x86_init_iommu		iommu;
};
/**
 * struct x86_cpuinit_ops - platform specific cpu hotplug setups
 * @setup_percpu_clockev:	set up the per cpu clock event device
 */
struct x86_cpuinit_ops {
	void (*setup_percpu_clockev)(void);
};
/**
 * struct x86_platform_ops - platform specific runtime functions
 * @calibrate_tsc:		calibrate TSC
 * @get_wallclock:		get time from HW clock like RTC etc.
 * @set_wallclock:		set time back to HW clock
 * @is_untracked_pat_range	exclude from PAT logic
 */
struct x86_platform_ops {
	unsigned long (*calibrate_tsc)(void);
	unsigned long (*get_wallclock)(void);
	int (*set_wallclock)(unsigned long nowtime);
	void (*iommu_shutdown)(void);
	bool (*is_untracked_pat_range)(u64 start, u64 end);
};
extern struct x86_init_ops x86_init;
extern struct x86_cpuinit_ops x86_cpuinit;
extern struct x86_platform_ops x86_platform;
extern void x86_init_noop(void);
extern void x86_init_uint_noop(unsigned int unused);
#line 9 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/mpspec.h" 2
extern int apic_version[
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_NUMA)
256
#endif
#if !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA)
255
#endif
];
extern int pic_mode;
#if definedEx(CONFIG_X86_32)
/*
 * Summit or generic (i.e. installer) kernels need lots of bus entries.
 * Maximum 256 PCI busses, plus 1 ISA bus in each of 4 cabinets.
 */
extern unsigned int def_to_bigsmp;
extern u8 apicid_2_node[];
#if definedEx(CONFIG_X86_NUMAQ)
extern int mp_bus_id_to_node[260];
extern int mp_bus_id_to_local[260];
extern int quad_local_to_mp_bus_id [
#if definedEx(CONFIG_SMP)
8
#endif
#if !definedEx(CONFIG_SMP)
1
#endif
/4][4];
#endif
#endif
#if !definedEx(CONFIG_X86_32)
/* Each PCI slot may be a combo card with its own bus.  4 IRQ pins per slot. */
#endif
#if !definedEx(CONFIG_MCA) && definedEx(CONFIG_EISA) || definedEx(CONFIG_MCA)
extern int mp_bus_id_to_type[
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_NUMA)
260
#endif
#if !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA)
256
#endif
];
#endif
extern unsigned long mp_bus_not_pci[(((
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_NUMA)
260
#endif
#if !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA)
256
#endif
) + (8 * sizeof(long)) - 1) / (8 * sizeof(long)))];
extern unsigned int boot_cpu_physical_apicid;
extern unsigned int max_physical_apicid;
extern int mpc_default_type;
extern unsigned long mp_lapic_addr;
#if definedEx(CONFIG_X86_LOCAL_APIC)
extern int smp_found_config;
#endif
#if !definedEx(CONFIG_X86_LOCAL_APIC)
#endif
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void get_smp_config(void)
{
	x86_init.mpparse.get_smp_config(0);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void early_get_smp_config(void)
{
	x86_init.mpparse.get_smp_config(1);
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void find_smp_config(void)
{
	x86_init.mpparse.find_smp_config();
}
#if definedEx(CONFIG_X86_MPPARSE)
extern void early_reserve_e820_mpc_new(void);
extern int enable_update_mptable;
extern int default_mpc_apic_id(struct mpc_cpu *m);
extern void default_smp_read_mpc_oem(struct mpc_table *mpc);
#if definedEx(CONFIG_X86_IO_APIC)
extern void default_mpc_oem_bus_info(struct mpc_bus *m, char *str);
#endif
#if !definedEx(CONFIG_X86_IO_APIC)
#endif
extern void default_find_smp_config(void);
extern void default_get_smp_config(unsigned int early);
#endif
#if !definedEx(CONFIG_X86_MPPARSE)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void early_reserve_e820_mpc_new(void) { }
#endif
void __attribute__ ((__section__(".cpuinit.text"))) __attribute__((__cold__)) generic_processor_info(int apicid, int version);
#if definedEx(CONFIG_ACPI)
extern void mp_register_ioapic(int id, u32 address, u32 gsi_base);
extern void mp_override_legacy_irq(u8 bus_irq, u8 polarity, u8 trigger,
				   u32 gsi);
extern void mp_config_acpi_legacy_irqs(void);
struct device;
extern int mp_register_gsi(struct device *dev, u32 gsi, int edge_level,
				 int active_high_low);
extern int acpi_probe_gsi(void);
#if definedEx(CONFIG_X86_IO_APIC)
extern int mp_find_ioapic(int gsi);
extern int mp_find_ioapic_pin(int ioapic, int gsi);
#endif
#endif
#if !definedEx(CONFIG_ACPI)
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int acpi_probe_gsi(void)
{
	return 0;
}
#endif
struct physid_mask {
	unsigned long mask[(((
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_NUMA)
256
#endif
#if !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA)
255
#endif
) + (8 * sizeof(long)) - 1) / (8 * sizeof(long)))];
};
typedef struct physid_mask physid_mask_t;
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 unsigned long physids_coerce(physid_mask_t *map)
{
	return map->mask[0];
}
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void physids_promote(unsigned long physids, physid_mask_t *map)
{
	bitmap_zero((*map).mask, 
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_NUMA)
256
#endif
#if !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA)
255
#endif
);
	map->mask[0] = physids;
}
/* Note: will create very large stack frames if physid_mask_t is big */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 void physid_set_mask_of_physid(int physid, physid_mask_t *map)
{
	bitmap_zero((*map).mask, 
#if definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && !definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && !definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) || definedEx(CONFIG_X86_32) && definedEx(CONFIG_NEED_MULTIPLE_NODES) && definedEx(CONFIG_NUMA)
256
#endif
#if !definedEx(CONFIG_X86_32) && !definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_SMP) && !definedEx(CONFIG_NUMA) && definedEx(CONFIG_X86_LOCAL_APIC) || !definedEx(CONFIG_X86_32) && definedEx(CONFIG_SMP) && definedEx(CONFIG_NUMA)
255
#endif
);
	set_bit(physid, (*map).mask);
}
extern physid_mask_t phys_cpu_present_map;
extern int generic_mps_oem_check(struct mpc_table *, char *, char *);
extern int default_acpi_madt_oem_check(char *, char *);
#endif
#line 50 "/usr0/home/ckaestne/work/TypeChef/LinuxAnalysis/linux-2.6.33.3/arch/x86/include/asm/topology.h" 2
#if definedEx(CONFIG_X86_32)
/* Mappings between logical cpu number and node number */
extern int cpu_to_node_map[];
/* Returns the number of the node containing CPU 'cpu' */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int cpu_to_node(int cpu)
{
	return cpu_to_node_map[cpu];
}
#endif



#if !definedEx(CONFIG_X86_32)
/* Mappings between logical cpu number and node number */
#if definedEx(CONFIG_SMP)
#if definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(".discard"), unused)) char __pcpu_scope_x86_cpu_to_node_map; extern __attribute__((section(".data.percpu" "")))  __typeof__(int) per_cpu__x86_cpu_to_node_map
#endif
#if !definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(".data.percpu" "")))  __typeof__(int) per_cpu__x86_cpu_to_node_map
#endif
; extern __typeof__(int) *x86_cpu_to_node_map_early_ptr; extern __typeof__(int) x86_cpu_to_node_map_early_map[]
#endif
#if !definedEx(CONFIG_SMP)
#if definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(".discard"), unused)) char __pcpu_scope_x86_cpu_to_node_map; extern __attribute__((section(".data" "")))  __typeof__(int) per_cpu__x86_cpu_to_node_map
#endif
#if !definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(".data" "")))  __typeof__(int) per_cpu__x86_cpu_to_node_map
#endif
#endif
;
/* Returns the number of the current Node. */
#if definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(".discard"), unused)) char __pcpu_scope_node_number; extern __attribute__((section(
#if definedEx(CONFIG_SMP)
".data.percpu"
#endif
#if !definedEx(CONFIG_SMP)
".data"
#endif
 "")))  __typeof__(int) per_cpu__node_number
#endif
#if !definedEx(CONFIG_DEBUG_FORCE_WEAK_PER_CPU)
extern __attribute__((section(
#if definedEx(CONFIG_SMP)
".data.percpu"
#endif
#if !definedEx(CONFIG_SMP)
".data"
#endif
 "")))  __typeof__(int) per_cpu__node_number
#endif
;
#if definedEx(CONFIG_DEBUG_PER_CPU_MAPS)
extern int cpu_to_node(int cpu);
extern int early_cpu_to_node(int cpu);
#endif
#if !definedEx(CONFIG_DEBUG_PER_CPU_MAPS)
/* Returns the number of the node containing CPU 'cpu' */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int cpu_to_node(int cpu)
{
	return 
#if definedEx(CONFIG_SMP)
(*({ unsigned long __ptr; __asm__ ("" : "=r"(__ptr) : "0"((&per_cpu__x86_cpu_to_node_map))); (typeof((&per_cpu__x86_cpu_to_node_map))) (__ptr + (((__per_cpu_offset[cpu])))); }))
#endif
#if !definedEx(CONFIG_SMP)
(*((void)(cpu), &per_cpu__x86_cpu_to_node_map))
#endif
;
}
/* Same function but used if called before per_cpu areas are setup */
static 
#if !definedEx(CONFIG_OPTIMIZE_INLINING)
inline __attribute__((always_inline))
#endif
#if definedEx(CONFIG_OPTIMIZE_INLINING)
inline
#endif
 int early_cpu_to_node(int cpu)
{
	return 
#if definedEx(CONFIG_SMP)
*((x86_cpu_to_node_map_early_ptr) ? &(x86_cpu_to_node_map_early_ptr)[cpu] : &(*({ unsigned long __ptr; __asm__ ("" : "=r"(__ptr) : "0"((&per_cpu__x86_cpu_to_node_map))); (typeof((&per_cpu__x86_cpu_to_node_map))) (__ptr + (((__per_cpu_offset[cpu])))); })))
#endif
#if !definedEx(CONFIG_SMP)
(*((void)(cpu), &per_cpu__x86_cpu_to_node_map))
#endif
;
}
#endif
#endif
/* Mappings between node number and cpus on that node. */
